
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../Reservoir%20computing%20and%20extreme%20learning%20machines%20for%20non-linear%20time-series%20data%20analysis%20-%2011.05.22/">
      
      
        <link rel="next" href="../Scientists%20discover%20how%20the%20brain%20keeps%20the%20urge%20to%20act%20in%20check%20%20Champalimaud%20Foundation%20-%2012.07.22/">
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.6">
    
    
      
        <title>Reservoir computing approaches to recurrent neural network training - ScienceDirect - obsidian-mkdocs template</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.ded33207.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="pink" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#reservoir-computing-approaches-to-recurrent-neural-network-training-sciencedirect" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="obsidian-mkdocs template" class="md-header__button md-logo" aria-label="obsidian-mkdocs template" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            obsidian-mkdocs template
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Reservoir computing approaches to recurrent neural network training - ScienceDirect
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="pink" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="pink" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="obsidian-mkdocs template" class="md-nav__button md-logo" aria-label="obsidian-mkdocs template" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    obsidian-mkdocs template
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          10 Projects
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          10 Projects
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/10%20Projects/" class="md-nav__link">
        10 Projects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Active%20Projects%20Overview/" class="md-nav__link">
        Active Projects Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/All%20Projects%20Overview/" class="md-nav__link">
        All Projects Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Open%20Projects%20Overview/" class="md-nav__link">
        Open Projects Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Projects%20Overview%20Template/" class="md-nav__link">
        Projects Overview Template
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Projects%20Table/" class="md-nav__link">
        Projects Table
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7" id="__nav_2_7_label" tabindex="0">
          1 Main Research
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7">
          <span class="md-nav__icon md-icon"></span>
          1 Main Research
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_1" id="__nav_2_7_1_label" tabindex="0">
          Global Trigger Local Back Propagation
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_1">
          <span class="md-nav__icon md-icon"></span>
          Global Trigger Local Back Propagation
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Global%20Trigger%20Local%20Back-Propagation/Global%20Trigger%20Local%20Back-Propagation/" class="md-nav__link">
        Global Trigger Local Back Propagation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_2" id="__nav_2_7_2_label" tabindex="0">
          Make the fucking timing figure
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_2">
          <span class="md-nav__icon md-icon"></span>
          Make the fucking timing figure
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Make%20the%20fucking%20timing%20figure/Figure%20Descriptions/" class="md-nav__link">
        Figure Descriptions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Make%20the%20fucking%20timing%20figure/Make%20the%20fucking%20timing%20figure/" class="md-nav__link">
        Make the fucking timing figure
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_3" id="__nav_2_7_3_label" tabindex="0">
          PhD Proposal
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_3">
          <span class="md-nav__icon md-icon"></span>
          PhD Proposal
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/Nicolai%27s%20Comments%20on%20Draft%205/" class="md-nav__link">
        Nicolai's Comments on Draft 5
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20Proposal%20tasks/" class="md-nav__link">
        PhD Proposal tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20Proposal/" class="md-nav__link">
        PhD Proposal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Outline/" class="md-nav__link">
        PhD proposal Outline
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_3_5" id="__nav_2_7_3_5_label" tabindex="0">
          PhD proposal Drafts
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_7_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_3_5">
          <span class="md-nav__icon md-icon"></span>
          PhD proposal Drafts
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20%20Draft%208%20Research%20Plan/" class="md-nav__link">
        PhD Proposal  Draft 8 Research Plan
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%201/" class="md-nav__link">
        1 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%202%20-%20cut%20and%20paste%20palette/" class="md-nav__link">
        1 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%202/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%204/" class="md-nav__link">
        PhD Proposal Draft 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%205%20CUT%20AND%20PASTE%20PALETTE/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%205.1/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%205/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%206.1/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%206/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%207%20latex%20conversion/" class="md-nav__link">
        PhD Proposal Draft 7 latex conversion
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%207/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Mega%20Frakenstein%20Collage%20Draft%203/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20draft%202%20-%20outline/" class="md-nav__link">
        1 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20proposal%20Drafts/" class="md-nav__link">
        PhD proposal Drafts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/Research%20Question%20Section%20Draft%202/" class="md-nav__link">
        Research Question Section Draft 2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_4" id="__nav_2_7_4_label" tabindex="0">
          Recreating SBF Model
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_4">
          <span class="md-nav__icon md-icon"></span>
          Recreating SBF Model
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Abstract%20for%20SBFA%20-%2027.03.23/" class="md-nav__link">
        Abstract for SBFA   27.03.23
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Brief%20Overview%20of%20SBF%20Model/" class="md-nav__link">
        Brief Overview of SBF Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Outline%20of%20SBF%20Model%20for%20Supervision%20Meeting/" class="md-nav__link">
        Outline of SBF Model for Supervision Meeting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Outline%20of%20SBF%20Model/" class="md-nav__link">
        Outline of SBF Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Recreating%20SBF%20Model/" class="md-nav__link">
        Recreating SBF Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_5" id="__nav_2_7_5_label" tabindex="0">
          SBF   Automata Experiment
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_5">
          <span class="md-nav__icon md-icon"></span>
          SBF   Automata Experiment
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Algorithm%20Outline/" class="md-nav__link">
        Algorithm Outline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Experiment%20Environment%20Variants/" class="md-nav__link">
        Experiment Environment Variants
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/First%20python%20implementation%20-%20conditions%20and%20results/" class="md-nav__link">
        First python implementation   conditions and results
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Pseudocode%20for%20Modular%20Arrangement/" class="md-nav__link">
        Pseudocode for Modular Arrangement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Pseudocode%20for%20SBF%20Automata/" class="md-nav__link">
        Pseudocode for SBF Automata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBF%20-%20Automata%20-%20Design%201/" class="md-nav__link">
        The SBF Automata Design
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBF%20-%20Automata%20Algorithm%20Formalized/" class="md-nav__link">
        SBF   Automata Algorithm Formalized
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBF%20-%20Automata%20Experiment/" class="md-nav__link">
        SBF   Automata Experiment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20-%20Normalized%20Automata%20Vote/" class="md-nav__link">
        SBFA   Normalized Automata Vote
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20Development%20Log/" class="md-nav__link">
        SBFA Development Log
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20Experiment%20Development/" class="md-nav__link">
        SBFA Experiment Development
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20Experiment%20Results%2010.02.23/" class="md-nav__link">
        SBFA Experiment Results 10.02.23
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Weighted%20Majority%20Vote%20Version%2027.01.23/" class="md-nav__link">
        Weighted Majority Vote Version [[27.01.23]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_5_14" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_5_14" id="__nav_2_7_5_14_label" tabindex="0">
          Ooman Collab
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_7_5_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_5_14">
          <span class="md-nav__icon md-icon"></span>
          Ooman Collab
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Ooman%20Collab/Ooman%20Collab/" class="md-nav__link">
        Formerly the Automata Oscilllators Project
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_6" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_6" id="__nav_2_7_6_label" tabindex="0">
          SNN Models
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_6">
          <span class="md-nav__icon md-icon"></span>
          SNN Models
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SNN%20Models/SNN%20Models%20/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_7" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_7" id="__nav_2_7_7_label" tabindex="0">
          Survey Paper
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_7">
          <span class="md-nav__icon md-icon"></span>
          Survey Paper
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Survey%20Paper/Survey%20Paper%20Tasks/" class="md-nav__link">
        Tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Survey%20Paper/Survey%20Paper/" class="md-nav__link">
        Survey Paper
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_7_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_7_3" id="__nav_2_7_7_3_label" tabindex="0">
          How to Conduct Survey
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_7_7_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_7_3">
          <span class="md-nav__icon md-icon"></span>
          How to Conduct Survey
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Survey%20Paper/How%20to%20Conduct%20Survey/How%20to%20Conduct%20Survey%20/" class="md-nav__link">
        How to Conduct Survey 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_8" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_8" id="__nav_2_7_8_label" tabindex="0">
          Timing Tasks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_8">
          <span class="md-nav__icon md-icon"></span>
          Timing Tasks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Timing%20Tasks/Timing%20Tasks%20Research/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8" >
      
      
      
        <label class="md-nav__link" for="__nav_2_8" id="__nav_2_8_label" tabindex="0">
          2 Alt. Research
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_8">
          <span class="md-nav__icon md-icon"></span>
          2 Alt. Research
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_8_1" id="__nav_2_8_1_label" tabindex="0">
          Stable Diffusion Project
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_8_1">
          <span class="md-nav__icon md-icon"></span>
          Stable Diffusion Project
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/2%20Alt.%20Research/Stable%20Diffusion%20Project/Stable%20Diffusion%20Project/" class="md-nav__link">
        Stable Diffusion Project
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_8_2" id="__nav_2_8_2_label" tabindex="0">
          Vicente Collab
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_8_2">
          <span class="md-nav__icon md-icon"></span>
          Vicente Collab
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/2%20Alt.%20Research/Vicente%20Collab/Vicente%20Collab/" class="md-nav__link">
        Vicente Collab
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_9" >
      
      
      
        <label class="md-nav__link" for="__nav_2_9" id="__nav_2_9_label" tabindex="0">
          3 PhD Administrative Stuff
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_9">
          <span class="md-nav__icon md-icon"></span>
          3 PhD Administrative Stuff
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/Applying%20for%20trip%20expenses/" class="md-nav__link">
        How to apply for trip expenses
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/PhD%20Administrative%20Stuff/" class="md-nav__link">
        [Resources for Ph.D. candidates at TKD](https://ansatt.oslomet.no/en/ressursside-for-phd-kandidater-ved-tkd1)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/PhD%20Courses/" class="md-nav__link">
        PhD Courses
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/welcome%20letter/" class="md-nav__link">
        ![[Velkomstbrev ph.d.-kandidater internt - engelsk 1.pdf]]
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10" id="__nav_2_10_label" tabindex="0">
          4 Conferences & Schools
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10">
          <span class="md-nav__icon md-icon"></span>
          4 Conferences & Schools
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_1" id="__nav_2_10_1_label" tabindex="0">
          DeepLearn 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_1">
          <span class="md-nav__icon md-icon"></span>
          DeepLearn 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/DeepLearn%202022%20Travel/" class="md-nav__link">
        DeepLearn 2022 budget proposal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/DeepLearn%202022/" class="md-nav__link">
        DeepLearn 2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/DeepLearn%20Lecture%20Selections/" class="md-nav__link">
        [[DeepLearn 2022]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/Sean%20Meyn%20-%20DeepLearn%202022%20summer%20-%2025.07.22/" class="md-nav__link">
        Sean Meyn   DeepLearn 2022 summer   25.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_1_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_1_5" id="__nav_2_10_1_5_label" tabindex="0">
          Schuller
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_10_1_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_1_5">
          <span class="md-nav__icon md-icon"></span>
          Schuller
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/schuller/Bjorn%20Schuller%20-%20DeepLearn%202022%20summer%20-%2025.07.22/" class="md-nav__link">
        Motivations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_2" id="__nav_2_10_2_label" tabindex="0">
          MLSS^N 2022 Krakow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_2">
          <span class="md-nav__icon md-icon"></span>
          MLSS^N 2022 Krakow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/MLSSN%20Review/" class="md-nav__link">
        MLSSN Review
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/MLSS%5EN%202022%20Krakow/" class="md-nav__link">
        Dataview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Our%20Contribution%20to%20Continual%20Learning/" class="md-nav__link">
        PDF
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_2_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_2_4" id="__nav_2_10_2_4_label" tabindex="0">
          Talk Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_10_2_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_2_4">
          <span class="md-nav__icon md-icon"></span>
          Talk Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Andrea%20Tagliasacchi/" class="md-nav__link">
        Andrea Tagliasacchi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Andrew%20Saxe/" class="md-nav__link">
        Andrew Saxe
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Ewa%20Szczurek/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Friedemann%20Zenke%20MLSSN%20Lecture%20-%2030.06.22/" class="md-nav__link">
        Lecture Material
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Grabaska-Barwinska%20MLSSN%20Lecture%20-%2001.07.22/" class="md-nav__link">
        A rapid and efficient learning rule
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Jan%20Chorowski%20MLSSN%20%20-%2002.07.22/" class="md-nav__link">
        Jan Chorowski MLSSN    02.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Joao%20Henriques/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Joao%20Sacremento%20Lecture%20-%2029.06.22/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Piotr%20Milos%20Lecture%20MLSS%20-%2002.07.22/" class="md-nav__link">
        Piotr Milos Lecture MLSS   02.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Poster%20Session%20notes/" class="md-nav__link">
        Poster Session notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Rafal%20Bogcaz/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Razvan%20Pascnau%20Lecture%20-%2027.06.22/" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Rianne%20Van%20Den%20Berg%20Lecture/" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Tomasz%20Trzcinski%20MLSS%20-%2002.07.22/" class="md-nav__link">
        Tomasz Trzcinski MLSS   02.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Zieba%20MLSSN%20Lecture%20-%2001.07.22/" class="md-nav__link">
        Discrete normalizing flows
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11" id="__nav_2_11_label" tabindex="0">
          5 Presentations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11">
          <span class="md-nav__icon md-icon"></span>
          5 Presentations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/5%20Presentations/" class="md-nav__link">
        5 Presentations
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_2" id="__nav_2_11_2_label" tabindex="0">
          Present on SNN MAB
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_2">
          <span class="md-nav__icon md-icon"></span>
          Present on SNN MAB
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Present%20on%20SNN%20MAB/Present%20on%20SNN%20MAB/" class="md-nav__link">
        Present on SNN MAB
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_3" id="__nav_2_11_3_label" tabindex="0">
          Presenting on Petter 2018
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_3">
          <span class="md-nav__icon md-icon"></span>
          Presenting on Petter 2018
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20on%20Petter%202018/Petter%202018%20Slides%20-%20Draft%201/" class="md-nav__link">
        Petter 2018 Slides   Draft 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20on%20Petter%202018/Petter%202018%20Slides%20-%20Draft%202/" class="md-nav__link">
        Petter 2018 Slides   Draft 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20on%20Petter%202018/Presenting%20on%20Petter%202018/" class="md-nav__link">
        Presenting on Petter 2018
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_4" id="__nav_2_11_4_label" tabindex="0">
          Presenting to Ooman
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_4">
          <span class="md-nav__icon md-icon"></span>
          Presenting to Ooman
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20to%20Ooman/Presenting%20to%20Oomman/" class="md-nav__link">
        RL, SNNs, and Representing time
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_5" id="__nav_2_11_5_label" tabindex="0">
          Templates
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_5">
          <span class="md-nav__icon md-icon"></span>
          Templates
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/templates/tpl/" class="md-nav__link">
        Tpl
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/templates/tpl2/" class="md-nav__link">
        Tpl2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12" id="__nav_2_12_label" tabindex="0">
          6 Teaching
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12">
          <span class="md-nav__icon md-icon"></span>
          6 Teaching
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_1" id="__nav_2_12_1_label" tabindex="0">
          ACIT4420   Python Programming Course Autumn 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_1">
          <span class="md-nav__icon md-icon"></span>
          ACIT4420   Python Programming Course Autumn 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4420%20-%20Python%20Programming%20Course%20Autumn%202022/ACIT4420%20-%20Python%20Programming%20Course%20Autumn%202022/" class="md-nav__link">
        ACIT4420 - Python Scripting
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_2" id="__nav_2_12_2_label" tabindex="0">
          ACIT4620   Comp. Intel Course Autmun 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_2">
          <span class="md-nav__icon md-icon"></span>
          ACIT4620   Comp. Intel Course Autmun 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/" class="md-nav__link">
        [[ACIT4620 Log]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/ACIT4620%20Log/" class="md-nav__link">
        [[04.10.22]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/Recommended%20Changes%20to%20Course/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_3" id="__nav_2_12_3_label" tabindex="0">
          DATA3900 Bachelors Thesis Supervision
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_3">
          <span class="md-nav__icon md-icon"></span>
          DATA3900 Bachelors Thesis Supervision
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/DATA%203900%20Group%20-%20Microsoft%20Norge%20AS/" class="md-nav__link">
        DATA 3900 Group   Microsoft Norge AS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/DATA3900%20Bachelors%20Thesis%20Supervision/" class="md-nav__link">
        DATA3900 Bachelors Thesis Supervision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/Group%20-%20Intility%20AS/" class="md-nav__link">
        Group   Intility AS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/MSFT%20Bacheloroppgave/" class="md-nav__link">
        MSFT Bacheloroppgave
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_4" id="__nav_2_12_4_label" tabindex="0">
          TA courses Autumn 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_4">
          <span class="md-nav__icon md-icon"></span>
          TA courses Autumn 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/TA%20courses%20Autumn%202022/TA%20courses%20Autumn%202022%20/" class="md-nav__link">
        TA Course for Fall 2022
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13" id="__nav_2_13_label" tabindex="0">
          7 Obsidian Vault Meta
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_13_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13">
          <span class="md-nav__icon md-icon"></span>
          7 Obsidian Vault Meta
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Body%20of%20Knowledge%20Without%20Organs/" class="md-nav__link">
        Body of Knowledge without Organs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Obsidian%20Vault%20Meta/" class="md-nav__link">
        Obsidian Vault Meta
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Obsidian%20web%20browsing%20-12.01.23/" class="md-nav__link">
        Obsidian web browsing  12.01.23
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/obsidian%20meta%20log/" class="md-nav__link">
        Obsidian meta log
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13_5" id="__nav_2_13_5_label" tabindex="0">
          Better Reading
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_13_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13_5">
          <span class="md-nav__icon md-icon"></span>
          Better Reading
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Better%20Reading/Better%20Reading%20%20/" class="md-nav__link">
        Better Reading  
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13_6" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13_6" id="__nav_2_13_6_label" tabindex="0">
          Showing off my obsidian wokflow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_13_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13_6">
          <span class="md-nav__icon md-icon"></span>
          Showing off my obsidian wokflow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Showing%20off%20my%20obsidian%20wokflow/Showing%20off%20my%20obsidian%20wokflow/" class="md-nav__link">
        Showing off my obsidian wokflow
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13_7" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13_7" id="__nav_2_13_7_label" tabindex="0">
          Tracking People
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_13_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13_7">
          <span class="md-nav__icon md-icon"></span>
          Tracking People
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Tracking%20People/Tracking%20People/" class="md-nav__link">
        Tracking People
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_14" >
      
      
      
        <label class="md-nav__link" for="__nav_2_14" id="__nav_2_14_label" tabindex="0">
          AI Movie Series
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_14">
          <span class="md-nav__icon md-icon"></span>
          AI Movie Series
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI%20Movie%20Series/AI%20Movie%20Series/" class="md-nav__link">
        AI Movie Series
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_15" >
      
      
      
        <label class="md-nav__link" for="__nav_2_15" id="__nav_2_15_label" tabindex="0">
          AI Art Fair Project
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_15_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_15">
          <span class="md-nav__icon md-icon"></span>
          AI Art Fair Project
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Art%20Fair%20Project/AI-Art%20Fair%20Project/" class="md-nav__link">
        AI Art Fair Project
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_16" >
      
      
      
        <label class="md-nav__link" for="__nav_2_16" id="__nav_2_16_label" tabindex="0">
          AI Lab Retreat Dec. 5 7 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_16_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_16">
          <span class="md-nav__icon md-icon"></span>
          AI Lab Retreat Dec. 5 7 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/AI-Lab%20Retreat%20Dec.%205-7%202022/" class="md-nav__link">
        AI Lab Retreat Dec. 5 7 2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Contributions%20Challenge/" class="md-nav__link">
        Contributions Challenge
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Free%20writing%20test%202/" class="md-nav__link">
        Free writing assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Group%20collaboration%20for%20living%20documents/" class="md-nav__link">
        A New Publication Method for a Future of Shared Knowledge
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Problem%20challenge/" class="md-nav__link">
        So here is a problem
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/test%20free%20writing/" class="md-nav__link">
        Free writing exercises
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_17" >
      
      
      
        <label class="md-nav__link" for="__nav_2_17" id="__nav_2_17_label" tabindex="0">
          AI Lab Wiki
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_17_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_17">
          <span class="md-nav__icon md-icon"></span>
          AI Lab Wiki
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Wiki/AI-Lab%20Wiki%20/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_18" >
      
      
      
        <label class="md-nav__link" for="__nav_2_18" id="__nav_2_18_label" tabindex="0">
          Hugging Face Deep Reinforcement Learning Course
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_18_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_18">
          <span class="md-nav__icon md-icon"></span>
          Hugging Face Deep Reinforcement Learning Course
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%201/" class="md-nav__link">
        RL Framework
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%202/" class="md-nav__link">
        HF DRL   Unit 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%203/" class="md-nav__link">
        HF DRL   Unit 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%204/" class="md-nav__link">
        HF DRL   Unit 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%205/" class="md-nav__link">
        HF DRL   Unit 5
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20Coding%20-%20Unit%204/" class="md-nav__link">
        HF DRL Coding   Unit 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/" class="md-nav__link">
        Hugging Face Deep Reinforcement Learning Course
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_19" >
      
      
      
        <label class="md-nav__link" for="__nav_2_19" id="__nav_2_19_label" tabindex="0">
          IKT460 G Reinforcement Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_19_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_19">
          <span class="md-nav__icon md-icon"></span>
          IKT460 G Reinforcement Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/IKT460%20Lecture%201/" class="md-nav__link">
        IKT460 Lecture 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/IKT460%20Lecture%202/" class="md-nav__link">
        Top Comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/IKT460-G%20Reinforcement%20Learning/" class="md-nav__link">
        IKT460 G Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/Project%20Proposal%20Outline/" class="md-nav__link">
        Project Proposal Outline
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_20" >
      
      
      
        <label class="md-nav__link" for="__nav_2_20" id="__nav_2_20_label" tabindex="0">
          Lab Social Media
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_20_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_20">
          <span class="md-nav__icon md-icon"></span>
          Lab Social Media
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Lab%20Social%20Media/Lab%20Social%20Media/" class="md-nav__link">
        Lab Social Media
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_21" >
      
      
      
        <label class="md-nav__link" for="__nav_2_21" id="__nav_2_21_label" tabindex="0">
          Laptop Environemnt
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_21_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_21">
          <span class="md-nav__icon md-icon"></span>
          Laptop Environemnt
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Laptop%20Environemnt/Laptop%20Environment/" class="md-nav__link">
        Laptop Environment
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_22" >
      
      
      
        <label class="md-nav__link" for="__nav_2_22" id="__nav_2_22_label" tabindex="0">
          Learning Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_22_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_22">
          <span class="md-nav__icon md-icon"></span>
          Learning Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Learning%20Python/Learning%20Python/" class="md-nav__link">
        Learning Python
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_23" >
      
      
      
        <label class="md-nav__link" for="__nav_2_23" id="__nav_2_23_label" tabindex="0">
          Making Posters
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_23_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_23">
          <span class="md-nav__icon md-icon"></span>
          Making Posters
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Making%20Posters/Making%20Posters/" class="md-nav__link">
        Making Posters
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Making%20Posters/Neuromorphics%20Poster%20-%20Staging/" class="md-nav__link">
        Neuromorphics Poster   Staging
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Making%20Posters/Neuromorphics%20Poster%202nd%20draft/" class="md-nav__link">
        Title
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_24" >
      
      
      
        <label class="md-nav__link" for="__nav_2_24" id="__nav_2_24_label" tabindex="0">
          New Business
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_24_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_24">
          <span class="md-nav__icon md-icon"></span>
          New Business
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/New%20Business/New%20Business%20/" class="md-nav__link">
        New Business 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_25" >
      
      
      
        <label class="md-nav__link" for="__nav_2_25" id="__nav_2_25_label" tabindex="0">
          Norwegian Courses A1 and A2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_25_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_25">
          <span class="md-nav__icon md-icon"></span>
          Norwegian Courses A1 and A2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Norwegian%20Courses%20A1%20and%20A2/Norwegian%20Courses%20A1%20and%20A2%20/" class="md-nav__link">
        Norwegian Courses A1 and A2 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_26" >
      
      
      
        <label class="md-nav__link" for="__nav_2_26" id="__nav_2_26_label" tabindex="0">
          Obsidian Addons and Todos
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_26_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_26">
          <span class="md-nav__icon md-icon"></span>
          Obsidian Addons and Todos
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/Example%20Slides%20-%20Miniml/" class="md-nav__link">
        Example Slides   Miniml
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/Example%20Slides/" class="md-nav__link">
        Example Slides
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/MD%20Slides/" class="md-nav__link">
        MD Slides
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/Obsidian%20Addons%20and%20Todos/" class="md-nav__link">
        Obsidian Addons and Todos
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_27" >
      
      
      
        <label class="md-nav__link" for="__nav_2_27" id="__nav_2_27_label" tabindex="0">
          PENG9560 Topics in Artificial Intelligence and Machine Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_27_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_27">
          <span class="md-nav__icon md-icon"></span>
          PENG9560 Topics in Artificial Intelligence and Machine Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Mandatory%20Assignment%20for%20Module%20I%20-%20Computational%20Intelligence/" class="md-nav__link">
        Mandatory Assignment for Module I   Computational Intelligence
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%201/" class="md-nav__link">
        PENG9560 Assignment Draft 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%202/" class="md-nav__link">
        PENG9560 Assignment Draft 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%203/" class="md-nav__link">
        PENG9560 Assignment Draft 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%204/" class="md-nav__link">
        PENG9560 Assignment Draft 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Outline%20Assignment%201/" class="md-nav__link">
        Drafts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/" class="md-nav__link">
        PENG9560 Topics in Artificial Intelligence and Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_27_8" >
      
      
      
        <label class="md-nav__link" for="__nav_2_27_8" id="__nav_2_27_8_label" tabindex="0">
          Presentation
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_27_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_27_8">
          <span class="md-nav__icon md-icon"></span>
          Presentation
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20-%20Draft%201%20-%20PENG9560/" class="md-nav__link">
        Presentation   Draft 1   PENG9560
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20-%20Draft%202%20-%20PENG9560/" class="md-nav__link">
        Presentation   Draft 2   PENG9560
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20-%20Draft%203%20-%20PENG9560/" class="md-nav__link">
        Presentation   Draft 3   PENG9560
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20Outline%20-%20PENG9560/" class="md-nav__link">
        Presentation Outline   PENG9560
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_28" >
      
      
      
        <label class="md-nav__link" for="__nav_2_28" id="__nav_2_28_label" tabindex="0">
          Video   How Critical is Brain Criticality
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_28_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_28">
          <span class="md-nav__icon md-icon"></span>
          Video   How Critical is Brain Criticality
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Video%20-%20How%20Critical%20is%20Brain%20Criticality/Video%20-%20How%20Critical%20is%20Brain%20Criticality/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_29" >
      
      
      
        <label class="md-nav__link" for="__nav_2_29" id="__nav_2_29_label" tabindex="0">
          Video Production
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_29_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_29">
          <span class="md-nav__icon md-icon"></span>
          Video Production
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Video%20Production/Video%20Production%20/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_30" >
      
      
      
        <label class="md-nav__link" for="__nav_2_30" id="__nav_2_30_label" tabindex="0">
          Workplan 2022 23 for Gustavo
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_30_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_30">
          <span class="md-nav__icon md-icon"></span>
          Workplan 2022 23 for Gustavo
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Workplan%202022-23%20for%20Gustavo/Workplan%202022-23%20for%20Gustavo/" class="md-nav__link">
        Workplan 2022 23 for Gustavo
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          50 Reading
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          50 Reading
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/50%20Reading/" class="md-nav__link">
        50 Reading
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          PDF Searches
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          PDF Searches
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/PDF%20Searches/" class="md-nav__link">
        Searching in pdfs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/fixed%20interval%201/" class="md-nav__link">
        Paton_Buonomano_2018_The Neural Basis of Timing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/fixed%20interval/" class="md-nav__link">
        Fixed interval
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/interval/" class="md-nav__link">
        Interval
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/time%20interval/" class="md-nav__link">
        Time interval
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          Reading Lists
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          Reading Lists
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Lists/Recent%20Reading/" class="md-nav__link">
        Recent Reading
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
          Reading Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          Reading Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Gerstner%20et%20al.%202018/" class="md-nav__link">
        Gerstner et al. 2018
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mehonic%202022/" class="md-nav__link">
        Mehonic 2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Oomen%202017/" class="md-nav__link">
        Oomen 2017
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Parent%202004/" class="md-nav__link">
        Parent 2004
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Payeur%20et%20al.%202021/" class="md-nav__link">
        Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_6" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_6" id="__nav_3_4_6_label" tabindex="0">
          Buzsáki and Vöröslakos (2023)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_6">
          <span class="md-nav__icon md-icon"></span>
          Buzsáki and Vöröslakos (2023)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Brain%20rhythms%20have%20come%20of%20age%20-%20comments/" class="md-nav__link">
        Brain rhythms have come of age   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Brain%20rhythms%20have%20come%20of%20age%20-%20glossary/" class="md-nav__link">
        Brain rhythms have come of age   glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Brain%20rhythms%20have%20come%20of%20age%20-%20topics/" class="md-nav__link">
        Brain rhythms have come of age   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/" class="md-nav__link">
        Buzsáki and Vöröslakos (2023)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_7" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_7" id="__nav_3_4_7_label" tabindex="0">
          Chen 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_7">
          <span class="md-nav__icon md-icon"></span>
          Chen 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022%20-%20comments/" class="md-nav__link">
        Chen 2022   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022%20-%20glossary/" class="md-nav__link">
        Formulae
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022%20-%20topics/" class="md-nav__link">
        Chen 2022   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022/" class="md-nav__link">
        Deep Reinforcement Learning with Spiking Q-learning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_8" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_8" id="__nav_3_4_8_label" tabindex="0">
          Granmo 2010
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_8">
          <span class="md-nav__icon md-icon"></span>
          Granmo 2010
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Granmo%202010/Granmo%202010%20Comments/" class="md-nav__link">
        Personal Summary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Granmo%202010/Granmo%202010%20Topics/" class="md-nav__link">
        Learning Automata (LA)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Granmo%202010/Granmo%202010/" class="md-nav__link">
        Granmo 2010
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_9" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_9" id="__nav_3_4_9_label" tabindex="0">
          Hartcher O'Brien, Brighouse, Levitan (2016)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_9">
          <span class="md-nav__icon md-icon"></span>
          Hartcher O'Brien, Brighouse, Levitan (2016)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29%20-%20comments/" class="md-nav__link">
        Hartcher O'Brien, Brighouse, Levitan (2016)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29%20-%20glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29%20-%20topics/" class="md-nav__link">
        Hartcher O'Brien, Brighouse, Levitan (2016)   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/" class="md-nav__link">
        Hartcher O'Brien, Brighouse, Levitan (2016)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_10" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_10" id="__nav_3_4_10_label" tabindex="0">
          Hasegawa and Sakata 2015
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_10">
          <span class="md-nav__icon md-icon"></span>
          Hasegawa and Sakata 2015
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hasegawa%20and%20Sakata%202015/Hasegawa%20and%20Sakata%202015/" class="md-nav__link">
        Hasegawa and Sakata 2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hasegawa%20and%20Sakata%202015/Hasegawa%2C%20and%20Sakata%202015%20-%20comments/" class="md-nav__link">
        Hasegawa, and Sakata 2015   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hasegawa%20and%20Sakata%202015/Hasegawa%2C%20and%20Sakata%202015%20-%20topics/" class="md-nav__link">
        Hasegawa, and Sakata 2015   topics
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_11" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_11" id="__nav_3_4_11_label" tabindex="0">
          Mello 2016
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_11">
          <span class="md-nav__icon md-icon"></span>
          Mello 2016
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016%20-%20Glossary/" class="md-nav__link">
        ABBREVIATIONS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016%20-%20Topics/" class="md-nav__link">
        Mello 2016   Topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016%20-%20comments/" class="md-nav__link">
        Mello 2016   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016/" class="md-nav__link">
        Mello 2016
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_12" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_12" id="__nav_3_4_12_label" tabindex="0">
          Mello, Soares, Paton 2015
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_12">
          <span class="md-nav__icon md-icon"></span>
          Mello, Soares, Paton 2015
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015%20-%20comments/" class="md-nav__link">
        Mello, Soares, Paton 2015   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015%20-%20glossary/" class="md-nav__link">
        Mello, Soares, Paton 2015   glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015%20-%20topics/" class="md-nav__link">
        Mello, Soares, Paton 2015   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015/" class="md-nav__link">
        Mello, Soares, Paton 2015
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_13" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_13" id="__nav_3_4_13_label" tabindex="0">
          O’Byrne & Jerbi (2022)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_13_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_13">
          <span class="md-nav__icon md-icon"></span>
          O’Byrne & Jerbi (2022)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/O%27Byrne%20%26%20Jerbi%20%282022%29%20-%20comments/" class="md-nav__link">
        O'Byrne & Jerbi (2022)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/O%27Byrne%20%26%20Jerbi%20%282022%29%20-%20topics/" class="md-nav__link">
        Topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/" class="md-nav__link">
        O’Byrne & Jerbi (2022)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_14" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_14" id="__nav_3_4_14_label" tabindex="0">
          Petter, Gershman, Meck (2018)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_14">
          <span class="md-nav__icon md-icon"></span>
          Petter, Gershman, Meck (2018)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29%20-%20comments/" class="md-nav__link">
        Petter, Gershman, Meck (2018)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29%20-%20glossary/" class="md-nav__link">
        Pavlovian Conditioning Protocol
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29%20-%20topics/" class="md-nav__link">
        Reward prediction error (RPE)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29/" class="md-nav__link">
        Petter, Gershman, Meck (2018)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_15" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_15" id="__nav_3_4_15_label" tabindex="0">
          Ponulak 2011
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_15_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_15">
          <span class="md-nav__icon md-icon"></span>
          Ponulak 2011
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Ponulak%202011/Ponulak%202011%20-%20comments/" class="md-nav__link">
        Ponulak 2011   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Ponulak%202011/Ponulak%202011%20-%20topics/" class="md-nav__link">
        Ponulak 2011   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Ponulak%202011/Ponulak%202011/" class="md-nav__link">
        Introduction to spiking neural networks: Information processing, learning and applications
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_16" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_16" id="__nav_3_4_16_label" tabindex="0">
          Sun 2020
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_16_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_16">
          <span class="md-nav__icon md-icon"></span>
          Sun 2020
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020%20-%20comments/" class="md-nav__link">
        Sun 2020   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020%20-%20glossary/" class="md-nav__link">
        Sun 2020   glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020%20-%20topics/" class="md-nav__link">
        Sun 2020   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020/" class="md-nav__link">
        A Review of Designs and Applications of Echo State Networks
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_17" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_17" id="__nav_3_4_17_label" tabindex="0">
          Voelkner 2019
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_17_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_17">
          <span class="md-nav__icon md-icon"></span>
          Voelkner 2019
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Voelkner%202019/Voelkner%202019%20-%20comments/" class="md-nav__link">
        Voelkner 2019   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Voelkner%202019/Voelkner%202019%20-%20topics/" class="md-nav__link">
        Voelkner 2019   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Voelkner%202019/Voelkner%202019/" class="md-nav__link">
        Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_18" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_18" id="__nav_3_4_18_label" tabindex="0">
          Yazidi 2021
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_18_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_18">
          <span class="md-nav__icon md-icon"></span>
          Yazidi 2021
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yazidi%202021/Yazidi%202021%20-%20Comments/" class="md-nav__link">
        Yazidi 2021   Comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yazidi%202021/Yazidi%202021%20-%20Topics/" class="md-nav__link">
        load balancing (LB)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yazidi%202021/Yazidi%202021/" class="md-nav__link">
        Yazidi 2021
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_19" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_19" id="__nav_3_4_19_label" tabindex="0">
          Yin 2017
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_19_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_19">
          <span class="md-nav__icon md-icon"></span>
          Yin 2017
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%202017/Yin%202017%20-%20comments/" class="md-nav__link">
        Yin 2017   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%202017/Yin%202017%20-%20topics/" class="md-nav__link">
        Yin 2017   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%202017/Yin%202017/" class="md-nav__link">
        Yin 2017
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_20" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_20" id="__nav_3_4_20_label" tabindex="0">
          Yin et al. (2022)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_20_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_20">
          <span class="md-nav__icon md-icon"></span>
          Yin et al. (2022)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29%20-%20comments/" class="md-nav__link">
        Yin et al. (2022)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29%20-%20glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29%20-%20topics/" class="md-nav__link">
        Topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29/" class="md-nav__link">
        Yin et al. (2022)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
          Videos
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          Videos
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Cosyne%202022%20Tutorial%20on%20Spiking%20Neural%20Networks/" class="md-nav__link">
        Cosyne 2022 Tutorial on Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Eliasmith%20%20-%20Spiking%20Neural%20Networks%20for%20More%20Efficient%20AI%20Algorithms/" class="md-nav__link">
        https://www.youtube.com/watch?v=PeW-TN3P1hk
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Hopfield%20Networks%20is%20All%20You%20Need/" class="md-nav__link">
        Hopfield Networks is All You Need
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/JNS%20Lecture%20Will%20Dabney%20-%20A%20Distributional%20Code%20for%20Value%20in%20Dopamine-Based%20Reinforcement%20Learning/" class="md-nav__link">
        A Distributional Code for Value in Dopamine-Based Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Neural%20representations%20of%20time%2C%20space%20and%20other%20continuous%20variables/" class="md-nav__link">
        Neural representations of time, space and other continuous variables
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
      
      
      
        <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
          Zotero Papers
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          Zotero Papers
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Zotero%20Papers/Zotero%20Papers/" class="md-nav__link">
        Zotero Papers
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
      
      
      
        <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
          Citations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_7">
          <span class="md-nav__icon md-icon"></span>
          Citations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/%40pateriaHierarchicalReinforcementLearning2021/" class="md-nav__link">
        @pateriaHierarchicalReinforcementLearning2021
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/%40voelkerLegendreMemoryUnits2019/" class="md-nav__link">
        @voelkerLegendreMemoryUnits2019
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20Anand%20Subramoney%2C%20Franz%20Scherr%2C%20Guillaume%20Bellec%2C%20Elias%20Hajek%2C%20Darjan%20Salaj%2C%20Robert%20Legenstein%2C%20Wolfgang%20Maass%20-%20/" class="md-nav__link">
        Citation Anand Subramoney, Franz Scherr, Guillaume Bellec, Elias Hajek, Darjan Salaj, Robert Legenstein, Wolfgang Maass   
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20Eleni%20Vasilaki%2C%20Nicolas%20Fr%C3%A9maux%2C%20Robert%20Urbanczik%2C%20Walter%20Senn%2C%20Wulfram%20Gerstner%20-%202009/" class="md-nav__link">
        Citation Eleni Vasilaki, Nicolas Frémaux, Robert Urbanczik, Walter Senn, Wulfram Gerstner   2009
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20Peter%20Diehl%2C%20Matthew%20Cook%20-%202015/" class="md-nav__link">
        Citation Peter Diehl, Matthew Cook   2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20SlowProcessesNeurons/" class="md-nav__link">
        Citation SlowProcessesNeurons
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
      
      
      
        <label class="md-nav__link" for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
          Tweets
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_8">
          <span class="md-nav__icon md-icon"></span>
          Tweets
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/tweets/Twitter%20-%20VnVrinda%20-%20How%20to%20search%20and%20read%20papers%20-%2011.07.22/" class="md-nav__link">
        Twitter   VnVrinda   How to search and read papers   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/tweets/Twitter%20-%20YanliangShi%20-%2020.07.22/" class="md-nav__link">
        Twitter   YanliangShi   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/tweets/Twitter%20-%20hisspikeness%20-%2023.07.22/" class="md-nav__link">
        Twitter   hisspikeness   23.07.22
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
      
      
      
        <label class="md-nav__link" for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
          Zot2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_9">
          <span class="md-nav__icon md-icon"></span>
          Zot2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40abelesCorticonicsNeuralCircuits1991/" class="md-nav__link">
        Corticonics: Neural Circuits of the Cerebral Cortex
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40allmanPathophysiologicalDistortionsTime2012/" class="md-nav__link">
        Pathophysiological distortions in time perception and timed performance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40balciPeakIntervalProcedure2020/" class="md-nav__link">
        The Peak Interval Procedure in Rodents: A Tool for Studying the Neurobiological Basis of Interval Timing and Its Alterations in Models of Human Disease
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40beggsCortexCriticalPoint2022/" class="md-nav__link">
        The Cortex and the Critical Point: Understanding the Power of Emergence.
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bellecSolutionLearningDilemma2020/" class="md-nav__link">
        A solution to the learning dilemma for recurrent networks of spiking neurons
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bendorBiasingContentHippocampal2012/" class="md-nav__link">
        Biasing the content of hippocampal replay during sleep
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bengioBiologicallyPlausibleDeep2016/" class="md-nav__link">
        Towards Biologically Plausible Deep Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40biTimeRepresentationNeural2019/" class="md-nav__link">
        Time representation in neural network models trained to perform interval timing tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40botvinickDeepReinforcementLearning2020/" class="md-nav__link">
        Deep Reinforcement Learning and its Neuroscientific Implications
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40brahlekTransportPropertiesTopological2015/" class="md-nav__link">
        @brahlekTransportPropertiesTopological2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bretteSimulationNetworksSpiking2007/" class="md-nav__link">
        Simulation of networks of spiking neurons: A review of tools and strategies
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bronsteinGeometricDeepLearning2021/" class="md-nav__link">
        Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40buzsakiBrainRhythmsHave2023/" class="md-nav__link">
        Brain rhythms have come of age
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40caoExplanatoryModelsNeuroscience2021/" class="md-nav__link">
        Explanatory models in neuroscience: Part 1 -- taking mechanistic abstraction seriously
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40carvalhoTemporalBisectionProcedure2019/" class="md-nav__link">
        Temporal Bisection Procedure
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40chenDeepReinforcementLearning2022/" class="md-nav__link">
        Deep Reinforcement Learning with Spiking Q-learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40chilkuriParallelizingLegendreMemory2021/" class="md-nav__link">
        Parallelizing Legendre Memory Unit Training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40cramerSurrogateGradientsAnalog2022/" class="md-nav__link">
        Surrogate gradients for analog neuromorphic computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40cruzActionSuppressionReveals2022/" class="md-nav__link">
        Action suppression reveals opponent parallel control via striatal circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40deverettIntervalTimingDeep2019/" class="md-nav__link">
        Interval timing in deep reinforcement learning agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40erdemExploringRelationshipsEffort2020/" class="md-nav__link">
        Exploring relationships between effort, motion, and sound in new musical instruments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40eshraghianIntroduction2022/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40fangwei123456SpikingJelly2022/" class="md-nav__link">
        SpikingJelly
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40fremauxNeuromodulatedSpikeTimingDependentPlasticity2016/" class="md-nav__link">
        Neuromodulated Spike-Timing-Dependent Plasticity, and Theory of Three-Factor Learning Rules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40furberSpiNNakerProject2014/" class="md-nav__link">
        The SpiNNaker Project
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gallonialessandroSNUFA2022Behavioral/" class="md-nav__link">
        SNUFA 2022 - Behavioral Timescale Synaptic Plasticity (BTSP) for credit assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gaoCorticalColumnWholebrain/" class="md-nav__link">
        Cortical column and whole-brain imaging with molecular contrast and nanoscale resolution
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gershmanReinforcementLearningEpisodic2017/" class="md-nav__link">
        Reinforcement learning and episodic memory in humans and animals: an integrative framework
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gershmanTimeRepresentationReinforcement2014/" class="md-nav__link">
        Time representation in reinforcement learning models of the basal ganglia
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gerstnerEligibilityTracesPlasticity2018/" class="md-nav__link">
        @gerstnerEligibilityTracesPlasticity2018
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40goudarEncodingSensoryMotor2018/" class="md-nav__link">
        Encoding sensory and motor patterns as time-invariant trajectories in recurrent neural networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40grondinTimingTimePerception2010/" class="md-nav__link">
        Timing and time perception: A review of recent behavioral and neuroscience findings and theoretical directions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40guOscillatoryMultiplexingNeural2015a/" class="md-nav__link">
        Oscillatory multiplexing of neural population codes for interval timing and working memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hamedaniDeepSpikingDelayed2020/" class="md-nav__link">
        Deep Spiking Delayed Feedback Reservoirs and Its Application in Spectrum Sensing of MIMO-OFDM Dynamic Spectrum Sharing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hardyEncodingTimeFeedforward2018/" class="md-nav__link">
        Encoding Time in Feedforward Trajectories of a Recurrent Neural Network Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hartcher-obrienSingleMechanismAccount2016/" class="md-nav__link">
        A single mechanism account of duration and rate processing via the pacemaker-accumulator and beat frequency models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hasegawaModelMultisecondTiming2015/" class="md-nav__link">
        A model of multisecond timing behaviour under peak-interval procedures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hesselRainbowCombiningImprovements2017/" class="md-nav__link">
        Rainbow: Combining Improvements in Deep Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40howardEfficientNeuralComputation2019/" class="md-nav__link">
        Efficient Neural Computation in the Laplace Domain
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40jazayeriNeuralMechanismSensing2015/" class="md-nav__link">
        @jazayeriNeuralMechanismSensing2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40jensenSelfOrganizedCriticalityEmergent1998/" class="md-nav__link">
        Self-Organized Criticality: Emergent Complex Behavior in Physical and Biological Systems
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40jirsaEntropyFreeEnergy2022/" class="md-nav__link">
        @jirsaEntropyFreeEnergy2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40khajehabdollahiWhenBeCritical2022/" class="md-nav__link">
        When to Be Critical? Performance and Evolvability in Different Regimes of Neural Ising Agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40kononowiczTimingTimePerception2018/" class="md-nav__link">
        Timing and Time Perception
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40kumarSpikingActivityPropagation2010/" class="md-nav__link">
        Spiking activity propagation in neuronal networks: reconciling different perspectives on neural coding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40lentResourceSelectionCognitive2018/" class="md-nav__link">
        Resource Selection in Cognitive Networks With Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40liMiceInferProbabilistic2013/" class="md-nav__link">
        Mice infer probabilistic models for timing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40liuHumanLevelControlDirectly2022/" class="md-nav__link">
        Human-Level Control Through Directly Trained Deep Spiking $Q$-Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40liuMultiobjectiveReinforcementLearning2015/" class="md-nav__link">
        Multiobjective Reinforcement Learning: A Comprehensive Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40loefflerModularityMultitaskingNeuromemristive2021a/" class="md-nav__link">
        Modularity and multitasking in neuro-memristive reservoir networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40maassNetworksSpikingNeurons1997/" class="md-nav__link">
        Networks of spiking neurons: The third generation of neural network models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40marsiliQuantifyingRelevanceLearning2022/" class="md-nav__link">
        Quantifying Relevance in Learning and Inference
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40matellNeuropsychologicalMechanismsInterval2000/" class="md-nav__link">
        Neuropsychological mechanisms of interval timing behavior
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40mehonicBraininspiredComputingNeeds2022/" class="md-nav__link">
        Brain-inspired computing needs a master plan
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40melloNeuralBehavioralMechanisms2016/" class="md-nav__link">
        Neural and Behavioral Mechanisms of Interval Timing in the Striatum
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40melloScalablePopulationCode2015/" class="md-nav__link">
        A Scalable Population Code for Time in the Striatum
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40mnihHumanlevelControlDeep2015/" class="md-nav__link">
        Human-level control through deep reinforcement learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40montanoGridgraphModelingEmergent2022/" class="md-nav__link">
        Grid-graph modeling of emergent neuromorphic dynamics and heterosynaptic plasticity in memristive nanonetworks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40montemurroPhaseofFiringCodingNatural2008/" class="md-nav__link">
        Phase-of-Firing Coding of Natural Visual Stimuli in Primary Visual Cortex
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40murrayLearningMultipleVariablespeed2017/" class="md-nav__link">
        Learning multiple variable-speed sequences in striatum via cortical tutoring
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40naudBurstdependentSynapticPlasticity/" class="md-nav__link">
        Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40neftciSurrogateGradientLearning2019/" class="md-nav__link">
        Surrogate Gradient Learning in Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40nicolaSupervisedLearningSpiking2017/" class="md-nav__link">
        Supervised learning in spiking neural networks with FORCE training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40obyrneHowCriticalBrain2022/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40oneillPlayItAgain2010/" class="md-nav__link">
        Play it again: reactivation of waking experience and memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40parisottoStabilizingTransformersReinforcement2020/" class="md-nav__link">
        Stabilizing Transformers for Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40patelImprovedRobustnessReinforcement2019/" class="md-nav__link">
        Improved robustness of reinforcement learning policies upon conversion to spiking neuronal network platforms applied to Atari Breakout game
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40patonNeuralBasisTiming2018/" class="md-nav__link">
        The Neural Basis of Timing: Distributed Mechanisms for Diverse Functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40payeurBurstdependentSynapticPlasticity2021/" class="md-nav__link">
        Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40petterIntegratingModelsInterval2018/" class="md-nav__link">
        Integrating Models of Interval Timing and Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40petterTemporalProcessingIntrinsic2016/" class="md-nav__link">
        Temporal Processing by Intrinsic Neural Network Dynamics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pfeifferDeepLearningSpiking2018/" class="md-nav__link">
        Deep Learning With Spiking Neurons: Opportunities and Challenges
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40phuongFormalAlgorithmsTransformers2022/" class="md-nav__link">
        Formal Algorithms for Transformers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoAssessingRobustnessCritical2022/" class="md-nav__link">
        Assessing the robustness of critical behavior in stochastic cellular automata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoBidirectionalLearningRobust2019%20%282%29/" class="md-nav__link">
        Bidirectional Learning for Robust Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoBidirectionalLearningRobust2019/" class="md-nav__link">
        Bidirectional Learning for Robust Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoGeneralRepresentationDynamical2019/" class="md-nav__link">
        A general representation of dynamical systems for reservoir computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40ponulakIntroductionSpikingNeural2011/" class="md-nav__link">
        Introduction to spiking neural networks: Information processing, learning and applications
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40princeCurrentStateFuture2022/" class="md-nav__link">
        Current State and Future Directions for Learning in Biological Recurrent Neural Networks: A Perspective Piece
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40ramsauerHopfieldNetworksAll2020/" class="md-nav__link">
        Hopfield Networks is All You Need
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40rolnickExperienceReplayContinual2019a/" class="md-nav__link">
        Experience Replay for Continual Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40rossbroichFluctuationdrivenInitializationSpiking2022/" class="md-nav__link">
        Fluctuation-driven initialization for spiking neural network training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40rueckauerConversionContinuousValuedDeep2017/" class="md-nav__link">
        Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40schmittNeuromorphicHardwareLoop2017/" class="md-nav__link">
        Neuromorphic hardware in the loop: Training a deep spiking network on the BrainScaleS wafer-scale system
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40shankarScaleInvariantInternalRepresentation2012/" class="md-nav__link">
        A Scale-Invariant Internal Representation of Time
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40shiScalarTimingTheory2022/" class="md-nav__link">
        Beyond Scalar Timing Theory: Integrating Neural Oscillators with Computational Accessibility in Memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40shiSpatialTemporalCorrelations2022/" class="md-nav__link">
        Spatial and temporal correlations in neural networks with structured connectivity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40songRewardbasedTrainingRecurrent2017/" class="md-nav__link">
        Reward-based training of recurrent neural networks for cognitive and value-based tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40spinolaWhatHumansLearn2013/" class="md-nav__link">
        What do humans learn in a double, temporal bisection task: Absolute or relative stimulus durations?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40sunReviewDesignsApplications2020/" class="md-nav__link">
        A Review of Designs and Applications of Echo State Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40sungSimultaneousEmulationSynaptic2022/" class="md-nav__link">
        Simultaneous emulation of synaptic and intrinsic plasticity using a memristive synapse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40suvrathanSTDPDiverseFunctionally2019a/" class="md-nav__link">
        Beyond STDP—towards diverse and functionally relevant plasticity rules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40swearingenPatternRespondingPeakInterval2010/" class="md-nav__link">
        The Pattern of Responding in the Peak-Interval Procedure with Gaps: An Individual-Trials Analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tallotNeuralEncodingTime2020/" class="md-nav__link">
        Neural encoding of time in the animal brain
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tanStrategyBenchmarkConverting2020/" class="md-nav__link">
        Strategy and Benchmark for Converting Deep Q-Networks to Event-Driven Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tangDeepReinforcementLearning2020/" class="md-nav__link">
        Deep Reinforcement Learning with Population-Coded Spiking Neural Network for Continuous Control
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tavanaeiDeepLearningSpiking2019/" class="md-nav__link">
        Deep learning in spiking neural networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tekiPersistenceMemoryHow2017/" class="md-nav__link">
        The Persistence of Memory: How the Brain Encodes Time in Memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tello-ramosTimePlaceLearning2015/" class="md-nav__link">
        Time–place learning in wild, free-living hummingbirds
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40varellaModelPeakintervalTask2019/" class="md-nav__link">
        A model for the peak-interval task based on neural oscillation-delimited states
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40vasilakiSpikeBasedReinforcementLearning2009/" class="md-nav__link">
        Spike-Based Reinforcement Learning in Continuous State and Action Space: When Policy Gradient Methods Fail
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40vigneronCriticalSurveySTDP2020/" class="md-nav__link">
        A critical survey of STDP in Spiking Neural Networks for Pattern Recognition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40voelkerDynamicalSystemsSpiking2019/" class="md-nav__link">
        Dynamical Systems in Spiking Neuromorphic Hardware
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40voelkerLegendreMemoryUnits2019/" class="md-nav__link">
        Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40voelkerSpikePerformanceTraining2021/" class="md-nav__link">
        A Spike in Performance: Training Hybrid-Spiking Neural Networks with Quantized Activation Functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40vogelsSignalPropagationLogic2005/" class="md-nav__link">
        Signal Propagation and Logic Gating in Networks of Integrate-and-Fire Neurons
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wangConvergentEfficientDeep2022/" class="md-nav__link">
        Convergent and Efficient Deep Q Network Algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wangDeepReinforcementLearning2022/" class="md-nav__link">
        Deep Reinforcement Learning: A Survey
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wengTianshouHighlyModularized2022/" class="md-nav__link">
        Tianshou: a Highly Modularized Deep Reinforcement Learning Library
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wiesnerMeasuringComplexity2020/" class="md-nav__link">
        Measuring complexity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40williamsNeuralBurstCodes2021/" class="md-nav__link">
        Neural burst codes disguised as rate codes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wilsonInfluenceMultipleTemporal2015/" class="md-nav__link">
        The influence of multiple temporal memories in the peak-interval procedure
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wimmerRewardLearningWorking2022/" class="md-nav__link">
        Reward learning and working memory: Effects of massed versus spaced training and post-learning delay period
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinAccurateOnlineTraining2022/" class="md-nav__link">
        Accurate online training of dynamical spiking neural networks through Forward Propagation Through Time
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinEffectiveEfficientComputation2020/" class="md-nav__link">
        Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinIntervaltimingProtocolsTheir2017/" class="md-nav__link">
        Interval-timing Protocols and Their Relevancy to the Study of Temporal Cognition and Neurobehavioral Genetics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinOscillationCoincidenceDetectionModels2022/" class="md-nav__link">
        Oscillation/Coincidence-Detection Models of Reward-Related Timing in Corticostriatal Circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zenkeSuperSpikeSupervisedLearning2018/" class="md-nav__link">
        SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zenkeSynapticPlasticityNeural2013/" class="md-nav__link">
        Synaptic Plasticity in Neural Networks Needs Homeostasis with a Fast Rate Detector
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zenkeVisualizingJointFuture2021/" class="md-nav__link">
        Visualizing a joint future of neuroscience and neuromorphic engineering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zhouEncodingTimeNeural2022/" class="md-nav__link">
        Encoding time in neural dynamic regimes with distinct computational tradeoffs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/Diehl%20and%20Cook%202015/" class="md-nav__link">
        Unsupervised learning of digit recognition using spike-timing-dependent plasticity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/goldwasserPlantingUndetectableBackdoors2022/" class="md-nav__link">
        goldwasserPlantingUndetectableBackdoors2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/wiesnerMeasuringComplexity2020/" class="md-nav__link">
        ['Measuring complexity']
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          70 Wiki
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          70 Wiki
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
          Articles
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          Articles
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../A%20Comprehensive%20Guide%20to%20Convolutional%20Neural%20Networks%20%E2%80%94%20the%20ELI5%20way%20-%2003.04.22/" class="md-nav__link">
        A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way   03.04.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Strange%20Illusion%20Shows%20The%20Human%20Brain%20Mess%20With%20Time%20to%20Maintain%20Our%20Expectations%20-%2008.04.22/" class="md-nav__link">
        Strange Illusion Shows The Human Brain Mess With Time to Maintain Our Expectations   08.04.22
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_3" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4_1_3" id="__nav_4_1_3_label" tabindex="0">
          MD Clips Old
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_3_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4_1_3">
          <span class="md-nav__icon md-icon"></span>
          MD Clips Old
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20Neural%20Mechanism%20for%20Sensing%20and%20Reproducing%20a%20Time%20Interval%20-%2023.06.22/" class="md-nav__link">
        A Neural Mechanism for Sensing and Reproducing a Time Interval   23.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20Neural%20Mechanism%20for%20Sensing%20and%20Reproducing%20a%20Time%20Interval%20-%20PubMed%20-%2023.06.22/" class="md-nav__link">
        A Neural Mechanism for Sensing and Reproducing a Time Interval   PubMed   23.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20Scalable%20Population%20Code%20for%20Time%20in%20the%20Striatum%20-%2008.06.22/" class="md-nav__link">
        A Scalable Population Code for Time in the Striatum - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20model%20of%20multisecond%20timing%20behaviour%20under%20peak-interval%20procedures%20-%2011.07.22/" class="md-nav__link">
        A model of multisecond timing behaviour under peak interval procedures   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20review%20of%20learning%20in%20biologically%20plausible%20spiking%20neural%20networks%20-%2011.05.22/" class="md-nav__link">
        A review of learning in biologically plausible spiking neural networks - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Action%20suppression%20reveals%20opponent%20parallel%20control%20via%20striatal%20circuits%20-%2012.07.22/" class="md-nav__link">
        Action suppression reveals opponent parallel control via striatal circuits   12.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../An%20experimental%20unification%20of%20reservoir%20computing%20methods%20-%2011.05.22/" class="md-nav__link">
        An experimental unification of reservoir computing methods - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Computational%20roles%20of%20plastic%20probabilistic%20synapses%20-%2010.05.22/" class="md-nav__link">
        Computational roles of plastic probabilistic synapses - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Deep%20Learning%20With%20Spiking%20Neurons%20Opportunities%20and%20Challenges%20-%2011.05.22/" class="md-nav__link">
        Frontiers | Deep Learning With Spiking Neurons: Opportunities and Challenges | Neuroscience
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Design%20of%20deep%20echo%20state%20networks%20-%2009.05.22/" class="md-nav__link">
        Design of deep echo state networks - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Encoding%20sensory%20and%20motor%20patterns%20as%20time-invariant%20trajectories%20in%20recurrent%20neural%20networks%20-%2020.07.22/" class="md-nav__link">
        Encoding sensory and motor patterns as time invariant trajectories in recurrent neural networks   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Ensemble%20Coding%20of%20Vocal%20Control%20in%20Birdsong%20-%2021.06.22/" class="md-nav__link">
        Ensemble Coding of Vocal Control in Birdsong   21.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Hopfield%20network%20-%20Scholarpedia%20-%2022.06.22/" class="md-nav__link">
        Hopfield network   Scholarpedia   22.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20Review%20Articles%20-%2009.06.22/" class="md-nav__link">
        How to Review Articles   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20conduct%20a%20review%20-%2009.06.22/" class="md-nav__link">
        How to conduct a review   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20write%20a%20review%20article%20%20Writing%20your%20paper%20%20Author%20Services%20-%2009.06.22/" class="md-nav__link">
        How to write a review article  Writing your paper  Author Services   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20write%20a%20superb%20literature%20review%20-%2009.06.22/" class="md-nav__link">
        How to write a superb literature review
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Human-level%20control%20through%20deep%20reinforcement%20learning%20-%2013.05.22/" class="md-nav__link">
        Human-level control through deep reinforcement learning | Nature
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Integrating%20Models%20of%20Interval%20Timing%20and%20Reinforcement%20Learning%20-%2011.07.22/" class="md-nav__link">
        Integrating Models of Interval Timing and Reinforcement Learning   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../L0%20Norm%2C%20L1%20Norm%2C%20L2%20Norm%20%26%20L-Infinity%20Norm%20-%20Sara%20Iris%20Garcia%20-%20Medium%20-%2013.06.22/" class="md-nav__link">
        L0 Norm, L1 Norm, L2 Norm & L Infinity Norm   Sara Iris Garcia   Medium   13.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Learning%20multiple%20variable-speed%20sequences%20in%20striatum%20via%20cortical%20tutoring%20-%2020.07.22/" class="md-nav__link">
        Learning multiple variable speed sequences in striatum via cortical tutoring   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Learning%20offline%20memory%20replay%20in%20biological%20and%20artificial%20reinforcement%20learning%20-%2010.05.22/" class="md-nav__link">
        Learning offline: memory replay in biological and artificial reinforcement learning - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Local%20Lyapunov%20exponents%20of%20deep%20echo%20state%20networks%20-%2009.05.22/" class="md-nav__link">
        Local Lyapunov exponents of deep echo state networks - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Pad%C3%A9%20approximant%20-%20Scholarpedia%20-%2009.05.22/" class="md-nav__link">
        Padé approximant   Scholarpedia   09.05.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Peer%20Review%20Writing%20Guide%20-%20Reviewer%20Resources%20-%20Volunteer%20-%2009.06.22/" class="md-nav__link">
        Peer Review Writing Guide   Reviewer Resources   Volunteer   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../RNN%2C%20LSTM%20%26%20GRU%20-%2009.05.22/" class="md-nav__link">
        RNN, LSTM & GRU
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Reinforcement%20learning%20and%20episodic%20memory%20in%20humans%20and%20animals%20an%20integrative%20framework%20-%2020.07.22/" class="md-nav__link">
        Reinforcement learning and episodic memory in humans and animals an integrative framework   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Reinforcement%20learning%20with%20Marr%20-%2020.07.22/" class="md-nav__link">
        Reinforcement learning with Marr   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Research%20Guides%20Publishing%20in%20the%20Sciences%20How%20to%20Write%20a%20Scientific%20Literature%20Review%20-%2009.06.22/" class="md-nav__link">
        Research Guides Publishing in the Sciences How to Write a Scientific Literature Review   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Reservoir%20computing%20and%20extreme%20learning%20machines%20for%20non-linear%20time-series%20data%20analysis%20-%2011.05.22/" class="md-nav__link">
        Reservoir computing and extreme learning machines for non-linear time-series data analysis - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Reservoir computing approaches to recurrent neural network training - ScienceDirect
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Reservoir computing approaches to recurrent neural network training - ScienceDirect
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#excerpt" class="md-nav__link">
    Excerpt
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1. Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-formalism" class="md-nav__link">
    2. Formalism
  </a>
  
    <nav class="md-nav" aria-label="2. Formalism">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-formulation-of-the-problem" class="md-nav__link">
    2.1. Formulation of the problem
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-expansions-and-kernels-in-non-temporal-tasks" class="md-nav__link">
    2.2. Expansions and kernels in non-temporal tasks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-expansions-in-temporal-tasks" class="md-nav__link">
    2.3. Expansions in temporal tasks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-recurrent-neural-networks" class="md-nav__link">
    2.4. Recurrent neural networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25-classical-training-of-rnns" class="md-nav__link">
    2.5. Classical training of RNNs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-reservoir-methods" class="md-nav__link">
    3. Reservoir methods
  </a>
  
    <nav class="md-nav" aria-label="3. Reservoir methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-echo-state-networks" class="md-nav__link">
    3.1. Echo State Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-liquid-state-machines" class="md-nav__link">
    3.2. Liquid State Machines
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-evolino" class="md-nav__link">
    3.3. Evolino
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-backpropagation-decorrelation" class="md-nav__link">
    3.4. Backpropagation-Decorrelation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-temporal-recurrent-networks" class="md-nav__link">
    3.5. Temporal Recurrent Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-other-exotic-types-of-reservoirs" class="md-nav__link">
    3.6. Other (exotic) types of reservoirs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#37-other-overviews-of-reservoir-methods" class="md-nav__link">
    3.7. Other overviews of reservoir methods
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-our-classification-of-reservoir-recipes" class="md-nav__link">
    4. Our classification of reservoir recipes
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-generic-reservoir-recipes" class="md-nav__link">
    5. Generic reservoir recipes
  </a>
  
    <nav class="md-nav" aria-label="5. Generic reservoir recipes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-classical-esn-approach" class="md-nav__link">
    5.1. Classical ESN approach
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-different-topologies-of-the-reservoir" class="md-nav__link">
    5.2. Different topologies of the reservoir
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-modular-reservoirs" class="md-nav__link">
    5.3. Modular reservoirs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54-time-delayed-vs-instantaneous-connections" class="md-nav__link">
    5.4. Time-delayed vs. instantaneous connections
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#55-leaky-integrator-neurons-and-speed-of-dynamics" class="md-nav__link">
    5.5. Leaky integrator neurons and speed of dynamics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-unsupervised-reservoir-adaptation" class="md-nav__link">
    6. Unsupervised reservoir adaptation
  </a>
  
    <nav class="md-nav" aria-label="6. Unsupervised reservoir adaptation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-goodness-measures-of-the-reservoir-activations" class="md-nav__link">
    6.1. “Goodness” measures of the reservoir activations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-unsupervised-local-methods" class="md-nav__link">
    6.2. Unsupervised local methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-unsupervised-global-methods" class="md-nav__link">
    6.3. Unsupervised global methods
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-supervised-reservoir-pre-training" class="md-nav__link">
    7. Supervised reservoir pre-training
  </a>
  
    <nav class="md-nav" aria-label="7. Supervised reservoir pre-training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-optimization-of-global-reservoir-parameters" class="md-nav__link">
    7.1. Optimization of global reservoir parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-evolutionary-methods" class="md-nav__link">
    7.2. Evolutionary methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#73-other-types-of-supervised-reservoir-tuning" class="md-nav__link">
    7.3. Other types of supervised reservoir tuning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#74-trained-auxiliary-feedbacks" class="md-nav__link">
    7.4. Trained auxiliary feedbacks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#75-reinforcement-learning" class="md-nav__link">
    7.5. Reinforcement learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-readouts-from-the-reservoirs" class="md-nav__link">
    8. Readouts from the reservoirs
  </a>
  
    <nav class="md-nav" aria-label="8. Readouts from the reservoirs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-single-layer-readout" class="md-nav__link">
    8.1. Single-layer readout
  </a>
  
    <nav class="md-nav" aria-label="8.1. Single-layer readout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#811-linear-regression" class="md-nav__link">
    8.1.1. Linear regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#812-online-adaptive-output-weight-training" class="md-nav__link">
    8.1.2. Online adaptive output weight training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#813-svm-style-readout" class="md-nav__link">
    8.1.3. SVM-style readout
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-feedbacks-and-stability-issues" class="md-nav__link">
    8.2. Feedbacks and stability issues
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#83-readouts-for-classificationrecognition" class="md-nav__link">
    8.3. Readouts for classification/recognition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#84-readouts-beyond-supervised-learning" class="md-nav__link">
    8.4. Readouts beyond supervised learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#85-multilayer-readouts" class="md-nav__link">
    8.5. Multilayer readouts
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#86-readouts-with-delays" class="md-nav__link">
    8.6. Readouts with delays
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#87-combining-several-readouts" class="md-nav__link">
    8.7. Combining several readouts
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#88-hierarchies" class="md-nav__link">
    8.8. Hierarchies
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-discussion" class="md-nav__link">
    9. Discussion
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#acknowledgments" class="md-nav__link">
    Acknowledgments
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cited-by-1641" class="md-nav__link">
    Cited by (1641)
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Scientists%20discover%20how%20the%20brain%20keeps%20the%20urge%20to%20act%20in%20check%20%20Champalimaud%20Foundation%20-%2012.07.22/" class="md-nav__link">
        Scientists discover how the brain keeps the urge to act in check  Champalimaud Foundation   12.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Spiking%20Neural%20Networks%20and%20online%20learning%20An%20overview%20and%20perspectives%20-%2010.05.22/" class="md-nav__link">
        Spiking Neural Networks and online learning: An overview and perspectives - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action%20-%2006.05.22/" class="md-nav__link">
        State–action–reward–state–action   06.05.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Temporal%20Bisection%20Procedure%20-%2020.06.22/" class="md-nav__link">
        Temporal Bisection Procedure   20.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../The%20Pattern%20of%20Responding%20in%20the%20Peak-Interval%20Procedure%20with%20Gaps%20An%20Individual-Trials%20Analysis%20-%2011.07.22/" class="md-nav__link">
        The Pattern of Responding in the Peak Interval Procedure with Gaps An Individual Trials Analysis   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../The%20influence%20of%20multiple%20temporal%20memories%20in%20the%20peak-interval%20procedure%20-%2011.07.22%20%281%29/" class="md-nav__link">
        The influence of multiple temporal memories in the peak interval procedure   11.07.22 (1)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../The%20influence%20of%20multiple%20temporal%20memories%20in%20the%20peak-interval%20procedure%20-%2011.07.22/" class="md-nav__link">
        The influence of multiple temporal memories in the peak interval procedure   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Time%20representation%20in%20reinforcement%20learning%20models%20of%20the%20basal%20ganglia%20-%2020.07.22/" class="md-nav__link">
        Time representation in reinforcement learning models of the basal ganglia   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Unified%20pre-%20and%20postsynaptic%20long-term%20plasticity%20enables%20reliable%20and%20flexible%20learning%20-%2011.05.22/" class="md-nav__link">
        Unified pre- and postsynaptic long-term plasticity enables reliable and flexible learning | eLife
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../What%20do%20humans%20learn%20in%20a%20double%2C%20temporal%20bisection%20task%20Absolute%20or%20relative%20stimulus%20durations%20-%2020.06.22/" class="md-nav__link">
        What do humans learn in a double, temporal bisection task Absolute or relative stimulus durations   20.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Wikipedia%20-%20Adiabatic%20process%20-%2022.06.22/" class="md-nav__link">
        Wikipedia   Adiabatic process   22.06.22
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_4" id="__nav_4_1_4_label" tabindex="0">
          MD webclips
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_4">
          <span class="md-nav__icon md-icon"></span>
          MD webclips
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_4_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_4_1" id="__nav_4_1_4_1_label" tabindex="0">
          MD Clips New
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_4_1_4_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_4_1">
          <span class="md-nav__icon md-icon"></span>
          MD Clips New
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../MD_webclips/MD_Clips_New/Surrogate%20gradients%20for%20analog%20neuromorphic%20computing%20-%2006.12.22/" class="md-nav__link">
        Surrogate gradients for analog neuromorphic computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../MD_webclips/MD_Clips_New/What%20is%20the%20Difference%20Between%20Gradient%20Descent%20and%20Gradient%20Ascent%20-%2012.01.23/" class="md-nav__link">
        What is the Difference Between Gradient Descent and Gradient Ascent?
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_5" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_5" id="__nav_4_1_5_label" tabindex="0">
          Markdown Clips
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_5">
          <span class="md-nav__icon md-icon"></span>
          Markdown Clips
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips/A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning%20-%20MachineLearningMastery.com%20-%2008.02.23.md/" class="md-nav__link">
        A Gentle Introduction to Cross-Entropy for Machine Learning - MachineLearningMastery.com
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips/Human-level%20control%20through%20deep%20reinforcement%20learning%20-%2020.02.23.md/" class="md-nav__link">
        Human-level control through deep reinforcement learning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_6" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_6" id="__nav_4_1_6_label" tabindex="0">
          Markdown Clips old
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_6">
          <span class="md-nav__icon md-icon"></span>
          Markdown Clips old
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/A%20model%20for%20the%20peak-interval%20task%20based%20on%20neural%20oscillation-delimited%20states%20-%2017.11.22.md/" class="md-nav__link">
        A model for the peak-interval task based on neural oscillation-delimited states
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/A%20single%20mechanism%20account%20of%20duration%20and%20rate%20processing%20via%20the%20pacemaker-accumulator%20and%20beat%20frequency%20models%20-%2016.11.22.md/" class="md-nav__link">
        A single mechanism account of duration and rate processing via the pacemaker-accumulator and beat frequency models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Beyond%20STDP%20%E2%80%94%20towards%20diverse%20and%20functionally%20relevant%20plasticity%20rules%20-%2012.10.22/" class="md-nav__link">
        Beyond STDP — towards diverse and functionally relevant plasticity rules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Geometric%20foundations%20of%20Deep%20Learning%20-%20Towards%20Data%20Science%20-%2006.10.22/" class="md-nav__link">
        Geometric foundations of Deep Learning - Towards Data Science
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Grid-graph%20modeling%20of%20emergent%20neuromorphic%20dynamics%20and%20heterosynaptic%20plasticity%20in%20memristive%20nanonetworks%20-%2007.10.22/" class="md-nav__link">
        Grid-graph modeling of emergent neuromorphic dynamics and heterosynaptic plasticity in memristive nanonetworks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/How%20critical%20is%20brain%20criticality%20-%20%20Original/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/How%20critical%20is%20brain%20criticality%20-%2020.10.22/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/How%20critical%20is%20brain%20criticality%20-%2022.10.22/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Modularity%20and%20multitasking%20in%20neuro-memristive%20reservoir%20networks%20-%2007.10.22/" class="md-nav__link">
        Modularity and multitasking in neuro-memristive reservoir networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Oscillatory%20multiplexing%20of%20neural%20population%20codes%20for%20interval%20timing%20and%20working%20memory%20-%2021.11.22.md/" class="md-nav__link">
        Oscillatory multiplexing of neural population codes for interval timing and working memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/The%20Persistence%20of%20Memory%20How%20the%20Brain%20Encodes%20Time%20in%20Memory%20-%2021.11.22.md/" class="md-nav__link">
        The Persistence of Memory: How the Brain Encodes Time in Memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Timing%20Intervals%20Using%20Population%20Synchrony%20and%20Spike%20Timing%20Dependent%20Plasticity%20-%2017.11.22.md/" class="md-nav__link">
        Timing Intervals Using Population Synchrony and Spike Timing Dependent Plasticity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Weighted%20Majority%20Algorithm%20-%20A%20beautiful%20algorithm%20for%20Learning%20from%20Experts/" class="md-nav__link">
        Weighted Majority Algorithm: A beautiful algorithm for Learning from Experts
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
      
      
      
        <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          Pages
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          Pages
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Apical%20Dendrite/" class="md-nav__link">
        Apical Dendrite
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Echo%20State%20Networks/" class="md-nav__link">
        Echo State Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Grid%20Cell%20-%20Time%20Cell%20Interactions/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Hierarchical%20Reinforcement%20Learning/" class="md-nav__link">
        Hierarchical Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Legendre%20Memory%20Unit%20%28LMU%29/" class="md-nav__link">
        Legendre Memory Unit (LMU)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Liquid%20State%20Machine/" class="md-nav__link">
        Liquid State Machine
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Recurrent%20Neural%20Network%20%28RNN%29/" class="md-nav__link">
        Nomenclature around degree of recurrence or moment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Reservoir%20Computing/" class="md-nav__link">
        Reservoir Computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Spiking%20Neural%20Networks%20%28SNN%29/" class="md-nav__link">
        Spiking Neural Networks (SNN)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Striatal%20Beat%20Frequency%20Model/" class="md-nav__link">
        Striatal Beat Frequency Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Striatal%20Beat%20Frequency/" class="md-nav__link">
        Striatal Beat Frequency
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Three-Factor%20Learning%20Rule/" class="md-nav__link">
        Search
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
      
      
      
        <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
          People
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          People
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Anna%20Levina/" class="md-nav__link">
        Anna Levina
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Friedemann%20Zenke/" class="md-nav__link">
        Friedemann Zenke
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Jan%20Tore%20L%C3%B8nning/" class="md-nav__link">
        Jan Tore Lønning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Jeffrey%20Allan%20Lugowe/" class="md-nav__link">
        Jeffrey Allan Lugowe
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Kai%20Olav%20Ellefsen/" class="md-nav__link">
        Kai Olav Ellefsen
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Katalin%20Vertes/" class="md-nav__link">
        Katalin Vertes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Klas%20Henning%20Pettersen/" class="md-nav__link">
        Klas Henning Pettersen
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Sven%20Peter%20N%C3%A4sholm/" class="md-nav__link">
        Sven Peter Näsholm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/people/" class="md-nav__link">
        In this folder
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
          Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/ADAM%20Optimizer/" class="md-nav__link">
        ADAM Optimizer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Adiabatic%20Process/" class="md-nav__link">
        Adiabatic Process
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Bellman%20Equation/" class="md-nav__link">
        From [[HF DRL - Unit 2]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Chaos/" class="md-nav__link">
        Chaos
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Complexity/" class="md-nav__link">
        Reading
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Continuous%20Markov%20Decision%20Process/" class="md-nav__link">
        Continuous Markov Decision Process
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Convolutional%20Neural%20Network%20%28CNN%29/" class="md-nav__link">
        Convolutional Neural Network (CNN)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Cortico-basal%20ganglia-thalamo-cortical%20loop/" class="md-nav__link">
        Cortico basal ganglia thalamo cortical loop
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Credit%20Assignment%20Problem/" class="md-nav__link">
        Credit Assignment Problem
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Criticality/" class="md-nav__link">
        Criticality
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Cross%20Entropy/" class="md-nav__link">
        Cross Entropy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Deep%20Q-Learning/" class="md-nav__link">
        Deep Q Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Double%20DQN/" class="md-nav__link">
        Double DQN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Dyna-Q/" class="md-nav__link">
        Dyna Q
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Echo%20State%20Property%20%28ESP%29/" class="md-nav__link">
        Echo State Property (ESP)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Eligibility%20Trace/" class="md-nav__link">
        Commentary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/End-to-End%20Reinforcement-Learning/" class="md-nav__link">
        End to End Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Epsilon%20Decay/" class="md-nav__link">
        ϵ-Decay Strategies
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Ergodicity/" class="md-nav__link">
        Ergodicity / Ergodic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Excitatory%20-%20Inhibitory%20Oscillation%20%28EIO-SBF%29/" class="md-nav__link">
        Excitatory   Inhibitory Oscillation (EIO SBF)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Excitatory%20-%20Inhibitory%20postsynaptic%20potential%20%28EPSP%29%20-%20%28IPSP%29/" class="md-nav__link">
        Excitatory   Inhibitory postsynaptic potential (EPSP)   (IPSP)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Firing-Rate%20RNN%20Model/" class="md-nav__link">
        Firing Rate RNN Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Fixed%20Interval%20%28FI%29%20procedure/" class="md-nav__link">
        Main
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Geometric%20Deep%20Learning/" class="md-nav__link">
        Geometric Deep Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Impairment%20effect/" class="md-nav__link">
        Impairment effect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Interval%20Timing/" class="md-nav__link">
        Interval Timing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Izhikevich%20%20simple%20spiking%20neurons/" class="md-nav__link">
        Izhikevich  simple spiking neurons
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Kernel/" class="md-nav__link">
        Kernel
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/L2-Norm/" class="md-nav__link">
        L2 Norm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Leaky%20Integrate-and-Fire/" class="md-nav__link">
        Leaky Integrate and Fire
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Long%20Short-Term%20Memory%20%28LSTM%29/" class="md-nav__link">
        Long Short Term Memory (LSTM)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Marr%E2%80%99s%20three%20levels%20of%20analysis/" class="md-nav__link">
        Marr’s three levels of analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Membrane%20Time%20Constant/" class="md-nav__link">
        Membrane Time Constant
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Multi-Armed%20Bandit%20Problem/" class="md-nav__link">
        Multi Armed Bandit Problem
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Multiplicative%20Weights%20Update%20Algorithm/" class="md-nav__link">
        About
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neural%20Cellular%20Automata/" class="md-nav__link">
        Neural Cellular Automata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neural%20Oscillations/" class="md-nav__link">
        Gamma
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neuromorphics/" class="md-nav__link">
        Neuromorphics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neuroscientific%20AI/" class="md-nav__link">
        Neuroscientific AI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Nomenclature%20for%20Experiments/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Novelty%20Signal/" class="md-nav__link">
        Novelty Signal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Nullcline/" class="md-nav__link">
        Nullcline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/On%20Off%20Policy/" class="md-nav__link">
        On Off Policy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Optuna/" class="md-nav__link">
        Optuna
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Pad%C3%A9%20approximants/" class="md-nav__link">
        Padé approximants
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Pareto%20Optimality/" class="md-nav__link">
        Pareto Optimality
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Peak%20Interval%20%28PI%29%20procedure/" class="md-nav__link">
        Peak Interval (PI) procedure
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Physical%20Symbol%20System%20Hypothesis%20%28PSSH%29/" class="md-nav__link">
        Physical Symbol System Hypothesis (PSSH)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Power%20Law%20distribution/" class="md-nav__link">
        Power Law distribution
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Q-Learning%20v.%20SARSA/" class="md-nav__link">
        Q Learning v. SARSA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Q-Learning/" class="md-nav__link">
        Q Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/REINFORCE/" class="md-nav__link">
        REINFORCE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Randomness%2C%20Disorder%2C%20and%20Noise/" class="md-nav__link">
        Randomness, Disorder, and Noise
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/ReLU/" class="md-nav__link">
        ReLU
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Relevance/" class="md-nav__link">
        Relevance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Reward%20Prediction%20Error/" class="md-nav__link">
        Reward Prediction Error
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/SARSA%20Algorithm/" class="md-nav__link">
        SARSA Algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Scalar%20Expectancy%20Theory/" class="md-nav__link">
        Scalar Expectancy Theory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Semi-Markov%20Decision%20Process%20%28SMDP%29/" class="md-nav__link">
        Semi Markov Decision Process (SMDP)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Separation%20Property/" class="md-nav__link">
        Separation Property
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Softmax/" class="md-nav__link">
        Softmax
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Spike%20Response%20Model%20neuron/" class="md-nav__link">
        Extracts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Stochastic%20Gradient%20Descent/" class="md-nav__link">
        Stochastic Gradient Descent
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Striatal%20Beat%20Frequency%20model%20%28SBF%29/" class="md-nav__link">
        Referenced in:
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Surrogate%20Gradient%20Learning/" class="md-nav__link">
        Surrogate Gradient Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Target%20v%20behavior%20Policy/" class="md-nav__link">
        Target v behavior Policy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Temporal%20Bisection%20Task/" class="md-nav__link">
        Temporal Bisection Task
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Temporal%20Difference%20%28TD%29/" class="md-nav__link">
        Temporal Difference (TD)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Three-Factor%20Learning%20Rule/" class="md-nav__link">
        Three Factor Learning Rule
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Three-Factor%20Learning/" class="md-nav__link">
        Three Factor Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Tigerprop/" class="md-nav__link">
        Tigerprop
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Time%20Invariance/" class="md-nav__link">
        Definition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Timing%20Tasks/" class="md-nav__link">
        Timing Tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Topics%20-%20Striatal%20Beat%20Frequency%20model%20%28SBF%29/" class="md-nav__link">
        Referenced in:
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Vogels-Abbott%20Benchmark/" class="md-nav__link">
        Vogels Abbott Benchmark
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Weber%27s%20Law/" class="md-nav__link">
        (Weber’s law):
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Weighted%20majority%20algorithm/" class="md-nav__link">
        Weighted majority algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Wilson-Cowan%20rate%20model/" class="md-nav__link">
        Wilson Cowan rate model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/behavior%20theory%20of%20timing%20%28BeT%29/" class="md-nav__link">
        [[@hasegawaModelMultisecondTiming2015|Takayuki Hasegawa, Shogo Sakata 2015]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/bias-variance%20trade-off/" class="md-nav__link">
        Bias variance trade off
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/catastrophic%20forgetting/" class="md-nav__link">
        Catastrophic forgetting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/central%20tendency%20effects/" class="md-nav__link">
        Central tendency effects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/derivative%20log%20trick/" class="md-nav__link">
        Derivative log trick
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/discount%20factor/" class="md-nav__link">
        Discount factor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/e-prop/" class="md-nav__link">
        E prop
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/learned%20embedding/" class="md-nav__link">
        Learned embedding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/local%20plasticity/" class="md-nav__link">
        Local plasticity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/multiple%20clock%20or%20flexible%20clock%20hypothesis/" class="md-nav__link">
        Multiple clock or flexible clock hypothesis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/pacemaker%20accumulator%20models%20%28PA%29/" class="md-nav__link">
        Reading
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/perceptual%20aliasing/" class="md-nav__link">
        Perceptual aliasing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/primitive%20actions/" class="md-nav__link">
        Primitive actions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/rescaling/" class="md-nav__link">
        Rescaling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/stride/" class="md-nav__link">
        Stride
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/temporal%20abstraction/" class="md-nav__link">
        Temporal abstraction
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          AZ. Assets
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          AZ. Assets
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../AZ.%20Assets/Outline%20or%20Goals%20Draft/" class="md-nav__link">
        Outline or Goals Draft
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          Features
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Features
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Features/LaTeX%20Math%20Support/" class="md-nav__link">
        LaTeX Math Support
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Features/Mermaid%20Diagrams/" class="md-nav__link">
        Mermaid diagrams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Features/Text%20Formatting/" class="md-nav__link">
        Text Formatting
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          Topic 1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Topic 1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Topic%201/Note%201/" class="md-nav__link">
        Note 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Topic%201/Note%202/" class="md-nav__link">
        Note 2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#excerpt" class="md-nav__link">
    Excerpt
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1. Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-formalism" class="md-nav__link">
    2. Formalism
  </a>
  
    <nav class="md-nav" aria-label="2. Formalism">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-formulation-of-the-problem" class="md-nav__link">
    2.1. Formulation of the problem
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-expansions-and-kernels-in-non-temporal-tasks" class="md-nav__link">
    2.2. Expansions and kernels in non-temporal tasks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-expansions-in-temporal-tasks" class="md-nav__link">
    2.3. Expansions in temporal tasks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-recurrent-neural-networks" class="md-nav__link">
    2.4. Recurrent neural networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25-classical-training-of-rnns" class="md-nav__link">
    2.5. Classical training of RNNs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-reservoir-methods" class="md-nav__link">
    3. Reservoir methods
  </a>
  
    <nav class="md-nav" aria-label="3. Reservoir methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-echo-state-networks" class="md-nav__link">
    3.1. Echo State Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-liquid-state-machines" class="md-nav__link">
    3.2. Liquid State Machines
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-evolino" class="md-nav__link">
    3.3. Evolino
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-backpropagation-decorrelation" class="md-nav__link">
    3.4. Backpropagation-Decorrelation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-temporal-recurrent-networks" class="md-nav__link">
    3.5. Temporal Recurrent Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-other-exotic-types-of-reservoirs" class="md-nav__link">
    3.6. Other (exotic) types of reservoirs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#37-other-overviews-of-reservoir-methods" class="md-nav__link">
    3.7. Other overviews of reservoir methods
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-our-classification-of-reservoir-recipes" class="md-nav__link">
    4. Our classification of reservoir recipes
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-generic-reservoir-recipes" class="md-nav__link">
    5. Generic reservoir recipes
  </a>
  
    <nav class="md-nav" aria-label="5. Generic reservoir recipes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-classical-esn-approach" class="md-nav__link">
    5.1. Classical ESN approach
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-different-topologies-of-the-reservoir" class="md-nav__link">
    5.2. Different topologies of the reservoir
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-modular-reservoirs" class="md-nav__link">
    5.3. Modular reservoirs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54-time-delayed-vs-instantaneous-connections" class="md-nav__link">
    5.4. Time-delayed vs. instantaneous connections
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#55-leaky-integrator-neurons-and-speed-of-dynamics" class="md-nav__link">
    5.5. Leaky integrator neurons and speed of dynamics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-unsupervised-reservoir-adaptation" class="md-nav__link">
    6. Unsupervised reservoir adaptation
  </a>
  
    <nav class="md-nav" aria-label="6. Unsupervised reservoir adaptation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-goodness-measures-of-the-reservoir-activations" class="md-nav__link">
    6.1. “Goodness” measures of the reservoir activations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-unsupervised-local-methods" class="md-nav__link">
    6.2. Unsupervised local methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-unsupervised-global-methods" class="md-nav__link">
    6.3. Unsupervised global methods
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-supervised-reservoir-pre-training" class="md-nav__link">
    7. Supervised reservoir pre-training
  </a>
  
    <nav class="md-nav" aria-label="7. Supervised reservoir pre-training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-optimization-of-global-reservoir-parameters" class="md-nav__link">
    7.1. Optimization of global reservoir parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-evolutionary-methods" class="md-nav__link">
    7.2. Evolutionary methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#73-other-types-of-supervised-reservoir-tuning" class="md-nav__link">
    7.3. Other types of supervised reservoir tuning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#74-trained-auxiliary-feedbacks" class="md-nav__link">
    7.4. Trained auxiliary feedbacks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#75-reinforcement-learning" class="md-nav__link">
    7.5. Reinforcement learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-readouts-from-the-reservoirs" class="md-nav__link">
    8. Readouts from the reservoirs
  </a>
  
    <nav class="md-nav" aria-label="8. Readouts from the reservoirs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-single-layer-readout" class="md-nav__link">
    8.1. Single-layer readout
  </a>
  
    <nav class="md-nav" aria-label="8.1. Single-layer readout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#811-linear-regression" class="md-nav__link">
    8.1.1. Linear regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#812-online-adaptive-output-weight-training" class="md-nav__link">
    8.1.2. Online adaptive output weight training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#813-svm-style-readout" class="md-nav__link">
    8.1.3. SVM-style readout
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-feedbacks-and-stability-issues" class="md-nav__link">
    8.2. Feedbacks and stability issues
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#83-readouts-for-classificationrecognition" class="md-nav__link">
    8.3. Readouts for classification/recognition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#84-readouts-beyond-supervised-learning" class="md-nav__link">
    8.4. Readouts beyond supervised learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#85-multilayer-readouts" class="md-nav__link">
    8.5. Multilayer readouts
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#86-readouts-with-delays" class="md-nav__link">
    8.6. Readouts with delays
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#87-combining-several-readouts" class="md-nav__link">
    8.7. Combining several readouts
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#88-hierarchies" class="md-nav__link">
    8.8. Hierarchies
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-discussion" class="md-nav__link">
    9. Discussion
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#acknowledgments" class="md-nav__link">
    Acknowledgments
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cited-by-1641" class="md-nav__link">
    Cited by (1641)
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="reservoir-computing-approaches-to-recurrent-neural-network-training-sciencedirect">Reservoir computing approaches to recurrent neural network training - ScienceDirect<a class="headerlink" href="#reservoir-computing-approaches-to-recurrent-neural-network-training-sciencedirect" title="Permanent link">&para;</a></h1>
<blockquote>
<h2 id="excerpt">Excerpt<a class="headerlink" href="#excerpt" title="Permanent link">&para;</a></h2>
<p>Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reser…</p>
</blockquote>
<hr />
<p><a href="https://www.sciencedirect.com/journal/computer-science-review" title="Go to Computer Science Review on ScienceDirect"><img alt="Elsevier" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/0f41bf0a82e6bb06d7c9c594fb6e5a3548742fb2/image/elsevier-non-solus.png" /></a></p>
<p><a href="https://www.sciencedirect.com/journal/computer-science-review/vol/3/issue/3"><img alt="Computer Science Review" src="https://ars.els-cdn.com/content/image/1-s2.0-S1574013709X00043-cov150h.gif" /></a></p>
<h2 id="abstract">Abstract<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h2>
<p>Echo State Networks and Liquid State Machines introduced a new paradigm in artificial <a href="https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network" title="Learn more about recurrent neural network from ScienceDirect's AI-generated Topic Pages">recurrent neural network</a> (RNN) training, where an RNN (the <em>reservoir</em>) is generated randomly and only a readout is trained. The paradigm, becoming known as <em>reservoir computing</em>, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to <em>using different methods for training the reservoir and the readout</em>. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual <a href="https://www.sciencedirect.com/topics/computer-science/classification" title="Learn more about classification from ScienceDirect's AI-generated Topic Pages">classification</a> of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.</p>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000136"><strong>Next</strong></a></li>
</ul>
<h2 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h2>
<p>Artificial <em><a href="https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network" title="Learn more about recurrent neural networks from ScienceDirect's AI-generated Topic Pages">recurrent neural networks</a></em> (RNNs) represent a large and varied class of computational models that are designed by more or less detailed analogy with biological brain modules. In an RNN numerous abstract <em>neurons</em> (also called <em>units</em> or <em>processing elements</em>) are interconnected by likewise abstracted <em>synaptic connections</em> (or <em>links</em>), which enable <em>activations</em> to propagate through the network. The characteristic feature of RNNs that distinguishes them from the more widely used <em><a href="https://www.sciencedirect.com/topics/computer-science/feedforward-neural-network" title="Learn more about feedforward neural networks from ScienceDirect's AI-generated Topic Pages">feedforward neural networks</a></em> is that the connection topology possesses cycles. The existence of cycles has a profound impact:</p>
<p>•</p>
<p>An RNN may develop a self-sustained temporal activation dynamics along its recurrent connection pathways, even in the absence of input. Mathematically, this renders an RNN to be a <em><a href="https://www.sciencedirect.com/topics/computer-science/dynamical-system" title="Learn more about dynamical system from ScienceDirect's AI-generated Topic Pages">dynamical system</a></em>, while <a href="https://www.sciencedirect.com/topics/computer-science/feedforward-network" title="Learn more about feedforward networks from ScienceDirect's AI-generated Topic Pages">feedforward networks</a> are <em>functions</em>.</p>
<p>•</p>
<p>If driven by an input signal, an RNN preserves in its internal state a <a href="https://www.sciencedirect.com/topics/computer-science/nonlinear-transformation" title="Learn more about nonlinear transformation from ScienceDirect's AI-generated Topic Pages">nonlinear transformation</a> of the input history — in other words, it has a <em>dynamical memory</em>, and is able to process temporal context information.</p>
<p>This review article concerns a particular subset of RNN-based research in two aspects:</p>
<p>•</p>
<p>RNNs are used for a variety of scientific purposes, and at least two major classes of RNN models exist: they can be used for purposes of modeling biological brains, or as engineering tools for technical applications. The first usage belongs to the field of computational neuroscience, while the second frames RNNs in the realms of <a href="https://www.sciencedirect.com/topics/computer-science/machine-learning" title="Learn more about machine learning from ScienceDirect's AI-generated Topic Pages">machine learning</a>, the theory of computation, and <a href="https://www.sciencedirect.com/topics/computer-science/nonlinear-signal" title="Learn more about nonlinear signal from ScienceDirect's AI-generated Topic Pages">nonlinear signal</a> processing and control. While there are interesting connections between the two attitudes, this survey focuses on the latter, with occasional borrowings from the first.</p>
<p>•</p>
<p>From a dynamical systems perspective, there are two main classes of RNNs. Models from the first class are characterized by an energy-minimizing stochastic dynamics and symmetric connections. The best known instantiations are <a href="https://www.sciencedirect.com/topics/computer-science/hopfield-network" title="Learn more about Hopfield networks from ScienceDirect's AI-generated Topic Pages">Hopfield networks</a> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b1">[1]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b2">[2]</a>, <a href="https://www.sciencedirect.com/topics/computer-science/boltzmann-machine" title="Learn more about Boltzmann machines from ScienceDirect's AI-generated Topic Pages">Boltzmann machines</a> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b3">[3]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b4">[4]</a>, and the recently emerging Deep Belief Networks <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b5">[5]</a>. These networks are mostly trained in some <a href="https://www.sciencedirect.com/topics/computer-science/unsupervised-learning" title="Learn more about unsupervised learning from ScienceDirect's AI-generated Topic Pages">unsupervised learning</a> scheme. Typical targeted <a href="https://www.sciencedirect.com/topics/computer-science/network-functionality" title="Learn more about network functionalities from ScienceDirect's AI-generated Topic Pages">network functionalities</a> in this field are <a href="https://www.sciencedirect.com/topics/computer-science/associative-memory" title="Learn more about associative memories from ScienceDirect's AI-generated Topic Pages">associative memories</a>, <a href="https://www.sciencedirect.com/topics/computer-science/data-compression" title="Learn more about data compression from ScienceDirect's AI-generated Topic Pages">data compression</a>, the unsupervised modeling of data distributions, and static pattern <a href="https://www.sciencedirect.com/topics/computer-science/classification" title="Learn more about classification from ScienceDirect's AI-generated Topic Pages">classification</a>, where the model is run for multiple time steps per single input instance to reach some type of convergence or equilibrium (but see e.g., <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b6">[6]</a> for extension to temporal data). The mathematical background is rooted in statistical physics. In contrast, the second big class of RNN models typically features a deterministic update dynamics and directed connections. Systems from this class implement nonlinear filters, which transform an input time series into an output time series. The mathematical background here is nonlinear dynamical systems. The standard training mode is supervised. This survey is concerned only with RNNs of this second type, and when we speak of <em>RNNs</em> later on, we will exclusively refer to such systems.<a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fn1">1</a></p>
<p>RNNs (of the second type) appear as highly promising and fascinating tools for nonlinear time series processing applications, mainly for two reasons. First, it can be shown that under fairly mild and general assumptions, such RNNs are universal approximators of dynamical systems <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b7">[7]</a>. Second, biological brain modules almost universally exhibit recurrent connection pathways too. Both observations indicate that RNNs should potentially be powerful tools for engineering applications.</p>
<p>Despite this widely acknowledged potential, and despite a number of successful academic and practical applications, the impact of RNNs in nonlinear modeling has remained limited for a long time. The main reason for this lies in the fact that RNNs are difficult to train by gradient-descent-based methods, which aim at iteratively reducing the training error. While a number of training algorithms have been proposed (a brief overview is given in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.5">2.5</a>), these all suffer from the following shortcomings:</p>
<p>•</p>
<p>The gradual change of network parameters during learning drives the network dynamics through bifurcations <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b8">[8]</a>. At such points, the gradient information degenerates and may become ill-defined. As a consequence, convergence cannot be guaranteed.</p>
<p>•</p>
<p>A single parameter update can be computationally expensive, and many update cycles may be necessary. This results in long training times, and renders RNN training feasible only for relatively small networks (in the order of tens of units).</p>
<p>•</p>
<p>It is intrinsically hard to learn dependences requiring long-range memory, because the necessary gradient information exponentially dissolves over time <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b9">[9]</a> (but see the Long Short-Term Memory networks <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b10">[10]</a> for a possible escape).</p>
<p>•</p>
<p>Advanced training algorithms are mathematically involved and need to be parameterized by a number of global control parameters, which are not easily optimized. As a result, such algorithms need substantial skill and experience to be successfully applied.</p>
<p>In this situation of slow and difficult progress, in 2001 a fundamentally new approach to RNN design and training was proposed independently by Wolfgang Maass under the name of <em>Liquid State Machines</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11">[11]</a> and by Herbert Jaeger under the name of <em>Echo State Networks</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a>. This approach, which had predecessors in computational neuroscience <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b13">[13]</a> and subsequent ramifications in machine learning as the <em>Backpropagation-Decorrelation</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b14">[14]</a> learning rule, is now increasingly often collectively referred to as <em>Reservoir Computing</em> (RC). The RC paradigm avoids the shortcomings of gradient-descent RNN training listed above, by setting up RNNs in the following way:</p>
<p>•</p>
<p>A recurrent neural network is <em>randomly</em> created and remains unchanged during training. This RNN is called the <em>reservoir</em>. It is passively excited by the input signal and maintains in its state a nonlinear transformation of the input history.</p>
<p>•</p>
<p>The desired output signal is generated as a <a href="https://www.sciencedirect.com/topics/computer-science/linear-combination" title="Learn more about linear combination from ScienceDirect's AI-generated Topic Pages">linear combination</a> of the neuron’s signals from the input-excited reservoir. This linear combination is obtained by linear regression, using the teacher signal as a target.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fig1">Fig. 1</a> graphically contrasts previous methods of RNN training with the RC approach.</p>
<p><img alt="" src="https://ars.els-cdn.com/content/image/1-s2.0-S1574013709000173-gr1.jpg" /></p>
<ol>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S1574013709000173-gr1.jpg" title="Download full-size image">Download : Download full-size image</a></li>
</ol>
<p>Fig. 1. A. Traditional gradient-descent-based <a href="https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network" title="Learn more about RNN from ScienceDirect's AI-generated Topic Pages">RNN</a> training methods adapt all connection weights (bold arrows), including input-to-RNN, RNN-internal, and RNN-to-output weights. B. In Reservoir Computing, only the RNN-to-output weights are adapted.</p>
<p>Reservoir Computing methods have quickly become popular, as witnessed for instance by a theme issue of <a href="https://www.sciencedirect.com/topics/computer-science/neural-networks" title="Learn more about Neural Networks from ScienceDirect's AI-generated Topic Pages">Neural Networks</a> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b15">[15]</a>, and today constitute one of the basic paradigms of RNN modeling <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b16">[16]</a>. The main reasons for this development are the following:</p>
<p><strong>Modeling accuracy.</strong> RC has starkly outperformed previous methods of nonlinear system identification, prediction and classification, for instance in predicting chaotic dynamics (three orders of magnitude improved accuracy <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17">[17]</a>), nonlinear wireless <a href="https://www.sciencedirect.com/topics/computer-science/channel-equalization" title="Learn more about channel equalization from ScienceDirect's AI-generated Topic Pages">channel equalization</a> (two orders of magnitude improvement <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17">[17]</a>), the Japanese Vowel benchmark (zero test error rate, previous best: 1.8% <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18">[18]</a>), financial forecasting (winner of the international forecasting competition NN3<a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fn2">2</a>), and in isolated spoken digits recognition (improvement of word error rate on benchmark from 0.6% of previous best system to 0.2% <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b19">[19]</a>, and further to 0% test error in recent unpublished work).</p>
<p><strong>Modeling capacity.</strong> RC is computationally universal for continuous-time, continuous-value real-time systems modeled with bounded resources (including time and value resolution) <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b20">[20]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b21">[21]</a>.</p>
<p><strong>Biological plausibility.</strong> Numerous connections of RC principles to architectural and dynamical properties of <a href="https://www.sciencedirect.com/topics/computer-science/mammalian-brain" title="Learn more about mammalian brains from ScienceDirect's AI-generated Topic Pages">mammalian brains</a> have been established. RC (or closely related models) provides explanations of why biological brains can carry out accurate computations with an “inaccurate” and noisy physical substrate <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b22">[22]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b23">[23]</a>, especially accurate timing <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b24">[24]</a>; of the way in which visual information is superimposed and processed in <a href="https://www.sciencedirect.com/topics/computer-science/primary-visual-cortex" title="Learn more about primary visual cortex from ScienceDirect's AI-generated Topic Pages">primary visual cortex</a> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b25">[25]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b26">[26]</a>; of how cortico-basal pathways support the representation of sequential information; and RC offers a functional interpretation of the cerebellar circuitry <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b27">[27]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b28">[28]</a>. A central role is assigned to an RC circuit in a series of models explaining sequential information processing in human and primate brains, most importantly of speech signals <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b13">[13]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b29">[29]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b30">[30]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b31">[31]</a>.</p>
<p><strong>Extensibility and parsimony.</strong> A notorious conundrum of <a href="https://www.sciencedirect.com/topics/computer-science/neural-network-research" title="Learn more about neural network research from ScienceDirect's AI-generated Topic Pages">neural network research</a> is how to extend previously learned models by new items without impairing or destroying previously learned representations (<em>catastrophic interference</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b32">[32]</a>). RC offers a simple and principled solution: new items are represented by new output units, which are appended to the previously established output units of a given reservoir. Since the output weights of different output units are independent of each other, catastrophic interference is a non-issue.</p>
<p>These encouraging observations should not mask the fact that RC is still in its infancy, and significant further improvements and extensions are desirable. Specifically, just simply creating a reservoir at random is unsatisfactory. It seems obvious that, when addressing a specific modeling task, a specific reservoir design that is adapted to the task will lead to better results than a naive random creation. Thus, the main stream of research in the field is today directed at understanding the effects of reservoir characteristics on task performance, and at developing suitable reservoir design and adaptation methods. Also, new ways of reading out from the reservoirs, including combining them into larger structures, are devised and investigated. While shifting from the initial idea of having a fixed randomly created reservoir and training only the readout, the current paradigm of reservoir computing remains (and differentiates itself from other RNN training approaches) as producing/training the reservoir and the readout separately and differently.</p>
<p>This review offers a conceptual classification and a comprehensive survey of this research.</p>
<p>As is true for many areas of machine learning, methods in reservoir computing converge from different fields and come with different names. We would like to make a distinction here between these differently named “tradition lines”, which we like to call <em>brands</em>, and the actual finer-grained ideas on producing good reservoirs, which we will call <em>recipes</em>. Since recipes can be useful and mixed across different brands, this review focuses on classifying and surveying them. To be fair, it has to be said that the authors of this survey associate themselves mostly with the Echo State Networks brand, and thus, willingly or not, are influenced by its mindset.</p>
<p><strong>Overview.</strong> We start by introducing a generic notational framework in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2">2</a>. More specifically, we define what we mean by <em>problem</em> or <em>task</em> in the context of machine learning in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.1">2.1</a>. Then we define a general notation for expansion (or kernel) methods for both non-temporal (Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.2">2.2</a>) and temporal (Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.3">2.3</a>) tasks, introduce our notation for recurrent neural networks in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.4">2.4</a>, and outline classical training methods in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.5">2.5</a>. In Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3">3</a> we detail the foundations of Reservoir Computing and proceed by naming the most prominent brands. In Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec4">4</a> we introduce our classification of the reservoir generation/adaptation recipes, which transcends the boundaries between the brands. Following this classification we then review universal (Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5">5</a>), unsupervised (Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6">6</a>), and supervised (Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7">7</a>) reservoir generation/adaptation recipes. In Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8">8</a> we provide a classification and review the techniques for reading the outputs from the reservoirs reported in literature, together with discussing various practical issues of readout training. A final discussion (Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec9">9</a>) wraps up the entire picture.</p>
<h2 id="2-formalism">2. Formalism<a class="headerlink" href="#2-formalism" title="Permanent link">&para;</a></h2>
<h3 id="21-formulation-of-the-problem">2.1. Formulation of the problem<a class="headerlink" href="#21-formulation-of-the-problem" title="Permanent link">&para;</a></h3>
<p>Let a <em>problem</em> or a <em>task</em> in our context of <a href="https://www.sciencedirect.com/topics/computer-science/machine-learning" title="Learn more about machine learning from ScienceDirect's AI-generated Topic Pages">machine learning</a> be defined as a problem of learning a functional relation between a given input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;u&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> and a desired output <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span>, where <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/math&gt;\)</span>, and <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/math&gt;\)</span> is the number of data points in the training <em>dataset</em> <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;{&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. A <em>non-temporal</em> task is where the data points are independent of each other and the goal is to learn a function <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> such that <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is minimized, where <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;/math&gt;\)</span> is an error measure, for instance, the normalized root-mean-square error (NRMSE) (1)<span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msqrt is="true"&gt;&lt;mrow is="true"&gt;&lt;mfrac is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;〈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;‖&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;‖&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;〉&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;〈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;‖&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;〈&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;〉&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;‖&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;〉&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> where <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;‖&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;‖&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> stands for the <a href="https://www.sciencedirect.com/topics/computer-science/euclidean-distance" title="Learn more about Euclidean distance from ScienceDirect's AI-generated Topic Pages">Euclidean distance</a> (or norm).</p>
<p>A <em>temporal</em> task is where <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> are signals in a discrete time domain <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/math&gt;\)</span>, and the goal is to learn a function <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> such that <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is minimized. Thus the difference between the temporal and non-temporal task is that the function <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> we are trying to learn has memory in the first case and is memoryless in the second. In both cases the underlying assumption is, of course, that the functional dependence we are trying to learn actually exists in the data. For the temporal case this spells out as data adhering to an additive noise model of the form <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;θ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is the relation to be learned by <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;θ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> is a noise term, limiting the learning precision, i.e., the precision of matching the learned <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> to <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>.</p>
<p>Whenever we say that the task or the problem is learned <em>well</em>, or with good <em>accuracy</em> or <em>precision</em>, we mean that <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is small. Normally one part of the <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/math&gt;\)</span> data points is used for training the model and another part (unseen during the training) for testing it. When speaking about output errors and <em>performance</em> or <em>precision</em> we will have <em>testing</em> errors in mind (if not explicitly specified otherwise). Also <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/math&gt;\)</span>, denoting the discrete time, will often be used omitting its range <span class="arithmatex">\(&lt;math&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/math&gt;\)</span>.</p>
<h3 id="22-expansions-and-kernels-in-non-temporal-tasks">2.2. Expansions and kernels in non-temporal tasks<a class="headerlink" href="#22-expansions-and-kernels-in-non-temporal-tasks" title="Permanent link">&para;</a></h3>
<p>Many tasks cannot be accurately solved by a simple <a href="https://www.sciencedirect.com/topics/computer-science/linear-relation" title="Learn more about linear relation from ScienceDirect's AI-generated Topic Pages">linear relation</a> between the <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, i.e., a linear model <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> (where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;u&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span>) gives big errors <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> regardless of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>. In such situations one has to resort to <em>nonlinear</em> models. A number of generic and widely used approaches to nonlinear modeling are based on the idea of nonlinearly expanding the input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> into a high-dimensional feature vector <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span>, and then utilizing those features using linear methods, for instance by linear regression or computing for a linear separation <a href="https://www.sciencedirect.com/topics/computer-science/hyperplanes" title="Learn more about hyperplane from ScienceDirect's AI-generated Topic Pages">hyperplane</a>, to get a reasonable <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span>. Solutions of this kind can be expressed in the form (2)<span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> are the trained output weights. Typically <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;≫&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;u&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, and we will often consider <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> as included in <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. There is also typically a constant <em>bias</em> value added to <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2">(2)</a>, which is omitted here and in other equations for brevity. The bias can be easily implemented, having one of the features in <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> constant (e.g., <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span>) and a corresponding column in <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>. Some models extend <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2">(2)</a> to (3)<span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;[&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is some <a href="https://www.sciencedirect.com/topics/computer-science/nonlinear-function" title="Learn more about nonlinear function from ScienceDirect's AI-generated Topic Pages">nonlinear function</a> (e.g., a sigmoid applied element-wise). For the sake of simplicity we will consider this definition as equivalent to <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2">(2)</a>, since <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> can be eliminated from <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> by redefining the target as <span class="arithmatex">\(&lt;math&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> (and the error function <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, if desired). Note that <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2">(2)</a> is a special case of <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3">(3)</a>, with <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> being the identity.</p>
<p>Functions <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> that transform an input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> into a (higher-dimensional) vector <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> are often called <em>kernels</em> (and traditionally denoted <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ϕ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>) in this context. Methods using kernels often employ the <em>kernel trick</em>, which refers to the option afforded by many kernels of computing inner products in the (high-dimensional, hence expensive) feature space of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> more cheaply in the original space populated by <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span>. The term <em><a href="https://www.sciencedirect.com/topics/computer-science/kernel-function" title="Learn more about kernel function from ScienceDirect's AI-generated Topic Pages">kernel function</a></em> has acquired a close association with the kernel trick. Since here we will not exploit the kernel trick, in order to avoid confusion we will use the more neutral term of an <em>expansion</em> function for <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, and refer to methods using such functions as <em>expansion methods</em>. These methods then include <em><a href="https://www.sciencedirect.com/topics/computer-science/support-vector-machine" title="Learn more about Support Vector Machines from ScienceDirect's AI-generated Topic Pages">Support Vector Machines</a></em> (which standardly do use the kernel trick), <em><a href="https://www.sciencedirect.com/topics/computer-science/feedforward-neural-network" title="Learn more about Feedforward Neural Networks from ScienceDirect's AI-generated Topic Pages">Feedforward Neural Networks</a></em>, <em><a href="https://www.sciencedirect.com/topics/computer-science/radial-basis-function" title="Learn more about Radial Basis Function from ScienceDirect's AI-generated Topic Pages">Radial Basis Function</a></em> approximators, <em>Slow Feature Analysis</em>, and various <em>Probability Mixture</em> models, among many others. Feedforward neural networks are also often referred to as <em>(multilayer) perceptrons</em> in the literature.</p>
<p>While training the output <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is a well defined and understood problem, producing a good expansion function <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> generally involves more creativity. In many expansion methods, e.g., Support Vector Machines, the function is chosen “by hand” (most often through trial-and-error) and is fixed.</p>
<h3 id="23-expansions-in-temporal-tasks">2.3. Expansions in temporal tasks<a class="headerlink" href="#23-expansions-in-temporal-tasks" title="Permanent link">&para;</a></h3>
<p>Many temporal methods are based on the same principle. The difference is that in a temporal task the function to be learned depends also on the history of the input, as discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.1">2.1</a>. Thus, the expansion function has memory: <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, i.e., it is an expansion of the current input and its (potentially infinite) history. Since this function has an unbounded number of parameters, practical implementations often take an alternative, recursive, definition: (4)<span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mtext is="true"&gt;.&lt;/mtext&gt;&lt;/math&gt;\)</span></p>
<p>The output <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is typically produced in the same way as for non-temporal methods by <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2">(2)</a> or <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3">(3)</a>.</p>
<p>In addition to the nonlinear expansion, as in the non-temporal tasks, such <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> could be seen as a type of a spatial embedding of the temporal information of <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. This, for example, enables capturing higher-dimensional dynamical attractors <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> of the system being modeled by <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> from a series of lower-dimensional observations <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> the system is emitting, which is shown to be possible by <em>Takens’s theorem</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b33">[33]</a>.</p>
<h3 id="24-recurrent-neural-networks">2.4. Recurrent neural networks<a class="headerlink" href="#24-recurrent-neural-networks" title="Permanent link">&para;</a></h3>
<p>The type of recurrent neural networks that we will consider most of the time in this review is a straightforward implementation of <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd4">(4)</a>. The nonlinear expansion with memory here leads to a <em>state vector</em> of the form (5)<span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;mspace width="1em" class="quad" is="true"&gt;&lt;/mspace&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> where <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> is a vector of reservoir neuron activations at a time step <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/math&gt;\)</span>, <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is the neuron <a href="https://www.sciencedirect.com/topics/computer-science/activation-function" title="Learn more about activation function from ScienceDirect's AI-generated Topic Pages">activation function</a>, usually the symmetric <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, or the positive logistic (or Fermi) sigmoid, applied element-wise, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;u&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> is the input weight matrix, and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> is a weight matrix of internal network connections. The network is usually started with the initial state <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;0&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span>. Bias values are again omitted in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5">(5)</a> in the same way as in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2">(2)</a>. The readout <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> of the network is implemented as in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3">(3)</a>.</p>
<p>Some models of RNNs extend <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5">(5)</a> as (6)<span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;mspace width="1em" class="quad" is="true"&gt;&lt;/mspace&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> is an optional output feedback weight matrix.</p>
<h3 id="25-classical-training-of-rnns">2.5. Classical training of RNNs<a class="headerlink" href="#25-classical-training-of-rnns" title="Permanent link">&para;</a></h3>
<p>The classical approach to supervised training of RNNs, known as <em><a href="https://www.sciencedirect.com/topics/computer-science/gradient-descent" title="Learn more about gradient descent from ScienceDirect's AI-generated Topic Pages">gradient descent</a></em>, is by iteratively adapting all weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, and possibly <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> (which as a whole we denote <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;all&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> for brevity) according to their estimated gradients <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;∂&lt;/mi&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mo is="true"&gt;/&lt;/mo&gt;&lt;mi is="true"&gt;∂&lt;/mi&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;all&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, in order to minimize the output error <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. A classical example of such methods is Real-Time Recurrent Learning <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b34">[34]</a>, where the estimation of <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;∂&lt;/mi&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mo is="true"&gt;/&lt;/mo&gt;&lt;mi is="true"&gt;∂&lt;/mi&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;all&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is done recurrently, forward in time. Conversely, error <a href="https://www.sciencedirect.com/topics/computer-science/backpropagation" title="Learn more about backpropagation from ScienceDirect's AI-generated Topic Pages">backpropagation</a> (BP) methods for training RNNs, which are derived as extensions of the BP method for feedforward neural networks <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b35">[35]</a>, estimate <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;∂&lt;/mi&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mo is="true"&gt;/&lt;/mo&gt;&lt;mi is="true"&gt;∂&lt;/mi&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;all&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> by propagating <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> backwards through network connections and time. The BP group of methods is arguably the most prominent in classical RNN training, with the classical example in this group being Backpropagation Through Time <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b36">[36]</a>. It has a runtime complexity of <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;O&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> per weight update per time step for a single output <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span>, compared to <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;O&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;4&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> for Real-Time Recurrent Learning.</p>
<p>A systematic unifying overview of many classical gradient descent RNN training methods is presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b37">[37]</a>. The same contribution also proposes a new approach, often referred to by others as Atiya–Parlos Recurrent Learning (APRL). It estimates gradients with respect to neuron activations <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;∂&lt;/mi&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mo is="true"&gt;/&lt;/mo&gt;&lt;mi is="true"&gt;∂&lt;/mi&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> (instead of weights directly) and gradually adapts the weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;all&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> to move the activations <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> into the desired directions. The method is shown to converge faster than previous ones. See Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.4">3.4</a> for more implications of APRL and bridging the gap between the classical gradient descent and the reservoir computing methods.</p>
<p>There are also other versions of supervised RNN training, formulating the training problem differently, such as using <a href="https://www.sciencedirect.com/topics/computer-science/extended-kalman-filter" title="Learn more about Extended Kalman Filters from ScienceDirect's AI-generated Topic Pages">Extended Kalman Filters</a> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b38">[38]</a> or the Expectation-Maximization algorithm <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b39">[39]</a>, as well as dealing with special types of RNNs, such as Long Short-Term Memory <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b40">[40]</a> modular networks capable of learning long-term dependences.</p>
<p>There are many more, arguably less prominent, methods and their modifications for RNN training that are not mentioned here, as this would lead us beyond the scope of this review. The very fact of their multiplicity suggests that there is no clear winner in all aspects. Despite many advances that the methods cited above have introduced, they still have multiple common shortcomings, as pointed out in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec1">1</a>.</p>
<h2 id="3-reservoir-methods">3. Reservoir methods<a class="headerlink" href="#3-reservoir-methods" title="Permanent link">&para;</a></h2>
<p>Reservoir computing methods differ from the “traditional” designs and learning techniques listed above in that they make a conceptual and computational separation between a dynamic <em>reservoir</em> — an <a href="https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network" title="Learn more about RNN from ScienceDirect's AI-generated Topic Pages">RNN</a> as a nonlinear temporal expansion function — and a recurrence-free (usually linear) <em>readout</em> that produces the desired output from the expansion.</p>
<p>This separation is based on the understanding (common with kernel methods) that <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> serve different purposes: <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> expands the input history <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;/math&gt;\)</span> into a rich enough reservoir state space <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span>, while <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> combines the neuron signals <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> into the desired output signal <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. In the linear readout case <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2">(2)</a>, for each dimension <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> an output weight vector <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> in the same space <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> is found such that (7)<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;≈&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> while the “purpose” of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is to contain a rich enough representation to make this possible.</p>
<p>Since the expansion and the readout serve different purposes, training/generating them separately and even with different goal functions makes sense. The readout <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mstyle mathvariant="italic" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is essentially a non-temporal function, learning which is relatively simple. On the other hand, setting up the reservoir such that a “good” state expansion <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> emerges is an ill-understood challenge in many respects. The “traditional” RNN training methods do not make the conceptual separation of a reservoir vs. a readout, and train both reservoir-internal and output weights in technically the same fashion. Nonetheless, even in traditional methods the ways of defining the <a href="https://www.sciencedirect.com/topics/computer-science/error-gradient" title="Learn more about error gradients from ScienceDirect's AI-generated Topic Pages">error gradients</a> for the output <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and the internal units <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> are inevitably different, reflecting that an explicit target <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is available only for the output units. Analyses of traditional training algorithms have furthermore revealed that the learning dynamics of internal vs. output weights exhibit systematic and striking differences. This theme will be expanded in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.4">3.4</a>.</p>
<p>Currently, reservoir computing is a vivid fresh RNN research stream, which has recently gained wide attention due to the reasons pointed out in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec1">1</a>.</p>
<p>We proceed to review the most prominent “named” reservoir methods, which we call here <em>brands</em>. Each of them has its own history, a specific mindset, specific types of reservoirs, and specific insights.</p>
<h3 id="31-echo-state-networks">3.1. Echo State Networks<a class="headerlink" href="#31-echo-state-networks" title="Permanent link">&para;</a></h3>
<p><em>Echo State Networks</em> (ESNs) <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b16">[16]</a> represent one of the two pioneering reservoir computing methods. The approach is based on the observation that if a random RNN possesses certain algebraic properties, training only a linear readout from it is often sufficient to achieve excellent performance in practical applications. The untrained RNN part of an ESN is called a <em>dynamical reservoir</em>, and the resulting states <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> are termed <em>echoes</em> of its input history <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a>—this is where reservoir computing draws its name from.</p>
<p>ESNs standardly use simple sigmoid neurons, i.e., reservoir states are computed by <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5">(5)</a> or <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd6">(6)</a>, where the <a href="https://www.sciencedirect.com/topics/computer-science/nonlinear-function" title="Learn more about nonlinear function from ScienceDirect's AI-generated Topic Pages">nonlinear function</a> <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is a sigmoid, usually the <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> function. Leaky integrator neuron models represent another frequent option for ESNs, which is discussed in depth in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.5">5.5</a>. Classical recipes of producing the ESN reservoir (which is in essence <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>) are outlined in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.1">5.1</a>, together with input-independent properties of the reservoir. Input-dependent measures of the quality of the activations <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> in the reservoir are presented in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1">6.1</a>.</p>
<p>The readout from the reservoir is usually linear <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3">(3)</a>, where <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is included as part of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, which can also be spelled out in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3">(3)</a> explicitly as (8)<span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;[&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;|&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;u&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> is the learned output weight matrix, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is the output neuron <a href="https://www.sciencedirect.com/topics/computer-science/activation-function" title="Learn more about activation function from ScienceDirect's AI-generated Topic Pages">activation function</a> (usually the identity) applied component-wise, and <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;|&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;/math&gt;\)</span> stands for a vertical concatenation of vectors. The original and most popular batch training method to compute <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is linear regression, discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.1.1">8.1.1</a>, or a computationally cheap online training discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.1.2">8.1.2</a>.</p>
<p>The initial ESN publications <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b41">[41]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b42">[42]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b43">[43]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17">[17]</a> were framed in settings of <a href="https://www.sciencedirect.com/topics/computer-science/machine-learning" title="Learn more about machine learning from ScienceDirect's AI-generated Topic Pages">machine learning</a> and <a href="https://www.sciencedirect.com/topics/computer-science/nonlinear-signal" title="Learn more about nonlinear signal from ScienceDirect's AI-generated Topic Pages">nonlinear signal</a> processing applications. The original theoretical contributions of early ESN research concerned algebraic properties of the reservoir that make this approach work in the first place (the <em>echo state property</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a> discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.1">5.1</a>) and analytical results characterizing the dynamical short-term memory capacity of reservoirs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b41">[41]</a> discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1">6.1</a>.</p>
<h3 id="32-liquid-state-machines">3.2. Liquid State Machines<a class="headerlink" href="#32-liquid-state-machines" title="Permanent link">&para;</a></h3>
<p><em>Liquid State Machines</em> (LSMs) <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11">[11]</a> are the other pioneering reservoir method, developed independently from and simultaneously with ESNs. LSMs were developed from a computational neuroscience background, aiming at elucidating the principal computational properties of neural microcircuits <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11">[11]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b20">[20]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b44">[44]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b45">[45]</a>. Thus LSMs use more sophisticated and biologically realistic models of spiking integrate-and-fire neurons and dynamic synaptic connection models in the reservoir. The connectivity among the neurons often follows topological and metric constraints that are biologically motivated. In the LSM literature, the reservoir is often referred to as the <em>liquid</em>, following an intuitive metaphor of the excited states as ripples on the surface of a pool of water. Inputs to LSMs also usually consist of spike trains. In their readouts LSMs originally used multilayer feedforward <a href="https://www.sciencedirect.com/topics/computer-science/neural-networks" title="Learn more about neural networks from ScienceDirect's AI-generated Topic Pages">neural networks</a> (of either spiking or sigmoid neurons), or linear readouts similar to ESNs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11">[11]</a>. Additional mechanisms for averaging spike trains to get real-valued outputs are often employed.</p>
<p>RNNs of the LSM-type with spiking neurons and more sophisticated synaptic models are usually more difficult to implement, to correctly set up and tune, and typically more expensive to emulate on digital computers<a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fn3">3</a> than simple ESN-type “weighted sum and nonlinearity” RNNs. Thus they are less widespread for engineering applications of RNNs than the latter. However, while the ESN-type neurons only emulate mean firing rates of biological neurons, spiking neurons are able to perform more complicated information processing, due to the time coding of the information in their signals (i.e., the exact timing of each firing also matters). Also findings on various mechanisms in natural neural circuits are more easily transferable to these more biologically-realistic models (there is more on this in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2">6.2</a>).</p>
<p>The main theoretical contributions of the LSM brand to Reservoir Computing consist in analytical characterizations of the computational power of such systems <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11">[11]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b21">[21]</a> discussed in Sections <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1">6.1 “Goodness” measures of the reservoir activations</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7.4">7.4 Trained auxiliary feedbacks</a>.</p>
<h3 id="33-evolino">3.3. Evolino<a class="headerlink" href="#33-evolino" title="Permanent link">&para;</a></h3>
<p><em>Evolino</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b46">[46]</a> transfers the idea of ESNs from an RNN of simple sigmoidal units to a Long Short-Term Memory type of RNNs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b40">[40]</a> constructed from units capable of preserving memory for long periods of time. In Evolino the weights of the reservoir are trained using evolutionary methods, as is also done in some extensions of ESNs, both discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7.2">7.2</a>.</p>
<h3 id="34-backpropagation-decorrelation">3.4. Backpropagation-Decorrelation<a class="headerlink" href="#34-backpropagation-decorrelation" title="Permanent link">&para;</a></h3>
<p>The idea of separation between a reservoir and a readout function has also been arrived at from the point of view of optimizing the performance of the RNN training algorithms that use error backpropagation, as already indicated in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.5">2.5</a>. In an analysis of the weight dynamics of an RNN trained using the APRL learning algorithm <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b47">[47]</a>, it was revealed that the output weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> of the network being trained change quickly, while the hidden weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> change slowly and in the case of a single output <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> the changes are column-wise coupled. Thus in effect APRL decouples the RNN into a quickly adapting output and a slowly adapting reservoir. Inspired by these findings a new iterative/online RNN training method, called <em>BackPropagation-DeCorrelation</em> (BPDC), was introduced <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b14">[14]</a>. It approximates and significantly simplifies the APRL method, and applies it only to the output weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, turning it into an online RC method. BPDC uses the reservoir update equation defined in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd6">(6)</a>, where output feedbacks <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> are essential, with the same type of units as ESNs. BPDC learning is claimed to be insensitive to the parameters of fixed reservoir weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>. BPDC boasts fast learning times and thus is capable of tracking quickly changing signals. As a downside of this feature, the trained network quickly forgets the previously seen data and is highly biased by the recent data. Some remedies for reducing this effect are reported in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b48">[48]</a>. Most of applications of BPDC in the literature are for tasks having one-dimensional outputs <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span>; however BPDC is also successfully applied to <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;&gt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span>, as recently demonstrated in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b49">[49]</a>.</p>
<p>From a conceptual perspective we can define a range of RNN training methods that gradually bridge the gap between the classical BP and reservoir methods:</p>
<p>1.</p>
<p>Classical BP methods, such as <a href="https://www.sciencedirect.com/topics/computer-science/backpropagation" title="Learn more about Backpropagation from ScienceDirect's AI-generated Topic Pages">Backpropagation</a> Through Time (BPTT) <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b36">[36]</a>;</p>
<p>2.</p>
<p>Atiya–Parlos recurrent learning (APRL) <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b37">[37]</a>;</p>
<p>3.</p>
<p>BackPropagation-DeCorrelation (BPDC) <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b14">[14]</a>;</p>
<p>4.</p>
<p>Echo State Networks (ESNs) <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b16">[16]</a>.</p>
<p>In each method of this list the focus of training gradually moves from the entire network towards the output, and convergence of the training is faster in terms of iterations, with only a single “iteration” in case 4. At the same time the potential expressiveness of the RNN, as per the same number of units in the NN, becomes weaker. All methods in the list primarily use the same type of simple sigmoid neuron model.</p>
<h3 id="35-temporal-recurrent-networks">3.5. Temporal Recurrent Networks<a class="headerlink" href="#35-temporal-recurrent-networks" title="Permanent link">&para;</a></h3>
<p>This summary of RC brands would be incomplete without a spotlight directed at Peter F. Dominey’s decade-long research suite on cortico-striatal circuits in the human brain (e.g., <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b13">[13]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b29">[29]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b31">[31]</a>, and many more). Although this research is rooted in empirical cognitive neuroscience and functional neuroanatomy and aims at elucidating complex neural structures rather than theoretical computational principles, it is probably Dominey who first clearly spelled out the RC principle: <em>“(…) there is no learning in the recurrent connections</em> [within a <a href="https://www.sciencedirect.com/topics/computer-science/subnetwork" title="Learn more about subnetwork from ScienceDirect's AI-generated Topic Pages">subnetwork</a> corresponding to a reservoir]<em>, only between the State</em> [i.e., reservoir] <em>units and the Output units. Second, adaptation is based on a simple</em> <em><a href="https://www.sciencedirect.com/topics/computer-science/associative-learning" title="Learn more about associative learning from ScienceDirect's AI-generated Topic Pages">associative learning</a></em> <em>mechanism (…)”</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b50">[50]</a>. It is also in this article where Dominey brands the neural reservoir module as a <em>Temporal Recurrent Network</em>. The learning algorithm, to which Dominey alludes, can be seen as a version of the Least Mean Squares discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.1.2">8.1.2</a>. At other places, Dominey emphasizes the randomness of the connectivity in the reservoir: <em>“It is worth noting that the simulated recurrent prefrontal network relies on fixed randomized recurrent connections, (…)”</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b51">[51]</a>. Only in early 2008 did Dominey and “computational” RC researchers become aware of each other.</p>
<h3 id="36-other-exotic-types-of-reservoirs">3.6. Other (exotic) types of reservoirs<a class="headerlink" href="#36-other-exotic-types-of-reservoirs" title="Permanent link">&para;</a></h3>
<p>As is clear from the discussion of the different reservoir methods so far, a variety of neuron models can be used for the reservoirs. Using different activation functions inside a single reservoir might also improve the richness of the echo states, as is illustrated, for example, by inserting some neurons with wavelet-shaped activation functions into the reservoir of ESNs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b52">[52]</a>. A hardware implementation friendly version of reservoirs composed of stochastic bitstream neurons was proposed in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b53">[53]</a>.</p>
<p>In fact the reservoirs do not necessarily need to be neural networks, governed by dynamics similar to <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5">(5)</a>. Other types of high-dimensional <a href="https://www.sciencedirect.com/topics/computer-science/dynamical-system" title="Learn more about dynamical systems from ScienceDirect's AI-generated Topic Pages">dynamical systems</a> that can take an input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and have an observable state <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> (which does not necessarily fully describe the state of the system) can be used as well. In particular this makes the reservoir paradigm suitable for harnessing the computational power of unconventional hardware, such as analog electronics <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b54">[54]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b55">[55]</a>, biological neural tissue <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b26">[26]</a>, optical <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b56">[56]</a>, quantum, or physical “computers”. The last of these was demonstrated (taking the “reservoir” and “liquid” idea quite literally) by feeding the input via mechanical actuators into a reservoir full of water, recording the state of its surface optically, and successfully training a readout <a href="https://www.sciencedirect.com/topics/computer-science/multilayer-perceptron" title="Learn more about multilayer perceptron from ScienceDirect's AI-generated Topic Pages">multilayer perceptron</a> on several <a href="https://www.sciencedirect.com/topics/computer-science/classification-task" title="Learn more about classification tasks from ScienceDirect's AI-generated Topic Pages">classification tasks</a> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b57">[57]</a>. An idea of treating a computer-simulated gene regulation network of <em>Escherichia Coli</em> bacteria as the reservoir, a sequence of chemical stimuli as an input, and measures of protein levels and mRNAs as an output is explored in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b58">[58]</a>.</p>
<h3 id="37-other-overviews-of-reservoir-methods">3.7. Other overviews of reservoir methods<a class="headerlink" href="#37-other-overviews-of-reservoir-methods" title="Permanent link">&para;</a></h3>
<p>An experimental comparison of LSM, ESN, and BPDC reservoir methods with different neuron models, even beyond the standard ones used for the respective methods, and different parameter settings is presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b59">[59]</a>. A brief and broad overview of reservoir computing is presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b60">[60]</a>, with an emphasis on applications and hardware implementations of reservoir methods. The editorial in the “Neural Networks” journal special issue on ESNs and LSMs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b15">[15]</a> offers a short introduction to the topic and an overview of the articles in the issue (most of which are also surveyed here). An older and much shorter part of this overview, covering only reservoir adaptation techniques, is available as a technical report <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b61">[61]</a>.</p>
<h2 id="4-our-classification-of-reservoir-recipes">4. Our classification of reservoir recipes<a class="headerlink" href="#4-our-classification-of-reservoir-recipes" title="Permanent link">&para;</a></h2>
<p>The successes of applying RC methods to benchmarks (see the listing in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec1">1</a>) outperforming classical fully trained <a href="https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network" title="Learn more about RNNs from ScienceDirect's AI-generated Topic Pages">RNNs</a> do not imply that randomly generated reservoirs are optimal and cannot be improved. In fact, “random” is almost by definition an antonym to “optimal”. The results rather indicate the need for some novel methods of training/generating the reservoirs that are very probably not a direct extension of the way the output is trained (as in BP). Thus besides application studies (which are not surveyed here), the bulk of current RC research on reservoir methods is devoted to optimal reservoir design, or reservoir optimization algorithms.</p>
<p>It is worth mentioning at this point that the general “no free lunch” principle in supervised <a href="https://www.sciencedirect.com/topics/computer-science/machine-learning" title="Learn more about machine learning from ScienceDirect's AI-generated Topic Pages">machine learning</a> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b62">[62]</a> states that there can exist no bias of a model which would universally improve the accuracy of the model for <em>all</em> possible problems. In our context this can be translated into a claim that no single type of reservoir can be optimal for all types of problems.</p>
<p>In this review we will try to survey all currently investigated ideas that help producing “good” reservoirs. We will classify those ideas into three major groups based on their universality:</p>
<p>•</p>
<p><em>Generic</em> guidelines/methods of producing good reservoirs irrespective of the task (both the input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and the desired output <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>);</p>
<p>•</p>
<p><em>Unsupervised</em> pre-training of the reservoir with respect to the given input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, but not the target <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>;</p>
<p>•</p>
<p><em>Supervised</em> pre-training of the reservoir with respect to both the given input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and the desired output <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>.</p>
<p>These three classes of methods are discussed in the following three sections. Note that many of the methods to some extend transcend the boundaries of these three classes, but will be classified according to their main principle.</p>
<h2 id="5-generic-reservoir-recipes">5. Generic reservoir recipes<a class="headerlink" href="#5-generic-reservoir-recipes" title="Permanent link">&para;</a></h2>
<p>The most classical methods of producing reservoirs all fall into this category. All of them generate reservoirs randomly, with topology and weight characteristics depending on some preset parameters. Even though they are not optimized for a particular input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> or target <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, a good manual selection of the parameters is to some extent task-dependent, complying with the “no free lunch” principle just mentioned.</p>
<h3 id="51-classical-esn-approach">5.1. Classical ESN approach<a class="headerlink" href="#51-classical-esn-approach" title="Permanent link">&para;</a></h3>
<p>Some of the most generic guidelines of producing good reservoirs were presented in the papers that introduced ESNs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b42">[42]</a>. Motivated by an intuitive goal of producing a “rich” set of dynamics, the recipe is to generate a <strong>(i)</strong><em>big</em>, <strong>(ii)</strong><em>sparsely</em> and <strong>(iii)</strong><em>randomly</em> connected, reservoir. This means that (i) <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is sufficiently large, with order ranging from tens to thousands, (ii) the weight matrix <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is sparse, with several to 20 per cent of possible connections, and (iii) the weights of the connections are usually generated randomly from a uniform distribution symmetric around the zero value. This design rationale aims at obtaining <em>many</em>, due to (i), reservoir activation signals, which are only <em>loosely coupled</em>, due to (ii), and <em>different</em>, due to (iii).</p>
<p>The input weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and the optional output feedback weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> are usually dense (they can also be sparse like <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>) and generated randomly from a uniform distribution. The exact scaling of both matrices and an optional shift of the input (a constant value added to <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>) are the few other free parameters that one has to choose when “baking” an ESN. The rules of thumb for them are the following. The scaling of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and shifting of the input depends on how much <a href="https://www.sciencedirect.com/topics/computer-science/nonlinearities" title="Learn more about nonlinearity from ScienceDirect's AI-generated Topic Pages">nonlinearity</a> of the processing unit the task needs: if the inputs are close to 0, the <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;/math&gt;\)</span> neurons tend to operate with activations close to 0, where they are essentially linear, while inputs far from 0 tend to drive them more towards saturation where they exhibit more nonlinearity. The shift of the input may help to overcome undesired consequences of the symmetry around 0 of the <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;/math&gt;\)</span> neurons with respect to the sign of the signals. Similar effects are produced by scaling the bias inputs to the neurons (i.e., the column of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> corresponding to constant input, which often has a different scaling factor than the rest of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>). The scaling of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is in practice limited by a threshold at which the ESN starts to exhibit an unstable behavior, i.e., the output feedback loop starts to amplify (the errors of) the output and thus enters a diverging generative mode. In <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b42">[42]</a>, these and related pieces of advice are given without a formal justification.</p>
<p>An important element for ESNs to work is that the reservoir should have the <em>echo state property</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a>. This condition in essence states that the effect of a previous state <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and a previous input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> on a future state <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> should vanish gradually as time passes (i.e., <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;mo is="true"&gt;→&lt;/mo&gt;&lt;mi is="true"&gt;∞&lt;/mi&gt;&lt;/math&gt;\)</span>), and not persist or even get amplified. For most practical purposes, the echo state property is assured if the reservoir weight matrix <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is scaled so that its <a href="https://www.sciencedirect.com/topics/computer-science/spectral-radius" title="Learn more about spectral radius from ScienceDirect's AI-generated Topic Pages">spectral radius</a> <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> (i.e., the largest absolute eigenvalue) satisfies <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;&lt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a>. Or, using another term, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is <em>contractive</em>. The fact that <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;&lt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> almost always ensures the echo state property has led to an unfortunate misconception which is expressed in many RC publications, namely, that <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;&lt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> amounts to a necessary and sufficient condition for the echo state property. This is wrong. The mathematically correct connection between the spectral radius and the echo state property is that the latter is violated if <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;&gt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> <em>in reservoirs using the</em> <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;/math&gt;\)</span> <em>function as neuron nonlinearity, and for zero input</em>. Contrary to widespread misconceptions, the echo state property can be obtained even if <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;&gt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> for non-zero input (including bias inputs to neurons), and it may be lost even if <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;&lt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span>, although it is hard to construct systems where this occurs (unless <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;&gt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> for the nonlinearity <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/math&gt;\)</span>), and in practice this does not happen.</p>
<p>The optimal value of <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> should be set depending on the amount of memory and nonlinearity that the given task requires. A rule of thumb, likewise discussed in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a>, is that <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> should be close to 1 for tasks that require long memory and accordingly smaller for the tasks where a too long memory might in fact be harmful. Larger <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> also have the effect of driving signals <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> into more nonlinear regions of <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;/math&gt;\)</span> units (further from 0) similarly to <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>. Thus scalings of both <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> have a similar effect on nonlinearity of the ESN, while their difference determines the amount of memory.</p>
<p>A rather conservative rigorous <em>sufficient</em> condition of the echo state property for any kind of inputs <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> (including zero) and states <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> (with <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;/math&gt;\)</span> nonlinearity) being <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;max&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;&lt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span>, where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;max&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is the largest <a href="https://www.sciencedirect.com/topics/computer-science/singular-value" title="Learn more about singular value from ScienceDirect's AI-generated Topic Pages">singular value</a> of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, was proved in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a>. Recently, a less restrictive sufficient condition, namely, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;inf&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;D&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;mi mathvariant="script" is="true"&gt;D&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;max&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;D&lt;/mi&gt;&lt;/mstyle&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;D&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;&lt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span>, where <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;D&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> is an arbitrary matrix, minimizing the so-called <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;D&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span>-norm <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;max&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;D&lt;/mi&gt;&lt;/mstyle&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;D&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, from a set <span class="arithmatex">\(&lt;math&gt;&lt;mi mathvariant="script" is="true"&gt;D&lt;/mi&gt;&lt;mo is="true"&gt;⊂&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> of <a href="https://www.sciencedirect.com/topics/computer-science/diagonal-matrix" title="Learn more about diagonal matrices from ScienceDirect's AI-generated Topic Pages">diagonal matrices</a>, has been derived in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b63">[63]</a>. This sufficient condition approaches the necessary <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;inf&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;D&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;mi mathvariant="script" is="true"&gt;D&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;max&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;D&lt;/mi&gt;&lt;/mstyle&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;D&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;→&lt;/mo&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span>, <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;&lt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span>, e.g., when <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is a normal or a triangular (permuted) matrix. A rigorous sufficient condition for the echo state property is rarely ensured in practice, with a possible exception being critical control tasks, where provable stability under any conditions is required.</p>
<h3 id="52-different-topologies-of-the-reservoir">5.2. Different topologies of the reservoir<a class="headerlink" href="#52-different-topologies-of-the-reservoir" title="Permanent link">&para;</a></h3>
<p>There have been attempts to find topologies of the ESN reservoir different from sparsely randomly connected ones. Specifically, small-world <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b64">[64]</a>, scale-free <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b65">[65]</a>, and biologically inspired connection topologies generated by spatial growth <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b66">[66]</a> were tested for this purpose in a careful study <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b67">[67]</a>, which we point out here due to its relevance although it was obtained only as a BSc thesis. The NRMS error <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd1">(1)</a> of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> as well as the eigenvalue spread of the cross-correlation matrix of the activations <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> (necessary for a fast online learning described in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.1.2">8.1.2</a>; see Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1">6.1</a> for details) were used as the performance measures of the topologies. This work also explored an exhaustive brute-force search of topologies of tiny networks (motifs) of four units, and then combining successful motives (in terms of the eigenvalue spread) into larger networks. The investigation, unfortunately, concludes that <em>“(…) none of the investigated network topologies was able to perform significantly better than simple random networks, both in terms of eigenvalue spread as well as testing error”</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b67">[67]</a>. This, however, does not serve as a proof that similar approaches are futile. An indication of this is the substantial variation in ESN performance observed among randomly created reservoirs, which is, naturally, more pronounced in smaller reservoirs (e.g., <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b68">[68]</a>).</p>
<p>In contrast, LSMs often use a biologically plausible connectivity structure and weight settings. In the original form they model a single cortical microcolumn <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11">[11]</a>. Since the model of both the connections and the neurons themselves is quite sophisticated, it has a large number of free parameters to be set, which is done manually, guided by biologically observed parameter ranges, e.g., as found in the rat somatosensory cortex <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b69">[69]</a>. This type of model also delivers good performance for practical applications of speech recognition <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b69">[69]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b70">[70]</a> (and many similar publications by the latter authors). Since LSMs aim at accuracy of modeling natural neural structures, less biologically plausible connectivity patterns are usually not explored.</p>
<p>It has been demonstrated that much more detailed biological neural circuit models, which use anatomical and neurophysiological data-based laminar (i.e., cortical layer) connectivity structures and Hodgkin–Huxley model neurons, improve the information-processing capabilities of the models <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b23">[23]</a>. Such highly realistic (for present-day standards) models <em>“perform significantly better than control circuits (which are lacking the laminar structures but are otherwise identical with regard to their components and overall connection statistics) for a wide variety of fundamental information-processing tasks”</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b23">[23]</a>.</p>
<p>Different from this direction of research, there are also explorations of using even simpler topologies of the reservoir than the classical ESN. It has been demonstrated that the reservoir can even be an unstructured feed-forward network with time-delayed connections if the finite limited memory window that it offers is sufficient for the task at hand <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b71">[71]</a>. A degenerate case of a “reservoir” composed of linear units and a diagonalized <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and unitary inputs <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> was considered in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b72">[72]</a>. A one-dimensional lattice (ring) topology was used for a reservoir, together with an adaptation of the reservoir discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2">6.2</a>, in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b73">[73]</a>. A special kind of excitatory and inhibitory neurons connected in a one-dimensional spatial arrangement was shown to produce interesting chaotic behavior in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b74">[74]</a>.</p>
<p>A tendency that higher ranks of the <a href="https://www.sciencedirect.com/topics/computer-science/connectivity-matrix" title="Learn more about connectivity matrix from ScienceDirect's AI-generated Topic Pages">connectivity matrix</a> <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;mask&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> (where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;mask&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> if <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;≠&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/math&gt;\)</span>, and <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/math&gt;\)</span> otherwise, for <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>) correlate with lower ESN output errors was observed in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b75">[75]</a>. Connectivity patterns of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> such that <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;∞&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;≡&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;lim&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;mo is="true"&gt;→&lt;/mo&gt;&lt;mi is="true"&gt;∞&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> (<span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> standing for “<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> to the power <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span>” and approximating weights of the cumulative indirect connections by paths of length <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span> among the reservoir units) is neither fully connected, nor all-zero, are claimed to give a broader distribution of ESN prediction performances, thus including best performing reservoirs, than random sparse connectivities in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b76">[76]</a>. A <a href="https://www.sciencedirect.com/topics/computer-science/permutation-matrix" title="Learn more about permutation matrix from ScienceDirect's AI-generated Topic Pages">permutation matrix</a> with a medium number and different lengths of connected cycles, or a general <a href="https://www.sciencedirect.com/topics/computer-science/orthogonal-matrix" title="Learn more about orthogonal matrix from ScienceDirect's AI-generated Topic Pages">orthogonal matrix</a>, are suggested as candidates for such <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>s.</p>
<h3 id="53-modular-reservoirs">5.3. Modular reservoirs<a class="headerlink" href="#53-modular-reservoirs" title="Permanent link">&para;</a></h3>
<p>One of the shortcomings of conventional ESN reservoirs is that, even though they are sparse, the activations are still coupled so strongly that the ESN is poor in dealing with different time scales simultaneously, e.g., predicting several superimposed generators. This problem was successfully tackled by dividing the reservoir into decoupled sub-reservoirs and introducing inhibitory connections among all the sub-reservoirs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b77">[77]</a>. For the approach to be effective, the inhibitory connections must predict the activations of the sub-reservoirs one time step ahead. To achieve this the inhibitory connections are heuristically computed from (the rest of) <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, or the sub-reservoirs are updated in a sequence and the real activations of the already updated sub-reservoirs are used.</p>
<p>The Evolino approach introduced in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.3">3.3</a> can also be classified as belonging to this group, as the LSTM <a href="https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network" title="Learn more about RNN from ScienceDirect's AI-generated Topic Pages">RNN</a> used for its reservoir consists of specific small memory-holding modules (which could alternatively be regarded as more complicated units of the network).</p>
<p>Approaches relying on combining outputs from several separate reservoirs will be discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.8">8.8</a>.</p>
<h3 id="54-time-delayed-vs-instantaneous-connections">5.4. Time-delayed vs. instantaneous connections<a class="headerlink" href="#54-time-delayed-vs-instantaneous-connections" title="Permanent link">&para;</a></h3>
<p>Another time-related limitation of the classical ESNs pointed out in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78">[78]</a> is that no matter how many neurons are contained in the reservoir, it (like any other fully <a href="https://www.sciencedirect.com/topics/computer-science/recurrent-network" title="Learn more about recurrent network from ScienceDirect's AI-generated Topic Pages">recurrent network</a> with all connections having a time delay) has only a single layer of neurons (<a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fig2">Fig. 2</a>). This makes it intrinsically unsuitable for some types of problems. Consider a problem where the mapping from <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> to <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is a very complex, nonlinear one, and the data in neighboring time steps are almost independent (i.e., little memory is required), as e.g., the “meta-learning” task in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b79">[79]</a>.<a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fn4">4</a> Consider a single time step <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/math&gt;\)</span>: signals from the input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> propagate only through one untrained layer of weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, through the nonlinearity <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/math&gt;\)</span> influence the activations <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, and reach the output <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> through the trained weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> (<a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fig2">Fig. 2</a>). Thus ESNs are not capable of producing a very complex <em>instantaneous</em> mapping from <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> to <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> using a realistic number of neurons, which could (only) be effectively done by a multilayer FFNN (not counting some non-NN-based methods). Delaying the target <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> by <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span> time steps would in fact make the signals coming from <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> “cross” the nonlinearities <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> times before reaching <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, but would mix the information from different time steps in <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, breaking the required virtually independent mapping <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;→&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, if no special structure of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is imposed.</p>
<p><img alt="" src="https://ars.els-cdn.com/content/image/1-s2.0-S1574013709000173-gr2.jpg" /></p>
<ol>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S1574013709000173-gr2.jpg" title="Download full-size image">Download : Download full-size image</a></li>
</ol>
<p>Fig. 2. Signal flow diagram of the standard ESN.</p>
<p>As a possible remedy Layered ESNs were introduced in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78">[78]</a>, where a part (up to almost half) of the reservoir connections can be instantaneous and the rest take one time step for the signals to propagate as in normal ESNs. Randomly generated Layered ESNs, however, do not offer a consistent improvement for large classes of tasks, and pre-training methods of such reservoirs have not yet been investigated.</p>
<p>The issue of standard ESNs not having enough trained layers is also discussed and addressed in a broader context in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.8">8.8</a>.</p>
<h3 id="55-leaky-integrator-neurons-and-speed-of-dynamics">5.5. Leaky integrator neurons and speed of dynamics<a class="headerlink" href="#55-leaky-integrator-neurons-and-speed-of-dynamics" title="Permanent link">&para;</a></h3>
<p>In addition to the basic sigmoid units, leaky integrator neurons were suggested to be used in ESNs from the point of their introduction <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a>. This type of neuron performs a leaky integration of its activation from previous time steps. Today a number of versions of leaky integrator neurons are often used in ESNs, which we will call here <em>leaky integrator ESNs</em> (LI-ESNs) where the distinction is needed. The main two groups are those using leaky integration before application of the <a href="https://www.sciencedirect.com/topics/computer-science/activation-function" title="Learn more about activation function from ScienceDirect's AI-generated Topic Pages">activation function</a> <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, and after. One example of the latter (in the discretized time case) has reservoir dynamics governed by (9)<span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> where <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/math&gt;\)</span> is a compound time gap between two consecutive time steps divided by the time constant of the system and <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;/math&gt;\)</span> is the decay (or leakage) rate <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b81">[81]</a>. Another popular (and we believe, preferable) design can be seen as setting <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> and redefining <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;δ&lt;/mi&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/math&gt;\)</span> in Eq. <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd9">(9)</a> as the leaking rate <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;/math&gt;\)</span> to control the “speed” of the dynamics, (10)<span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> which in effect is an exponential moving average, has only one additional parameter and the <a href="https://www.sciencedirect.com/topics/computer-science/desirable-property" title="Learn more about desirable property from ScienceDirect's AI-generated Topic Pages">desirable property</a> that neuron activations <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> never go outside the boundaries defined by <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. Note that the simple ESN <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5">(5)</a> is a special case of LI-ESNs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd9">(9)</a> or <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd10">(10)</a> with <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span>. As a corollary, an LI-ESN with a good choice of the parameters can always perform <em>at least</em> as well as a corresponding simple ESN. With the introduction of the new parameter <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;/math&gt;\)</span> (and <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/math&gt;\)</span>), the condition for the echo state property is redefined <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a>. A natural constraint on the two new parameters is <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;[&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd9">(9)</a>, and <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;[&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd10">(10)</a> — a neuron should neither retain, nor leak, more activation than it had. The effect of these parameters on the final performance of ESNs was investigated in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18">[18]</a> and <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b82">[82]</a>. The latter contribution also considers applying the leaky integrator in different places of the model and resampling the signals as an alternative.</p>
<p>The additional parameters of the LI-ESN control the “speed” of the reservoir dynamics. Small values of <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/math&gt;\)</span> result in reservoirs that react slowly to the input. By changing these parameters it is possible to shift the effective interval of frequencies in which the reservoir is working. Along these lines, time warping invariant ESNs (TWIESNs) — an architecture that can deal with strongly time-warped signals — were outlined in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b81">[81]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18">[18]</a>. This architecture varies <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/math&gt;\)</span> on-the-fly in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd9">(9)</a>, directly depending on the speed at which the input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is changing.</p>
<p>From a signal processing point of view, the exponential moving average on the neuron activation <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd10">(10)</a> does a simple <em>low-pass</em> filtering of its activations with the <a href="https://www.sciencedirect.com/topics/computer-science/cutoff-frequency" title="Learn more about cutoff frequency from ScienceDirect's AI-generated Topic Pages">cutoff frequency</a> (11)<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mfrac is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;mi is="true"&gt;π&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> where <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;/mstyle&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/math&gt;\)</span> is the <a href="https://www.sciencedirect.com/topics/computer-science/discretization" title="Learn more about discretization from ScienceDirect's AI-generated Topic Pages">discretization</a> time step. This makes the neurons average out the frequencies above <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and enables tuning the reservoirs for particular frequencies. Elaborating further on this idea, <em>high-pass</em> neurons, that produce their activations by subtracting from the unfiltered activation <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5">(5)</a> the low-pass filtered one <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd10">(10)</a>, and <em>band-pass</em> neurons, that combine the low-pass and high-pass ones, were introduced <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b83">[83]</a>. The authors also suggested mixing neurons with different passbands inside a single ESN reservoir, and reported that a single reservoir of such kind is able to predict/generate signals having structure on different timescales.</p>
<p>Following this line of thought, <a href="https://www.sciencedirect.com/topics/computer-science/infinite-impulse-response" title="Learn more about Infinite Impulse Response from ScienceDirect's AI-generated Topic Pages">Infinite Impulse Response</a> (IIR) band-pass filters having sharper cutoff characteristics were tried on neuron activations in ESNs with success in several types of signals <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b84">[84]</a>. Since the filters often introduce an undesired phase shift to the signals, a time delay for the activation of each neuron was learned and applied before the linear readout from the reservoir. A successful application of Butterworth band-pass filters in ESNs is reported in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b85">[85]</a>.</p>
<p>Connections between neurons that have different time delays (more than one time step) can actually also be used inside the recurrent part, which enables the network to operate on different timescales simultaneously and learn longer-term dependences <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b86">[86]</a>. This idea has been tried for RNNs trained by error <a href="https://www.sciencedirect.com/topics/computer-science/backpropagation" title="Learn more about backpropagation from ScienceDirect's AI-generated Topic Pages">backpropagation</a>, but could also be useful for multi-timescale reservoirs. Long-term dependences can also be learned using the reservoirs mentioned in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.3">3.3</a>.</p>
<h2 id="6-unsupervised-reservoir-adaptation">6. Unsupervised reservoir adaptation<a class="headerlink" href="#6-unsupervised-reservoir-adaptation" title="Permanent link">&para;</a></h2>
<p>In this section we describe reservoir training/generation methods that try to optimize some measure defined on the activations <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> of the reservoir, for a given input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, but regardless of the desired output <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. In Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1">6.1</a> we survey measures that are used to estimate the quality of the reservoir, irrespective of the methods optimizing them. Then local, Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2">6.2</a>, and global, Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.3">6.3</a> unsupervised reservoir training methods are surveyed.</p>
<h3 id="61-goodness-measures-of-the-reservoir-activations">6.1. “Goodness” measures of the reservoir activations<a class="headerlink" href="#61-goodness-measures-of-the-reservoir-activations" title="Permanent link">&para;</a></h3>
<p>The classical feature that reservoirs should possess is the echo state property, defined in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.1">5.1</a>. Even though this property depends on the concrete input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, usually in practice its existence is not measured explicitly, and only the <a href="https://www.sciencedirect.com/topics/computer-science/spectral-radius" title="Learn more about spectral radius from ScienceDirect's AI-generated Topic Pages">spectral radius</a> <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is selected to be &lt;1 irrespective of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, or just tuned for the final performance. A measure of short-term <em>memory capacity</em>, evaluating how well <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> can be reconstructed by the reservoir as <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> after various delays <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span>, was introduced in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b41">[41]</a>.</p>
<p>The two necessary and sufficient conditions for LSMs to work were introduced in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11">[11]</a>. A <em>separation property</em> measures the distance between different states <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> caused by different input sequences <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span>. The measure is refined for binary ESN-type reservoirs in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b87">[87]</a> with a <a href="https://www.sciencedirect.com/topics/computer-science/generalization" title="Learn more about generalization from ScienceDirect's AI-generated Topic Pages">generalization</a> in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b88">[88]</a>. An <em>approximation property</em> measures the capability of the readout to produce a desired output <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> from <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span>, and thus is not an unsupervised measure, but is included here for completeness.</p>
<p>Methods for estimating the computational power and generalization capability of neural reservoirs were presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b89">[89]</a>. The proposed measure for computational power, or <em>kernel quality</em>, is obtained in the following way. Take <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span> different input sequences (or segments of the same signal) <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, where <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span>, and <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>. For each input <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/math&gt;\)</span> take the resulting reservoir state <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, and collect them into a matrix <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;M&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span>, where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is some fixed time after the appearance of <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> in the input. Then the rank <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;/math&gt;\)</span> of the matrix <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;M&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> is the measure. If <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span>, this means that all the presented inputs can be separated by a linear readout from the reservoir, and thus the reservoir is said to have a <em>linear separation property</em>. For estimating the generalization capability of the reservoir, the same procedure can be performed with <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/math&gt;\)</span> (<span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;mo is="true"&gt;≫&lt;/mo&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span>) inputs <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/math&gt;\)</span>, that represent the set of all possible inputs. If the resultant rank <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;/math&gt;\)</span> is substantially smaller than the size <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/math&gt;\)</span> of the training set, the reservoir generalizes well. These two measures are more targeted to tasks of time series <a href="https://www.sciencedirect.com/topics/computer-science/classification" title="Learn more about classification from ScienceDirect's AI-generated Topic Pages">classification</a>, but can also be revealing in predicting the performance of regression <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b90">[90]</a>.</p>
<p>A much-desired measure to minimize is the eigenvalue spread (EVS, the ratio of the maximal eigenvalue to the minimal eigenvalue) of the cross-correlation matrix of the activations <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. A small EVS is necessary for an online training of the ESN output by a computationally cheap and stable stochastic <a href="https://www.sciencedirect.com/topics/computer-science/gradient-descent" title="Learn more about gradient descent from ScienceDirect's AI-generated Topic Pages">gradient descent</a> algorithm outlined in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.1.2">8.1.2</a> (see, e.g., <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b91">[91]</a>, chapter 5.3, for the mathematical reasons that render this mandatory). In classical ESNs the EVS sometimes reaches 1012 or even higher <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b92">[92]</a>, which makes the use of stochastic gradient descent training unfeasible. Other commonly desirable features of the reservoir are small pairwise correlation of the reservoir activations <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, or a large entropy of the <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> distribution (e.g., <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b92">[92]</a>). The latter is a rather popular measure, as discussed later in this review. A criterion for maximizing the local information transmission of each <a href="https://www.sciencedirect.com/topics/computer-science/individual-neuron" title="Learn more about individual neuron from ScienceDirect's AI-generated Topic Pages">individual neuron</a> was investigated in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b93">[93]</a> (more in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2">6.2</a>).</p>
<p>The so-called <em>edge of chaos</em> is a region of parameters of a <a href="https://www.sciencedirect.com/topics/computer-science/dynamical-system" title="Learn more about dynamical system from ScienceDirect's AI-generated Topic Pages">dynamical system</a> at which it operates at the boundary between the chaotic and non-chaotic behavior. It is often claimed (but not undisputed; see, e.g., <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b94">[94]</a>) that at the edge of chaos many types of dynamical systems, including binary systems and reservoirs, possess high computational power <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b87">[87]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b95">[95]</a>. It is intuitively clear that the edge of chaos in reservoirs can only arise when the effect of inputs on the reservoir state does not die out quickly; thus such reservoirs can potentially have high memory capacity, which is also demonstrated in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b95">[95]</a>. However, this does not universally imply that such reservoirs are optimal <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b90">[90]</a>. The edge of chaos can be empirically detected (even for biological networks) by measuring <a href="https://www.sciencedirect.com/topics/computer-science/lyapunov-exponent" title="Learn more about Lyapunov exponents from ScienceDirect's AI-generated Topic Pages">Lyapunov exponents</a> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b95">[95]</a>, even though such measurements are not trivial (and often involve a degree of expert judgment) for high-dimensional noisy systems. For reservoirs of simple binary threshold units this can be done more simply by computing the <a href="https://www.sciencedirect.com/topics/computer-science/hamming-distance" title="Learn more about Hamming distances from ScienceDirect's AI-generated Topic Pages">Hamming distances</a> between trajectories of the states <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b87">[87]</a>. There is also an empirical observation that, while changing different parameter settings of a reservoir, the best performance in a given task correlates with a Lyapunov exponent specific to that task <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b59">[59]</a>. The optimal exponent is related to the amount of memory needed for the task, as discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.1">5.1</a>. It was observed in ESNs with no input that when <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is slightly greater than 1, the internally generated signals are periodic oscillations, whereas for larger values of <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, the signals are more irregular and even chaotic <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b96">[96]</a>. Even though stronger inputs <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> can push the dynamics of the reservoirs out of the chaotic regime and thus make them useful for computation, no reliable benefit of such a mode of operation was found in the last contribution.</p>
<p>In contrast to ESN-type reservoirs of real-valued units, simple binary threshold units exhibit a more immediate transition from damped to chaotic behavior without intermediate periodic oscillations <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b87">[87]</a>. This difference between the two types of <a href="https://www.sciencedirect.com/topics/computer-science/activation-function" title="Learn more about activation functions from ScienceDirect's AI-generated Topic Pages">activation functions</a>, including intermediate <em>quantized</em> ones, in ESN-type reservoirs was investigated more closely in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b88">[88]</a>. The investigation showed that reservoirs of binary units are more sensitive to the topology and the connection weight parameters of the network in their transition between damped and chaotic behavior, and computational performance, than the real-valued ones. This difference can be related to the similar apparent difference in sensitivity of the ESNs and LSM-type reservoirs of firing units, discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.2">5.2</a>.</p>
<h3 id="62-unsupervised-local-methods">6.2. Unsupervised local methods<a class="headerlink" href="#62-unsupervised-local-methods" title="Permanent link">&para;</a></h3>
<p>A natural strategy for improving reservoirs is to mimic biology (at a high level of abstraction) and count on <em>local</em> adaptation rules. “Local” here means that parameters pertaining to some neuron <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/math&gt;\)</span> are adapted on the basis of no other information than the activations of neurons directly connected with neuron <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/math&gt;\)</span>. In fact all local methods are almost exclusively unsupervised, since the information on the performance <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;/math&gt;\)</span> at the output is unreachable in the reservoir.</p>
<p>First attempts to decrease the eigenvalue spread in ESNs by classical Hebbian <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b97">[97]</a> (inspired by synaptic plasticity in biological brains) or Anti-Hebbian learning gave no success <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b92">[92]</a>. A modification of Anti-Hebbian learning, called <em>Anti-Oja</em> learning, is reported to improve the performance of ESNs in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b98">[98]</a>.</p>
<p>On the more biologically realistic side of the RC research with spiking neurons, local unsupervised adaptations are very natural to use. In fact, LSMs had used synaptic connections with realistic short-term dynamic adaptation, as proposed by <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b99">[99]</a>, in their reservoirs from the very beginning <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11">[11]</a>.</p>
<p>The Hebbian learning principle is usually implemented in spiking NNs as <em>spike-time-dependent plasticity</em> (STDP) of synapses. STDP is shown to improve the separation property of LSMs for real-world speech data, but not for random inputs <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span>, in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b100">[100]</a>. The authors however were uncertain whether manually optimizing the parameters of the STDP adaptation (which they did) or the ones for generating the reservoir would result in a larger performance gain for the same effort spent. STDP is shown to work well with time-coded readouts from the reservoir in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b101">[101]</a>.</p>
<p>Biological neurons are widely observed to adapt their intrinsic excitability, which often results in <a href="https://www.sciencedirect.com/topics/computer-science/exponential-distribution" title="Learn more about exponential distributions from ScienceDirect's AI-generated Topic Pages">exponential distributions</a> of firing rates, as observed in visual cortex (e.g., <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b102">[102]</a>). This homeostatic adaptation mechanism, called <em>intrinsic plasticity</em> (IP), has recently attracted a wide attention in the reservoir computing community. Mathematically, the exponential distribution maximizes the entropy of a non-negative random variable with a fixed mean; thus it enables the neurons to transmit maximal information for a fixed metabolic cost of firing. An IP learning rule for spiking model neurons aimed at this goal was first presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b103">[103]</a>.</p>
<p>For a more abstract model of the neuron, having a continuous Fermi sigmoid activation function <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mo is="true"&gt;:&lt;/mo&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;mo is="true"&gt;→&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, the IP rule was derived as a proportional control that changes the steepness and offset of the sigmoid to get an exponential-like output distribution in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b104">[104]</a>. A more elegant gradient IP learning rule for the same purpose was presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b93">[93]</a>, which is similar to the information maximization approach in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b105">[105]</a>. Applying IP with Fermi neurons in reservoir computing significantly improves the performance of BPDC-trained networks <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b106">[106]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b107">[107]</a>, and is shown to have a positive effect on offline trained ESNs, but can cause stability problems for larger reservoirs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b106">[106]</a>. An ESN reservoir with IP-adapted Fermi neurons is also shown to enable predicting several superimposed oscillators <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b108">[108]</a>.</p>
<p>An adaptation of the IP rule to <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;/math&gt;\)</span> neurons (<span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mo is="true"&gt;:&lt;/mo&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;mo is="true"&gt;→&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>) that results in a zero-mean Gaussian-like distribution of activations was first presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b73">[73]</a> and investigated more in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b55">[55]</a>. The IP-adapted ESNs were compared with classical ones, both having Fermi and <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;/math&gt;\)</span> neurons, in the latter contribution. IP was shown to (modestly) improve the performance in all cases. It was also revealed that ESNs with Fermi neurons have significantly smaller short-term memory capacity (as in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1">6.1</a>) and worse performance in a synthetic NARMA prediction task, while having a slightly better performance in a speech recognition task, compared to <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;/math&gt;\)</span> neurons. The same type of <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;/math&gt;\)</span> neurons adapted by IP aimed at <a href="https://www.sciencedirect.com/topics/computer-science/laplacian-distribution" title="Learn more about Laplacian distributions from ScienceDirect's AI-generated Topic Pages">Laplacian distributions</a> are investigated in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b109">[109]</a>. In general, IP gives more control on the working points of the reservoir <a href="https://www.sciencedirect.com/topics/computer-science/nonlinearities" title="Learn more about nonlinearity from ScienceDirect's AI-generated Topic Pages">nonlinearity</a> sigmoids. The slope (first derivative) and the curvature (second derivative) of the sigmoid at the point around which the activations are centered by the IP rule affect the effective spectral radius and the nonlinearity of the reservoir, respectively. Thus, for example, centering <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;/math&gt;\)</span> activations around points other than 0 is a good idea if no quasi-linear behavior is desired. IP has recently become employed in reservoirs as a standard practice by several research groups.</p>
<p>Overall, an information-theoretic view on adaptation of spiking neurons has a long history in computational neuroscience. Even better than maximizing just any information in the output of a neuron is maximizing <em>relevant</em> information. In other words, in its output the neuron should encode the inputs in such a way as to preserve maximal information about some (local) target signal. This is addressed in a general information-theoretical setting by the <em>Information Bottleneck</em> (IB) method <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b110">[110]</a>. A learning rule for a spiking neuron that maximizes mutual information between its inputs and its output is presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b111">[111]</a>. A more general IB learning rule, transferring the general ideas of IB method to spiking neurons, is introduced in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b112">[112]</a> and <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b113">[113]</a>. Two <em>semi-local</em> training scenarios are presented in these two contributions. In the first, a neuron optimizes the mutual information of its output with outputs of some neighboring neurons, while minimizing the mutual information with its inputs. In the second, two neurons reading from the same signals maximize their information throughput, while keeping their inputs statistically independent, in effect performing Independent Component Analysis (ICA). A simplified online version of the IB training rule with a variation capable of performing <a href="https://www.sciencedirect.com/topics/computer-science/principle-component-analysis" title="Learn more about Principle Component Analysis from ScienceDirect's AI-generated Topic Pages">Principle Component Analysis</a> (PCA) was recently introduced in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b114">[114]</a>. In addition, it assumes slow semi-local target signals, which is more biologically plausible. The approaches described in this paragraph are still waiting to be tested in the reservoir computing setting.</p>
<p>It is also of great interest to understand how different types of plasticity observed in biological brains interact when applied together and what effect this has on the quality of reservoirs. The interaction of the IP with Hebbian synaptic plasticity in a single Fermi neuron is investigated in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b104">[104]</a> and further in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b115">[115]</a>. The synergy of the two plasticities is shown to result in a better specialization of the neuron that finds heavy-tail directions in the input. An interaction of IP with a neighborhood-based Hebbian learning in a layer of such neurons was also shown to maximize information transmission, perform nonlinear ICA, and result in an emergence of orientational Gabor-like receptive fields in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b116">[116]</a>. The interaction of STDP with IP in an LSM-like reservoir of simple sparsely spiking neurons was investigated in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b117">[117]</a>. The interaction turned out to be a non-trivial one, resulting in networks more robust to perturbations of the state <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and having a better short-time memory and time series prediction performance.</p>
<p>A recent approach of combining STDP with a biologically plausible reinforcement signal is discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7.5">7.5</a>, as it is not unsupervised.</p>
<h3 id="63-unsupervised-global-methods">6.3. Unsupervised global methods<a class="headerlink" href="#63-unsupervised-global-methods" title="Permanent link">&para;</a></h3>
<p>Here we review <a href="https://www.sciencedirect.com/topics/computer-science/unsupervised-method" title="Learn more about unsupervised methods from ScienceDirect's AI-generated Topic Pages">unsupervised methods</a> that optimize reservoirs based on <em>global</em> information of the reservoir activations induced by the given input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, but irrespective of the target <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, like for example the measures discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1">6.1</a>. The intuitive goal of such methods is to produce good representations of (the history of) <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> in <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> for any (and possibly several) <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>.</p>
<p>A biologically inspired unsupervised approach with a reservoir trying to predict itself is proposed in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b118">[118]</a>. An additional output <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;z&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span>, <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;z&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;z&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> from the reservoir is trained on the target <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;z&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, where <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> are the activations of the reservoir before applying the neuron transfer function <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, i.e., <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mo is="true"&gt;tanh&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. Then, in the application phase of the trained networks, the original activations <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, which result from <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, are mixed with the self-predictions <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;z&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> obtained from <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;z&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, with a certain mixing ratio <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mi is="true"&gt;α&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;:&lt;/mo&gt;&lt;mi is="true"&gt;α&lt;/mi&gt;&lt;/math&gt;\)</span>. The coefficient <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;α&lt;/mi&gt;&lt;/math&gt;\)</span> determines how much the reservoir is relying on the external input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and how much on the internal self-prediction <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;z&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. With <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;α&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/math&gt;\)</span> we have the classical ESN and with <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;α&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> we have an “autistic” reservoir that does not react to the input. Intermediate values of <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;α&lt;/mi&gt;&lt;/math&gt;\)</span> close to 1 were shown to enable reservoirs to generate slow, highly <a href="https://www.sciencedirect.com/topics/computer-science/nonlinear-signal" title="Learn more about nonlinear signals from ScienceDirect's AI-generated Topic Pages">nonlinear signals</a> that are hard to get otherwise.</p>
<p>An algebraic unsupervised way of generating ESN reservoirs was proposed in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b119">[119]</a>. The idea is to linearize the ESN update equation <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5">(5)</a> locally around its current state <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> at every time step <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/math&gt;\)</span> to get a linear approximation of <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5">(5)</a> as <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;A&lt;/mi&gt;&lt;/mstyle&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;B&lt;/mi&gt;&lt;/mstyle&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, where <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;A&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;B&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> are time (<span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/math&gt;\)</span>)-dependent matrices corresponding to <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> respectively. The approach aims at distributing the predefined complex eigenvalues of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;A&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> uniformly within the unit circle on the <span class="arithmatex">\(&lt;math&gt;&lt;mi mathvariant="double-struck" is="true"&gt;C&lt;/mi&gt;&lt;/math&gt;\)</span> plane. The reservoir matrix <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is obtained analytically from the set of these predefined eigenvalues and a given input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. The motivation for this is, as for Kautz filters <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b120">[120]</a> in linear systems, that if the target <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is unknown, it is best to have something like an <a href="https://www.sciencedirect.com/topics/computer-science/orthogonal-basis" title="Learn more about orthogonal basis from ScienceDirect's AI-generated Topic Pages">orthogonal basis</a> in <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, from which any <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> could, on average, be constructed well. The spectral radius of the reservoir is suggested to be set by hand (according to the correlation time of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, which is an indication of a memory span needed for the task), or by adapting the bias value of the reservoir units to minimize the output error (which actually renders this method <em>supervised</em>, as in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7">7</a>). Reservoirs generated this way are shown to yield higher average entropy of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> distribution, higher short-term memory capacity (both measures mentioned in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1">6.1</a>), and a smaller output error on a number of synthetic problems, using relatively small reservoirs (<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;20&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mn is="true"&gt;30&lt;/mn&gt;&lt;/math&gt;\)</span>). However, a more extensive empirical comparison of this type of reservoir with the classical ESN one is still lacking.</p>
<h2 id="7-supervised-reservoir-pre-training">7. Supervised reservoir pre-training<a class="headerlink" href="#7-supervised-reservoir-pre-training" title="Permanent link">&para;</a></h2>
<p>In this section we discuss methods for training reservoirs to perform a specific given task, i.e., not only the concrete input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, but also the desired output <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is taken into account. Since a linear readout from a reservoir is quickly trained, the suitability of a candidate reservoir for a particular task (e.g., in terms of NRMSE <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd1">(1)</a>) is inexpensive to check. Notice that even for most methods of this class the explicit target signal <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is not technically required for training the reservoir itself, but only for evaluating it in an outer loop of the adaptation process.</p>
<h3 id="71-optimization-of-global-reservoir-parameters">7.1. Optimization of global reservoir parameters<a class="headerlink" href="#71-optimization-of-global-reservoir-parameters" title="Permanent link">&para;</a></h3>
<p>In Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.1">5.1</a> we discussed guidelines for the manual choice of global parameters for reservoirs of ESNs. This approach works well only with experience and a good intuitive grasp on nonlinear dynamics. A systematic <a href="https://www.sciencedirect.com/topics/computer-science/gradient-descent-method" title="Learn more about gradient descent method from ScienceDirect's AI-generated Topic Pages">gradient descent method</a> of optimizing the global parameters of LI-ESNs (recalled from Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.5">5.5</a>) to fit them to a given task is presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18">[18]</a>. The investigation shows that the error surfaces in the combined global parameter and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> spaces may have very high curvature and multiple local minima. Thus, gradient descent methods are not always practical.</p>
<h3 id="72-evolutionary-methods">7.2. Evolutionary methods<a class="headerlink" href="#72-evolutionary-methods" title="Permanent link">&para;</a></h3>
<p>As one can see from the previous sections of this review, optimizing reservoirs is generally challenging, and breakthrough methods remain to be found. On the other hand, checking the performance of a resulting ESN is relatively inexpensive, as said. This brings in <em>evolutionary</em> methods for the reservoir pre-training as a natural strategy.</p>
<p>Recall that the classical method generates a reservoir randomly; thus the performance of the resulting ESN varies slightly (and for small reservoirs not so slightly) from one instance to another. Then indeed, an “evolutionary” method as naive as “generate <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span> reservoirs, pick the best” will outperform the classical method (“generate a reservoir”) with probability <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;/&lt;/mo&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span>, even though the improvement might be not striking.</p>
<p>Several evolutionary approaches on optimizing reservoirs of ESNs are presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b121">[121]</a>. The first approach was to carry out an evolutionary search on the parameters for generating <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>: <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, and the connection density of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>. Then an <a href="https://www.sciencedirect.com/topics/computer-science/evolutionary-algorithm" title="Learn more about evolutionary algorithm from ScienceDirect's AI-generated Topic Pages">evolutionary algorithm</a> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b122">[122]</a> was used on individuals consisting of all the weight matrices <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> of small (<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;5&lt;/mn&gt;&lt;/math&gt;\)</span>) reservoirs. A variant with a reduced search space was also tried where the weights, but not the topology, of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> were explored, i.e., elements of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> that were zero initially always stayed zero. The empirical results of modeling the motion of an underwater robot showed superiority of the methods over other state-of-art methods, and that the topology-restricted adaptation of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is almost as effective as the full one.</p>
<p>Another approach of optimizing the reservoir <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> by a greedy evolutionary search is presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b75">[75]</a>. Here the same idea of separating the topology and weight sizes of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> to reduce the search space was independently used, but the search was, conversely, restricted to the connection topology. This approach also was demonstrated to yield on average 50% smaller (and much more stable) error in predicting the behavior of a mass–spring–damper system with small (<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;20&lt;/mn&gt;&lt;/math&gt;\)</span>) reservoirs than without the genetic optimization.</p>
<p>Yet another way of reducing the search space of the reservoir parameters is constructing a big reservoir weight matrix <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> in a fractal fashion by repeatedly applying <em>Kronecker</em> self-multiplication to an initial small matrix, called the <em>Kronecker kernel</em> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b123">[123]</a>. This contribution showed that among <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>s constructed in this way some yield ESN performance similar to the best unconstrained <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>s; thus only the good weights of the small Kronecker kernel need to be found by evolutionary search for producing a well-performing reservoir.</p>
<p>Evolino <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b46">[46]</a>, introduced in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.3">3.3</a>, is another example of adapting a reservoir (in this case an LSTM network) using a genetic search.</p>
<p>It has been recently demonstrated that by adapting only the slopes of the reservoir unit <a href="https://www.sciencedirect.com/topics/computer-science/activation-function" title="Learn more about activation functions from ScienceDirect's AI-generated Topic Pages">activation functions</a> <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> by a state-of-art evolutionary algorithm, and having <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> random and fixed, a prediction performance of an ESN can be achieved close to the best of classical ESNs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b68">[68]</a>.</p>
<p>In addition to (or instead of) adapting the reservoirs, an evolutionary search can also be applied in training the readouts, such as readouts with no explicit <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, as discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.4">8.4</a>.</p>
<h3 id="73-other-types-of-supervised-reservoir-tuning">7.3. Other types of supervised reservoir tuning<a class="headerlink" href="#73-other-types-of-supervised-reservoir-tuning" title="Permanent link">&para;</a></h3>
<p>A greedy pruning of neurons from a big reservoir has been shown in a recent initial attempt <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b124">[124]</a> to often give a (bit) better classification performance for the same final <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> than just a randomly created reservoir of the same size. The effect of neuron removal to the reservoir dynamics, however, has not been addressed yet.</p>
<h3 id="74-trained-auxiliary-feedbacks">7.4. Trained auxiliary feedbacks<a class="headerlink" href="#74-trained-auxiliary-feedbacks" title="Permanent link">&para;</a></h3>
<p>While reservoirs have a natural capability of performing complex real-time analog computations with fading memory <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11">[11]</a>, an analytical investigation has shown that they can approximate any <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span>-order differential equation (with persistent memory) if extended with <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span> trained feedbacks <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b21">[21]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b125">[125]</a>. This is equivalent to simulating any <a href="https://www.sciencedirect.com/topics/computer-science/turing-machines" title="Learn more about Turing machine from ScienceDirect's AI-generated Topic Pages">Turing machine</a>, and thus also means universal digital computing. In the presence of noise (or finite precision) the memory becomes limited in such models, but they still can simulate Turing machines with finite tapes.</p>
<p>This theory has direct implications for reservoir computing; thus different ideas on how the power of ESNs could be improved along its lines are explored in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78">[78]</a>. It is done by defining auxiliary targets, training additional outputs of ESNs on these targets, and feeding the outputs back to the reservoir. Note that this can be implemented in the usual model with feedback connections <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd6">(6)</a> by extending the original output <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> with additional dimensions that are trained before training the original (final) output. The auxiliary targets are constructed from <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and/or <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> or some additional knowledge of the modeled process. The intuition is that the feedbacks could shift the internal dynamics of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> in the directions that would make them better linearly combinable into <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. The investigation showed that for some types of tasks there are natural candidates for such auxiliary targets, which improve the performance significantly. Unfortunately, no universally applicable methods for producing auxiliary targets are known such that the targets would be both easy to learn and improve the accuracy of the final output <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. In addition, training multiple outputs with feedback connections <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> makes the whole procedure more complicated, as cyclical dependences between the trained outputs (one must take care of the order in which the outputs are trained) as well as stability issues discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.2">8.2</a> arise. Despite these obstacles, we perceive this line of research as having a big potential.</p>
<h3 id="75-reinforcement-learning">7.5. Reinforcement learning<a class="headerlink" href="#75-reinforcement-learning" title="Permanent link">&para;</a></h3>
<p>In the line of biologically inspired local unsupervised adaptation methods discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2">6.2</a>, an STDP modulated by a reinforcement signal has recently emerged as a powerful learning mechanism, capable of explaining some famous findings in neuroscience (biofeedback in monkeys), as demonstrated in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b126">[126]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b127">[127]</a> and references thereof. The learning mechanism is also well biologically motivated as it uses a local unsupervised STDP rule and a reinforcement (i.e., reward) feedback, which is present in biological brains in a form of chemical signaling, e.g., by the level of dopamine. In the RC framework this learning rule has been successfully applied for training readouts from the reservoirs so far in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b127">[127]</a>, but could in principle be applied inside the reservoir too.</p>
<p>Overall the authors of this review believe that reinforcement learning methods are natural candidates for reservoir adaptation, as they can immediately exploit the knowledge of how well the output is learned inside the reservoir without the problems of error <a href="https://www.sciencedirect.com/topics/computer-science/backpropagation" title="Learn more about backpropagation from ScienceDirect's AI-generated Topic Pages">backpropagation</a>. They can also be used in settings where no explicit target <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is available. We expect to see more applications of reinforcement learning in reservoir computing in the future.</p>
<h2 id="8-readouts-from-the-reservoirs">8. Readouts from the reservoirs<a class="headerlink" href="#8-readouts-from-the-reservoirs" title="Permanent link">&para;</a></h2>
<p>Conceptually, training a readout from a reservoir is a common supervised non-temporal task of mapping <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> to <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. This is a well investigated domain in <a href="https://www.sciencedirect.com/topics/computer-science/machine-learning" title="Learn more about machine learning from ScienceDirect's AI-generated Topic Pages">machine learning</a>, much more so than learning temporal mappings with memory. A large choice of methods is available, and in principle any of them can be applied. Thus we will only briefly go through the ones reported to be successful in the literature.</p>
<h3 id="81-single-layer-readout">8.1. Single-layer readout<a class="headerlink" href="#81-single-layer-readout" title="Permanent link">&para;</a></h3>
<p>By far the most popular readout method from the ESN reservoirs is the originally proposed <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a> simple linear readout, as in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3">(3)</a> (we will consider it as equivalent to <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd8">(8)</a>, i.e., <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> being part of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>). It is shown to be often sufficient, as reservoirs provide a rich enough pool of signals for solving many application-relevant and benchmark tasks, and is very efficient to train, since optimal solutions can be found analytically.</p>
<h4 id="811-linear-regression">8.1.1. Linear regression<a class="headerlink" href="#811-linear-regression" title="Permanent link">&para;</a></h4>
<p>In batch mode, learning of the output weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2">(2)</a> can be phrased as solving a system of <a href="https://www.sciencedirect.com/topics/computer-science/linear-equations" title="Learn more about linear equations from ScienceDirect's AI-generated Topic Pages">linear equations</a> (12)<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;Y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> with respect to <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, where <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> are all <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> produced by presenting the reservoir with <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;Y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> are all <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, both collected into respective matrices over the training period <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/math&gt;\)</span>. Usually <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> data from the beginning of the training run are discarded (they come before <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span>), since they are contaminated by initial transients.</p>
<p>Since typically the goal is minimizing a quadratic error <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;Y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> as in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd1">(1)</a> and <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;mo is="true"&gt;&gt;&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, to solve <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd12">(12)</a> one usually employs methods for finding <em>least square</em> solutions of <em>overdetermined</em> systems of linear equations (e.g., <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b128">[128]</a>), the problem also known as <em>linear regression</em>. One direct method is calculating the Moore–Penrose pseudoinverse <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span>, and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> as (13)<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;Y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mtext is="true"&gt;.&lt;/mtext&gt;&lt;/math&gt;\)</span> Direct pseudoinverse calculations exhibit high numerical stability, but are expensive memory-wise for large state-collecting matrices <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span>, thereby limiting the size of the reservoir <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and/or the number of training samples <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/math&gt;\)</span>.</p>
<p>This issue is resolved in the <em>normal equations</em> formulation of the problem<a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fn5">5</a>: (14)<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;Y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mtext is="true"&gt;.&lt;/mtext&gt;&lt;/math&gt;\)</span> A naive solution of it would be (15)<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;Y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mtext is="true"&gt;.&lt;/mtext&gt;&lt;/math&gt;\)</span> Note that in this case <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;Y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> do not depend on the length <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/math&gt;\)</span> of the training sequence, and can be calculated incrementally while the training data are passed through the reservoir. Thus, having these two matrices collected, the solution complexity of <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd15">(15)</a> does not depend on <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/math&gt;\)</span> either in time or in space. Also, intermediate values of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> can be calculated in the middle of running through the training data, e.g., for an early assessment of the performance, making this a “semi-online” training method.</p>
<p>The method <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd15">(15)</a> has lower numerical stability, compared to <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd13">(13)</a>, but the problem can be mitigated by using the pseudoinverse <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> instead of the real inverse <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> (which usually also works faster). In addition, this method enables one to introduce <em>ridge</em>, or <em>Tikhonov</em>, <a href="https://www.sciencedirect.com/topics/computer-science/regularization" title="Learn more about regularization from ScienceDirect's AI-generated Topic Pages">regularization</a> elegantly: (16)<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;Y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;I&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> where <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;I&lt;/mi&gt;&lt;/mstyle&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="double-struck" is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;×&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> is the <a href="https://www.sciencedirect.com/topics/computer-science/identity-matrix" title="Learn more about identity matrix from ScienceDirect's AI-generated Topic Pages">identity matrix</a> and <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;α&lt;/mi&gt;&lt;/math&gt;\)</span> is a regularization factor. In addition to improving the numerical stability, the regularization in effect reduces the magnitudes of entries in <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, thus mitigating sensitivity to noise and overfitting; see Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.2">8.2</a> for more details. All this makes <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd16">(16)</a> a highly recommendable choice for learning outputs from the reservoirs.</p>
<p>Another alternative for solving <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd14">(14)</a> is decomposing the matrix <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> into a product of two <a href="https://www.sciencedirect.com/topics/computer-science/triangular-matrix" title="Learn more about triangular matrices from ScienceDirect's AI-generated Topic Pages">triangular matrices</a> via Cholesky or LU decomposition, and solving <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd14">(14)</a> by two steps of substitution, avoiding (pseudo-)inverses completely. The <a href="https://www.sciencedirect.com/topics/computer-science/cholesky-decomposition" title="Learn more about Cholesky decomposition from ScienceDirect's AI-generated Topic Pages">Cholesky decomposition</a> is the more numerically stable of the two.</p>
<p><em>Weighted</em> regression can be used for training linear readouts by multiplying both <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and the corresponding <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> by different weights over time, thus emphasizing some time steps <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/math&gt;\)</span> over others. Multiplying certain recorded <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and corresponding <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> by <span class="arithmatex">\(&lt;math&gt;&lt;msqrt is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/math&gt;\)</span> has the same emphasizing effect as if they appeared in the training sequence <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span> times.</p>
<p>When the reservoir is made from spiking neurons and thus <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> becomes a collection of spike trains, smoothing by low-pass filtering may be applied to it before doing the linear regression, or it can be done directly on <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11">[11]</a>. For more on linear regression based on spike train data, see <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b129">[129]</a>.</p>
<p>Evolutionary search for training linear readouts can also be employed. State-of-art evolutionary methods are demonstrated to be able to achieve the same record levels of precision for supervised tasks as with the best applications of linear regression in ESN training <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b68">[68]</a>. Their much higher computational cost is justifiable in settings where no explicit <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is available, discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.4">8.4</a>.</p>
<h4 id="812-online-adaptive-output-weight-training">8.1.2. Online adaptive output weight training<a class="headerlink" href="#812-online-adaptive-output-weight-training" title="Permanent link">&para;</a></h4>
<p>Some applications require online model adaptation, e.g., in online adaptive <a href="https://www.sciencedirect.com/topics/computer-science/channel-equalization" title="Learn more about channel equalization from ScienceDirect's AI-generated Topic Pages">channel equalization</a> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17">[17]</a>. In such cases one typically minimizes an error that is exponentially discounted going back in time. <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> here acts as an adaptive <a href="https://www.sciencedirect.com/topics/computer-science/linear-combiner" title="Learn more about linear combiner from ScienceDirect's AI-generated Topic Pages">linear combiner</a>. The simplest way to train <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is to use stochastic <a href="https://www.sciencedirect.com/topics/computer-science/gradient-descent" title="Learn more about gradient descent from ScienceDirect's AI-generated Topic Pages">gradient descent</a>. The method is familiar as the <em>Least Mean Squares</em> (LMS) algorithm in linear signal processing <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b91">[91]</a>, and has many extensions and modifications. Its convergence performance is unfortunately severely impaired by <a href="https://www.sciencedirect.com/topics/computer-science/largest-eigenvalue" title="Learn more about large eigenvalue from ScienceDirect's AI-generated Topic Pages">large eigenvalue</a> spreads of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span>, as mentioned in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1">6.1</a>.</p>
<p>An alternative to LMS, known in linear signal processing as the <em>Recursive Least Squares</em> (RLS) algorithm, is insensitive to the detrimental effects of eigenvalue spread and boasts a much faster convergence because it is a second-order method. The downside is that RLS is computationally more expensive (order <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;O&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> per time step instead of <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;O&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> for LMS, for <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;y&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span>) and notorious for numerical stability issues. Demonstrations of RLS are presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17">[17]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b43">[43]</a>. A careful and comprehensive comparison of variants of RLS is carried out in a Master’s thesis <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b130">[130]</a>, which we mention here because it will be helpful for practitioners.</p>
<p>The BackPropagation-DeCorrelation (BPDC) algorithm discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.4">3.4</a> is another powerful method for online training of single-layer readouts with feedback connections from the reservoirs.</p>
<p>Simple forms of adaptive online learning, such as LMS, are also more biologically plausible than batch-mode training. From spiking neurons a firing time-coded (instead of a more common firing rate-coded) output for <a href="https://www.sciencedirect.com/topics/computer-science/classification" title="Learn more about classification from ScienceDirect's AI-generated Topic Pages">classification</a> can also be trained by only adapting the delays of the output connections <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b101">[101]</a>. And firing rate-coded readouts can be trained by a biologically-realistic reward-modulated STDP <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b127">[127]</a>, mentioned in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2">6.2</a>.</p>
<h4 id="813-svm-style-readout">8.1.3. SVM-style readout<a class="headerlink" href="#813-svm-style-readout" title="Permanent link">&para;</a></h4>
<p>Continuing the analogy between the temporal and non-temporal expansion methods, discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2">2</a>, the reservoir can be considered a temporal kernel, and the standard linear readout <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> from it can be trained using the same loss functions and regularizations as in Support Vector Machines (SVMs) or <a href="https://www.sciencedirect.com/topics/computer-science/support-vector-regression" title="Learn more about Support Vector Regression from ScienceDirect's AI-generated Topic Pages">Support Vector Regression</a> (SVR). Different versions of this approach are proposed and investigated in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b131">[131]</a>.</p>
<p>A standard <a href="https://www.sciencedirect.com/topics/computer-science/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages">SVM</a> (having its own kernel) can also be used as a readout from a continuous-value reservoir <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b132">[132]</a>. Similarly, special kernel types could be applied in reading out from spiking (LSM-type) reservoirs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b133">[133]</a> (and references therein).</p>
<h3 id="82-feedbacks-and-stability-issues">8.2. Feedbacks and stability issues<a class="headerlink" href="#82-feedbacks-and-stability-issues" title="Permanent link">&para;</a></h3>
<p>Stability issues (with reservoirs having the echo state property) usually only occur in <em>generative</em> setups where a model trained on (one step) signal prediction is later run in a generative mode, looping its output <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> back into the input as <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. Note that this is equivalent to a model with output feedbacks <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd6">(6)</a> and no input at all (<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;u&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/math&gt;\)</span>), which is usually trained using <em>teacher forcing</em> (i.e., feeding <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> as <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> for the feedbacks during the training run) and later is run freely to generate signals as <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> in the first case is equivalent to <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> in the second one. Models having feedbacks <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> may also suffer from instability while driven with external input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, i.e., not in a purely generative mode.</p>
<p>The reason for these instabilities is that even if the model can predict the signal quite accurately, going through the feedback loop of connections <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> (or <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>) small errors get amplified, making <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> diverge from the intended <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>.</p>
<p>One way to look at this for trained linear outputs is to consider the feedback loop connections <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> as part of the reservoir <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>. Putting <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd6">(6)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2">(2)</a> together we get (17)<span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;in&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;[&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mtext is="true"&gt;,&lt;/mtext&gt;&lt;/math&gt;\)</span> where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;ofb&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> forms the “extended reservoir” connections, which we will call <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;∗&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> for brevity (as in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78">[78]</a> Section 3.2). If the <a href="https://www.sciencedirect.com/topics/computer-science/spectral-radius" title="Learn more about spectral radius from ScienceDirect's AI-generated Topic Pages">spectral radius</a> of the extended reservoir <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;∗&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is very large we can expect unstable behavior. A more detailed analysis using Laplace transformations and a sufficient condition for stability is presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b134">[134]</a>. On the other hand, for purely generative tasks, <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;∗&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;&lt;&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> would mean that the generated signal would die out, which is not desirable in most cases. Thus producing a generator with stable dynamics is often not trivial.</p>
<p>Quite generally, models trained with clean (noise-free) data for the best one-time-step prediction diverge fast in the generative mode, as they are too “sharp” and not noise-robust. A classical remedy is adding some noise to reservoir states <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12">[12]</a> during the training. This way the generator forms a stable attractor by learning how to come to the desired next output <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> from a neighborhood of the current state <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, having seen it perturbed by noise during training. Setting the right amount of noise is a delicate balance between the sharpness (of the prediction) and the stability of the generator. Alternatively, adding noise to <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> can be seen as a form of regularization in training, as it in effect also emphasizes the diagonal of matrix <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="normal" is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span> in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd16">(16)</a>. A similar effect can be achieved using ridge regression <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd16">(16)</a> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b135">[135]</a>, or to some extent even pruning of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b136">[136]</a>. Ridge regression <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd16">(16)</a> is the least computationally expensive to do of the three, since the reservoir does not need to be rerun with the data to test different values of the regularization factor <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;α&lt;/mi&gt;&lt;/math&gt;\)</span>.</p>
<p>Using different modifications of signals for teacher forcing, like mixing <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> with noise, or in some cases using pure strong noise, during the training also has an effect on the final performance and stability, as discussed in Section 5.4 of <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78">[78]</a>.</p>
<h3 id="83-readouts-for-classificationrecognition">8.3. Readouts for classification/recognition<a class="headerlink" href="#83-readouts-for-classificationrecognition" title="Permanent link">&para;</a></h3>
<p>The time series classification or temporal pattern detection tasks that need a category indicator (as opposed to real values) as an output can be implemented in two main ways. The most common and straightforward way is having a real-valued output for each class (or a single output and a threshold for the two-class classifier), and interpreting the strengths of the outputs as votes for the corresponding classes, or even class probabilities (several options are discussed in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18">[18]</a>). Often the most probable class is taken as the decision. A simple target <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> for this approach is a constant <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/math&gt;\)</span> signal for the right class <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/math&gt;\)</span> and 0 for the others in the range of <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/math&gt;\)</span> where the indicating output is expected. More elaborate shapes of <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> can improve classification performance, depending on the task (e.g., <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b81">[81]</a>). With spiking neurons the direct classification based on time coding can be learned and done, e.g., the class is assigned depending on which output fires first <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b101">[101]</a>.</p>
<p>The main alternative to direct class indications is to use predictive classifiers, i.e., train different predictors to predict different classes and assign a class to a new example corresponding to the predictor that predicts it best. Here the quality of each predictor serves as the output strength for the corresponding class. The method is quite popular in automated speech recognition (e.g., Section 6 in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b137">[137]</a> for an overview). However, in Section 6.5 of <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b137">[137]</a> the author argues against this approach, at least in its straightforward form, pointing out some weaknesses, like the lack of specificity, and negative practical experience.</p>
<p>For both approaches a weighting scheme can be used for both training (like in weighted regression) and integrating the class votes, e.g., putting more emphasis on the end of the pattern when sufficient information has reached the <a href="https://www.sciencedirect.com/topics/computer-science/classification-machine-learning" title="Learn more about classifier from ScienceDirect's AI-generated Topic Pages">classifier</a> to make the decision.</p>
<p>An advanced version of ESN-based predictive classifier, where for each class there is a set of competitively trained predictors and <a href="https://www.sciencedirect.com/topics/computer-science/dynamic-programming" title="Learn more about dynamic programming from ScienceDirect's AI-generated Topic Pages">dynamic programming</a> is used to find the optimal sequence of them, is reported to be much more noise robust than a standard Hidden Markov Model in spoken word recognition <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b138">[138]</a>.</p>
<h3 id="84-readouts-beyond-supervised-learning">8.4. Readouts beyond supervised learning<a class="headerlink" href="#84-readouts-beyond-supervised-learning" title="Permanent link">&para;</a></h3>
<p>Even though most of the readout types from reservoirs reported in the literature are trained in a purely supervised manner, i.e., making <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> match an explicitly given <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, the reservoir computing paradigm lends itself to settings where no <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is available. A typical such setting is <em><a href="https://www.sciencedirect.com/topics/computer-science/reinforcement-learning" title="Learn more about reinforcement learning from ScienceDirect's AI-generated Topic Pages">reinforcement learning</a></em> where only a feedback on the model’s performance is available. Note that an explicit <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is not required for the reservoir adaptation methods discussed in Sections <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5">5 Generic reservoir recipes</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6">6 Unsupervised reservoir adaptation</a> of this survey by definition. Even most of the adaptation methods classified as <em>supervised</em> in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7">7</a> do not need an explicit <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, as long as one can evaluate the performance of the reservoir. Thus they can be used without modification, provided that unsupervised training and evaluation of the output is not prohibitively expensive or can be done simultaneously with reservoir adaptation. In this section we will give some pointers on training readouts using reinforcement learning.</p>
<p>A biologically inspired learning rule of Spike-Time-Dependent Plasticity (STDP) modulated by a reinforcement signal has been successfully applied for training a readout of firing neurons from the reservoirs of the same LSTM-type in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b127">[127]</a>.</p>
<p><a href="https://www.sciencedirect.com/topics/computer-science/evolutionary-algorithm" title="Learn more about Evolutionary algorithms from ScienceDirect's AI-generated Topic Pages">Evolutionary algorithms</a> are a natural candidate for training outputs in a non-supervised manner. Using a genetic search with crossover and mutation to find optimal output weights <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> of an ESN is reported in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b139">[139]</a>. Such an ESN is successfully applied for a hard reinforcement learning task of direct <a href="https://www.sciencedirect.com/topics/computer-science/adaptive-control-systems" title="Learn more about adaptive control from ScienceDirect's AI-generated Topic Pages">adaptive control</a>, replacing a classical indirect controller.</p>
<p>ESNs trained with a simple “ (1+1)” evolution strategy for an unsupervised artificial embryogeny (the so-called “flag”) problem are shown to perform very well in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b140">[140]</a>.</p>
<p>An ESN trained with a state-of-art evolutionary continuous parameter optimization method (CMA-ES) shows comparable performance in a benchmark double pole balancing problem to the best <a href="https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network" title="Learn more about RNN from ScienceDirect's AI-generated Topic Pages">RNN</a> topology-learning methods in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b68">[68]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b141">[141]</a>. For this problem the best results are obtained when the spectral radius <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is adapted together with <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>. The same contributions also validate the CMA-ES readout training method on a standard supervised prediction task, achieving the same excellent precision (MSE of the order <span class="arithmatex">\(&lt;math&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mn is="true"&gt;15&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\)</span>) as the state-of-art with linear regression. Conversely, the best results for this task were achieved with <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;ρ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> fixed and training only <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>. An even more curious finding is that almost as good results were achieved by only adapting slopes of the reservoir <a href="https://www.sciencedirect.com/topics/computer-science/activation-function" title="Learn more about activation functions from ScienceDirect's AI-generated Topic Pages">activation functions</a> <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mo is="true"&gt;⋅&lt;/mo&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and having <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> fixed, as mentioned in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7.2">7.2</a>.</p>
<h3 id="85-multilayer-readouts">8.5. Multilayer readouts<a class="headerlink" href="#85-multilayer-readouts" title="Permanent link">&para;</a></h3>
<p><a href="https://www.sciencedirect.com/topics/computer-science/multilayer-perceptron" title="Learn more about Multilayer perceptrons from ScienceDirect's AI-generated Topic Pages">Multilayer perceptrons</a> (MLPs) as readouts, trained by error backpropagation, were used from the very beginnings of LSMs <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11">[11]</a> and ESNs (unpublished). They are theoretically more powerful and expressive in their instantaneous mappings from <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> to <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> than linear readouts, and are thus suitable for particularly nonlinear outputs, e.g., in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b142">[142]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b143">[143]</a>. In both cases the MLP readouts are trained by error backpropagation. On the other hand they are significantly harder to train than an optimal single-layer linear regression, thus often giving inferior results compared to the latter in practice.</p>
<p>Some experience in training MLPs as ESN readouts, including network initialization, using stochastic, batch, and semi-batch gradients, adapting learning rates, and combining with regression-training of the last layer of the MLP, is presented in Section 5.3 of <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78">[78]</a>.</p>
<h3 id="86-readouts-with-delays">8.6. Readouts with delays<a class="headerlink" href="#86-readouts-with-delays" title="Permanent link">&para;</a></h3>
<p>While the readouts from reservoirs are usually recurrence-free, this does not mean that they may not have memory. In some approaches they do, or rather some memory is inserted between the reservoir and the readout.</p>
<p>Learning a delay for each neuron in an ESN reservoir <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> in addition to the output weight from it is investigated in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b84">[84]</a>. Cross-correlation (simple or generalized) is used to optimally align activations of each neuron in <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> with <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, and then activations with the delays <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;delayed&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> are used to find <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> in a usual way. This approach potentially enables utilizing the computational power of the reservoir more efficiently. In a time-coded output from a spiking reservoir the output connection delays can actually be the only thing that is learned <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b101">[101]</a>.</p>
<p>For time series <a href="https://www.sciencedirect.com/topics/computer-science/classification-task" title="Learn more about classification tasks from ScienceDirect's AI-generated Topic Pages">classification tasks</a> the decision can be based on a readout from a joined reservoir state <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;joined&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;[&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> that is a concatenation of the reservoir states from different moments <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mo is="true"&gt;…&lt;/mo&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> in time during the time series <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18">[18]</a>. This approach, compared to only using the last state of the given time series, moves the emphasis away from the ending of the series, depending on how the support times <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> are spread. It is also more expressive, since it has <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span> times more trainable parameters in <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;out&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> for the same size of the reservoir <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>. As a consequence, it is also more prone to overfitting. It is also possible to integrate intervals of states in some way, e.g., use <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;∗&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mfrac is="true"&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;∑&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;m&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;m&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> instead of using a single snapshot of the states <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>.</p>
<p>An approach of treating a finite history of reservoir activations <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> (similar to <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;X&lt;/mi&gt;&lt;/mstyle&gt;&lt;/math&gt;\)</span> in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd12">(12)</a>) as a two-dimensional image, and training a minimum average correlations energy filter as the readout for dynamical pattern recognition is presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b144">[144]</a>.</p>
<p>Even though in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec1">1</a> we stated that the RNNs considered in this survey are used as nonlinear filters, which transform an input time series into an output time series, ESNs can also be utilized for non-temporal (defined in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.1">2.1</a>) tasks <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;{&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;target&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> by presenting an ESN with the same input <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> for many time steps, letting the ESN converge to a fixed-point <a href="https://www.sciencedirect.com/topics/computer-science/attractor-state" title="Learn more about attractor state from ScienceDirect's AI-generated Topic Pages">attractor state</a> <span class="arithmatex">\(&lt;math&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;∞&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> (which it does if it possesses echo state property) and reading the output from the attractor state <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mi is="true"&gt;y&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;u&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;∞&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b145">[145]</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b146">[146]</a>.</p>
<h3 id="87-combining-several-readouts">8.7. Combining several readouts<a class="headerlink" href="#87-combining-several-readouts" title="Permanent link">&para;</a></h3>
<p>Segmenting of the spatially embedded trajectory of <span class="arithmatex">\(&lt;math&gt;&lt;mstyle mathvariant="bold" is="true"&gt;&lt;mi is="true"&gt;x&lt;/mi&gt;&lt;/mstyle&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> by <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span>-means clustering and assigning a separate “responsible” linear readout for each cluster is investigated in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b147">[147]</a>. This approach increases the expressiveness of the ESN by having <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span> linear readouts trained and an online switching mechanism among them. Bigger values of <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;k&lt;/mi&gt;&lt;/math&gt;\)</span> are shown to compensate for smaller sizes <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mtext is="true"&gt;x&lt;/mtext&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> of the reservoirs to get the same level of performance.</p>
<p>A benchmark-record-breaking approach of taking an average of outputs from many (1000) different instances of tiny (<span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;4&lt;/mn&gt;&lt;/math&gt;\)</span>) trained ESNs is presented in Section 5.2.2 of <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18">[18]</a>. The approach is also combined with reading from different support times as discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.6">8.6</a> of this survey. Averaging outputs over 20 instances of ESNs was also shown to refine the prediction of chaotic time series in supporting online material of <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17">[17]</a>.</p>
<p>Using dynamic programing to find sequences in multiple sets of predicting readouts for classification <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b138">[138]</a> was already mentioned at the end of Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.3">8.3</a>.</p>
<h3 id="88-hierarchies">8.8. Hierarchies<a class="headerlink" href="#88-hierarchies" title="Permanent link">&para;</a></h3>
<p>Following the analogy between the ESNs and non-temporal <a href="https://www.sciencedirect.com/topics/computer-science/kernel-method" title="Learn more about kernel methods from ScienceDirect's AI-generated Topic Pages">kernel methods</a>, ESNs would be called “type-1 shallow architectures” according to the classification proposed in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b148">[148]</a>. The reservoir adaptation techniques reviewed in our article would make ESNs “type-3 shallow architectures”, which are more expressive. However, the authors in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b148">[148]</a> argue that any type of <em>shallow</em> (i.e., non-hierarchical) architectures is incapable of learning really complex intelligent tasks. This suggests that for demandingly complex tasks the adaptation of a single reservoir might not be enough and a hierarchical architecture of ESNs might be needed, e.g., such as presented in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b149">[149]</a>. Here the outputs of a higher level in the hierarchy serve as coefficients of mixing (or voting on) outputs from a lower one. The structure can have an arbitrary number of layers. Only the outputs from the reservoirs of each layer are trained simultaneously, using stochastic gradient descent and error backpropagation through the layers. The structure is demonstrated to discover features on different timescales in an unsupervised way when being trained for predicting a synthetic time series of interchanging generators. On the downside, such hierarchies require many epochs to train, and suffer from a similar problem of vanishing gradients, as deep <a href="https://www.sciencedirect.com/topics/computer-science/feedforward-neural-network" title="Learn more about feedforward neural networks from ScienceDirect's AI-generated Topic Pages">feedforward neural networks</a> or gradient-descent methods for fully trained RNNs. They also do not scale-up yet to real-world demanding problems. Research on hierarchically structured RC models has only just begun.</p>
<h2 id="9-discussion">9. Discussion<a class="headerlink" href="#9-discussion" title="Permanent link">&para;</a></h2>
<p>The striking success of the original RC methods in outperforming fully trained <a href="https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network" title="Learn more about RNNs from ScienceDirect's AI-generated Topic Pages">RNNs</a> in many (though not all) tasks, established an important milestone, or even a turning point, in the research of RNN training. The fact that a randomly generated fixed RNN with only a linear readout trained consistently outperforms state-of-art RNN training methods had several consequences:</p>
<p>•</p>
<p>First of all it revealed that we do not really know how to train RNNs well, and something new is needed. The error <a href="https://www.sciencedirect.com/topics/computer-science/backpropagation" title="Learn more about backpropagation from ScienceDirect's AI-generated Topic Pages">backpropagation</a> methods, which had caused a breakthrough in feedforward <a href="https://www.sciencedirect.com/topics/computer-science/neural-network-training" title="Learn more about RNN training from ScienceDirect's AI-generated Topic Pages">neural network training</a> (up to a certain depth), and had also become the most popular training methods for RNNs, are hardly unleashing their full potential.</p>
<p>•</p>
<p>Neither are the classical RC methods yet exploiting the full potential of RNNs, since they use a random RNN, which is unlikely to be optimal, and a linear readout, which is quite limited by the quality of the signals it is combining. But they give a quite tough performance reference for more sophisticated methods.</p>
<p>•</p>
<p>The separation between the RNN reservoir and the readout provides a good <em>platform</em> to try out all kinds of RNN adaptation methods in the reservoir and see how much they can actually improve the performance over randomly created RNNs. This is particularly well suited for testing various biology-inspired RNN adaptation mechanisms, which are almost exclusively local and unsupervised, in how they can improve learning of a supervised task.</p>
<p>•</p>
<p>In parallel, it enables all types of powerful non-temporal methods to be applied for reading out of the reservoir.</p>
<p>This platform is the current <em>paradigm of RC</em>: using different methods for <strong>(i)</strong> producing/adapting the reservoir, and <strong>(ii)</strong> training different types of readouts. It enables looking for good (i) and (ii) methods independently, and combining the best practices from both research directions. The platform has been actively used by many researchers, ever since the first ESNs and LSMs appeared. This research in both (i) and (ii) directions, together with theoretical insights, like what characterizes a “good” reservoir, constitutes the modern field of RC.</p>
<p>In this review, together with motivating the new paradigm, we have provided a comprehensive survey of all this RC research. We introduced a natural taxonomy of the reservoir generation/adaptation techniques (i) with three big classes of methods (generic, unsupervised, and supervised), depending on their universality with respect to the input and desired output of the task. Inside each class, methods are also grouped into major directions of approaches, taking different inspirations. We also surveyed all types of readouts from the reservoirs (ii) reported in the literature, including the ones containing several layers of <a href="https://www.sciencedirect.com/topics/computer-science/nonlinearities" title="Learn more about nonlinearities from ScienceDirect's AI-generated Topic Pages">nonlinearities</a>, combining several time steps, or several reservoirs, among others. We also briefly discussed some practical issues of training the most popular types of readouts in a tutorial-like fashion.</p>
<p>The survey is transcending the boundaries among several traditional methods that fall under the umbrella of RC, generalizing the results to the whole RC field and pointing out relations, where applicable.</p>
<p>Even though this review is quite extensive, we tried to keep it concise, outlining only the basic ideas of each contribution. We did not try to include <em>every</em> contribution relating to RC in this survey, but only the ones highlighting the main research directions. Publications only reporting applications of reservoir methods, but not proposing any interesting modifications of them, were left out. Since this review is aimed at a (fast) moving target, which RC is, some (especially very new) contributions might have been missed unintentionally.</p>
<p>In general, the RC field is still very young, but very active and quickly expanding. While the original first RC methods made an impact that could be called a small revolution, current RC research is more in a phase of a (rapid) evolution. The multiple new modifications of the original idea are gradually increasing the performance of the methods. While with no striking breakthroughs lately, the progress is steady, establishing some of the extensions as common practices to build on further. There are still many promising directions to be explored, hopefully leading to breakthroughs in the near future.</p>
<p>While the tasks for which RNNs are applied nowadays often are quite complex, hardly any of them could yet be called truly <em>intelligent</em>, as compared to the human level of intelligence. The fact that RC methods perform well in many of these simple tasks by no means indicates that there is little space left for their improvement. More complex tasks and adequate solutions are still to meet each other in RC. We further provide some of our (subjective, or even speculative) outlooks on the future of RC.</p>
<p>The elegant simplicity of the classical ESNs gives many benefits in these simple applications, but it also has some intrinsic limitations (as, for example, discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.4">5.4</a>) that must be overcome in some way or other. Since the RNN model is by itself biologically inspired, looking at real brains is a natural (literally) source of inspiration on how to do that. RC models may reasonably explain some aspects of how small portions of the brain work, but if we look at the bigger picture, the brain is far from being just a big blob of randomly connected neurons. It has a complex structure that is largely predefined before even starting to learn. In addition, there are many learning mechanisms observed in the real brain, as briefly outlined in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2">6.2</a>. It is very probable that there is no single easily implementable underlying rule which can explain all learning.</p>
<p>The required complexity in the context of RC can be achieved in two basic ways: either <strong>(i)</strong> by giving the reservoir a more complex internal structure, like that discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.3">5.3</a> or <strong>(ii)</strong> externally building structures combining several reservoirs and readouts, like those discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.8">8.8</a>. Note that the two ways correspond to the above-mentioned dichotomy of the RC research and are not mutually exclusive. An “externally” (ii) built structure can also be regarded as a single complex reservoir (i) and a readout from it all can be trained.</p>
<p>An internal auto-structuring of the reservoir (i) through an (unsupervised) training would be conceptually appealing and nature-like, but not yet quite feasible at the current state of knowledge. A robust realization of such a learning algorithm would signify a breakthrough in the generation/training of artificial NNs. Most probably such an approach would combine several competing learning mechanisms and goals, and require a careful parameter selection to balance them, and thus would not be easy to successfully apply. In addition, changing the structure of the RNN during the adaptive training would lead to bifurcations in the training process, as in <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b8">[8]</a>, which makes learning very difficult.</p>
<p>Constructing external architectures or several reservoirs can be approached as more of an engineering task. The structures can be hand-crafted, based on the specifics of the application, and, in some cases, trained entirely supervised, each reservoir having a predefined function and a target signal for its readout. While such approaches are successfully being applied in practice, they are very case-specific, and not quite in the scope of the research reviewed here, since in essence they are just applications of (several instances of) the classical RC methods in bigger settings.</p>
<p>However, generic structures of multiple reservoirs (ii) that can be trained with no additional information, such as discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.8">8.8</a>, are of high interest. Despite their current state being still an “embryo”, and the difficulties pointed out earlier, the authors of this review see this direction as highly promising.</p>
<p>Biological inspiration and progress of neuroscience in understanding how real brains work are beneficial for both (i) and (ii) approaches. Well understood natural principles of local neural adaptation and development can be relatively easily transfered to artificial reservoirs (i), and reservoirs internally structured to more closely resemble cortical microcolumns in the brain have been shown to perform better <a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#b23">[23]</a>. Understanding how different brain areas interact could also help in building external structures of reservoirs (ii) better suited for nature-like tasks.</p>
<p>In addition to processing and “understanding” multiple scales of time and abstraction in the data, which hierarchical models promise to solve, other features still lacking in the current RC (and overall RNN) methods include robustness and stability of pattern generation. A possible solution to this could be a homeostasis-like self-regulation in the RNNs. Other intelligence-tending features as selective longer-term memory or active attention are also not yet well incorporated.</p>
<p>In short, RC is not the end, but an important stepping-stone in the big journey of developing RNNs, ultimately leading towards building artificial and comprehending natural intelligence.</p>
<h2 id="acknowledgments">Acknowledgments<a class="headerlink" href="#acknowledgments" title="Permanent link">&para;</a></h2>
<p>This work is partially supported by Planet Intelligent Systems GmbH, a private company with an inspiring interest in fundamental research. The authors are also thankful to Benjamin Schrauwen, Michael Thon, and an anonymous reviewer of this journal for their helpful constructive feedback.</p>
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb1">[1]</a></p>
<p>John J. Hopfield</p>
<p><strong>Hopfield network</strong></p>
<p>Scholarpedia, 2 (5) (2007), p. 1977</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb2">[2]</a></p>
<p>John J. Hopfield</p>
<p><strong>Neural networks and physical systems with emergent collective computational abilities</strong></p>
<p>Proceedings of the National Academy of Sciences of the United States of America, 79 (1982), pp. 2554-2558</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb3">[3]</a></p>
<p>Geoffrey E. Hinton</p>
<p><strong>Boltzmann machine</strong></p>
<p>Scholarpedia, 2 (5) (2007), p. 1668</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb4">[4]</a></p>
<p>David H. Ackley, Geoffrey E. Hinton, Terrence J. Sejnowski</p>
<p><strong>A learning algorithm for Boltzmann machines</strong></p>
<p>Cognitive Science, 9 (1985), pp. 147-169</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb5">[5]</a></p>
<p>Geoffrey E. Hinton, Ruslan Salakhutdinov</p>
<p><strong>Reducing the dimensionality of data with neural networks</strong></p>
<p>Science, 313 (5786) (2006), pp. 504-507</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb6">[6]</a></p>
<p>Graham W. Taylor, Geoffrey E. Hinton, Sam Roweis</p>
<p><strong>Modeling human motion using binary latent variables</strong></p>
<p>Advances in Neural Information Processing Systems 19, NIPS 2006, MIT Press, Cambridge, MA (2007), pp. 1345-1352</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb7">[7]</a></p>
<p>Ken-ichi Funahashi, Yuichi Nakamura</p>
<p><strong>Approximation of dynamical systems by continuous time recurrent neural networks</strong></p>
<p>Neural Networks, 6 (1993), pp. 801-806</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb8">[8]</a></p>
<p>Kenji Doya, Bifurcations in the learning of recurrent neural networks, in: Proceedings of IEEE International Symposium on Circuits and Systems 1992, vol. 6, 1992, pp. 2777–2780</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb9">[9]</a></p>
<p>Yoshua Bengio, Patrice Simard, Paolo Frasconi</p>
<p><strong>Learning long-term dependencies with gradient descent is difficult</strong></p>
<p>IEEE Transactions on Neural Networks, 5 (2) (1994), pp. 157-166</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb10">[10]</a></p>
<p>Felix A. Gers, Jürgen Schmidhuber, Fred A. Cummins</p>
<p><strong>Learning to forget: Continual prediction with LSTM</strong></p>
<p>Neural Computation, 12 (10) (2000), pp. 2451-2471</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb11">[11]</a></p>
<p>Wolfgang Maass, Thomas Natschläger, Henry Markram</p>
<p><strong>Real-time computing without stable states: A new framework for neural computation based on perturbations</strong></p>
<p>Neural Computation, 14 (11) (2002), pp. 2531-2560</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb12">[12]</a></p>
<p>Herbert Jaeger, The “echo state” approach to analysing and training recurrent neural networks, Technical Report GMD Report 148, German National Research Center for Information Technology, 2001</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb13">[13]</a></p>
<p>Peter F. Dominey</p>
<p><strong>Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning</strong></p>
<p>Biological Cybernetics, 73 (1995), pp. 265-274</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb14">[14]</a></p>
<p>Jochen J. Steil, Backpropagation-decorrelation: Recurrent learning with O(N) complexity, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2004, IJCNN 2004, vol. 2, 2004, pp. 843–848</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb15">[15]</a></p>
<p>Herbert Jaeger, Wolfgang Maass, José C. Príncipe</p>
<p><strong>Special issue on echo state networks and liquid state machines — Editorial</strong></p>
<p>Neural Networks, 20 (3) (2007), pp. 287-289</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb16">[16]</a></p>
<p>Herbert Jaeger</p>
<p><strong>Echo state network</strong></p>
<p>Scholarpedia, 2 (9) (2007), p. 2330</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb17">[17]</a></p>
<p>Herbert Jaeger, Harald Haas</p>
<p><strong>Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication</strong></p>
<p>Science (2004), pp. 78-80</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb18">[18]</a></p>
<p>Herbert Jaeger, Mantas Lukoševičius, Dan Popovici, Udo Siewert</p>
<p><strong>Optimization and applications of echo state networks with leaky-integrator neurons</strong></p>
<p>Neural Networks, 20 (3) (2007), pp. 335-352</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb19">[19]</a></p>
<p>David Verstraeten, Benjamin Schrauwen, Dirk Stroobandt, Reservoir-based techniques for speech recognition, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2006, IJCNN 2006, 2006 pp. 1050–1053</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb20">[20]</a></p>
<p>Wolfgang Maass, Thomas Natschläger, Henry Markram</p>
<p><strong>A model for real-time computation in generic neural microcircuits</strong></p>
<p>Advances in Neural Information Processing Systems 15, NIPS 2002, MIT Press, Cambridge, MA (2003), pp. 213-220</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb21">[21]</a></p>
<p>Wolfgang Maass, Prashant Joshi, Eduardo D. Sontag</p>
<p><strong>Principles of real-time computing with feedback applied to cortical microcircuit models</strong></p>
<p>Advances in Neural Information Processing Systems 18, MIT Press, Cambridge, MA (2006), pp. 835-842</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb22">[22]</a></p>
<p>Dean V. Buonomano, Michael M. Merzenich</p>
<p><strong>Temporal information transformed into a spatial code by a neural network with realistic properties</strong></p>
<p>Science, 267 (1995), pp. 1028-1030</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb23">[23]</a></p>
<p>Stefan Haeusler, Wolfgang Maass</p>
<p><strong>A statistical analysis of information-processing properties of lamina-specific cortical microcircuit models</strong></p>
<p>Cerebral Cortex, 17 (1) (2007), pp. 149-162</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb24">[24]</a></p>
<p>Uma R. Karmarkar, Dean V. Buonomano</p>
<p><strong>Timing in the absence of clocks: Encoding time in neural network states</strong></p>
<p>Neuron, 53 (3) (2007), pp. 427-438</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb25">[25]</a></p>
<p>Garrett B. Stanley, Fei F. Li, Yang Dan</p>
<p><strong>Reconstruction of natural scenes from ensemble responses in the lateral genicualate nucleus</strong></p>
<p>Journal of Neuroscience, 19 (18) (1999), pp. 8036-8042</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb26">[26]</a></p>
<p>Danko Nikolić, Stefan Haeusler, Wolf Singer, Wolfgang Maass</p>
<p><strong>Temporal dynamics of information content carried by neurons in the primary visual cortex</strong></p>
<p>Advances in Neural Information Processing Systems 19, NIPS 2006, MIT Press, Cambridge, MA (2007), pp. 1041-1048</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb27">[27]</a></p>
<p>Werner M. Kistler, Chris I. De Zeeuw</p>
<p><strong>Dynamical working memory and timed responses: The role of reverberating loops in the olivo-cerebellar system</strong></p>
<p>Neural Computation, 14 (2002), pp. 2597-2626</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb28">[28]</a></p>
<p>Tadashi Yamazaki, Shigeru Tanaka</p>
<p><strong>The cerebellum as a liquid state machine</strong></p>
<p>Neural Networks, 20 (3) (2007), pp. 290-297</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb29">[29]</a></p>
<p>Peter F. Dominey, Michel Hoen, Jean-Marc Blanc, Taïssia Lelekov-Boissard</p>
<p><strong>Neurological basis of language and sequential cognition: Evidence from simulation, aphasia, and ERP studies</strong></p>
<p>Brain and Language, 86 (2003), pp. 207-225</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb30">[30]</a></p>
<p>Jean-Marc Blanc, Peter F. Dominey</p>
<p><strong>Identification of prosodic attitudes by atemporal recurrent network</strong></p>
<p>Cognitive Brain Research, 17 (2003), pp. 693-699</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb31">[31]</a></p>
<p>Peter F. Dominey, Michel Hoen, Toshio Inui</p>
<p><strong>A neurolinguistic model of grammatical construction processing</strong></p>
<p>Journal of Cognitive Neuroscience, 18 (12) (2006), pp. 2088-2107</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb32">[32]</a></p>
<p>Robert M. French</p>
<p><strong>Catastrophic interference in connectionist networks</strong></p>
<p>L. Nadel (Ed.), Encyclopedia of Cognitive Science, Volume 1, Nature Publishing Group (2003), pp. 431-435</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb33">[33]</a></p>
<p>Floris Takens</p>
<p><strong>Detecting strange attractors in turbulence</strong></p>
<p>Proceedings of a Symposium on Dynamical Systems and Turbulence, LNM, vol. 898, Springer (1981), pp. 366-381</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb34">[34]</a></p>
<p>Ronald J. Williams, David Zipser</p>
<p><strong>A learning algorithm for continually running fully recurrent neural networks</strong></p>
<p>Neural Computation, 1 (1989), pp. 270-280</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb35">[35]</a></p>
<p>David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams</p>
<p><strong>Learning internal representations by error propagation</strong></p>
<p>Neurocomputing: Foundations of research, MIT Press, Cambridge, MA, USA (1988), pp. 673-695</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb36">[36]</a></p>
<p>Paul J. Werbos</p>
<p><strong>Backpropagation through time: What it does and how to do it</strong></p>
<p>Proceedings of the IEEE, 78 (10) (1990), pp. 1550-1560</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb37">[37]</a></p>
<p>Amir F. Atiya, Alexander G. Parlos</p>
<p><strong>New results on recurrent network training: Unifying the algorithms and accelerating convergence</strong></p>
<p>IEEE Transactions on Neural Networks, 11 (3) (2000), pp. 697-709</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb38">[38]</a></p>
<p>Gintaras V. Puškorius, Lee A. Feldkamp</p>
<p><strong>Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks</strong></p>
<p>IEEE Transactions on Neural Networks, 5 (2) (1994), pp. 279-297</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb39">[39]</a></p>
<p>Sheng Ma, Chuanyi Ji</p>
<p><strong>Fast training of recurrent networks based on the EM algorithm</strong></p>
<p>IEEE Transactions on Neural Networks, 9 (1) (1998), pp. 11-26</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb40">[40]</a></p>
<p>Sepp Hochreiter, Jürgen Schmidhuber</p>
<p><strong>Long short-term memory</strong></p>
<p>Neural Computation, 9 (8) (1997), pp. 1735-1780</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb41">[41]</a></p>
<p>Herbert Jaeger, Short term memory in echo state networks, Technical Report GMD Report 152, German National Research Center for Information Technology, 2002</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb42">[42]</a></p>
<p>Herbert Jaeger, Tutorial on training recurrent neural networks, covering BPTT, RTRL, EKF and the “echo state network” approach, Technical Report GMD Report 159, German National Research Center for Information Technology, 2002</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb43">[43]</a></p>
<p>Herbert Jaeger</p>
<p><strong>Adaptive nonlinear system identification with echo state networks</strong></p>
<p>Advances in Neural Information Processing Systems 15, MIT Press, Cambridge, MA (2003), pp. 593-600</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb44">[44]</a></p>
<p>Thomas Natschläger, Henry Markram, Wolfgang Maass</p>
<p><strong>Computer models and analysis tools for neural microcircuits</strong></p>
<p>R. Kötter (Ed.), A Practical Guide to Neuroscience Databases and Associated Tools, Kluver Academic Publishers, Boston (2002)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb45">[45]</a></p>
<p>Wolfgang Maass, Thomas Natschläger, Henry Markram</p>
<p><strong>Computational models for generic cortical microcircuits</strong></p>
<p>J. Feng (Ed.), Computational Neuroscience: A Comprehensive Approach, CRC-Press (2002)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb46">[46]</a></p>
<p>Jürgen Schmidhuber, Daan Wierstra, Matteo Gagliolo, Faustino J. Gomez</p>
<p><strong>Training recurrent networks by Evolino</strong></p>
<p>Neural Computation, 19 (3) (2007), pp. 757-779</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb47">[47]</a></p>
<p>Ulf D. Schiller, Jochen J. Steil</p>
<p><strong>Analyzing the weight dynamics of recurrent learning algorithms</strong></p>
<p>Neurocomputing, 63C (2005), pp. 5-23</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb48">[48]</a></p>
<p>Jochen J. Steil</p>
<p><strong>Memory in backpropagation-decorrelation O(N) efficient online recurrent learning</strong></p>
<p>Proceedings of the 15th International Conference on Artificial Neural Networks, LNCS, vol. 3697, Springer (2005), pp. 649-654</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb49">[49]</a></p>
<p>Felix R. Reinhart, Jochen J. Steil, Recurrent neural autoassociative learning of forward and inverse kinematics for movement generation of the redundant PA-10 robot, in: Proceedings of the ECSIS Symposium on Learning and Adaptive Behaviors for Robotic Systems, LAB-RS, vol. 1, 2008, 35–40</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb50">[50]</a></p>
<p>Peter F. Dominey, Franck Ramus</p>
<p><strong>Neural network processing of natural language: I. Sensitivity to serial, temporal and abstract structure of language in the infant</strong></p>
<p>Language and Cognitive Processes, 15 (1) (2000), pp. 87-127</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb51">[51]</a></p>
<p>Peter F. Dominey</p>
<p><strong>From sensorimotor sequence to grammatical construction: Evidence from simulation and neurophysiology</strong></p>
<p>Adaptive Behaviour, 13 (4) (2005), pp. 347-361</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb52">[52]</a></p>
<p>Se Wang, Xiao-Jian Yang, Cheng-Jian Wei, Harnessing non-linearity by sigmoid-wavelet hybrid echo state networks (SWHESN), in: The 6th World Congress on Intelligent Control and Automation, WCICA 2006, 1, 2006, pp. 3014–3018</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb53">[53]</a></p>
<p>David Verstraeten, Benjamin Schrauwen, Dirk Stroobandt, Reservoir computing with stochastic bitstream neurons, in: Proceedings of the 16th Annual ProRISC Workshop, Veldhoven, The Netherlands, November 2005, pp. 454–459</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb54">[54]</a></p>
<p>Felix Schürmann, Karlheinz Meier, Johannes Schemmel</p>
<p><strong>Edge of chaos computation in mixed-mode VLSI - A hard liquid</strong></p>
<p>Advances in Neural Information Processing Systems 17, MIT Press, Cambridge, MA (2005), pp. 1201-1208</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb55">[55]</a></p>
<p>Benjamin Schrauwen, Marion Wardermann, David Verstraeten, Jochen J. Steil, Dirk Stroobandt</p>
<p><strong>Improving reservoirs using intrinsic plasticity</strong></p>
<p>Neurocomputing, 71 (2008), pp. 1159-1171</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb56">[56]</a></p>
<p>Kristof Vandoorne, Wouter Dierckx, Benjamin Schrauwen, David Verstraeten, Roel Baets, Peter Bienstman, Jan~Van Campenhout</p>
<p><strong>Toward optical signal processing using photonic reservoir computing</strong></p>
<p>Optics Express, 16 (15) (2008), pp. 11182-11192</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb57">[57]</a></p>
<p>Chrisantha Fernando, Sampsa Sojakka</p>
<p><strong>Pattern recognition in a bucket</strong></p>
<p>Proceedings of the 7th European Conference on Advances in Artificial Life, LNCS, vol. 2801, ECAL 2003, Springer (2003), pp. 588-597</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb58">[58]</a></p>
<p>Ben Jones, Dov Stekelo, Jon Rowe, Chrisantha Fernando, Is there a liquid state machine in the bacterium Escherichia coli?, in: Proceedings of the 1st IEEE Symposium on Artificial Life, ALIFE 2007, 1–5 April 2007, pp. 187–191</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb59">[59]</a></p>
<p>David Verstraeten, Benjamin Schrauwen, Michiel D’Haene, Dirk Stroobandt</p>
<p><strong>An experimental unification of reservoir computing methods</strong></p>
<p>Neural Networks, 20 (3) (2007), pp. 391-403</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb60">[60]</a></p>
<p>Benjamin Schrauwen, David Verstraeten, Jan Van Campenhout, An overview of reservoir computing: Theory, applications and implementations, in: Proceedings of the 15th European Symposium on Artificial Neural Networks, ESANN 2007, 2007, pp. 471–482</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb61">[61]</a></p>
<p>Mantas Lukoševičius, Herbert Jaeger, Overview of reservoir recipes, Technical Report No. 11, Jacobs University Bremen, 2007</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb62">[62]</a></p>
<p>David H. Wolpert, The supervised learning no-free-lunch theorems, in: Proceedings of the 6th Online World Conference on Soft Computing in Industrial Applications, WSC 2006, 2001, pp. 25–42</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb63">[63]</a></p>
<p>Michael Buehner, Peter Young</p>
<p><strong>A tighter bound for the echo state property</strong></p>
<p>IEEE Transactions on Neural Networks, 17 (3) (2006), pp. 820-824</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb64">[64]</a></p>
<p>Duncan J. Watts, Steven~H. Strogatz</p>
<p><strong>Collective dynamics of ‘small-world’ networks</strong></p>
<p>Nature, 393 (1998), pp. 440-442</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb65">[65]</a></p>
<p>Albert-Laszlo Barabasi, Reka Albert</p>
<p><strong>Emergence of scaling in random networks</strong></p>
<p>Science, 286 (1999), p. 509</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb66">[66]</a></p>
<p>Marcus Kaiser, Claus~C. Hilgetag</p>
<p><strong>Spatial growth of real-world networks</strong></p>
<p>Physical Review E, 69 (2004), p. 036103</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb67">[67]</a></p>
<p>Benjamin Liebald, Exploration of effects of different network topologies on the ESN signal crosscorrelation matrix spectrum, Bachelor’s Thesis, Jacobs University Bremen, 2004 <a href="http://www.eecs.jacobs-university.de/archive/bsc-2004/liebald.pdf">http://www.eecs.jacobs-university.de/archive/bsc-2004/liebald.pdf</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb68">[68]</a></p>
<p>Fei Jiang, Hugues Berry, Marc Schoenauer</p>
<p><strong>Supervised and evolutionary learning of echo state networks.</strong></p>
<p>Proceedings of 10th International Conference on Parallel Problem Solving from Nature, LNCS, vol. 5199, PPSN 2008, Springer (2008), pp. 215-224</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb69">[69]</a></p>
<p>Wolfgang Maass, Thomas Natschläger, Henry Markram</p>
<p><strong>Computational models for generic cortical microcircuits</strong></p>
<p>Computational Neuroscience: A Comprehensive Approach, Chapman &amp; Hall/CRC (2004), pp. 575-605</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb70">[70]</a></p>
<p>David Verstraeten, Benjamin Schrauwen, Dirk Stroobandt, Jan~Van Campenhout</p>
<p><strong>Isolated word recognition with the liquid state machine: A case study</strong></p>
<p>Information Processing Letters, 95 (6) (2005), pp. 521-528</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb71">[71]</a></p>
<p>Michal Čerňanský, Matej Makula, Feed-forward echo state networks, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2005, IJCNN 2005, vol. 3, 2005, pp. 1479–1482</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb72">[72]</a></p>
<p>Georg Fette, Julian Eggert</p>
<p><strong>Short term memory and pattern matching with simple echo state network</strong></p>
<p>Proceedings of the 15th International Conference on Artificial Neural Networks, LNCS, vol. 3696, ICANN 2005, Springer (2005), pp. 13-18</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb73">[73]</a></p>
<p>David Verstraeten, Benjamin Schrauwen, Dirk Stroobandt, Adapting reservoirs to get Gaussian distributions, in: Proceedings of the 15th European Symposium on Artificial Neural Networks, ESANN 2007, 2007, pp. 495–500</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb74">[74]</a></p>
<p>Carlos Lourenço, Dynamical reservoir properties as network effects, in: Proceedings of the 14th European Symposium on Artificial Neural Networks, ESANN 2006, 2006, pp. 503–508</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb75">[75]</a></p>
<p>Keith Bush, Batsukh Tsendjav, Improving the richness of echo state features using next ascent local search, in: Proceedings of the Artificial Neural Networks In Engineering Conference, St. Louis, MO, 2005, pp. 227–232</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb76">[76]</a></p>
<p>Márton~Albert Hajnal, András Lőrincz</p>
<p><strong>Critical echo state networks</strong></p>
<p>Proceedings of the 16th International Conference on Artificial Neural Networks, LNCS, vol. 4131, Springer (2006), pp. 658-667</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb77">[77]</a></p>
<p>Yanbo Xue, Le Yang, Simon Haykin</p>
<p><strong>Decoupled echo state networks with lateral inhibition</strong></p>
<p>Neural Networks, 20 (3) (2007), pp. 365-376</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb78">[78]</a></p>
<p>Mantas Lukoševičius, Echo state networks with trained feedbacks, Technical Report No. 4, Jacobs University Bremen, 2007</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb79">[79]</a></p>
<p>Danil V. Prokhorov, Lee A. Feldkamp, Ivan Yu. Tyukin, Adaptive behavior with fixed weights in RNN: An overview, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2002, IJCNN 2002, 2002, pp. 2018–2023</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb80">[80]</a></p>
<p>Mohamed Oubbati, Paul Levi, Michael Schanz, Meta-learning for adaptive identification of non-linear dynamical systems, in: Proceedings of the IEEE International Joint Symposium on Intelligent Control, June 2005, pp. 473–478</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb81">[81]</a></p>
<p>Mantas Lukoševičius, Dan Popovici, Herbert Jaeger, Udo Siewert, Time warping invariant echo state networks, Technical Report No. 2, Jacobs University Bremen, 2006</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb82">[82]</a></p>
<p>Benjamin Schrauwen, Jeroen Defour, David Verstraeten, Jan M. Van Campenhout</p>
<p><strong>The introduction of time-scales in reservoir computing, applied to isolated digits recognition</strong></p>
<p>Proceedings of the 17th International Conference on Artificial Neural Networks, LNCS, vol. 4668, Springer (2007), pp. 471-479</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb83">[83]</a></p>
<p>Udo Siewert, Welf Wustlich, Echo-state networks with band-pass neurons: Towards generic time-scale-independent reservoir structures, Internal Status Report, PLANET intelligent systems GmbH, 2007. Available online at <a href="http://snn.elis.ugent.be/">http://snn.elis.ugent.be/</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb84">[84]</a></p>
<p>Georg Holzmann, Echo state networks with filter neurons and a delay&amp;sum readout, Internal Status Report, Graz University of Technology, 2007. Available online at <a href="http://grh.mur.at/data/misc.html">http://grh.mur.at/data/misc.html</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb85">[85]</a></p>
<p>Francis wyffels, Benjamin Schrauwen, David Verstraeten, Stroobandt Dirk, Band-pass reservoir computing, in: Z. Hou, and N. Zhang (Eds.), Proceedings of the IEEE International Joint Conference on Neural Networks, 2008, IJCNN 2008, Hong Kong, 2008, pp. 3204–3209</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb86">[86]</a></p>
<p>Salah El Hihi, Yoshua Bengio</p>
<p><strong>Hierarchical recurrent neural networks for long-term dependencies</strong></p>
<p>Advances in Neural Information Processing Systems 8, MIT Press, Cambridge, MA (1996), pp. 493-499</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb87">[87]</a></p>
<p>Nils Bertschinger, Thomas Natschläger</p>
<p><strong>Real-time computation at the edge of chaos in recurrent neural networks</strong></p>
<p>Neural Computation, 16 (7) (2004), pp. 1413-1436</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb88">[88]</a></p>
<p>Benjamin Schrauwen, Lars Buesing, Robert Legenstein, On computational power and the order-chaos phase transition in reservoir computing, in: Advances in Neural Information Processing Systems 21, NIPS 2008, 2009, pp. 1425–1432</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb89">[89]</a></p>
<p>Wolfgang Maass, Robert A. Legenstein, Nils Bertschinger</p>
<p><strong>Methods for estimating the computational power and generalization capability of neural microcircuits</strong></p>
<p>Advances in Neural Information Processing Systems 17, NIPS 2004, MIT Press, Cambridge, MA (2005), pp. 865-872</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb90">[90]</a></p>
<p>Robert A. Legenstein, Wolfgang Maass</p>
<p><strong>Edge of chaos and prediction of computational performance for neural circuit models</strong></p>
<p>Neural Networks, 20 (3) (2007), pp. 323-334</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb91">[91]</a></p>
<p>Behrouz Farhang-Boroujeny</p>
<p><strong>Adaptive Filters: Theory and Applications</strong></p>
<p>Wiley (1998)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb92">[92]</a></p>
<p>Herbert Jaeger, Reservoir riddles: suggestions for echo state network research, Proceedings of the IEEE International Joint Conference on Neural Networks, 2005, IJCNN 2005, vol. 3, 2005, pp. 1460–1462</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb93">[93]</a></p>
<p>Jochen Triesch, A gradient rule for the plasticity of a neuron’s intrinsic excitability, in: Proceedings of the 13th European Symposium on Artificial Neural Networks, ESANN 2005, 2005, pp. 65–70</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb94">[94]</a></p>
<p>Melanie Mitchell, James P. Crutchfield, Peter T. Hraber</p>
<p><strong>Dynamics, computation, and the “edge of chaos”: A re-examination</strong></p>
<p>G. Cowan, D. Pines, D. Melzner (Eds.), Complexity: Metaphors, Models, and Reality, Addison-Wesley, Reading, MA (1994), pp. 497-513</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb95">[95]</a></p>
<p>Robert Legenstein, Wolfgang Maass</p>
<p><strong>What makes a dynamical system computationally powerful?</strong></p>
<p>S. Haykin, J. Príncipe, T. Sejnowski, J. McWhirter (Eds.), New Directions in Statistical Signal Processing: From Systems to Brain, MIT Press (2007), pp. 127-154</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb96">[96]</a></p>
<p>Mustafa C. Ozturk, José C. Príncipe, Computing with transiently stable states, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2005, IJCNN 2005, vol. 3, 2005, pp. 1467–1472</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb97">[97]</a></p>
<p>Donald O. Hebb</p>
<p><strong>The Organization of Behavior: A Neuropsychological Theory</strong></p>
<p>Wiley, New York (1949)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb98">[98]</a></p>
<p>Štefan Babinec, Jiří Pospíchal</p>
<p><strong>Improving the prediction accuracy of echo state neural networks by anti-Oja’s learning</strong></p>
<p>Proceedings of the 17th International Conference on Artificial Neural Networks, LNCS, vol. 4668, Springer (2007), pp. 19-28</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb99">[99]</a></p>
<p>Henry Markram, Yun Wang, Misha Tsodyks</p>
<p><strong>Differential signaling via the same axon of neocortical pyramidal neurons</strong></p>
<p>Proceedings of National Academy of Sciences USA, 95 (9) (1998), pp. 5323-5328</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb100">[100]</a></p>
<p>David Norton, Dan Ventura, Preparing more effective liquid state machines using Hebbian learning, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2006, IJCNN 2006, 2006, pp. 4243–4248</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb101">[101]</a></p>
<p>Hélène Paugam-Moisy, Regis Martinez, Samy Bengio</p>
<p><strong>Delay learning and polychronization for reservoir computing</strong></p>
<p>Neurocomputing, 71 (7–9) (2008), pp. 1143-1158</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb102">[102]</a></p>
<p>Roland Baddeley, Larry F. Abbott, Michael C.A. Booth, Frank Sengpeil, Toby Freeman, Edward A. Wakeman, Edmund T. Rolls</p>
<p><strong>Responses of neurons in primary and inferior temporal visual cortices to natural scenes</strong></p>
<p>Proceedings of the Royal Society of London B, 264 (1997), pp. 1775-1783</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb103">[103]</a></p>
<p>Martin Stemmler, Christof Koch</p>
<p><strong>How voltage-dependent conductances can adapt to maximize the information encoded by neuronal firing rate</strong></p>
<p>Nature Neuroscience, 2 (6) (1999), pp. 521-527</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb104">[104]</a></p>
<p>Jochen Triesch</p>
<p><strong>Synergies between intrinsic and synaptic plasticity in individual model neurons</strong></p>
<p>Advances in Neural Information Processing Systems 17, MIT Press, Cambridge, MA (2005), pp. 1417-1424</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb105">[105]</a></p>
<p>Anthony J. Bell, Terrence J. Sejnowski</p>
<p><strong>An information-maximization approach to blind separation and blind deconvolution</strong></p>
<p>Neural Computation, 7 (6) (1995), pp. 1129-1159</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb106">[106]</a></p>
<p>Jochen J. Steil</p>
<p><strong>Online reservoir adaptation by intrinsic plasticity for backpropagation-decorrelation and echo state learning</strong></p>
<p>Neural Networks, 20 (3) (2007), pp. 353-364</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb107">[107]</a></p>
<p>Marion Wardermann, Jochen J. Steil, Intrinsic plasticity for reservoir learning algorithms, in: Proceedings of the 15th European Symposium on Artificial Neural Networks, ESANN 2007, 2007, pp. 513–518</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb108">[108]</a></p>
<p>Jochen J. Steil, Several ways to solve the MSO problem, in: Proceedings of the 15th European Symposium on Artificial Neural Networks, ESANN 2007, 2007, pp. 489–494</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb109">[109]</a></p>
<p>Joschka Boedecker, Oliver Obst, Norbert Michael Mayer, Minoru Asada, Studies on reservoir initialization and dynamics shaping in echo state networks, in: Proceedings of the 17th European Symposium on Artificial Neural Networks, ESANN 2009, 2009 (in press)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb110">[110]</a></p>
<p>Naftali Tishby, Fernando~C. Pereira, William Bialek</p>
<p><strong>The information bottleneck method</strong></p>
<p>Proceedings of the 37th Annual Allerton Conference on Communication, Control and Computing (1999), pp. 368-377</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb111">[111]</a></p>
<p>Taro Toyoizumi, Jean-Pascal Pfister, Kazuyuki Aihara, Wulfram Gerstner</p>
<p><strong>Generalized Bienenstock–Cooper–Munro rule for spiking neurons that maximizes information transmission</strong></p>
<p>Proceedings of National Academy of Sciences USA, 102 (2005), pp. 5239-5244</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb112">[112]</a></p>
<p>Stefan Klampfl, Robert Legenstein, Wolfgang Maass</p>
<p><strong>Information bottleneck optimization and independent component extraction with spiking neurons</strong></p>
<p>Advances in Neural Information Processing Systems 19, ICANN 2007, MIT Press, Cambridge, MA (2007), pp. 713-720</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb113">[113]</a></p>
<p>Stefan Klampfl, Robert Legenstein, Wolfgang Maass</p>
<p><strong>Spiking neurons can learn to solve information bottleneck problems and to extract independent components</strong></p>
<p>Neural Computation, 21 (4) (2008), pp. 911-959</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb114">[114]</a></p>
<p>Lars Buesing, Wolfgang Maass</p>
<p><strong>Simplified rules and theoretical analysis for information bottleneck optimization and PCA with spiking neurons</strong></p>
<p>Advances in Neural Information Processing Systems 20, MIT Press, Cambridge, MA (2008), pp. 193-200</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb115">[115]</a></p>
<p>Jochen Triesch</p>
<p><strong>Synergies between intrinsic and synaptic plasticity mechanisms</strong></p>
<p>Neural Computation, 19 (4) (2007), pp. 885-909</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb116">[116]</a></p>
<p>Nicholas J. Butko, Jochen Triesch</p>
<p><strong>Learning sensory representations with intrinsic plasticity</strong></p>
<p>Neurocomputing (70) (2007), pp. 1130-1138</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb117">[117]</a></p>
<p>Andreea Lazar, Gordon Pipa, Jochen Triesch</p>
<p><strong>Fading memory and time series prediction in recurrent networks with different forms of plasticity</strong></p>
<p>Neural Networks, 20 (3) (2007), pp. 312-322</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb118">[118]</a></p>
<p>Norbert M. Mayer, Matthew Browne, Echo state networks and self-prediction, in: Revised Selected Papers of Biologically Inspired Approaches to Advanced Information Technology, BioADIT 2004, 2004, pp. 40–48</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb119">[119]</a></p>
<p>Mustafa C. Ozturk, Dongming Xu, José C. Príncipe</p>
<p><strong>Analysis and design of echo state networks</strong></p>
<p>Neural Computation, 19 (1) (2007), pp. 111-138</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb120">[120]</a></p>
<p>William H. Kautz</p>
<p><strong>Transient synthesis in the time domain</strong></p>
<p>IRE Transactions on Circuit Theory, 1 (3) (1954), pp. 29-39</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb121">[121]</a></p>
<p>Kazuo Ishii, Tijn van der Zant, Vlatko Bečanović, Paul Plöger, Identification of motion with echo state network, in: Proceedings of the OCEANS 2004 MTS/IEEE–TECHNO-OCEAN 2004 Conference, vol. 3, 2004, pp. 1205–1210</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb122">[122]</a></p>
<p>John H. Holland</p>
<p><strong>Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology Control and Artificial Intelligence</strong></p>
<p>MIT Press, Cambridge, MA, USA (1992)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb123">[123]</a></p>
<p>Ali Ajdari Rad, Mahdi Jalili, Martin Hasler</p>
<p><strong>Reservoir optimization in recurrent neural networks using Kronecker kernels</strong></p>
<p>Proceedings of IEEE International Symposium on Circuits and Systems 2008, IEEE (2008), pp. 868-871</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb124">[124]</a></p>
<p>Xavier Dutoit, Hendrik Van Brussel, Marnix Nutti, A first attempt of reservoir pruning for classification problems, in: Proceedings of the 15th European Symposium on Artificial Neural Networks, ESANN 2007, 2007, pp. 507–512</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb125">[125]</a></p>
<p>Wolfgang Maass, Prashant Joshi, Eduardo D. Sontag</p>
<p><strong>Computational aspects of feedback in neural circuits</strong></p>
<p>PLoS Computational Biology, 3 (1) (2007), p. e165+</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb126">[126]</a></p>
<p>Robert Legenstein, Dejan Pecevski, Wolfgang Maass</p>
<p><strong>Theoretical analysis of learning with reward-modulated spike-timing-dependent plasticity</strong></p>
<p>Advances in Neural Information Processing Systems 20, NIPS 2007, MIT Press, Cambridge, MA (2008), pp. 881-888</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb127">[127]</a></p>
<p>Robert Legenstein, Dejan Pecevski, Wolfgang Maass</p>
<p><strong>A learning theory for reward-modulated spike-timing-dependent plasticity with application to biofeedback</strong></p>
<p>PLoS Computational Biology, 4 (10) (2008), p. e1000180</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb128">[128]</a></p>
<p>A˚ke Björck</p>
<p><strong>Numerical Method for Least Squares Problems</strong></p>
<p>SIAM, Philadelphia, PA, USA (1996)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb129">[129]</a></p>
<p>Andrew Carnell, Daniel Richardson, Linear algebra for time series of spikes, in: Proceedings of the 13th European Symposium on Artificial Neural Networks, ESANN 2005, 2005, pp. 363–368</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb130">[130]</a></p>
<p>Ali U. Küçükemre, Echo state networks for adaptive filtering, University of Applied Sciences Bohn-Rhein-Sieg, Germany, April 2006. <a href="http://www.faculty.jacobs-university.de/hjaeger/pubs/Kucukemre.pdf">http://www.faculty.jacobs-university.de/hjaeger/pubs/Kucukemre.pdf</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb131">[131]</a></p>
<p>Zhinwei Shi, Min Han</p>
<p><strong>Support vector echo-state machine for chaotic time-series prediction</strong></p>
<p>IEEE Transactions on Neural Networks, 18 (2) (2007), pp. 359-372</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb132">[132]</a></p>
<p>Jürgen Schmidhuber, Matteo Gagliolo, Daan Wierstra, Faustino J. Gomez, Evolino for recurrent support vector machines. Technical Report, 2006</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb133">[133]</a></p>
<p>Benjamin Schrauwen, Jan~Van Campenhout</p>
<p><strong>Linking non-binned spike train kernels to several existing spike train metrics</strong></p>
<p>M. Verleysen (Ed.), Proceedings of the 14th European Symposium on Artificial Neural Networks, ESANN 2006, d-side publications, Evere (2006), pp. 41-46</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb134">[134]</a></p>
<p>Jochen J. Steil, Stability of backpropagation-decorrelation efficient O(N) recurrent learning, in: Proceedings of the 13th European Symposium on Artificial Neural Networks, ESANN 2005, 2005, pp. 43–48</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb135">[135]</a></p>
<p>Francis wyffels, Benjamin Schrauwen, Dirk Stroobandt</p>
<p><strong>Stable output feedback in reservoir computing using ridge regression</strong></p>
<p>Proceedings of the 18th International Conference on Artificial Neural Networks, LNCS, vol. 5163, ICANN 2008, Springer (2008), pp. 808-817</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb136">[136]</a></p>
<p>Xavier Dutoit, Benjamin Schrauwen, Jan Van Campenhout, Dirk Stroobandt, Hendrik Van Brussel, Marnix Nuttin, Pruning and regularization in reservoir computing: A first insight, in: Proceedings of the 16th European Symposium on Artificial Neural Networks, ESANN 2008, 2008, pp. 1–6</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb137">[137]</a></p>
<p>Joe Tebelskis, Ph.D. Thesis, Speech Recognition using Neural Networks, School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, 1995</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb138">[138]</a></p>
<p>Mark D. Skowronski, John G. Harris</p>
<p><strong>Automatic speech recognition using a predictive echo state network classifier</strong></p>
<p>Neural Networks, 20 (3) (2007), pp. 414-423</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb139">[139]</a></p>
<p>Dongming Xu, Jing Lan, José C. Príncipe, Direct adaptive control: An echo state network and genetic algorithm approach, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2005, IJCNN 2005, vol. 3, 2005, pp. 1483–1486</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb140">[140]</a></p>
<p>Alexandre Devert, Nicolas Bredeche, Marc Schoenauer</p>
<p><strong>Unsupervised learning of echo state networks: a case study in artificial embryogeny</strong></p>
<p>Proceedings of the 8th International Conference on Artificial Evolution, LNCS, vol. 4926, Springer (2008), pp. 278-290</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb141">[141]</a></p>
<p>Fei Jiang, Hugues Berry, Marc Schoenauer</p>
<p><strong>Unsupervised learning of echo state networks: Balancing the double pole</strong></p>
<p>Proceedings of the 10th Genetic and Evolutionary Computation Conference, ACM (2008), pp. 869-870</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb142">[142]</a></p>
<p>Keith Bush, Charles Anderson, Modeling reward functions for incomplete state representations via echo state networks, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2005, IJCNN 2005, vol. 5, 2005, pp. 2995–3000</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb143">[143]</a></p>
<p>Štefan Babinec, Jiří Pospíchal</p>
<p><strong>Merging echo state and feedforward neural networks for time series forecasting</strong></p>
<p>Proceedings of the 16th International Conference on Artificial Neural Networks, LNCS, vol. 4131, Springer (2006), pp. 367-375</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb144">[144]</a></p>
<p>Mustafa C. Ozturk, José C. Príncipe</p>
<p><strong>An associative memory readout for ESNs with applications to dynamical pattern recognition</strong></p>
<p>Neural Networks, 20 (3) (2007), pp. 377-390</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb145">[145]</a></p>
<p>Mark Embrechts, Luis Alexandre, Jonathan Linton, Reservoir computing for static pattern recognition, in: Proceedings of the 17th European Symposium on Artificial Neural Networks, ESANN 2009, 2009 (in press)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb146">[146]</a></p>
<p>Felix R. Reinhart, Jochen J. Steil, Attractor-based computation with reservoirs for online learning of inverse kinematics, in: Proceedings of the 17th European Symposium on Artificial Neural Networks, ESANN 2009, 2009 (in press)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb147">[147]</a></p>
<p>Keith Bush, Charles Anderson, Exploiting iso-error pathways in the N, k-plane to improve echo state network performance, 2006</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb148">[148]</a></p>
<p>Yoshua Bengio, Yann LeCun</p>
<p><strong>Scaling learning algorithms toward AI</strong></p>
<p>L. Bottou, O. Chapelle, D. DeCoste, J. Weston (Eds.), Large Scale Kernel Machines, MIT Press, Cambridge, MA (2007)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb149">[149]</a></p>
<p>Herbert Jaeger, Discovering multiscale dynamical features with hierarchical echo state networks, Technical Report No. 9, Jacobs University Bremen, 2007</p>
<h2 id="cited-by-1641">Cited by (1641)<a class="headerlink" href="#cited-by-1641" title="Permanent link">&para;</a></h2>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1574013709000173">View Abstract</a></p>
<p>Copyright © 2009 Elsevier Inc. All rights reserved.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.51198bba.min.js"></script>
      
        <script src="../../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
      
    
  </body>
</html>