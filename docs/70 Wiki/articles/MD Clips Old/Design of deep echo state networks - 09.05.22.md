---
created: 2022-05-09T16:48:30 (UTC +02:00)
tags: []
source: https://www.sciencedirect.com/science/article/pii/S0893608018302223
author: 
---

# Design of deep echo state networks - ScienceDirect



> ## Excerpt
> In this paper, we provide a novel approach to the architectural design of deep Recurrent Neural Networks using signal frequency analysis. In particula…

---
## 1\. Introduction

In the last years, the study of deep architectures aroused a great interest in the [neural network](https://www.sciencedirect.com/topics/neuroscience/neural-networks "Learn more about neural network from ScienceDirect's AI-generated Topic Pages") research community. Based on a hierarchical organization of multiple layers, such networks proved effective in developing a compositional internal representation of the input information, allowing to address challenging real-world problems from several application fields featured by complex data [Goodfellow et al., 2016](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b20), [Graves et al., 2013](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b21), [Krizhevsky et al., 2012](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b33), [Mohamed et al., 2012](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b39), [Sutskever et al., 2014](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b50). In particular, recent studies on deep [recurrent](https://www.sciencedirect.com/topics/engineering/recurrent "Learn more about recurrent from ScienceDirect's AI-generated Topic Pages") neural networks (RNNs) opened a way to develop novel models able to learn hierarchical temporal representations from signals characterized by multiple time-scales dynamics [Angelov and Sperduti, 2016](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b1), [Hermans and Schrauwen, 2013](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b23), [Hihi and Bengio, 1995](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b24), [Pascanu et al., 2014](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b41), [Schmidhuber, 2015](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b46).

In the field of randomized neural networks [Gallicchio et al., 2017](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b8), [Gallicchio, Micheli, and Tiňo, 2018](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b18), studies in the Reservoir Computing (RC) area [Lukoševičius and Jaeger, 2009](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b35), [Verstraeten et al., 2007](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b54) targeting ad-hoc [modular organizations](https://www.sciencedirect.com/topics/computer-science/modular-organization "Learn more about modular organizations from ScienceDirect's AI-generated Topic Pages") of Echo State Networks (ESNs) [Jaeger, 2001b](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b29), [Jaeger and Haas, 2004](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b31), showed promising results on time-series tasks (see e.g. [Jaeger, 2007](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b30), [Malik et al., 2017](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b38), [Triefenbach et al., 2013](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b52)). Recently, the RC/ESN framework has been explicitly extended towards the deep learning framework, allowing to analyze the intrinsic aspects of layering in stacked RNN architectures, at the same time providing efficient solutions for building deep RNNs [(Gallicchio, Micheli, & Pedrelli, 2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14). Studies on the DeepESN model [(Gallicchio, Micheli, & Pedrelli, 2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14) shed light on the significance of stacking layers of recurrent units, pointing out the inherent characterization of the system dynamics developed at the different layers in deep RNNs. Empirical results in [Gallicchio, Micheli, and Pedrelli (2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14) and [Gallicchio and Micheli (2016)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b10), as well as theoretical investigations in the field of [dynamical systems](https://www.sciencedirect.com/topics/computer-science/dynamical-system "Learn more about dynamical systems from ScienceDirect's AI-generated Topic Pages") [(Gallicchio & Micheli, 2017b)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b12) and [Lyapunov exponents](https://www.sciencedirect.com/topics/engineering/lyapunov-exponent "Learn more about Lyapunov exponents from ScienceDirect's AI-generated Topic Pages") [Gallicchio, Micheli, and Silvestri, 2017](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b16), [Gallicchio, Micheli, and Silvestri, 2018](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b17) highlighted the natural structured organization of the state dynamics developed by deep recurrent architectures even without training of the recurrent connections. Higher layers in the stack can progressively develop qualitatively different dynamics, and this natural differentiation results in a rich multiple time-scales representation of the temporal information. Moreover, the analysis in [Gallicchio, Micheli, and Pedrelli (2019)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b15) noticeably showed that such hierarchical organization of the temporal features developed by the dynamics of stacked recurrent layers still holds even in case of linear [activation functions](https://www.sciencedirect.com/topics/engineering/activation-function "Learn more about activation functions from ScienceDirect's AI-generated Topic Pages"). Further details on the analysis and advancements of DeepESN can be found in [Gallicchio and Micheli (2017a)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b11).

Overall, the analysis conducted so far in the RC context highlighted the potential advantages of layering as a factor of [architectural design](https://www.sciencedirect.com/topics/computer-science/architectural-design "Learn more about architectural design from ScienceDirect's AI-generated Topic Pages") in the development of a multiple time-scales [dynamical behavior](https://www.sciencedirect.com/topics/engineering/dynamical-behavior "Learn more about dynamical behavior from ScienceDirect's AI-generated Topic Pages"). Starting from this intrinsic characterization, we can thus ask whether the number of layers in the architecture is actually providing a sufficiently diversified behavior, and, on the other hand, whether adding new layers is still effective in terms of dynamical differentiation or not. In other words, in this paper we tackle the problem of how to choose the number of layers in a deep recurrent architecture, which currently represents one of the main open questions in the deep learning area. Differently from the work in [Pascanu et al. (2014)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b41), which describes different possible ways of introducing deepness into the architecture of an RNN trained with stochastic [gradient descent](https://www.sciencedirect.com/topics/engineering/gradient-descent "Learn more about gradient descent from ScienceDirect's AI-generated Topic Pages"), here we explicitly address the issue of how to operatively set the number of layers in deep stacked recurrent models, based on the [properties](https://www.sciencedirect.com/topics/mathematics/sigma-property "Learn more about properties from ScienceDirect's AI-generated Topic Pages") of the specific driving input signals and without the training of recurrent units.

Specifically, in this paper we propose an automatic method for the design of DeepESN, based on frequency analysis and aimed at appropriately exploiting the differentiation of temporal representation in deep recurrent architectures. The hypotheses (that delineate the scope of the work) are that the input signals are multi-scale and that the differences in the time scales are important for the learning task at hand. Given these hypotheses, we aim at exploiting such differences, tailoring the layered architecture to the characteristics of the input signals, by adding layers only as long as the changes in the frequency spectrum are effective through layering. This will be crucial for the final classification/regression performance, under the assumed conditions, proportionally to the effectiveness of the readout training in grasping/modulating the layered differentiation. Under such conditions, the proposed approach has the advantage to determine the proper number of recurrent layers avoiding to apply the training algorithm for each possible number of recurrent layers explored by the usual trial and error approach.

Besides, a secondary objective is also to bring attention to a more general methodological aspect concerning the analysis of multi-layered recurrent architectures by means of signal processing tools for investigation aims, with a focus on monitoring the filtering effect on input signals through the recurrent layers. This aspect is concretely exploited for design purpose in this work, providing an unsupervised approach to determine the number of layers for a deep recurrent architecture on the basis of the data at hand, while conserving a more general flavor for future research.

Based on the analysis of quantitative measures of frequency spectrum in the state space, we define an iterative procedure to assess the diversification of multiple time-scales dynamics among layers. First, we analyze and refine our design method on a controlled scenario characterized by signals with multiple time-scales dynamics, studying qualitative and quantitative aspects of frequency analysis in layers states. Then, we experimentally evaluate the method on challenging real-world tasks in the area of temporal processing of time-series featured by multiple times-scales, namely music processing and speech processing.

A further contribution of this work is to explicitly show, for the first time in the literature, the performance advantage on real-world tasks resulting from hierarchically structured recurrent state space organizations of deep RC models, through a comparative assessment with both fully trained neural network methodologies and RC-based approaches.

The paper is organized as follows. In Section [2](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec2) we first recall the basics of the standard (shallow) ESN approach (in Section [2.1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec2.1)) and then we introduce the DeepESN model (in Section [2.2](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec2.2)). In Section [3](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3) we define the proposed method for designing DeepESN architectures, providing an analysis of the involved advantages in terms of computational cost and analyzing it under an ad-hoc controlled scenario. In Section [4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec4) we evaluate our approach on music processing and speech processing tasks. We discuss the outcomes of our analysis in Section [5](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec5), and we present conclusions in Section [6](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec6). Further details on experimental results are provided in [Appendix](https://www.sciencedirect.com/science/article/pii/S0893608018302223#app).

## 2\. Deep reservoir computing

In this section, we introduce the deep RC framework. First, we briefly describe the standard shallow RC in Section [2.1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec2.1). After that, we introduce deep RC architectures in Section [2.2](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec2.2).

### 2.1. Shallow echo state networks

The ESN [(Jaeger & Haas, 2004)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b31) model is an efficient implementation of an [RNN](https://www.sciencedirect.com/topics/engineering/recurrent-neural-network "Learn more about RNN from ScienceDirect's AI-generated Topic Pages") within the RC framework [Lukoševičius and Jaeger, 2009](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b35), [Verstraeten et al., 2007](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b54). It is characterized by a [recurrent](https://www.sciencedirect.com/topics/engineering/recurrent "Learn more about recurrent from ScienceDirect's AI-generated Topic Pages") layer called reservoir and by a linear output layer called readout. The reservoir implements a discrete-time dynamical system through untrained recurrent connections and it provides a suffix-based Markovian representation of the input history [Gallicchio and Micheli, 2011](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b9), [Tiňo et al., 2007](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b51). The readout is a linear layer that computes the output of the network by exploiting the [temporal state](https://www.sciencedirect.com/topics/computer-science/temporal-state "Learn more about temporal state from ScienceDirect's AI-generated Topic Pages") representation developed by the reservoir part. In our work, we consider a variant of ESN called Leaky [Integrator](https://www.sciencedirect.com/topics/engineering/integrator "Learn more about Integrator from ScienceDirect's AI-generated Topic Pages") ESN (LI-ESN) [(Jaeger, Lukoševičius, Popovici, & Siewert, 2007)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b32), in which the reservoir contains leaky integrator units. Omitting the bias terms in the following formulas for the ease of notation, the [state transition function](https://www.sciencedirect.com/topics/computer-science/state-transition-function "Learn more about state transition function from ScienceDirect's AI-generated Topic Pages") of LI-ESN is defined as follows: (1)$<math><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><mi is="true">a</mi><mo is="true">)</mo></mrow><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">+</mo><mi is="true">a</mi><mspace width="0.16667em" is="true"></mspace><mi mathvariant="bold" is="true">tanh</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><mover accent="true" is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">,</mo></math>$where $<math><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">U</mi></mrow></msub></mrow></msup></math>$ and $<math><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub></mrow></msup></math>$ are respectively the input and the reservoir state at time $<math><mi is="true">t</mi></math>$, $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">U</mi></mrow></msub></mrow></msup></math>$ is the matrix of the input weights, $<math><mover accent="true" is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub></mrow></msup></math>$ is the matrix of the recurrent weights, $<math><mi is="true">a</mi><mo is="true">∈</mo><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">]</mo></mrow></math>$ is the leaky parameter and $<math><mi mathvariant="bold" is="true">tanh</mi></math>$ represents the element-wise application of the [hyperbolic tangent](https://www.sciencedirect.com/topics/engineering/hyperbolic-tangent "Learn more about hyperbolic tangent from ScienceDirect's AI-generated Topic Pages") [activation function](https://www.sciencedirect.com/topics/engineering/activation-function "Learn more about activation function from ScienceDirect's AI-generated Topic Pages"). The reservoir parameters are initialized in order to satisfy the Echo State [Property](https://www.sciencedirect.com/topics/mathematics/sigma-property "Learn more about Property from ScienceDirect's AI-generated Topic Pages") (ESP) [Gallicchio and Micheli, 2011](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b9), [Jaeger and Haas, 2004](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b31), [Yildiz et al., 2012](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b59) and after that they are left untrained. Accordingly, the values in matrix $<math><mover accent="true" is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></math>$ are randomly selected from a uniform distribution, e.g. in $<math><mrow is="true"><mo is="true">[</mo><mo is="true">−</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">]</mo></mrow></math>$, and then re-scaled in order to control spectral-related properties that influence the stability of network dynamics. Commonly, following the necessary condition for the ESP, and denoting by $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ the [spectral radius](https://www.sciencedirect.com/topics/engineering/spectral-radius "Learn more about spectral radius from ScienceDirect's AI-generated Topic Pages") operator (i.e. the largest absolute eigenvalue of its matrix argument), the weight values in $<math><mover accent="true" is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></math>$ are re-scaled to meet the following condition: (2)$<math><mi is="true">ρ</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><mi is="true">a</mi><mo is="true">)</mo></mrow><mi mathvariant="bold" is="true">I</mi><mo is="true">+</mo><mi is="true">a</mi><mover accent="true" is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></mrow></mfenced><mo is="true">&lt;</mo><mn is="true">1</mn><mo is="true">.</mo></math>$The values in matrix $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub></math>$ are randomly selected from a uniform distribution over $<math><mrow is="true"><mo is="true">[</mo><mo is="true">−</mo><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">,</mo><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">]</mo></mrow></math>$, where $<math><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub></math>$ is the input scaling parameter.

The output of the network at time $<math><mi is="true">t</mi></math>$ is computed by the readout through a [linear combination](https://www.sciencedirect.com/topics/engineering/linear-combination "Learn more about linear combination from ScienceDirect's AI-generated Topic Pages") of reservoir states, as follows: (3)$<math><mi mathvariant="bold" is="true">y</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mi is="true">o</mi><mi is="true">u</mi><mi is="true">t</mi></mrow></msub><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">.</mo></math>$The readout layer is the only part of the network that is trained, typically using pseudo-inversion or ridge [regression methods](https://www.sciencedirect.com/topics/computer-science/regression-method "Learn more about regression methods from ScienceDirect's AI-generated Topic Pages")[(Lukoševičius & Jaeger, 2009)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b35) computed through [singular value decomposition](https://www.sciencedirect.com/topics/engineering/singular-value-decomposition "Learn more about singular value decomposition from ScienceDirect's AI-generated Topic Pages") (SVD). In the following, we refer to the LI-ESN model described so far as *shallowESN*.

In the context of RC, a well known approach for unsupervised adaptation of the reservoir is given by Intrinsic Plasticity (IP) [Lukoševičius and Jaeger, 2009](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b35), [Schrauwen et al., 2008](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b47), [Steil, 2007](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b48), [Triesch, 2005](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b53), which is based on maximizing the entropy of the output distribution of the reservoir units. Specifically, IP aims at adjusting the parameters of the activation function of reservoir units to fit a desired distribution, which in case of $<math><mi is="true">t</mi><mi is="true">a</mi><mi is="true">n</mi><mi is="true">h</mi></math>$ is of Gaussian type. In our context, the application of the reservoir activation function for a generic reservoir unit can be expressed as $<math><mover accent="true" is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mo is="true">̃</mo></mrow></mover><mo is="true">=</mo><mi is="true">t</mi><mi is="true">a</mi><mi is="true">n</mi><mi is="true">h</mi><mrow is="true"><mo is="true">(</mo><mi is="true">g</mi><mspace width="0.16667em" is="true"></mspace><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">n</mi><mi is="true">e</mi><mi is="true">t</mi></mrow></msub><mo is="true">+</mo><mi is="true">b</mi><mo is="true">)</mo></mrow></math>$, where $<math><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">n</mi><mi is="true">e</mi><mi is="true">t</mi></mrow></msub></math>$, $<math><mover accent="true" is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mo is="true">̃</mo></mrow></mover></math>$, $<math><mi is="true">g</mi></math>$ and $<math><mi is="true">b</mi></math>$ are respectively the (net) input, the output, the gain and the bias of the activation function. The IP training procedure is performed on each reservoir unit on the basis of the IP rule [(Schrauwen et al., 2008)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b47) defined as: (4)$<math><mtable align="axis" class="array" is="true"><mtr is="true"><mtd columnalign="left" is="true"><mi is="true">Δ</mi><mi is="true">b</mi><mo is="true">=</mo><mo is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">η</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mo is="true">−</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mo is="true">∕</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo is="true">)</mo></mrow><mo is="true">+</mo><mrow is="true"><mo is="true">(</mo><mover accent="true" is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mo is="true">̃</mo></mrow></mover><mo is="true">∕</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo is="true">)</mo></mrow><mrow is="true"><mo is="true">(</mo><mn is="true">2</mn><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo is="true">+</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mo is="true">̃</mo></mrow></mover></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mover accent="true" is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mo is="true">̃</mo></mrow></mover><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">,</mo></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mi is="true">Δ</mi><mi is="true">g</mi><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">η</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mo is="true">∕</mo><mi is="true">g</mi><mo is="true">+</mo><mi is="true">Δ</mi><mi is="true">b</mi><mspace width="0.16667em" is="true"></mspace><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">n</mi><mi is="true">e</mi><mi is="true">t</mi></mrow></msub><mo is="true">,</mo></mtd></mtr></mtable></math>$where $<math><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub></math>$ are the mean and the standard deviation of the [Gaussian distribution](https://www.sciencedirect.com/topics/mathematics/gaussian-distribution "Learn more about Gaussian distribution from ScienceDirect's AI-generated Topic Pages") (that is the target of the adaptation process), $<math><msub is="true"><mrow is="true"><mi is="true">η</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub></math>$ is the learning rate, $<math><mi is="true">Δ</mi><mi is="true">g</mi></math>$ and $<math><mi is="true">Δ</mi><mi is="true">b</mi></math>$ respectively denote the update values for gain and bias of the IP iterative learning algorithm.

### 2.2. Deep echo state networks

The DeepESN model, recently introduced in [Gallicchio, Micheli, and Pedrelli (2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14), allowed to frame the ESN approach in the context of deep learning. The architecture of a DeepESN is characterized by a stacked hierarchy of reservoirs, as shown in [Fig. 1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig1). At each time-step $<math><mi is="true">t</mi></math>$, the first recurrent layer is fed by the external input $<math><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$, while each successive layer is fed by the output of the previous one in the stack. Although the pipelined architectural organization of the reservoir in DeepESN allows a general flexibility in the size of each layer, for the sake of simplicity here we consider a hierarchical reservoir setup with $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></math>$ recurrent layers each of which contains the same number of units $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub></math>$. Furthermore, in our notation, we use $<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">x</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub></mrow></msup></math>$ to denote the state of layer $<math><mi is="true">l</mi></math>$ at time $<math><mi is="true">t</mi></math>$.

Omitting the bias terms in the formulas for the ease of notation, the state transition function of the first layer is defined as follows: (5)$<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">x</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup><mo is="true">)</mo></mrow><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">x</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup><mi mathvariant="bold" is="true">tanh</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">x</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">,</mo></math>$while for every layer $<math><mi is="true">l</mi><mo is="true">&gt;</mo><mn is="true">1</mn></math>$ it is described by: (6)$<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">x</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">)</mo></mrow><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">x</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mi mathvariant="bold" is="true">tanh</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">x</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">x</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">,</mo></math>$where $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">U</mi></mrow></msub></mrow></msup></math>$ is the input [weight matrix](https://www.sciencedirect.com/topics/mathematics/weight-matrix "Learn more about weight matrix from ScienceDirect's AI-generated Topic Pages"), $<math><msup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub></mrow></msup></math>$ is the matrix of the recurrent weights for layer $<math><mi is="true">l</mi></math>$, $<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub></mrow></msup></math>$ is the matrix relative to the inter-layer connection weights from layer $<math><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn></math>$ to layer $<math><mi is="true">l</mi></math>$, $<math><msup is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$ is the leaky parameter at layer $<math><mi is="true">l</mi></math>$ and $<math><mi mathvariant="bold" is="true">tanh</mi></math>$ represents the element-wise application of the hyperbolic tangent.

As in the standard ESN approach, the reservoir component of a DeepESN is left untrained after initialization subject to stability constraints. In the case of DeepESN, such constraints are expressed by the conditions for the ESP of deep RC networks, given in [Gallicchio and Micheli (2017b)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b12). Accordingly, the weight values in the recurrent matrices $<math><msup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$, for $<math><mi is="true">l</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></math>$, are randomly initialized from a uniform distribution, e.g. in $<math><mrow is="true"><mo is="true">[</mo><mo is="true">−</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">]</mo></mrow></math>$, and then are rescaled to satisfy the necessary condition for the ESP of deep reservoirs [(Gallicchio & Micheli, 2017b)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b12), described by the following equation: (7)$<math><munder is="true"><mrow is="true"><mo class="qopname" is="true">max</mo></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">≤</mo><mi is="true">l</mi><mo is="true">≤</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></mrow></munder><mi is="true">ρ</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">)</mo></mrow><mi mathvariant="bold" is="true">I</mi><mo is="true">+</mo><msup is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><msup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></mrow></mfenced><mo is="true">=</mo><munder is="true"><mrow is="true"><mo class="qopname" is="true">max</mo></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">≤</mo><mi is="true">l</mi><mo is="true">≤</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></mrow></munder><msup is="true"><mrow is="true"><mi is="true">ρ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">&lt;</mo><mn is="true">1</mn><mo is="true">,</mo></math>$in which we used the notation $<math><msup is="true"><mrow is="true"><mi is="true">ρ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$ to denote the effective spectral radius of the reservoir system in the $<math><mi is="true">l</mi></math>$th layer. As regards input and inter-layer matrices, the values in $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub></math>$ and $<math><msubsup is="true"><mrow is="true"><mrow is="true"><mo is="true">{</mo><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">}</mo></mrow></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">=</mo><mn is="true">2</mn></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></mrow></msubsup></math>$ are randomly initialized from a uniform distribution and then re-scaled in the $<math><mrow is="true"><mo is="true">[</mo><mo is="true">−</mo><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">,</mo><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">]</mo></mrow></math>$ range.

Note that, as in the case of standard RC, in experimental assessments of DeepESNs, for each reservoir hyper-parameterization a number of different (independently initialized) instances are considered, all with the same reservoir hyper-parameters, but generated using different random seeds. In this paper, we refer to such instances as reservoir guesses. The performance of each hyper-parameterization is then obtained by averaging the performance achieved by the corresponding reservoir guesses.

As pertains to the output computation, although different patterns of state-output connectivity have been explored in recent literature in the case of deep recurrent models[Hermans and Schrauwen, 2013](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b23), [Pascanu et al., 2014](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b41), in this paper we focus on the case represented in [Fig. 1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig1), in which the states of all the reservoir layers are used to feed the readout. Specifically, considering the global state of the DeepESN as the composition of the states in the different layers, i.e. $<math><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">x</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">x</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub></mrow></msup></math>$, the output of the network at time $<math><mi is="true">t</mi></math>$, i.e. $<math><mi mathvariant="bold" is="true">y</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">Y</mi></mrow></msub></mrow></msup></math>$, is computed as follows: (8)$<math><mi mathvariant="bold" is="true">y</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mi is="true">o</mi><mi is="true">u</mi><mi is="true">t</mi></mrow></msub><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></math>$where $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mi is="true">o</mi><mi is="true">u</mi><mi is="true">t</mi></mrow></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">Y</mi></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub></mrow></msup></math>$ denotes the matrix of the output weights. Note that in the case of DeepESN, the readout formulation given in Eq. [(8)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd8) expresses a linear combination between the global reservoir state of the network $<math><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$ and the readout weight matrix $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">W</mi></mrow><mrow is="true"><mi is="true">o</mi><mi is="true">u</mi><mi is="true">t</mi></mrow></msub></math>$, i.e. a weighted sum of the states coming from all the reservoir layers in the architecture. In the training phase, this directly enables the model to differently weight the contribution of the multiple time-scales dynamics developed through the layers of the deep recurrent architecture, thus enhancing the quality of the temporal representation and the ability to approach temporal tasks for which such differentiation in dealing with the different time-scales is important. As regards the training algorithms, the output layer in DeepESNs is trained as in the standard RC paradigm [(Lukoševičius & Jaeger, 2009)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b35), i.e. using pseudo-inversion or ridge regression implemented through SVD.

Although the DeepESN model has been introduced in the neural networks’ literature only recently [Gallicchio and Micheli, 2016](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b10), [Gallicchio, Micheli, and Pedrelli, 2017](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14), the outcomes of its study already allowed to clearly highlight a number of major advantages that are *inherently* brought by a layered construction of RNN architectures [(Gallicchio & Micheli, 2018)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b13), i.e. even prior to training of the recurrent connections. These advantages, which contribute to delineate the characterizations of the DeepESN approach, are briefly summarized in the following.

$<math><mo is="true">•</mo></math>$

*Multiple temporal representations.* The hierarchical organization of the reservoir in successive layers is naturally reflected into the structure of the developed system dynamics. Specifically, it has been experimentally observed in [Gallicchio, Micheli, and Pedrelli (2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14) and [Gallicchio and Micheli (2016)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b10) that the global state of a DeepESN tends to develop a *multiple time-scales* representation of the input history, hierarchically ordered along the layers of the recurrent architecture. In particular, higher layers showed progressively slower dynamics in the conditions, setup and tasks analyzed in [Gallicchio, Micheli, and Pedrelli (2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14). In this regard, an interesting observation from the work in [Gallicchio, Micheli, and Pedrelli (2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14) is that the hierarchical structure of DeepESN state representations can be achieved even in the case in which all the reservoir layers share the same values for the hyper-parameters. Another relevant outcome of the work in [Gallicchio, Micheli, and Pedrelli (2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14) is that IP adaptation applied in conjunction with a layered recurrent organization is able to further enhance the effect of temporal scales differentiation across the layers.

The aspect of temporal scales differentiation in RC models has been addressed in literature also from a different, though related, line of architectural studies. These are based on the idea of structuring the reservoir into sub-groups, or sub-reservoirs, characterized by different dynamical properties with the aim to achieve a decoupling among the state dynamics [(Xue, Yang, & Haykin, 2007)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b57), an idea that has been pursued also outside of the RC context e.g. in [Yamashita and Tani (2008)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b58). A recent development, described in [Qiao, Li, Han, and Li (2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b42), proposed an [incremental approach](https://www.sciencedirect.com/topics/computer-science/incremental-approach "Learn more about incremental approach from ScienceDirect's AI-generated Topic Pages") to the construction of the sub-groups reservoir organization. Differently from the DeepESN model, all of these architectural variants are based on a structured but non-hierarchical organization of the recurrent dynamical part, i.e. they are shallow networks purposely designed to achieve a multiplicity of temporal scales by construction. The experimental comparison between the two approaches, already studied in [Gallicchio, Micheli, and Pedrelli (2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14), pointed out the actual relevance of a layered architectural construction, in which pools of recurrent units are progressively more distant from the input (and there is no feedback from higher to lower layers).

The multiplicity of temporal representations developed by the global internal state of a DeepESN has been also analyzed in terms of the frequency spectrum of state components in case of linear activation functions. In this case, results in [Gallicchio et al. (2019)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b15) pointed out the natural propensity of DeepESN states to produce a *multiple frequency representation* of the input information, distributed through layers, whereas (in the conditions analyzed in [Gallicchio et al., 2019](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b15)) higher layers tended to focus on lower frequencies.

$<math><mo is="true">•</mo></math>$

*Richness of reservoir states.* A hierarchical construction of the reservoir is also beneficial in terms of increasing the richness of the developed state dynamics. This has been experimentally observed in [Gallicchio, Micheli, and Pedrelli (2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14), by measuring the averaged entropy of DeepESN states, using the IP rule for unsupervised adaptation and in comparison to the shallow case. On the theoretical side, studies in the field of [dynamical system theory](https://www.sciencedirect.com/topics/engineering/dynamical-system-theory "Learn more about dynamical system theory from ScienceDirect's AI-generated Topic Pages") [(Gallicchio & Micheli, 2017b)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b12) showed that reservoir states in different layers of DeepESN are able to develop dynamics that are qualitatively different in terms of contractive behavior. Specifically, as analyzed in [Gallicchio and Micheli (2017b)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b12) (in a basic setting without IP learning), when the DeepESN is initialized using the same hyper-parameters for the scaling of reservoirs matrices in all the levels of the architecture, progressively higher layers tend to be characterized by progressively less contractive dynamics. Furthermore, stability analysis in presence of driving inputs, conducted through the study of [Lyapunov exponents](https://www.sciencedirect.com/topics/engineering/lyapunov-exponent "Learn more about Lyapunov exponents from ScienceDirect's AI-generated Topic Pages") in [Gallicchio, Micheli, and Silvestri, 2017](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b16), [Gallicchio, Micheli, and Silvestri, 2018](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b17), pointed out that layered RNN architectures, compared to shallow counterparts in condition of equal number of recurrent units, show a [dynamical behavior](https://www.sciencedirect.com/topics/engineering/dynamical-behavior "Learn more about dynamical behavior from ScienceDirect's AI-generated Topic Pages") that is naturally pushed closer to the *edge of chaos*. This represents a transition condition of states regime near which the RNN system exhibits a rich internal representation of the driving input signals and high performance in tasks requiring long [memory spans](https://www.sciencedirect.com/topics/engineering/memory-span "Learn more about memory spans from ScienceDirect's AI-generated Topic Pages") [Bertschinger and Natschläger, 2004](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b4), [Maass, 2007](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b37).

$<math><mo is="true">•</mo></math>$

*Memory Capacity*. The hierarchical reservoir organization in DeepESNs has also a natural positive effect on the short-term memory abilities of the network, which in the context of RC is commonly evaluated by means of the Memory Capacity (MC) task defined in [Jaeger (2001a)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b28). Experimental evidences reported in [Gallicchio, Micheli, and Pedrelli (2017)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14) showed that DeepESNs are able to considerably and consistently outperform corresponding shallow ESN settings (with the same total number of recurrent units) in terms of MC. A further performance gain on the MC task was found through the combination of DeepESN with IP adaptation, providing a concrete example of a case in which the introduction of depth in the RNN design is able to naturally enhance the advantages brought by IP in tasks (such as MC) on which this adaptation technique is already effective by itself [(Schrauwen et al., 2008)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b47). Moreover, recent results given in [Gallicchio (2018)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b7) indicated that the MC improvement that can be achieved by hierarchical reservoir models can be observed not only at a global scale, i.e. considering the entire network’s global dynamics, but also at the level of individual layers’ dynamics at increasing height in the architecture.

$<math><mo is="true">•</mo></math>$

*Efficiency.* From an architectural view-point, introducing a [layered reservoir](https://www.sciencedirect.com/topics/engineering/layered-reservoir "Learn more about layered reservoir from ScienceDirect's AI-generated Topic Pages") construction into the architecturalRNN/ESN design has also the effect of reducing the number of non-zero recurrent connections. This contributes to a number of inherent implications on the characterization of the developed system dynamics, as discussed in the previous points. Besides, layering turns out to be a striking advantageous setting also under the perspective of computational efficiency [(Gallicchio & Micheli, 2018)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b13), which in an RC context sums to the well-known efficiency of training algorithms. In particular, under the condition of a total number of recurrent units given by $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mspace width="0.16667em" is="true"></mspace><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub></math>$, while the cost of each state update for shallow ESNs (Eq. [(1)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd1)) amounts to $<math><mi mathvariant="script" is="true">O</mi><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mspace width="0.16667em" is="true"></mspace><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo is="true">)</mo></mrow></math>$, in the case of DeepESNs (Eqs. [(5)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd5) and [(6)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd6)) it is reduced, by the [sparsity](https://www.sciencedirect.com/topics/engineering/sparsity "Learn more about sparsity from ScienceDirect's AI-generated Topic Pages") of the connectivity given by the layering constraints, to $<math><mi mathvariant="script" is="true">O</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo is="true">)</mo></mrow></math>$. This straightforwardly implies a saving in the time complexity required by the reservoir operation that scales with the number of layers in which the same number of units is organized.

Finally, it is worth noticing that the use of IP in combination with deep reservoir organizations generally resulted effective in improving DeepESN performance under several aspects, as briefly recalled in the above points. In light of this observation, in this paper we use DeepESNs in synergy with the IP adaptation technique.

## 3\. Spectral analysis and depth of DeepESN

In the following sections, we introduce our approach based on the spectral analysis for automatically determining the depth of DeepESN architectures. First, in Section [3.1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3.1) we define the iterative algorithm, then in Section [3.2](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3.2) we discuss its advantages with respect to a standard cross-validation approach in terms of computational costs. In Section [3.3](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3.3) we assess our method on a controlled scenario. Finally, in Section [3.4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3.4) we experimentally analyze the role of the depth in deep [RNNs](https://www.sciencedirect.com/topics/engineering/recurrent-neural-network "Learn more about RNNs from ScienceDirect's AI-generated Topic Pages") using spectral analysis.

### 3.1. Method

In this section, we define an automatic algorithm, based on spectral analysis, to determine the depth of DeepESN architectures in which every layer encodes a different range of time-scales dynamics. Recent research findings [Gallicchio, Micheli, and Pedrelli, 2017](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14), [Gallicchio et al., 2019](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b15) (briefly discussed in Section [2.2](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec2.2)) showed that a stack of [recurrent](https://www.sciencedirect.com/topics/engineering/recurrent "Learn more about recurrent from ScienceDirect's AI-generated Topic Pages") layers can develop a multiple time-scales differentiation among the layers even considering the same value of the leaky rate for each recurrent layer and unit. These considerations allowed us to define a simple design method based on [building blocks](https://www.sciencedirect.com/topics/computer-science/building-blocks "Learn more about building blocks from ScienceDirect's AI-generated Topic Pages") (recurrent layers with same hyper-parameters) used to build up the deep architecture. As it is known, a recurrent layer can be studied as a filter [Holzmann and Hauser, 2010](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b26), [Jaeger et al., 2007](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b32), [Lukoševičius, 2012](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b34), [Wyffels et al., 2008](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b56). However, differently from filter design, in which the purpose is to design a filter with a specific cut-off frequency, in our work we aim to exploit the richness of dynamics represented in the state of each recurrent layer through the training of the readout component. In such a way, the output layer can adaptively choose a proper modulation of time-scales on the basis of the characteristics of the supervised task to solve. Therefore, the main idea behind the proposed automatic method for network design is to stop adding new layers whenever the filtering effects become negligible, i.e. when adding new layers essentially does not enrich anymore the multiplicity of temporal dynamics developed by the reservoir states.

In order to determine when the filtering effect becomes negligible, we perform a spectral analysis by computing the [spectral centroid](https://www.sciencedirect.com/topics/engineering/spectral-centroid "Learn more about spectral centroid from ScienceDirect's AI-generated Topic Pages") and the spectral spread (defined below in Eqs. [(10)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd10), [(11)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd11)) on the state signal of layers. Intuitively, they represent weighted average and bandwidth of frequency spectrum values computed on the state of recurrent layers over time. Spectral centroid tends to converge to a certain value as we add recurrent layers (further details in Section [3.4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3.4)). Therefore, we define a *stop condition* of the iterative algorithm to detect when the shift of spectral centroid converges: (9)$<math><mrow is="true"><mo is="true">|</mo><msup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">−</mo><msup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup><mo is="true">|</mo></mrow><mo is="true">≤</mo><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup><mi is="true">η</mi></math>$ where $<math><mn is="true">0</mn><mo is="true">&lt;</mo><mi is="true">η</mi><mo is="true">&lt;</mo><mn is="true">1</mn></math>$ (see Section [3.4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3.4) for details regarding the $<math><mi is="true">η</mi></math>$ value). $<math><msup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$ and $<math><msup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup></math>$ are the spectral centroid computed on state of layers $<math><mi is="true">l</mi></math>$ and $<math><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn></math>$ respectively, and $<math><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup></math>$ is the spectral spread computed on the state of layer $<math><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn></math>$. On the right-hand side of Eq. [(9)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd9), the $<math><mi is="true">η</mi></math>$ value is multiplied by $<math><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></msup></math>$ in order to take into account also the bandwidth of the spectrum.

In formulas, the spectral centroid (Eq. [(10)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd10)) and the spectral spread (Eq. [(11)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd11)) are defined as follows: (10)$<math><msup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">k</mi></mrow></munderover><msubsup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msubsup><msubsup is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msubsup><mo is="true">)</mo></mrow><mo is="true">∕</mo><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">k</mi></mrow></munderover><msubsup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msubsup></math>$ (11)$<math><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">=</mo><mroot is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">k</mi></mrow></munderover><msubsup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msubsup><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msubsup><mo is="true">−</mo><msup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">)</mo></mrow></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">)</mo></mrow><mo is="true">∕</mo><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">k</mi></mrow></munderover><msubsup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msubsup></mrow><mrow is="true"></mrow></mroot><mo is="true">,</mo></math>$where $<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">f</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">=</mo><mrow is="true"><mo is="true">[</mo><msubsup is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msubsup><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msubsup><mo is="true">]</mo></mrow></math>$ and $<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">p</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup><mo is="true">=</mo><mrow is="true"><mo is="true">[</mo><msubsup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msubsup><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msubsup><mo is="true">]</mo></mrow></math>$ respectively denote the normalized frequencies and the corresponding magnitudes of components, computed over time on the state of layer $<math><mi is="true">l</mi></math>$, i.e. $<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">x</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$, by the [FFT](https://www.sciencedirect.com/topics/mathematics/fft "Learn more about FFT from ScienceDirect's AI-generated Topic Pages") algorithm, whereas $<math><mi is="true">k</mi></math>$ is the number of frequency components (i.e., the length of both vectors $<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">p</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$ and $<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">f</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$). More in detail, the steps performed for the computation of $<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">f</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$ and $<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">p</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$ are detailed in Algorithm 1.

![](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx1.jpg)

1.  [Download : Download high-res image (209KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx1_lrg.jpg "Download high-res image (209KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx1.jpg "Download full-size image")

Algorithm 1 computes the frequency components for each unit of each reservoir guess. For each layer $<math><mi is="true">l</mi></math>$, terms $<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">p</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$ are obtained by averaging over the reservoir units and guesses. Depending on the number of time-steps, terms $<math><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">f</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$ are computed in order to have normalized frequency components measured as cycles/seconds.

Here we assume to consider a standard model selection process in which for each hyper-parameterization a certain number of reservoir guesses are instantiated (with different random initialization). Given a configuration of hyper-parameters of the model, denoted by $<math><mi is="true">θ</mi></math>$, the [design Algorithm](https://www.sciencedirect.com/topics/computer-science/algorithm-design "Learn more about design Algorithm from ScienceDirect's AI-generated Topic Pages") 2 selects a number of layers for the network’s architecture. The function $<math><mi is="true">c</mi><mi is="true">o</mi><mi is="true">m</mi><mi is="true">p</mi><mi is="true">u</mi><mi is="true">t</mi><mi is="true">e</mi><mi is="true">S</mi><mi is="true">t</mi><mi is="true">a</mi><mi is="true">t</mi><mi is="true">e</mi><mrow is="true"><mo is="true">(</mo><mo is="true">)</mo></mrow></math>$ called inside Algorithm 2, is composed by two steps, first, the IP Adaptation is performed (see Eq. [(4)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd4)) over the layers and, second, the state of the network is computed and returned. Finally, the number of layers is calculated before training the readout, incrementally considering new layers of recurrent units in the architecture until the stop condition in line 8 of Algorithm 2 (i.e., Eq. [(9)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd9)) is satisfied or the max number of layers (i.e., $<math><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></math>$) is reached.

![](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx2.jpg)

1.  [Download : Download high-res image (113KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx2_lrg.jpg "Download high-res image (113KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx2.jpg "Download full-size image")

### 3.2. Analysis of the computational cost

Typically, the number of layers in a deep (RNN) architecture is selected on a validation set through a cross-validation approach that results in an extremely expensive procedure from the computational point of view [(Angelov & Sperduti, 2016)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b1). The aim of the analysis provided in this sub-section is to quantify the advantage, in terms of computational cost, of the use of the proposed design algorithm for deep recurrent models with respect to a basic cross-validation approach.

In the case of deep recurrent models, a typical systematic procedure to choose the number of layers consists in training each hyper-parameterization of the network on a training set for every number of layers considered, i.e. using networks with a number of layers from 1 to a maximum number of layers $<math><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></math>$. The number of layers is then chosen (along with the other hyper-parameters) to maximize the performance achieved on a validation set. The basic cross-fold validation procedure, considering a number of $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">f</mi></mrow></msub></math>$ folds and $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">θ</mi></mrow></msub></math>$ hyper-parameterizations, is summarized in Algorithm 3.

![](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx3.jpg)

1.  [Download : Download high-res image (84KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx3_lrg.jpg "Download high-res image (84KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx3.jpg "Download full-size image")

![](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx4.jpg)

1.  [Download : Download high-res image (87KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx4_lrg.jpg "Download high-res image (87KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-fx4.jpg "Download full-size image")

For each hyper-parameterization $<math><mi is="true">θ</mi></math>$ and fold f, the cost of such procedure is given by: (12)$<math><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">d</mi><mi is="true">e</mi><mi is="true">e</mi><mi is="true">p</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></mrow></munderover><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">t</mi><mi is="true">r</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">,</mo></math>$where $<math><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">t</mi><mi is="true">r</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow></math>$ is the cost of training a deep recurrent architecture with $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></math>$ recurrent layers.

Within the context of the deep RC paradigm, considered in this paper, the readout layer is the only part of the network that is trained. Accordingly, for DeepESNs the training cost in Algorithm 3, i.e. the $<math><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">t</mi><mi is="true">r</mi></mrow></msub></math>$ term in Eq. [(12)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd12), is determined by the cost of training the readout. Considering a DeepESN with $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></math>$ layers, a training set with $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub></math>$ time-steps, and adopting a typical direct method based on [SVD](https://www.sciencedirect.com/topics/engineering/singular-value-decomposition "Learn more about SVD from ScienceDirect's AI-generated Topic Pages") for choosing the readout’s weights, the training cost is given by: (13)$<math><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">t</mi><mi is="true">r</mi><mtext is="true">_</mtext><mi is="true">r</mi><mi is="true">c</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mi mathvariant="script" is="true">O</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">.</mo></math>$Therefore, using the right-hand side of Eq. [(13)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd13) as training cost in Eq. [(12)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd12), we can see that the process of choosing the number of layers (for each hyper-parameterization $<math><mi is="true">θ</mi></math>$ and fold f) using the basic procedure described by Algorithm 3 is given by: (14)$<math><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">b</mi><mi is="true">a</mi><mi is="true">s</mi><mi is="true">i</mi><mi is="true">c</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></mrow></munderover><mi mathvariant="script" is="true">O</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mi mathvariant="script" is="true">O</mi><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><msubsup is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">.</mo></math>$

Compared to the basic procedure explained above, the design methodology proposed in this paper allows to restrict the number of cases for which training is applied, taking into account only the number of layers that are selected by the design Algorithm 2. The resulting selection process is illustrated in Algorithm 4.

We can note that the cost of Algorithm 2is dominated by the cost of performing the FFT, given by: (15)$<math><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">f</mi><mi is="true">f</mi><mi is="true">t</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mi mathvariant="script" is="true">O</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mspace width="0.16667em" is="true"></mspace><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mspace width="0.16667em" is="true"></mspace><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">V</mi></mrow></msub><mi is="true">l</mi><mi is="true">o</mi><mi is="true">g</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">V</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">,</mo></math>$where $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">V</mi></mrow></msub></math>$ is the size of the validation set. Overall, the cost entailed by the proposed Algorithm 4 is determined by the sum between $<math><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">f</mi><mi is="true">f</mi><mi is="true">t</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow></math>$ in Eq. [(15)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd15), which is linear in the total number of recurrent units $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mspace width="0.16667em" is="true"></mspace><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></math>$, and the cost $<math><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">t</mi><mi is="true">r</mi><mtext is="true">_</mtext><mi is="true">r</mi><mi is="true">c</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow></math>$ in Eq. [(13)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd13), which is quadratic in $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></math>$. Accordingly, reducing the total cost to the sole cost of the dominant operation (i.e., training the readout), the cost of Algorithm 4 (for each hyper-parameterization $<math><mi is="true">θ</mi></math>$ and fold f) can be expressed as follows: (16)$<math><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">d</mi><mi is="true">e</mi><mi is="true">s</mi><mi is="true">i</mi><mi is="true">g</mi><mi is="true">n</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mi mathvariant="script" is="true">O</mi><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><msubsup is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">.</mo></math>$

Overall, comparing $<math><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">d</mi><mi is="true">e</mi><mi is="true">s</mi><mi is="true">i</mi><mi is="true">g</mi><mi is="true">n</mi></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">C</mi></mrow><mrow is="true"><mi is="true">b</mi><mi is="true">a</mi><mi is="true">s</mi><mi is="true">i</mi><mi is="true">c</mi></mrow></msub></math>$ emerges that the cross-validation procedure that makes use of our proposed design algorithm leads to a clear reduction of the cost (per fold and per hyper-parameterization) that scales with the number of layers $<math><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></math>$. Notice that these benefits are even more enhanced when the number of hyper-parameterizations ($<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">θ</mi></mrow></msub></math>$) and folds ($<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">f</mi></mrow></msub></math>$) is considered in the total cost of the selection procedure. In such case, the reduction of the cost amounts to $<math><mi mathvariant="script" is="true">O</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">θ</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">f</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">)</mo></mrow></math>$, which could be quite high both considering deep networks and the typical high dimension of the hyper-parameter space in the RC applications.

### 3.3. Design experiments in a controlled scenario

In this section, we define an artificial task called Frequency Based [Classification](https://www.sciencedirect.com/topics/computer-science/classification "Learn more about Classification from ScienceDirect's AI-generated Topic Pages") (FBC) characterized by signals with multiple time-scales dynamics in order to analyze and refine our method on a controlled scenario. Accordingly, we consider an input sequence $<math><mi mathvariant="bold" is="true">s</mi></math>$ formed by a random concatenation of elements that belong to subsequences $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ or $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub></math>$. The subsequence $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ contains a sum of impulse trains with periods from $<math><mn is="true">3</mn></math>$ to $<math><mn is="true">29</mn></math>$, while $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub></math>$ contains trains with periods from $<math><mn is="true">3</mn></math>$ to $<math><mn is="true">31</mn></math>$. The [classification task](https://www.sciencedirect.com/topics/engineering/classification-task "Learn more about classification task from ScienceDirect's AI-generated Topic Pages") consists in determining, at each time-step $<math><mi is="true">t</mi></math>$, if the element $<math><mi mathvariant="bold" is="true">s</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$ is equal to $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$ or $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$. The FBC dataset is made publicly available for download.[1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fn1)

In formulas, let $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">signal</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math>$ be an impulse train with period $<math><mi is="true">i</mi></math>$ such that the element $<math><mi is="true">t</mi></math>$ of the signal (i.e., $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">signal</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$) is 1 every period $<math><mi is="true">i</mi></math>$ and 0 otherwise. The sequence $<math><mi mathvariant="bold" is="true">s</mi></math>$ is defined as follows: (17)$<math><mi mathvariant="bold" is="true">s</mi><mo is="true">=</mo><mrow is="true"><mo is="true">[</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mn is="true">0</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></msub><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">n</mi></mrow></msub></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">]</mo></mrow></math>$where $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub></mrow></msub><mo is="true">∈</mo><mrow is="true"><mo is="true">{</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo is="true">}</mo></mrow></math>$, $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">=</mo><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">3</mn></mrow><mrow is="true"><mn is="true">29</mn></mrow></msubsup><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">signal</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo is="true">=</mo><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">3</mn></mrow><mrow is="true"><mn is="true">31</mn></mrow></msubsup><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">signal</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math>$. Moreover, the index $<math><msub is="true"><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo is="true">∈</mo><mrow is="true"><mo is="true">{</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">}</mo></mrow></math>$ is different from $<math><msub is="true"><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></msub><mo is="true">∈</mo><mrow is="true"><mo is="true">{</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">}</mo></mrow></math>$ with a probability of 0.01, in formulas: (18)$<math><msub is="true"><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">t</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo is="true">=</mo><mfenced open="{" close="" is="true"><mrow is="true"><mtable align="axis" class="array" is="true"><mtr is="true"><mtd columnalign="left" is="true"><msub is="true"><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo is="true">+</mo><mn is="true">1</mn><mspace width="0.16667em" is="true"></mspace><mspace width="0.16667em" is="true"></mspace><mi is="true">m</mi><mi is="true">o</mi><mi is="true">d</mi><mspace width="0.16667em" is="true"></mspace><mspace width="0.16667em" is="true"></mspace><mn is="true">2</mn></mtd><mtd columnalign="left" is="true"><mtext is="true">with probability</mtext><mspace width="0.16667em" is="true"></mspace><mspace width="0.16667em" is="true"></mspace><mn is="true">0</mn><mo is="true">.</mo><mn is="true">01</mn></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><msub is="true"><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub></mtd><mtd columnalign="left" is="true"><mtext is="true">otherwise.</mtext></mtd></mtr></mtable></mrow></mfenced></math>$[Fig. 2](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig2) shows an excerpt of the sequence $<math><mi mathvariant="bold" is="true">s</mi></math>$. The continuous lines and the dashed lines indicate that the element $<math><mi mathvariant="bold" is="true">s</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$ belongs to $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ or $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub></math>$, respectively. Consider that the probability (defined in Eq. [(18)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd18)) to have a switch between subsequences $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub></math>$ is low, for instance in [Fig. 2](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig2), this happens only in time-steps, 1107, 1122 and 1194. Therefore, sequence $<math><mi mathvariant="bold" is="true">s</mi></math>$ tends to have long ranges with elements of the same subsequence $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math>$. In particular, in the generated sequence $<math><mi mathvariant="bold" is="true">s</mi></math>$ for this task, the average number of time-steps of such ranges is 88.2. Note that the information involved in a single element $<math><mi mathvariant="bold" is="true">s</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$ is not sufficient to discriminate the elements $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$ and $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$. Therefore, the capacity of the model in representing temporal dynamics of the past of $<math><mi mathvariant="bold" is="true">s</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$ is relevant to perform the correct classification.

The dataset contains a total number of $<math><mn is="true">6000</mn></math>$ time-steps, and it is split such that the first $<math><mn is="true">2000</mn></math>$ time-steps are used for training, the following $<math><mn is="true">2000</mn></math>$ for validation and the last $<math><mn is="true">2000</mn></math>$ for test, while the first $<math><mn is="true">20</mn></math>$ time-steps of the training set are used for the washout phase. The $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">y</mi></mrow><mrow is="true"><mi is="true">t</mi><mi is="true">a</mi><mi is="true">r</mi><mi is="true">g</mi><mi is="true">e</mi><mi is="true">t</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$ target, contained in the labeled dataset $<math><msubsup is="true"><mrow is="true"><mrow is="true"><mo is="true">{</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">y</mi></mrow><mrow is="true"><mi is="true">t</mi><mi is="true">a</mi><mi is="true">r</mi><mi is="true">g</mi><mi is="true">e</mi><mi is="true">t</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><mi mathvariant="bold" is="true">s</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">}</mo></mrow></mrow><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">6000</mn></mrow></msubsup></math>$, is defined as follows: $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">y</mi></mrow><mrow is="true"><mi is="true">t</mi><mi is="true">a</mi><mi is="true">r</mi><mi is="true">g</mi><mi is="true">e</mi><mi is="true">t</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mfenced open="{" close="" is="true"><mrow is="true"><mtable align="axis" class="array" is="true"><mtr is="true"><mtd columnalign="left" is="true"><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mspace width="0.16667em" is="true"></mspace><mn is="true">1</mn><mo is="true">]</mo></mrow></mrow><mrow is="true"><mi is="true">T</mi></mrow></msup></mtd><mtd columnalign="left" is="true"><mtext is="true">if</mtext><mspace width="0.16667em" is="true"></mspace><mspace width="0.16667em" is="true"></mspace><mspace width="0.16667em" is="true"></mspace><mi mathvariant="bold" is="true">s</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">[</mo><mn is="true">1</mn><mspace width="0.16667em" is="true"></mspace><mn is="true">0</mn><mo is="true">]</mo></mrow></mrow><mrow is="true"><mi is="true">T</mi></mrow></msup></mtd><mtd columnalign="left" is="true"><mtext is="true">if</mtext><mspace width="0.16667em" is="true"></mspace><mspace width="0.16667em" is="true"></mspace><mspace width="0.16667em" is="true"></mspace><mi mathvariant="bold" is="true">s</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">.</mo></mtd></mtr></mtable></mrow></mfenced></math>$

![](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr2.jpg)

1.  [Download : Download high-res image (319KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr2_lrg.jpg "Download high-res image (319KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr2.jpg "Download full-size image")

Fig. 2. A 300 time-step long excerpt of the sequence **s** for the [FBC task](https://www.sciencedirect.com/topics/engineering/classification-task "Learn more about FBC task from ScienceDirect's AI-generated Topic Pages").

The performance on the FBC task was evaluated in terms of [classification accuracy](https://www.sciencedirect.com/topics/engineering/classification-accuracy "Learn more about classification accuracy from ScienceDirect's AI-generated Topic Pages") (ACC), i.e., the percentage of correctly classified elements of the sequence $<math><mi mathvariant="bold" is="true">s</mi></math>$. We applied the design algorithm proposed in Section [3.1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3.1) to determine the number of DeepESN recurrent layers, considering the network’s hyper-parameterizations as specified by the ranges reported in [Table 1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#tbl1). We fixed the same number of recurrent units (i.e., $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">=</mo><mn is="true">100</mn></math>$) in each layer in order to have a comparable frequency spectrum of the temporal dynamics among layers’ states. Moreover, the recurrent layers considered have a small/medium number of units because our aim is to consider them as a sort of building blocks for the DeepESN architecture. Finally, we performed model selection on the validation set, using for each hyper-parameterization the number of layers computed by the design algorithm. For each reservoir hyper-parameterization, we independently generated 10 reservoir guesses, and the [predictive performance](https://www.sciencedirect.com/topics/computer-science/predictive-performance "Learn more about predictive performance from ScienceDirect's AI-generated Topic Pages") in the different cases has been averaged over such guesses. Moreover, the proposed approach was compared with a shallowESN (a DeepESN model with one recurrent layer). Each model is individually optimized with grid search on hyper-parameters values as specified in [Table 1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#tbl1), and on a range of total recurrent units in $<math><mrow is="true"><mo is="true">{</mo><mn is="true">100</mn><mo is="true">,</mo><mn is="true">200</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mn is="true">2000</mn><mo is="true">}</mo></mrow></math>$. We adopted an experimental setting in which the hyper-parameter values are the same for all layers, in particular, $<math><msup is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$ and $<math><msup is="true"><mrow is="true"><mi is="true">ρ</mi></mrow><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">l</mi><mo is="true">)</mo></mrow></mrow></msup></math>$ are the same for every layer $<math><mi is="true">l</mi></math>$. Note that the models are compared on a wide range of hyper-parameters: this range is large respect to the standard ranges used in literature for ESN and it is suitable to optimize both shallow and deep configuration without specific bias. The training of free parameters has been performed by means of ridge-regression with [regularization term](https://www.sciencedirect.com/topics/computer-science/regularization-term "Learn more about regularization term from ScienceDirect's AI-generated Topic Pages") $<math><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></math>$. For determining the depth of the DeepESN we used Algorithm 2 with $<math><mi is="true">η</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">01</mn></math>$ The choice of such $<math><mi is="true">η</mi></math>$ value is motivated and discussed in Section [3.4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3.4).

[Table 2](https://www.sciencedirect.com/science/article/pii/S0893608018302223#tbl2) shows the training, validation and test classification accuracy achieved by the DeepESN model with the optimal number of recurrent layers obtained by the design algorithm, compared with the accuracy achieved by the shallowESN model.

Table 1. Range of DeepESN and shallowESN hyper-parameters values for model selection in the considered tasks.

| Hyper-parameter | Empty Cell |
| --- | --- |
| Readout regularization $<math><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></math>$ | $<math><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">15</mn></mrow></msup><mo is="true">,</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">14</mn></mrow></msup><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mn is="true">0</mn></mrow></msup></math>$ |
| Input and inter-layer scaling $<math><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub></math>$ | 0.01, 0.1, 1, 10 |
| Leaking rate $<math><mi is="true">a</mi></math>$ | 0.1, 0.3, 0.5, 0.7, 0.9, 1.0 |
| Spectral radius $<math><mi is="true">ρ</mi></math>$ | 0.1, 0.3, 0.5, 0.7, 0.9, 1.0 |
| IP standard deviation $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub></math>$ | $<math><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn></math>$ |

The hyper-parameters that obtained the best performance on the validation set are $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">=</mo><mn is="true">7</mn></math>$ (by the design algorithm), $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">=</mo><mn is="true">100</mn></math>$, $<math><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">9</mn></math>$, $<math><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$, $<math><mi is="true">a</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">01</mn></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">10</mn></mrow></msup></math>$ for DeepESN and $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">=</mo><mn is="true">300</mn></math>$, $<math><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">9</mn></math>$, $<math><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></math>$, $<math><mi is="true">a</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">01</mn></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">7</mn></mrow></msup></math>$ for shallow ESN. Results in [Table 2](https://www.sciencedirect.com/science/article/pii/S0893608018302223#tbl2) show that DeepESN outperforms shallowESN on both validation and test sets, with an accuracy improvement of $<math><mn is="true">5</mn><mo is="true">.</mo><mn is="true">63</mn><mtext is="true">%</mtext></math>$ and $<math><mn is="true">2</mn><mo is="true">.</mo><mn is="true">46</mn><mtext is="true">%</mtext></math>$, respectively. It is worth to note that, noticeably, shallowESN obtains worse results on validation and test sets than DeepESN despite the difference between the two models is due to the number of layers in which the recurrent units are arranged, in condition of equal range of possible values of hyper-parameters for model selection. This is an instance of the fact that choosing the proper number of recurrent layers in a hierarchical architecture can play a big role in the representation of multiple time-scales dynamics involved in complex signals, with an effect also on the accuracy.

Table 2. Training (TR), validation (VL) and test (TS) [classification accuracy](https://www.sciencedirect.com/topics/engineering/classification-accuracy "Learn more about classification accuracy from ScienceDirect's AI-generated Topic Pages") (ACC) obtained by DeepESN and shallowESN on the [FBC task](https://www.sciencedirect.com/topics/engineering/classification-task "Learn more about FBC task from ScienceDirect's AI-generated Topic Pages").

| Model | TR ACC | VL ACC | TS ACC |
| --- | --- | --- | --- |
| DeepESN | **83.37 (0.0019)** **%** | **83.00 (0.0017)** **%** | **81.77 (0.0056)** **%** |
| shallowESN | 82.22 (0.0065)% | 77.37 (0.0196)% | 79.31 (0.0067)% |

In order to evaluate the quality of the proposed design algorithm in the selection of the number of layers, we compared the performance obtained by our approach with the results achieved using a DeepESN with a number of layers that goes from 1 to 20. [Fig. 3](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig3) shows the [classification errors](https://www.sciencedirect.com/topics/engineering/classification-error "Learn more about classification errors from ScienceDirect's AI-generated Topic Pages") ($<math><mo is="true">=</mo><mn is="true">100</mn><mspace width="0.16667em" is="true"></mspace><mo is="true">−</mo></math>$ACC %) obtained on the validation set by DeepESN considering a progressively larger number of layers. In this case, for the sake of analysis, the readout is trained for each configuration to show the comparison.

In [Fig. 3](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig3), the marker ‘X’ represents the number of layers selected by the design algorithm (7 in our case-study). Noteworthy, the design method added the optimal number of layers in the hierarchy, beside filtering aspects, also with respect to the final accuracy of the global model, i.e. allowing to obtain the lowest error among the considered configurations. In the studied case, these results show that the developed approach allowed us to accurately select the number of layers by just analyzing the time-scales diversification among the temporal dynamics developed in the hierarchy, avoiding to perform the training algorithm for each possible number of recurrent layers.

![](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr3.jpg)

1.  [Download : Download high-res image (115KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr3_lrg.jpg "Download high-res image (115KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr3.jpg "Download full-size image")

Fig. 3. [Classification error](https://www.sciencedirect.com/topics/engineering/classification-error "Learn more about Classification error from ScienceDirect's AI-generated Topic Pages") obtained on the validation set of the [FBC task](https://www.sciencedirect.com/topics/engineering/classification-task "Learn more about FBC task from ScienceDirect's AI-generated Topic Pages") by DeepESN architectures with a number of [recurrent](https://www.sciencedirect.com/topics/engineering/recurrent "Learn more about recurrent from ScienceDirect's AI-generated Topic Pages") layers up to 20. Results are obtained through model selection individually performed for each number of layers. The marker ‘X’ indicates the number of recurrent layers selected by the [design algorithm](https://www.sciencedirect.com/topics/computer-science/algorithm-design "Learn more about design algorithm from ScienceDirect's AI-generated Topic Pages").

The qualitative aspects of the temporal representation encoded in the state dynamics of the recurrent layers is further investigated by means of frequency analysis. [Fig. 4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig4) (a), (b), (c) and (d) show the frequency components computed over time on the state of layers 1, 3, 5 and 7 respectively.

Note that the frequency components of the impulse trains ($<math><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">3</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">4</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">5</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo></math>$ $<math><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">31</mn></math>$) that have smaller frequencies havesmaller magnitudes. In this task, the frequencies that discriminate $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub></math>$ from $<math><msub is="true"><mrow is="true"><mi mathvariant="bold" is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></math>$ are $<math><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">31</mn></math>$ and $<math><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">30</mn></math>$, represented by the azure and red vertical lines on the left of [Fig. 4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig4) (a), (b), (c) and (d). In [Fig. 4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig4) (a), the components around frequencies $<math><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">31</mn></math>$ and $<math><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">30</mn></math>$ have almost null magnitudes. Therefore, it is more difficult to perform the classification task by a 1-layered model, as indeed showed by the lower performance achieved by shallowESN on the task. In higher layers, instead, the frequency components around $<math><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">31</mn></math>$ and $<math><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">30</mn></math>$ are progressively more visible, showing the filtering effect that naturally results in the state computation taking place in successive layers of the architecture. The red range in [4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig4) (b), (c) and (d) represents the difference of frequency mean (i.e., the shift of the spectral centroid) between the current layer and the previous layer (the measure defined on the left side of Eq. [(9)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd9)) amplified by a factor of 10 in order to make it visually more clear. We can see that the frequency mean, represented by the marker ‘X’ in [4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig4) (b), (c) and (d), shifts progressively to the left in the higher layers towards low-frequency components. At layer 7 the condition in Eq. [(9)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd9) is satisfied and the design algorithm stops adding new recurrent layers. In this regard, it is also worth to note that, in the case illustrated in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig4), the magnitude of the frequency components, e.g. as measured in a range of 0.01 cyc/s around the vertical line indicated in the plots in correspondence of the frequency 1/31, in layer 7 are quantitatively amplified by more than 500% in comparison to layer 1. Given the quantitative and the qualitative results described so far, we can say that differently from the case of shallowESN, in the higher layers of the DeepESN reservoir hierarchy, the information of the frequency components around $<math><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">31</mn></math>$ and $<math><mn is="true">1</mn><mo is="true">∕</mo><mn is="true">30</mn></math>$ becomes more easily accessible and can be exploited in order to improve the classification accuracy. Overall, in the analyzed scenario, the proposed design algorithm allows to choose a proper number of recurrent layers, ensuring a rich representation of the input history and at the same time guaranteeing the time-scale differentiation in the hierarchy avoiding to consider further recurrent layers with similar dynamics.

![](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr4.jpg)

1.  [Download : Download high-res image (411KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr4_lrg.jpg "Download high-res image (411KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr4.jpg "Download full-size image")

Fig. 4. Frequency components computed over time on reservoir states, encoding the sequence defined in Eq. [(17)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd17), in progressively higher layers of DeepESN. (a) layer 1, (b) layer 3, (c) layer 5, (d) layer 7. The red range represents the shift of [spectral centroid](https://www.sciencedirect.com/topics/engineering/spectral-centroid "Learn more about spectral centroid from ScienceDirect's AI-generated Topic Pages") between current and previous layer, multiplied by a factor of 10. Normalized frequency is expressed in cycles per second (cyc/s). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

### 3.4. Experimental analysis of depth in the controlled scenario

In this section, we empirically evaluate how the spectral centroid, computed by Algorithm 1 over the layers, varies with the depth of the stacked recurrent architecture. This allows us to assess the considered $<math><mi is="true">η</mi></math>$ value and to evaluate the effectiveness of the Algorithm 2 and the stop condition defined in Eq. [(9)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd9).

A recurrent layer can be studied as a (low/high/band pass) filter [Holzmann and Hauser, 2010](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b26), [Jaeger et al., 2007](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b32), [Lukoševičius, 2012](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b34), [Wyffels et al., 2008](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b56). The effect of the filter determines a cut-off frequency and a roll-off, which tells how the frequency magnitude decreases for increasing (resp. decreasing) frequencies after (resp. before) the cut-off frequency in a low (resp. high) pass filter. Hence, a deep RNN architecture, such is DeepESN, operates as a stack of progressively applied filters. Stacking progressively more filters, i.e. adding layers to a DeepESN, leads the roll-off value to converge towards the cut-off frequency [(Jacob, 2004)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b27), which entails that the spectral centroid converges to a certain [asymptotic value](https://www.sciencedirect.com/topics/engineering/asymptotic-value "Learn more about asymptotic value from ScienceDirect's AI-generated Topic Pages"). Hence, when a new layer is added, its filtering effect in terms of shifting the spectral centroid of state signals progressively decreases.

In this context, the purpose of the proposed automatic method for network design is to stop adding new layers whenever the filtering effects become negligible, i.e. when adding new layers essentially does not enrich anymore the multiplicity of temporal dynamics developed by the reservoir states. Here, the role of the $<math><mi is="true">η</mi></math>$ parameter in Eq. [(9)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fd9) is to practically terminate the process of adding layers when the spectral centroid is close enough to its convergence point. Since the convergence can be asymptotic or some numerical errors can lead to small fluctuations in the shift of the spectral centroid computed through layers, the $<math><mi is="true">η</mi></math>$ value should not be 0. However, in order to reach a point that is close enough to the convergence, the $<math><mi is="true">η</mi></math>$ value should be sufficiently small. Overall, the choice of $<math><mi is="true">η</mi></math>$ stems from a reasonable trade-off between such conditions (as typical in any iterative algorithm with [asymptotic behavior](https://www.sciencedirect.com/topics/mathematics/asymptotic-behaviour "Learn more about asymptotic behavior from ScienceDirect's AI-generated Topic Pages"), which does not lose generality with a termination cut-off condition). Empirically, we observed that in all considered tasks a value of $<math><mi is="true">η</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">01</mn></math>$ meets this trade-off, being large enough to avoid chasing the [asymptote](https://www.sciencedirect.com/topics/engineering/asymptote "Learn more about asymptote from ScienceDirect's AI-generated Topic Pages") (or following the small fluctuations), and at the same time being sufficiently small to reach a point near the convergence. In order to empirically verify the soundness of this choice, we conducted several experimental evaluations on the FBC task, in this section, and on the real-world tasks (described in Sections [4.1 Polyphonic music tasks](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec4.1), [4.2 Speech recognition](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec4.2)), in [Appendix](https://www.sciencedirect.com/science/article/pii/S0893608018302223#app).

[Fig. 5](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig5) shows the trend of the spectral centroid obtained from the state of each recurrent layer of the DeepESN, optimized on the FBC task and considering the value of $<math><mi is="true">η</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">01</mn></math>$. The red vertical line represents the number of selected layers.

As expected, the layers progressively apply a filter to the signal. In particular, in the case represented in [Fig. 5](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig5), the resulting effect is that of a cascade of [low pass filters](https://www.sciencedirect.com/topics/engineering/lowpass-filter "Learn more about low pass filters from ScienceDirect's AI-generated Topic Pages"), with a decreasing trend of the spectral centroid as more layers are taken into account. More in general, the specific trend of this behavior might depend on the combined effect of the reservoir hyper-parameters, IP adaptation and characteristics of the input signal (on the considered real-world tasks both the cases of low pass filters and high pass filters are found — see [Appendix](https://www.sciencedirect.com/science/article/pii/S0893608018302223#app)). Moreover, as we can see in [Fig. 5](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig5) for the considered case, the effect of filtering clearly tends to decrease, with the shift of the spectral centroid approaching convergence as the depth of the recurrent architecture increases. Interestingly, in the case of FBC task convergence is achieved on the 7th layer, i.e. in correspondence to the optimal number of layers found for the task (see [Fig. 3](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig3)). Furthermore, from [Fig. 5](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig5), we note that, after convergence, some small fluctuations of the spectral centroid can be observed, which can be due to possible numerical errors of FFT operations and to the characteristics of the input signals (e.g., for the MuseData and Piano-midi.de tasks the convergence is smoother — see [Appendix](https://www.sciencedirect.com/science/article/pii/S0893608018302223#app)). In this respect, the adoption of $<math><mi is="true">η</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">01</mn></math>$ empirically shows to be a safe threshold value that is not too small to follow the fluctuations neither too large to determine an insufficient depth (too far from the convergence). Apart from the specific trend of the spectral centroid for increasing number of layers, the same qualitative considerations made here generally apply also for the real-world tasks considered in this paper, as reported more in detail in [Appendix](https://www.sciencedirect.com/science/article/pii/S0893608018302223#app).

![](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr5.jpg)

1.  [Download : Download high-res image (127KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr5_lrg.jpg "Download high-res image (127KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr5.jpg "Download full-size image")

Fig. 5. [Spectral centroid](https://www.sciencedirect.com/topics/engineering/spectral-centroid "Learn more about Spectral centroid from ScienceDirect's AI-generated Topic Pages") computed on the state of DeepESN layers optimized on [FBC task](https://www.sciencedirect.com/topics/engineering/classification-task "Learn more about FBC task from ScienceDirect's AI-generated Topic Pages"). The red vertical line indicates the number of layers selected by the [design algorithm](https://www.sciencedirect.com/topics/computer-science/algorithm-design "Learn more about design algorithm from ScienceDirect's AI-generated Topic Pages"). Threshold $<math><mi is="true">η</mi></math>$ value is set to 0.01. Normalized frequency is expressed in cycles per second (cyc/s).

Overall, the [properties](https://www.sciencedirect.com/topics/mathematics/sigma-property "Learn more about properties from ScienceDirect's AI-generated Topic Pages") of stacked filters are empirically evident in our experimental analysis that shows the convergence of the shift of the spectral centroid in both controlled scenario and real-world cases considered in this paper, and the non-critical role of the $<math><mi is="true">η</mi></math>$ value for the generality of the algorithm. At the same time, our investigation shows an analysis methodology (i.e. to follow the trend of the spectral centroid on the plot over the layers, similarly to [Fig. 5](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig5)) that can be helpful in assessing the useful (possibly different) $<math><mi is="true">η</mi></math>$ choice for the task at hand.

## 4\. Experimental assessment on real-world tasks

In this section we present the experimental assessment on real-world tasks of the DeepESN approach (Section [2.2](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec2.2)) constructed according to the design method proposed in Section [3](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3). Specifically, results achieved on tasks in the areas of music processing and speech processing are respectively presented in Sections [4.1 Polyphonic music tasks](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec4.1), [4.2 Speech recognition](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec4.2).

### 4.1. Polyphonic music tasks

In this section, we evaluate our model on polyphonic music tasks defined in [Boulanger-Lewandowski, Bengio, and Vincent (2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b5). In particular, we consider two datasets, namely Piano-midi.de[2](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fn2) and MuseData.[3](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fn3) These datasets are characterized by [high dimensionality](https://www.sciencedirect.com/topics/computer-science/high-dimensionality "Learn more about high dimensionality from ScienceDirect's AI-generated Topic Pages") and complex temporal dependencies involved at different time-scales, forming interesting benchmarks for [RNNs](https://www.sciencedirect.com/topics/engineering/recurrent-neural-network "Learn more about RNNs from ScienceDirect's AI-generated Topic Pages") [(Bengio, Boulanger-Lewandowski, & Pascanu, 2013)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b3). The twodatasets are characterized by complex piano and orchestral compositions with a number of simultaneous notes that ranges from 0 to 15. The musical compositions are represented by piano-rolls that were preprocessed from MIDI files. Training, validation and test sets of preprocessed piano-rolls are available on the website[4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fn4) of the authors of [Boulanger-Lewandowski et al. (2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b5). [Table 3](https://www.sciencedirect.com/science/article/pii/S0893608018302223#tbl3) shows the main characteristics of Piano-midi.de and MuseData preprocessed piano-rolls as provided in [Boulanger-Lewandowski et al. (2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b5).

In this representation, a musical composition is a sequence of 88- and 82-dimensional vectors for Piano-midi.de and MuseData tasks, respectively. In both cases, at each time-step a variable is set to 1 if the note is played and to 0 otherwise.

Table 3. The main characteristics of the preprocessed piano-rolls samples in training, validation and test set, as defined in [Boulanger-Lewandowski et al. (2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b5).

| Dataset | Split | \# Samples | Avg len | Min len | Max len |
| --- | --- | --- | --- | --- | --- |
| Piano-midi.de | Training | 87 | 872.5 | 111 | 3857 |
| Validation | 12 | 711.7 | 209 | 1637 |
| Test | 25 | 761.4 | 65 | 2645 |
| MuseData | Training | 524 | 467.9 | 9 | 3457 |
| Validation | 135 | 613.0 | 63 | 3723 |
| Test | 124 | 518.9 | 45 | 4273 |

A polyphonic music task is a next-step prediction task on high-dimensional vectors. In particular, the aim of the tasks is to predict the notes played at time-steps $<math><mi is="true">t</mi><mo is="true">+</mo><mn is="true">1</mn></math>$ (i.e., the vector $<math><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">+</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></math>$) given the notes played at time-step $<math><mi is="true">t</mi></math>$ (i.e., the vector $<math><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$). In order to compare and evaluate the classification performance of the models, we measured the expected frame-level accuracy (FL-ACC) defined as in [Bay, Ehmann, and Downie (2009)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b2) and adopted in polyphonic music tasks in [Boulanger-Lewandowski et al. (2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b5), computed as follows: (19)$<math><mtext is="true">FL-ACC</mtext><mo is="true">=</mo><mfrac is="true"><mrow is="true"><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">T</mi></mrow></munderover><mi is="true">T</mi><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow><mrow is="true"><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">T</mi></mrow></munderover><mi is="true">T</mi><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">T</mi></mrow></munderover><mi is="true">F</mi><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">T</mi></mrow></munderover><mi is="true">F</mi><mi is="true">N</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></mfrac><mo is="true">,</mo></math>$where $<math><mi is="true">T</mi></math>$ is the total number of time-steps of all sequences (i.e., musical compositions) considered for the evaluation and $<math><mi is="true">T</mi><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$, $<math><mi is="true">F</mi><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$ and $<math><mi is="true">F</mi><mi is="true">N</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$ are respectively the number of true positive notes, false positive notes and false negative notes predicted at time-step $<math><mi is="true">t</mi></math>$ (i.e., the vector $<math><mi mathvariant="bold" is="true">y</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></math>$).

In our experiments on these tasks, we used reservoirs initialized with $<math><mn is="true">10</mn><mtext is="true">%</mtext></math>$ of connectivity. For what regards DeepESN, the model selection process is performed by considering [recurrent](https://www.sciencedirect.com/topics/engineering/recurrent "Learn more about recurrent from ScienceDirect's AI-generated Topic Pages") layers with a number of units per layer $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub></math>$ varying in $<math><mrow is="true"><mo is="true">{</mo><mn is="true">50</mn><mo is="true">,</mo><mn is="true">100</mn><mo is="true">,</mo><mn is="true">200</mn><mo is="true">}</mo></mrow></math>$. As regards readout training, we used ridge-regression with a [regularization parameter](https://www.sciencedirect.com/topics/engineering/regularization-parameter "Learn more about regularization parameter from ScienceDirect's AI-generated Topic Pages") $<math><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></math>$ in $<math><mrow is="true"><mo is="true">{</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">3</mn></mrow></msup><mo is="true">,</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">2</mn></mrow></msup><mo is="true">,</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup><mo is="true">,</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mn is="true">0</mn></mrow></msup><mo is="true">}</mo></mrow></math>$. We performed the [design Algorithm](https://www.sciencedirect.com/topics/computer-science/algorithm-design "Learn more about design Algorithm from ScienceDirect's AI-generated Topic Pages") 2 considering a number of maximum recurrent layers $<math><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">=</mo><mn is="true">50</mn></math>$. All other aspects of experimental setup were as described in Section [3.3](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3.3), and the remaining hyper-parameters were chosen (for model selection purposes) from the same ranges as shown in [Table 1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#tbl1). To assess the effectiveness of the proposed methodology, we compared the performance achieved by DeepESNs built using the method proposed in Section [3](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec3), with the one obtained by shallowESNs, considering the same ranges for hyper-parameters values shown in [Table 1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#tbl1) and a range of total recurrent units in 1000–7000 and 1000–7200 (with a step of $<math><mn is="true">200</mn></math>$), for Piano-midi.de and MuseData respectively. Moreover, we compared the performance obtained by our model with the state-of-the-art approach that achieved the best FL-ACC results on the considered tasks [(Boulanger-Lewandowski et al., 2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b5), namely RNN-RBM. RNN-RBM is a sequence of Restricted Boltzmann machines (RBMs) whose parameters are the output of a deterministic RNN with proper constraint on the distribution of hidden units [(Boulanger-Lewandowski et al., 2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b5). Other examples of applications that assess the performance of RNNs models using the FL-ACC measure on the considered polyphonic music tasks are presented in [Pasa and Sperduti (2014)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b40), in which however the pre-processing of piano-rolls is different[5](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fn5) and, as this affects the performance, the corresponding results are thereby difficultly comparable.

[Table 4](https://www.sciencedirect.com/science/article/pii/S0893608018302223#tbl4) shows the FL-ACC obtained on the test set by DeepESN, shallowESN and RNN-RBM on Piano-midi.de and MuseData tasks. In Piano-midi.de task, the hyper-parameters that obtained the best performance on the validation set are $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">=</mo><mn is="true">35</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">=</mo><mn is="true">200</mn></math>$, $<math><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$, $<math><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$, $<math><mi is="true">a</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">7</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup></math>$ for DeepESN, and $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">=</mo><mn is="true">5000</mn></math>$, $<math><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">5</mn></math>$, $<math><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">01</mn></math>$, $<math><mi is="true">a</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">2</mn></mrow></msup></math>$ for shallow ESN. While for what regards MuseData task, the hyper-parameters that obtained the best performance on the validation set are $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">=</mo><mn is="true">36</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">=</mo><mn is="true">200</mn></math>$, $<math><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$, $<math><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$, $<math><mi is="true">a</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">7</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">2</mn></mrow></msup></math>$ for DeepESN, and $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub></math>$ $<math><mo is="true">=</mo><mn is="true">1</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">=</mo><mn is="true">6000</mn></math>$, $<math><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">3</mn></math>$, $<math><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">01</mn></math>$, $<math><mi is="true">a</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">3</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">2</mn></mrow></msup></math>$ for shallow ESN. Noteworthy, our approach achieved the best results on both the tasks, outperforming the state-of-the-art RNN-RBM model and leading to an improvement of the test accuracy of $<math><mn is="true">4</mn><mo is="true">.</mo><mn is="true">30</mn><mtext is="true">%</mtext></math>$ and $<math><mn is="true">2</mn><mo is="true">.</mo><mn is="true">41</mn><mtext is="true">%</mtext></math>$ for Piano-midi.de and MuseData, respectively. Moreover, DeepESN outperforms shallowESN with an improvement of $<math><mn is="true">1</mn><mo is="true">.</mo><mn is="true">46</mn><mtext is="true">%</mtext></math>$ and $<math><mn is="true">1</mn><mo is="true">.</mo><mn is="true">12</mn><mtext is="true">%</mtext></math>$ FL-ACC on Piano-midi.de and MuseData tasks, respectively.

Table 4. FL-ACC (and standard deviation on reservoir guesses, shown in parentheses) obtained on the test set of the Piano-midi.de and MuseData tasks by DeepESN, shallowESN and RNN-RBM.

| *Model* | Piano-midi.de | MuseData |
| --- | --- | --- |
| DeepESN | **33.22 (0.12)** **%** | **36.43 (0.05)** **%** |
| shallowESN | 31.76 (0.08)% | 35.31 (0.03)% |
| RNN-RBM [(Boulanger-Lewandowski et al., 2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b5) | 28.92% | 34.02% |

These results highlight the effectiveness of the proposed design approach to manage complex high-dimensional time-series on real-world tasks without re-training the readout for each configuration of the number of layers considered, outperforming at the same time the shallowESN.

We next analyzed how the choice of the number of layers performed by the design algorithm allows the DeepESN architecture to reach a good performance. To this end, we evaluated the quality of our design method by comparing the results obtained considering progressively more layers in the DeepESN architecture with the performance achieved by shallowESN with the same total number of recurrent units. To evaluate the choice performed by the design algorithm, we re-trained the readout for each number of recurrent layers until the number of layers selected by the design algorithm is reached, i.e. 35 for Piano-midi.de and 36 for MuseData. Thereby, for DeepESN we considered an architecture with reservoirs composed by 200 recurrent units each, and with a number of layers chosen in the range $<math><mrow is="true"><mo is="true">{</mo><mn is="true">5</mn><mo is="true">,</mo><mn is="true">10</mn><mo is="true">,</mo><mn is="true">15</mn><mo is="true">,</mo><mn is="true">20</mn><mo is="true">,</mo><mn is="true">25</mn><mo is="true">,</mo><mn is="true">30</mn><mo is="true">,</mo><mi is="true">s</mi><mi is="true">e</mi><mi is="true">l</mi><mi is="true">e</mi><mi is="true">c</mi><mi is="true">t</mi><mi is="true">e</mi><mi is="true">d</mi><mtext is="true">_</mtext><mi is="true">l</mi><mi is="true">a</mi><mi is="true">y</mi><mi is="true">e</mi><mi is="true">r</mi><mi is="true">s</mi><mo is="true">}</mo></mrow></math>$ where $<math><mi is="true">s</mi><mi is="true">e</mi><mi is="true">l</mi><mi is="true">e</mi><mi is="true">c</mi><mi is="true">t</mi><mi is="true">e</mi><mi is="true">d</mi><mtext is="true">_</mtext><mi is="true">l</mi><mi is="true">a</mi><mi is="true">y</mi><mi is="true">e</mi><mi is="true">r</mi><mi is="true">s</mi></math>$ equals $<math><mn is="true">35</mn></math>$ and $<math><mn is="true">36</mn></math>$ for Piano-midi.de and MuseData respectively. Accordingly, the number of total recurrent units considered for shallowESN is $<math><mrow is="true"><mo is="true">{</mo><mn is="true">1000</mn><mo is="true">,</mo><mn is="true">2000</mn><mo is="true">,</mo><mn is="true">3000</mn><mo is="true">,</mo><mn is="true">4000</mn><mo is="true">,</mo><mn is="true">5000</mn><mo is="true">,</mo><mn is="true">6000</mn><mo is="true">,</mo><mi is="true">m</mi><mi is="true">a</mi><mi is="true">x</mi><mtext is="true">_</mtext><mi is="true">u</mi><mi is="true">n</mi><mi is="true">i</mi><mi is="true">t</mi><mi is="true">s</mi><mo is="true">}</mo></mrow></math>$ where $<math><mi is="true">m</mi><mi is="true">a</mi><mi is="true">x</mi><mtext is="true">_</mtext><mi is="true">u</mi><mi is="true">n</mi><mi is="true">i</mi><mi is="true">t</mi><mi is="true">s</mi></math>$ is $<math><mn is="true">7000</mn></math>$ and $<math><mn is="true">7200</mn></math>$ for Piano-midi.de and MuseData, respectively. For each configuration of the number of DeepESN layers (total number of units for the shallow ESN), the networks were selected on the basis of the performance achieved on the validation set considering values of the hyper-parameters chosen from the ranges defined in [Table 1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#tbl1).

[Fig. 6](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig6), [Fig. 7](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig7) show the validation FL-ACC obtained by DeepESN and shallowESN on Piano-midi.de and MuseData, respectively, for increasing number of layers and total number of recurrent units. For the sake of graphical comparison, in the same figures we also plotted the test performance achieved by RNN-RBM in [Boulanger-Lewandowski et al. (2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b5) as a horizontal dashed line. As we can see from both [Fig. 6](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig6), [Fig. 7](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig7), DeepESN always performs better than shallowESN for every number of total recurrent units considered. Furthermore, the comparative plots clearly show that DeepESNs are able to reach (and even outperform) the performance achieved by shallowESNs with a much larger number of recurrent units, e.g. a DeepESN with only 1000 total recurrent units reaches a similar performance to what achieved by a shallowESN with a total reservoir size of 3000. Results in [Fig. 6](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig6), [Fig. 7](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig7) also point out that the choice of the design algorithm to select 35 layers for Piano-midi.de and 36 layers for MuseData is appropriate, especially in light of the [saturation effect](https://www.sciencedirect.com/topics/engineering/saturation-effect "Learn more about saturation effect from ScienceDirect's AI-generated Topic Pages") on the performance that can be appreciated in both cases after 30 layers. Moreover, looking at the relation between validation and test performance, we observed that DeepESN obtained validation FL-ACCs of 32.61 and 37.76 on Piano-midi.de and MuseData, respectively, with a deviation of 0.39 and 1.33 FL-ACCs from correspondent test errors. Such results highlight the effectiveness of the design choice also in what concerns the [generalization error](https://www.sciencedirect.com/topics/computer-science/generalization-error "Learn more about generalization error from ScienceDirect's AI-generated Topic Pages"). Overall, in the studied cases, these results show the good ability of our approach in the automatic selection of the proper number of recurrent layers, also approaching challenging real-world tasks, reaching a good performance able to outperform the state-of-the-art results and at the same time avoiding to build up a too complex model.

![](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr6.jpg)

1.  [Download : Download high-res image (166KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr6_lrg.jpg "Download high-res image (166KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr6.jpg "Download full-size image")

Fig. 6. Comparison between the FL-ACCs (and standard deviations on reservoir guesses represented by vertical intervals) obtained on the validation set of the Piano-midi.de task by DeepESN and shallowESN, considering a number of [recurrent](https://www.sciencedirect.com/topics/engineering/recurrent "Learn more about recurrent from ScienceDirect's AI-generated Topic Pages") layers in the range 5–35 and, correspondingly, a total number of recurrent units in the range 1000–7000 (results are obtained through model selection individually performed for each number of layers or total recurrent units for shallowESN). The dashed line at the bottom represents the test set performance achieved by RNN-RBM in [Boulanger-Lewandowski et al. (2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b5).

![](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr7.jpg)

1.  [Download : Download high-res image (158KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr7_lrg.jpg "Download high-res image (158KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr7.jpg "Download full-size image")

Fig. 7. Comparison between the FL-ACCs (and standard deviations on reservoir guesses represented by vertical intervals) obtained on the validation set of the MuseData task by DeepESN and shallowESN, considering a number of [recurrent](https://www.sciencedirect.com/topics/engineering/recurrent "Learn more about recurrent from ScienceDirect's AI-generated Topic Pages") layers in the range 5–36 and, correspondingly, a total number of recurrent units in the range 1000–7200 (results are obtained through model selection individually performed for each number of layers or total recurrent units for shallowESN). The dashed line at the bottom represents the test set performance achieved by RNN-RBM in [Boulanger-Lewandowski et al. (2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b5).

### 4.2. Speech recognition

In this section, we consider the isolated spoken digit recognition task discussed in [Verstraeten, Schrauwen, Stroobandt, and Van Campenhout (2005)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b55). This is a widely used task in the RC context (see e.g. [Rodan and Tiňo, 2011](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b43), [Rodan and Tiňo, 2012](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b44), [Verstraeten et al., 2007](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b54)), and it is characterized by multiple time-scales dependencies in high-dimensional sequences. The problem is modeled as a multi-class [classification task](https://www.sciencedirect.com/topics/engineering/classification-task "Learn more about classification task from ScienceDirect's AI-generated Topic Pages"), and it consists in recognizing ten (zero to nine) isolated spoken digits. Each digit is spoken 10 times by 5 different speakers. The 500 spoken digits are randomly split into training and test sets, each containing 250 sequences. As in works [Rodan and Tiňo, 2011](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b43), [Rodan and Tiňo, 2012](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b44), [Verstraeten et al., 2007](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b54), the model selection is performed by 10-fold cross-validation on the training set, and the error is evaluated using the Word Error Rate (WER). As proposed in [Verstraeten et al. (2007)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b54), the speech audio was preprocessed using a biological model of the human cochlea by [Lyon (1982)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b36), resulting in a 77-channel cochleagram.

For what regards DeepESN, the model selection process is performed as in Section [4.1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec4.1). Moreover, the performance obtained by DeepESN is compared with the one achieved by shallowESN model, considering a range of total recurrent units in 50–550 (with a step of $<math><mn is="true">50</mn></math>$). For this task, the design algorithm selected a DeepESN with 11 recurrent layers.

[Table 5](https://www.sciencedirect.com/science/article/pii/S0893608018302223#tbl5) shows the test WER obtained by DeepESN designed with the proposed design algorithm, shallowESN, Simple Circular Reservoir (SCR) [(Rodan & Tiňo, 2011)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b43) and Circular Reservoir with Jumps (CRJ) [(Rodan & Tiňo, 2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b44). The hyper-parameters that obtained the best performance in validation set are $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">=</mo><mn is="true">11</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">=</mo><mn is="true">50</mn></math>$, $<math><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">7</mn></math>$, $<math><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">10</mn></math>$, $<math><mi is="true">a</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup></math>$ for DeepESN and $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">R</mi></mrow></msub><mo is="true">=</mo><mn is="true">250</mn></math>$, $<math><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">1</mn></math>$, $<math><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">10</mn></math>$, $<math><mi is="true">a</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$, $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">I</mi><mi is="true">P</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn><mo is="true">.</mo><mn is="true">1</mn></math>$ and $<math><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">3</mn></mrow></msup></math>$ for shallow ESN. As results show, on this task the difference between the performance of DeepESN and shallowESN is remarkable, with a gap of 0.0502 WER on the test set. Note that, the state-of-the-art approaches SRC and CRJ, already reached good results in this task with respect to shallowESN, achieving a test WER of 0.0081 and 0.0046, respectively. Moreover, such results are obtained by CRJ and SCR models using 300 recurrent units. We can note from [Fig. 8](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig8) that the error obtained by DeepESN using 300 recurrent units remains lower than the error achieved by such models considering the same number of free parameters.

Table 5. Test WER (and standard deviation on folds, shown in parentheses) obtained on the speech recognition task by DeepESN, shallowESN, SRC and CRJ.

| *Model* | Test WER |
| --- | --- |
| DeepESN | $<math><mi mathvariant="bold" is="true">0.0028</mi></math>$ ($<math><mi mathvariant="bold" is="true">0.0005</mi></math>$) |
| shallowESN | 0.0530 (0.0318) |
| SRC [(Rodan & Tiňo, 2011)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b43) | 0.0081 (0.0022) |
| CRJ [(Rodan & Tiňo, 2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b44) | 0.0046 (0.0021) |

Remarkably, the number of layers selected by the proposed design algorithm allows the DeepESN model to outperform the state-of-the-art, reaching a test performance that is almost 4 and 2 times better than SRC and CRJ, respectively.

The effectiveness of the proposed approach is investigated by considering the results obtained by re-training the readout of DeepESNs with a progressively larger number of recurrent layers. Specifically, in this case the number of recurrent layers in the DeepESN architecture varied in the range 1–11, while the total number of recurrent units was in the range 50–550. The other hyper-parameters values were chosen from the ranges in [Table 1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#tbl1). Also in this case, the experimental analysis was conducted in comparison to the results achieved by shallowESN under the same experimental settings and with the same total number of recurrent units.

[Fig. 8](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig8) shows the validation WERs obtained by DeepESN and shallowESN by re-training the readout in correspondence of networks settings with a progressively larger number of layers (and recurrent units) in the architecture. In the same figure, we also indicated the test WERs achieved by the state-of-the-art models on the task, i.e. SRC and CRJ, as horizontal dash-dotted and dashed lines, respectively.

From [Fig. 8](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig8) we can see that DeepESN outperformed the shallowESN also on this task, for all the cases of total number of recurrent units considered. Moreover, we can see that DeepESNs required a smaller number of units to reach and outperform the performance of shallowESNs. For example, from the plot in [Fig. 8](https://www.sciencedirect.com/science/article/pii/S0893608018302223#fig8) we can see that DeepESNs with only $<math><mn is="true">150</mn></math>$ units in total were already able to beat the results of shallowESNs with even up to more than three times larger reservoirs (i.e. up to $<math><mn is="true">550</mn></math>$ units). In general, we can see that for increasing number of layers the validation performance of the DeepESN continues to improve (i.e. the WER continues to decrease), with a saturation effect that can be observed also in this case. Finally, note that the DeepESN with 11 layers obtained a validation WER of 0.0024, with a corresponding test WER of 0.0028 (i.e. the validation-test deviation is $<math><mo is="true">≈</mo><mn is="true">4</mn><mo is="true">×</mo><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">4</mn></mrow></msup></math>$ WER) which suggests that the choice made by our proposed design method results effective also with respect to the generalization error.

![](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr8.jpg)

1.  [Download : Download high-res image (181KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr8_lrg.jpg "Download high-res image (181KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0893608018302223-gr8.jpg "Download full-size image")

Fig. 8. Comparison of WERs (and standard deviations on folds represented by vertical intervals) obtained on the validation set by DeepESN and shallowESN considering a number of [recurrent](https://www.sciencedirect.com/topics/engineering/recurrent "Learn more about recurrent from ScienceDirect's AI-generated Topic Pages") layer in the range 1–11 and a total number of recurrent units in the range 50–550 on speech recognition task (results are obtained through model selection individually performed for each number of layers or total recurrent units for shallowESN). The horizontal dash-dotted and dashed lines respectively represent the test set performance achieved by SRC and CRJ in [Rodan and Tiňo (2012)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b44).

## 5\. Discussion

The fundamental goal of the work presented in this paper consisted in the development of a design strategy for automatizing the choice of the number of layers in DeepESNs. Essentially, we exploited the idea that each new layer should provide an internal state representation that, in terms of frequency spectrum, is sufficiently diversified with respect to those developed in the previous ones. Collectively, this ensures that the dynamical component of the network provides an advantageous trade-off between the richness of temporal information representation (multiplicity of temporal scales) and the resulting complexity (final number of layers).

The outcomes of our experimental analysis showed that the proposed design strategy is effective in the RC context, leading to DeepESN setups that on the one hand are able to fruitfully exploit the depth in the comparison with shallow ESN counterparts, and on the other hand outperform previous state-of-the-art results achieved by fully trained [RNN](https://www.sciencedirect.com/topics/engineering/recurrent-neural-network "Learn more about RNN from ScienceDirect's AI-generated Topic Pages") on real-world problems. It is worth to note also that such tasks are characterized by hundreds of sequences with high-dimensionality and very heterogeneous lengths in which the [regularization](https://www.sciencedirect.com/topics/engineering/regularization "Learn more about regularization from ScienceDirect's AI-generated Topic Pages") can play a relevant role in relation to our opportunistic and parsimonious construction.

From the point of view of filtering, unlike previous works [Gallicchio, Micheli, and Pedrelli, 2017](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b14), [Gallicchio et al., 2019](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b15), the qualitative analysis on the considered tasks empirically showed that [recurrent](https://www.sciencedirect.com/topics/engineering/recurrent "Learn more about recurrent from ScienceDirect's AI-generated Topic Pages") layers of a DeepESN architecture with IP adaptation can not only act as [low pass filter](https://www.sciencedirect.com/topics/engineering/lowpass-filter "Learn more about low pass filter from ScienceDirect's AI-generated Topic Pages") (as observed in previous analyses discussed in Section [1](https://www.sciencedirect.com/science/article/pii/S0893608018302223#sec1)), but also as [high pass filter](https://www.sciencedirect.com/topics/engineering/high-pass-filters "Learn more about high pass filter from ScienceDirect's AI-generated Topic Pages") (see [Appendix](https://www.sciencedirect.com/science/article/pii/S0893608018302223#app)). We believe that these observations can stimulate further analytical/theoretical studies on the characterization of the filtering effect operated by a recurrent layer, in particular, focusing on the hyper-parameters of reservoirs in hierarchical architectures.

The exploitation of our contribution can be also considered from the point of view of studies on the initialization and architectural [properties](https://www.sciencedirect.com/topics/mathematics/sigma-property "Learn more about properties from ScienceDirect's AI-generated Topic Pages") of fully trained multi-layered neural architectures with back-propagation (stochastic [gradient descent](https://www.sciencedirect.com/topics/engineering/gradient-descent "Learn more about gradient descent from ScienceDirect's AI-generated Topic Pages") approaches). This is particularly interesting in consideration of the difficulties that are typically encountered in training deep networks [(Glorot & Bengio, 2010)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b19), especially for studies regarding RNNs with ESN-based initialization, see [Sutskever, Martens, Dahl, and Hinton (2013)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b49). Indeed, initialization approaches based on pre-training analysis can influence the efficiency and the stability of the convergence of [gradient based algorithms](https://www.sciencedirect.com/topics/engineering/gradient-based-algorithm "Learn more about gradient based algorithms from ScienceDirect's AI-generated Topic Pages") in deep [nonlinear networks](https://www.sciencedirect.com/topics/engineering/nonlinear-networks "Learn more about nonlinear networks from ScienceDirect's AI-generated Topic Pages") [Jacob, 2004](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b27), [Saxe et al., 2013](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b45).

Investigations conducted in this paper also allowed to address other research issues arising hot debates in the [neural networks](https://www.sciencedirect.com/topics/neuroscience/neural-networks "Learn more about neural networks from ScienceDirect's AI-generated Topic Pages") community. In particular, two relevant instances of such questions regard the performance comparison between deep and shallow RNN models [Goodfellow et al., 2016](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b20), [Graves et al., 2013](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b21), [Pascanu et al., 2014](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b41), [Zhang et al., 2016](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b60), on the one hand, and between untrained (randomized) and trained RNNs, on the other.

As regards the former question, in this paper, through experimental comparisons between DeepESNs and ESNs, we practically demonstrated, at least in the considered tasks, the performance advantages that inherently stem from a suitable multi-layered organization of the recurrent part of the model (in the RC framework, i.e. taking aside the learning aspects of the recurrent part). At the same time, the possibility to effectively exploit the layering factor in the design of multi-layered [recurrent networks](https://www.sciencedirect.com/topics/engineering/recurrent-network "Learn more about recurrent networks from ScienceDirect's AI-generated Topic Pages"), using the approach proposed in this paper, paves the way for a grounded comparison between deep and shallow RNNs also in the context of trained models. Additionally, the results contributing to settle deep recurrent approaches, further encourage future analysis aimed at exploiting properly designed deep RNNs in modeling temporal information with latent compositionality under a generative setting.

As regards the concrete impact of supervised training of the recurrent connections in RNN applications (the second question above), the results of our experiments on the polyphonic music tasks showed that ESN-based approaches (both in the shallow and in the deep cases) are actually able to outperform fully trained complex RNN architectures that achieved previous state-of-the-art results (as represented, in this case, by the RNN-RBM model). Moreover, the good results obtained by DeepESNs on the isolated-word speech recognition task, also allow to foresee future comparisons between the untrained RC approach and trained gated RNNs, such as [Long Short Term Memory](https://www.sciencedirect.com/topics/engineering/long-short-term-memory "Learn more about Long Short Term Memory from ScienceDirect's AI-generated Topic Pages") [(Hochreiter & Schmidhuber, 1997)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b25) and Gated Recurrent Unit [(Cho et al., 2014)](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b6), popularized in the context of continuous (rather than isolated-word) speech recognition problems [Graves et al., 2013](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b21), [Graves and Schmidhuber, 2005](https://www.sciencedirect.com/science/article/pii/S0893608018302223#b22), although such analysis would necessarily take into account also other aspects of model design (e.g. bi-directionality or the use of ad-hoc loss functions) that deserve further considerations out the scopes of this paper.

## 6\. Conclusions

In this work, we have proposed a novel approach to address a fundamental issue in deep learning for sequence processing, i.e. the problem of how to choose the number of [recurrent](https://www.sciencedirect.com/topics/engineering/recurrent "Learn more about recurrent from ScienceDirect's AI-generated Topic Pages") layers in a deep recurrent architecture. Remembering the scope of the approach, namely that the input signals are featured by multiple time-scales and that the differences in the time-scales are important for the learning task at hand, we aim at exploiting such differences to tailor the layered architecture to the task. In turn, the trained output part of the model can exploit the differentiation provided through the layering for the performance aims on the learning task.

Indeed, framing our work in the deep RC area and making use of frequency analysis tools, we have defined an automatic [design algorithm](https://www.sciencedirect.com/topics/computer-science/algorithm-design "Learn more about design algorithm from ScienceDirect's AI-generated Topic Pages") for DeepESNs, aimed at exploiting as much as possible the differentiation of the temporal information representation naturally developed by recurrent hierarchies. Under the assumed conditions, the provided approach enables to choose the proper number of recurrent layers avoiding to perform the training of the readout part for each possible number of considered layers. As such, compared to a standard selection process, the proposed method allows to obtain a reduction of the time cost of model selection that scales with the number of layers.

On the experimental side, in order to assess the effect of the diversification of the frequency components enriching the state representation through layering, we have first analyzed the approach on a controlled scenario with a synthetic task characterized by signals with a predefined multiple time-scales dynamics. Quantitative and qualitative analysis on such task revealed that, in the considered experimental setting, the proposed design method is able to choose a proper number of layers reaching a better performance compared to alternative configurations with a different number of layers or with a shallow recurrent architecture. After that, we have assessed our approach on challenging real-world tasks in the areas of music and speech processing.

The results achieved on the considered tasks showed that DeepESNs designed by our automatic algorithm consistently improve the performance of shallow ESNs counterparts under the same experimental settings (and ranges for the hyper-parameter values). Noteworthy, the performance achieved by DeepESN outperforms the state-of-the-art results previously obtained by fully trained RNN-based models on real-world tasks and RC approaches on the speech recognition task. This, in turn, suggests that music and speech processing represent instances of applicative domains with multiple time-scales information that can benefit from the DeepESN approach. In this respect, at the best of our knowledge, the results presented in this paper represent also the first experimental evidence that RC networks with hierarchical reservoir organization consistently outperform RC shallow (one recurrent layer) architectures in challenging real-world tasks.

In conclusion, we believe that the design method proposed in this work can contribute to, and further stimulate, the development of approaches aimed to a principled automatic design of deep reservoir architectures in an information-based fashion, i.e. through quantitative and qualitative analysis of the dynamics emerging in the layers of stacked recurrent models.
