---
type: topics
status: open
priority: p4
creationtag: 2023-04-11 11:15
infotags:
citekey: petterIntegratingModelsInterval2018
---

# Reward prediction error (RPE)
the discrepancy between received and expected reward. This error is often attributed to the firing patterns of midbrain dopamine neurons (e.g., [89]). Recent findings, however, have challenged this long-held view by attributing the role of striatal dopamine to movement kinematics (e.g., head velocity) in unrestrained subjects performing in behavioral tasks involving the delivery of reward [90].

See the relation to Temporal Difference Learning in RL

# [[Marr’s three levels of analysis]] [7,8]
7. Marr, D. and Poggio, T. (1977) From understanding computation to understanding neural circuitry. Neurosci. Res. Prog. Bull. 15, 470–488 
8. Niv, Y. and Langdon, A. (2016) Reinforcement learning with Marr. Curr. Opin. Behav. Sci. 11, 67–73

# Scalar-Timing noise
The [[Weber's Law]] I believe where uncertainty increases scalarly with time ^vplsbh

# Belief-States
probability distributions over states 
>  The belief state model leads to the hypothesis that TD errors signaled by dopamine will refelct the evolution of state uncertainty over time [3]. Two recent studies [5,6] have tested this hypothesis in mice (see also [66]). 


# Structure Learning 
> In many environments, agents might not only be uncertain about the hidden state, but also be uncertain about the state space. This means agents must solve a structure learning problem simultaneously with the RL problem of maximizing rewards [68]. The formal framework of belief state RL (Box 2) can be naturally extended to the problem of structure learning. 
> 
> The aforementioned change detection (hopper-switch) paradigms [53,52,63] can be viewed as a form of structure learning. In these paradigms, the environment never returns to exactly the same state, so the belief state is in theory infniite dimensional. However, real-world environments often have recurring structure, where old states can reoccur in addition to the occurrence of new states. The computational problem facing the brain in these environments is therefore somewhat different from change detection. Instead of detecting whether the current observation is generated by either the same state as the previous observation or a new state, structure learning in recurrent environments needs to consider whether a previously encountered state has reoccurred [4,69]. 

68. Gershman, S.J. et al. (2015) Discovering latent causes in reinforcement learning. Curr. Opin. Behav. Sci. 5, 43–50
69. Gershman, S.J. and Daw, N.D. (2017) Reinforcement learning
and episodic memory in humans and animals: an integrative
framework. Annu. Rev. Psychol. 68, 101–128

# Interval Reproduction?
> A recent work [70] provides another example of structure learning in a temporal reproduction task. Human observers were tasked with reproducing the duration of a visual stimulus by holding down a key for an equivalent duration. 
> 
> Consistent with a large experimental literature (e.g., [71–73]), observers exhibited a central tendency effect: relative to the mean duration, long durations were underestimated, and short durations were overestimated [5]. This effect can be understood in terms of a Bayesian analysis [47,71,72,74], according to which noisy timing signals are combined with a prior over duration to form a posterior (i.e., the belief state). The expected duration under this belief state will be intermediate between the observed timing signal and the mean of the prior. 
> 
> Further, this research [70] asked whether the central tendency effect refelcts a single prior learned across environments, or instead refelcts separate priors learned for different environments. To answer this question, they interleaved stimuli from different distributions (e.g., sensory modality), and found that the reproduction bias refelcted a single combined prior, even when stimuli for the two distributions were presented in distinct spatial locations. By contrast, when distinct motor responses were required for the two distributions the reproduction bias refelcted separate priors, indicating no generalization. These results suggest that motor information exerts a stronger infulence on structure learning than sensory information (see also [75]).
70. Roach, N.W. et al. (2017) Generalization of prior information for rapid Bayesian time estimation. Proc. Natl. Acad. Sci. U. S. A. 114, 412–417
# RL and IT
> **Concluding Remarks and Future Directions** 
> Computational theories of RL and IT have matured independently, and only recently have they begun to intersect [4]. This review has highlighted several ways in which RL algorithms use temporal representations, which go beyond the traditional function approximation with a temporal basis set seen in model-free RL theories. 
> 
> Modern RL embraces a broader range of algorithms, including model-based algorithms that use IT to support felxible and rapid changes in behavior. Many open questions remain. One concerns our assumption that the environment can be modeled as an MDP. Markov dynamics are algorithmically useful, but they are also restrictive. Several lines of research suggest ways to get around the Markov assumption without entirely abandoning algorithmic efficiency, for example, using episodic memory [69] or a collection of timing elements that support learning at multiple scales [81,82]. 
> 
4. [ ]  #reading #p3 [[@gershmanTimeRepresentationReinforcement2014|Gershman, S.J. et al. (2014) Time representation in reinforcement learning models of the basal ganglia. Front. Comput. Neurosci. 7, 194]]
69. [ ] #reading #p3 [[@gershmanReinforcementLearningEpisodic2017|Gershman, S.J. and Daw, N.D. (2017) Reinforcement learning and episodic memory in humans and animals: an integrative framework. Annu. Rev. Psychol. 68, 101–128]]
81. [ ] #reading #p3 [[@howardEfficientNeuralComputation2019|Howard, M.W. et al. (2015) Efficient neural computation in the Laplace domain. In Proceedings of the 2015th International Conference on Cognitive Computation: Integrating Neural and Symbolic Approaches (Vol. 1583), pp. 61–68, CEUR-WS.org]]
82. [ ] #reading #p3 [[@shankarScaleInvariantInternalRepresentation2012|Shankar, K.H. and Howard, M.W. (2012) A scale-invariant internal representation of time. Neural Comput. 24, 134–193]]

> Another issue concerns the neural integration of IT and RL mechanisms. Specifically, there are distributed brain regions that support the processing of both IT and RL, yet information is integrated across different algorithmic learning systems to form a perceptual whole [83]. One useful way to better understand the interaction of these two feilds is through the use of biologically plausible neural networks. Recent work has shown how biologically plausible recurrent neural networks can be used to implement felxible timing [40,84]. These networks are trained using Hebbian rules [84] or supervised learning [40], rather than RL.
> 
> However, the insights from these network models may prove useful in constructing more biologically plausible and powerful implementations of RL algorithms (see [9] for a step in this direction). Overall, we encourage a more unifeid approach to the study of IT and RL going forward. It becomes clear the two disciplines are overlapping as they are traced across Marr’s three levels of analysis (see Outstanding Questions). 
9. [[@songRewardbasedTrainingRecurrent2017|Song, H.F. et al. (2017) Reward-based training of recurrent neural networks for cognitive and value-based tasks. eLife 6, e21492]] #reading #p3 
83. [[@petterTemporalProcessingIntrinsic2016|Petter, E.A. and Merchant, H. (2016) Temporal processing by intrinsic neural network dynamics. Timing Time Percept. 4, 399–410]] #reading #p3
84. [[@murrayLearningMultipleVariablespeed2017|Murray, J.M. (2017) Learning multiple variable-speed sequences in striatum via cortical tutoring. eLife 6, e26084]] #reading #p3
40. [[@goudarEncodingSensoryMotor2018|Goudar, V. and Buonomano, D.V. (2018) Encoding sensory and motor patterns as time-invariant trajectories in recurrent neural networks. eLife 7, e31134]] #reading #p3

# Temporal Difference (TD)
> The canonical example is temporal difference (TD) learning [14], which uses the discrepancy between received and expected reward (the TD error) to update its value estimates (see Box 1 for technical details). 
> 
> This algorithm has been influential in neuroscience due to the correspondence between the TD error and the firing of midbrain dopamine neurons [15,16]. The value function is thought to be encoded at corticostriatal synapses, where plasticity is modulated by dopamine release similar to the circuits subserving IT [17–21]. This dopamine-dependent plasticity functions within a precise time window (i.e., 0.3–2.0 s), which has been demonstrated using optical activation of dopaminergic and glutamatergic afferents [22].

- This is directly referencing RPE, thought hey do not talk about  it yet