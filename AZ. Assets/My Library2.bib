
@article{.paninskiNeuralDataScience2018,
  title = {Neural Data Science: Accelerating the Experiment-Analysis-Theory Cycle in Large-Scale Neuroscience},
  shorttitle = {Neural Data Science},
  author = {.Paninski, L and Cunningham, JP},
  year = {2018},
  month = jun,
  journal = {Current Opinion in Neurobiology},
  series = {Neurotechnologies},
  volume = {50},
  pages = {232--241},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2018.04.007},
  abstract = {Modern large-scale multineuronal recording methodologies, including multielectrode arrays, calcium imaging, and optogenetic techniques, produce single-neuron resolution data of a magnitude and precision that were the realm of science fiction twenty years ago. The major bottlenecks in systems and circuit neuroscience no longer lie in simply collecting data from large neural populations, but also in understanding this data: developing novel scientific questions, with corresponding analysis techniques and experimental designs to fully harness these new capabilities and meaningfully interrogate these questions. Advances in methods for signal processing, network analysis, dimensionality reduction, and optimal control\textemdash developed in lockstep with advances in experimental neurotechnology\textemdash promise major breakthroughs in multiple fundamental neuroscience problems. These trends are clear in a broad array of subfields of modern neuroscience; this review focuses on recent advances in methods for analyzing neural time-series data with single-neuronal precision.},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Paninski_Cunningham_2018_Neural data science.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@paninskiNeuralDataScience2018.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XBHICUBV/S0959438817302428.html}
}

@techreport{12UnitedStates2017,
  title = {(12) {{United States Patent}} (45) {{Date}} of {{Patent}}: (54) (71) (72) (73) (*) (21) (22) (65) (60) (51) (52) (58) {{MACHINE-LEARNING ACCELERATOR}} ({{MLA}}). {{INTEGRATED CIRCUIT FOR EXTRACTING FEATURES FROM SIGNALS AND PERFORMING INFERENCE COMPUTATIONS OTHER PUBLICATIONS}}},
  year = {2017},
  abstract = {A machine-learning accelerator (MLA) integrated circuit for extracting features from signals and performing inference computations is disclosed. The MLA integrated circuit includes a framework of finite state machine (FSM) kernels that are machine-learning algorithms implemented in hard ware. The MLA integrated circuit further includes a kernel controller having mathematical structures implemented in hardware in communication with the framework of FSM kernels. An arithmetic engine implemented in hardware within the MLA integrated circuit is in communication with the kernel controller to perform computations for the math ematical structures. In at least one embodiment, the MLA integrated circuit includes a compression decompression accelerator (CDA) implemented in hardware and coupled between a memory and the kernel controller for compressing data to be stored in the memory and for decompressing data retrieved from the memory. 19 Claims, 9 Drawing Sheets MACHINE-LEARNING 28 ACCELERATOR (MLA) BLOCK 8 coRDIC 70 MAXMIN 72},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3WISE485/Unknown - 2017 - (12) United States Patent (45) Date of Patent (54) (71) (72) (73) () (21) (22) (65) (60) (51) (52) (58) MACHINE-LEARNIN.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z9D4F75I/Unknown - 2017 - (12) United States Patent (45) Date of Patent (54) (71) (72) (73) () (21) (22) (65) (60) (51) (52) (58) MACHINE-LEAR(2).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@12UnitedStates2017.md}
}

@techreport{Aaser,
  title = {Towards Making a Cyborg: {{A}} Closed-Loop Reservoir-Neuro System},
  author = {Aaser, Peter and Knudsen, Martinius and Ramstad, Ola Huse and Van De Wijdeven, Rosanne and Nichele, Stefano and Sandvig, Ioanna and Tufte, Gunnar and Bauer, Ulrich Stefan and Halaas, {\O}yvind and Hendseth, Sverre and Sandvig, Axel and Valderhaug, Vibeke},
  abstract = {The human brain is a remarkable computing machine, i.e. vastly parallel, self-organizing, robust, and energy efficient. To gain a better understanding into how the brain works, a cyborg (cybernetic organism, a combination of machine and living tissue) is currently being made in an interdisciplinary effort, known as the Cyborg project. In this paper we describe how living cultures of neurons (biological neural networks) are successfully grown in-vitro over Micro-Electrode Arrays (MEAs), which allow them to be interfaced to a robotic body through electrical stimulation and neural recordings. Furthermore , we describe the bio-and nano-technological procedures utilized for the culture of such dissociated neural networks and the interface software and hardware framework used for creating a closed-loop hybrid neuro-system. A Reservoir Computing (RC) approach is used to harness the computational power of the neuronal culture.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VXWNG2XS/Aaser et al. - Unknown - Towards Making a Cyborg A Closed-Loop Reservoir-Neuro System.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@Aaser.md}
}

@article{aaserMainAbstract2011,
  title = {Main with {{Abstract}}},
  author = {Aaser, Peter},
  year = {2011},
  issn = {2013436106},
  doi = {10.1360/zd-2013-43-6-1064},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/R5VQ75VK/Aaser - 2011 - Main with Abstract.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@aaserMainAbstract2011.md}
}

@techreport{aaserMakingCyborgClosedLoop,
  title = {Towards {{Making}} a {{Cyborg}}: {{A Closed-Loop Reservoir-Neuro System}}},
  author = {Aaser, Peter and Knudsen, Martinius and Ramstad, Ola Huse and Van De Wijdeven, Rosanne and Nichele, Stefano and Sandvig, Ioanna and Tufte, Gunnar and Bauer, Ulrich Stefan and Halaas, {\O}yvind and Hendseth, Sverre and Sandvig, Axel and Valderhaug, Vibeke},
  abstract = {The human brain is a remarkable computing machine, i.e. vastly parallel, self-organizing, robust, and energy efficient. To gain a better understanding into how the brain works, a cyborg (cybernetic organism, a combination of machine and living tissue) is currently being made in an interdisciplinary effort, known as the Cyborg project. In this paper we describe how living cultures of neurons (biological neural networks) are successfully grown in-vitro over Micro-Electrode Arrays (MEAs), which allow them to be interfaced to a robotic body through electrical stimulation and neural recordings. Furthermore , we describe the bio-and nano-technological procedures utilized for the culture of such dissociated neural networks and the interface software and hardware framework used for creating a closed-loop hybrid neuro-system. A Reservoir Computing (RC) approach is used to harness the computational power of the neuronal culture.},
  note = {Test note},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LHFVLMVZ/Aaser et al. - Unknown - Towards Making a Cyborg A Closed-Loop Reservoir-Neuro System.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@aaserMakingCyborgClosedLoop.md}
}

@article{aaserMakingCyborgClosedLoop2017,
  title = {Towards {{Making}} a {{Cyborg}} : {{A Closed-Loop Reservoir-Neuro System}}},
  author = {Aaser, Peter and Knudsen, Martinius and Ramstad, Ola Huse and Wijdeven, Rosanne Van De and Nichele, Stefano and Sandvig, Ioanna and Tufte, Gunnar and Bauer, Ulrich Stefan and Halaas, {\O}yvind and Hendseth, Sverre and Sandvig, Axel and Valderhaug, Vibeke},
  year = {2017},
  journal = {Ecal},
  number = {September},
  pages = {430--437},
  issn = {978-0-262-34633-7},
  doi = {10.7551/ecal_a_072},
  abstract = {The human brain is a remarkable computing machine, i.e. vastly parallel, self-organizing, robust, and energy efficient. To gain a better understanding into how the brain works, a cyborg (cybernetic organism, a combination of machine and living tissue) is currently being made in an interdisciplinary effort, known as the Cyborg project. In this paper we describe how living cultures of neurons (biological neural networks) are successfully grown in-vitro over Micro-Electrode Arrays (MEAs), which allow them to be interfaced to a robotic body through electrical stimulation and neural recordings. Furthermore, we describe the bio- and nano-technological procedures utilized for the culture of such dissociated neural networks and the interface software and hardware framework used for creating a closed-loop hybrid neuro-system. A Reservoir Computing (RC) approach is used to harness the computational power of the neuronal culture.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DZVDBRBL/Aaser et al. - 2017 - Towards Making a Cyborg A Closed-Loop Reservoir-Neuro System.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@aaserMakingCyborgClosedLoop2017.md}
}

@article{abderrahmaneDesignSpaceExploration2020,
  title = {Design {{Space Exploration}} of {{Hardware Spiking Neurons}} for {{Embedded Artificial Intelligence}}},
  author = {Abderrahmane, Nassim and Lemaire, Edgar and Miramond, Beno{\^i}t},
  year = {2020},
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  pages = {366--386},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.09.024},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/E5HEGT8R/Abderrahmane et al_2020_Design Space Exploration of Hardware Spiking Neurons for Embedded Artificial.pdf}
}

@book{abelesCorticonicsNeuralCircuits1991,
  title = {Corticonics: {{Neural Circuits}} of the {{Cerebral Cortex}}},
  shorttitle = {Corticonics},
  author = {Abeles, M.},
  year = {1991},
  month = feb,
  publisher = {{Cambridge University Press}},
  abstract = {Understanding how the brain works is probably the greatest scientific and intellectual challenge of our generation. The cerebral cortex is the instrument by which we carry the most complex mental functions. Fortunately, there exists an immense body of knowledge concerning both cortical structure and the properties of single neurons in the cortex. With the advent of the supercomputer, there has been increased interest in neural network modeling. What is needed is a new approach to an understanding of the mammalian cerebral cortex that will provide a link between the physiological description and the computer model. This book meets that need by combining anatomy, physiology, and modeling to achieve a quantitative description of cortical function. The material is presented didactically, starting with descriptive anatomy and comprehensively examining all aspects of modeling. The book gradually leads the reader from the macroscopic cortical anatomy and standard electrophysiological properties of single neurons to neural network models and synfire chains. The most modern trends in neural network modeling are explored.},
  googlebooks = {v46SDOLJrLcC},
  isbn = {978-0-521-37617-4},
  langid = {english},
  keywords = {Medical / Physiology,Science / Life Sciences / Neuroscience}
}

@book{adamatzkyCollisionBasedComputing2002,
  title = {Collision-{{Based Computing}}},
  editor = {Adamatzky, Andrew},
  year = {2002},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-0129-1},
  abstract = {Dynamical systems are capable of performing computation in a reservoir computing paradigm. This paper presents a general representation of these systems as an artificial neural network (ANN). Initially, we implement the simplest dynamical system, a cellular automaton. The mathematical fundamentals behind an ANN are maintained, but the weights of the connections and the activation function are adjusted to work as an update rule in the context of cellular automata. The advantages of such implementation are its usage on specialized and optimized deep learning libraries, the capabilities to generalize it to other types of networks and the possibility to evolve cellular automata and other dynamical systems in terms of connectivity, update and learning rules. Our implementation of cellular automata constitutes an initial step towards a general framework for dynamical systems. It aims to evolve such systems to optimize their usage in reservoir computing and to model physical computing substrates.},
  isbn = {978-1-85233-540-3 978-1-4471-0129-1},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/S3H8A5Y2/Adamatzky - 2002 - Collision-Based Computing.pdf}
}

@book{adamatzkyCollisionBasedComputing2002a,
  title = {Collision-{{Based Computing}}},
  editor = {Adamatzky, Andrew},
  year = {2002},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-0129-1},
  abstract = {Dynamical systems are capable of performing computation in a reservoir computing paradigm. This paper presents a general representation of these systems as an artificial neural network (ANN). Initially, we implement the simplest dynamical system, a cellular automaton. The mathematical fundamentals behind an ANN are maintained, but the weights of the connections and the activation function are adjusted to work as an update rule in the context of cellular automata. The advantages of such implementation are its usage on specialized and optimized deep learning libraries, the capabilities to generalize it to other types of networks and the possibility to evolve cellular automata and other dynamical systems in terms of connectivity, update and learning rules. Our implementation of cellular automata constitutes an initial step towards a general framework for dynamical systems. It aims to evolve such systems to optimize their usage in reservoir computing and to model physical computing substrates.},
  isbn = {978-1-85233-540-3 978-1-4471-0129-1},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TQ48HI6E/Adamatzky - 2002 - Collision-Based Computing.pdf}
}

@article{adamsBayesianOnlineChangepoint2007,
  title = {Bayesian {{Online Changepoint Detection}}},
  author = {Adams, Ryan Prescott and MacKay, David J. C.},
  year = {2007},
  issn = {9781509028337},
  abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6GMNIM2C/Adams, MacKay - 2007 - Bayesian Online Changepoint Detection.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@adamsBayesianOnlineChangepoint2007.md}
}

@techreport{adlerApplicationsRANDOMFIELDS2015,
  title = {Applications of {{RANDOM FIELDS AND GEOMETRY Foundations}} and {{Case Studies}}},
  author = {Adler, Robert J and Taylor, Jonathan E and Worsley, Keith J},
  year = {2015},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XWGMSFWA/Adler, Taylor, Worsley - 2015 - Applications of RANDOM FIELDS AND GEOMETRY Foundations and Case Studies.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@adlerApplicationsRANDOMFIELDS2015.md}
}

@article{adlerRANDOMFIELDSGEOMETRY,
  title = {{{RANDOM FIELDS AND GEOMETRY}}},
  author = {Adler, Robert J and Taylor, Jonathan E and Worsley, Keith J},
  pages = {265},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/M6KP7BMF/Adler et al. - RANDOM FIELDS AND GEOMETRY.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@adlerRANDOMFIELDSGEOMETRY.md}
}

@article{aenuguTrainingSpikingNeural2020,
  title = {Training Spiking Neural Networks Using Reinforcement Learning},
  author = {Aenugu, Sneha},
  year = {2020},
  month = may,
  journal = {arXiv:2005.05941 [cs, stat]},
  eprint = {2005.05941},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neurons in the brain communicate with each other through discrete action spikes as opposed to continuous signal transmission in artificial neural networks. Therefore, the traditional techniques for optimization of parameters in neural networks which rely on the assumption of differentiability of activation functions are no longer applicable to modeling the learning processes in the brain. In this project, we propose biologically-plausible alternatives to backpropagation to facilitate the training of spiking neural networks. We primarily focus on investigating the candidacy of reinforcement learning (RL) rules in solving the spatial and temporal credit assignment problems to enable decision-making in complex tasks. In one approach, we consider each neuron in a multi-layer neural network as an independent RL agent forming a different representation of the feature space while the network as a whole forms the representation of the complex policy to solve the task at hand. In other approach, we apply the reparameterization trick to enable differentiation through stochastic transformations in spiking neural networks. We compare and contrast the two approaches by applying them to traditional RL domains such as gridworld, cartpole and mountain car. Further we also suggest variations and enhancements to enable future research in this area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VSKZY954/Aenugu_2020_Training spiking neural networks using reinforcement learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Q2RUNQC2/2005.html}
}

@article{aguileraUnifyingFrameworkMean2020,
  title = {A Unifying Framework for Mean Field Theories of Asymmetric Kinetic {{Ising}} Systems},
  author = {Aguilera, Miguel and Moosavi, S. Amin and Shimazaki, Hideaki},
  year = {2020},
  month = feb,
  abstract = {Kinetic Ising models are powerful tools for studying the non-equilibrium dynamics of complex, discrete systems and for analyzing their experimental recordings. However, the behaviour of the model is in general not tractable for large networks; therefore, mean field theories are frequently used to approximate its statistical properties. Many variants of the classical naive and TAP (i.e., second-order) mean field approximations have been proposed, each of which makes unique assumptions about the temporal evolution of the system's correlation structure. This disparity of methods makes it difficult to systematically advance mean field approaches beyond previous contributions. Here, we propose a unified framework for mean field theories of the dynamics of asymmetric kinetic Ising systems based on information geometry. The framework is built on Plefka expansions of the model around a simplified model obtained by an orthogonal projection to a sub-manifold of tractable probability distributions. This view not only unifies previous methods but also allows us to propose novel methods that, in contrast with traditional mean-field approaches, preserve correlations of the system, both in stationary and transient states. By comparing analytical approximations and exact numerical simulations, we show that the proposed methods provide more accurate estimates of covariance evolution in the system than classical equations, and consequently outperform previous mean field theories for solving the inverse Ising problem. In sum, our framework unifies and extends current mean-field approximations of kinetic Ising model, constituting a powerful tool for studying non-equilibrium dynamics of complex systems.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5D8ICDFW/Aguilera, Moosavi, Shimazaki - 2020 - A unifying framework for mean field theories of asymmetric kinetic Ising systems.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@aguileraUnifyingFrameworkMean2020.md}
}

@techreport{aguileraUNIFYINGFRAMEWORKMEAN2020,
  title = {A {{UNIFYING FRAMEWORK FOR MEAN FIELD THEORIES OF ASYMMETRIC KINETIC ISING SYSTEMS A PREPRINT}}},
  author = {Aguilera, Miguel and Moosavi, S Amin and Shimazaki, Hideaki},
  year = {2020},
  abstract = {Kinetic Ising models are powerful tools for studying the non-equilibrium dynamics of complex discrete systems and analyzing their experimental recordings. However, the behaviour of the model is in general not tractable for large networks; therefore, mean field theories are frequently used to approximate its statistical properties. Many variants of the classical naive and TAP (i.e., second-order) mean field approximations have been proposed, each of which makes unique assumptions about time evolution of the system's correlation structure. This disparity of methods makes it difficult to systematically advance the mean field approach over previous contributions. Here, we propose a unified framework for mean field theories of the dynamics of asymmetric kinetic Ising systems based on information geometry. The framework is built on Plefka expansions of the model around a simplified model obtained by an orthogonal projection to a sub-manifold of tractable probability distributions. This approach not only unifies previous methods but allows us to define novel methods that, in contrast with traditional mean-field approaches, preserve correlations of the system, both in stationary and transient states. By comparing analytical approximations and exact numerical simulations, we show that the proposed methods provide more accurate estimates for the evolution of equal-time and delayed covariance structures than classical equations, and consequently outperform previous mean field theories for solving the inverse Ising problem. In sum, our framework unifies and extends current mean-field approximations of kinetic Ising model, constituting a powerful tool for studying non-equilibrium dynamics of complex systems.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WJQHDIMN/Aguilera, Moosavi, Shimazaki - 2020 - A UNIFYING FRAMEWORK FOR MEAN FIELD THEORIES OF ASYMMETRIC KINETIC ISING SYSTEMS A PREPRINT(2).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XKFWMJ46/Aguilera, Moosavi, Shimazaki - 2020 - A UNIFYING FRAMEWORK FOR MEAN FIELD THEORIES OF ASYMMETRIC KINETIC ISING SYSTEMS A PREPRINT.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@aguileraUNIFYINGFRAMEWORKMEAN2020.md}
}

@article{akaikeNewLookStatistical1974,
  title = {A {{New Look}} at the {{Statistical Model Identification}}},
  author = {Akaike, Hirotugu},
  year = {1974},
  journal = {IEEE Transactions on Automatic Control},
  volume = {19},
  number = {6},
  pages = {716--723},
  doi = {10.1109/TAC.1974.1100705},
  abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximurn likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AlC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples. \textcopyright{} 1974, IEEE. All rights reserved.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RAUDJSET/01100705(1).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@akaikeNewLookStatistical1974.md}
}

@inproceedings{akellaTimeActionCoTraining2021,
  title = {Time and {{Action Co-Training}} in {{Reinforcement Learning Agents}}},
  booktitle = {Frontiers in {{Control Engineering}}},
  author = {Akella, Ashlesha and Lin, Chin-Teng},
  year = {2021},
  doi = {10.3389/fcteg.2021.722092},
  abstract = {This work trained a reinforcement learning agent to create its representation of time using a neural network with a population of recurrently connected nonlinear firing rate neurons to produce a neural trajectory that peaks at the ``time-to-act''; thus, it learns ``when'' to act. In formation control, a robot (or an agent) learns to align itself in a particular spatial alignment. However, in a few scenarios, it is also vital to learn temporal alignment along with spatial alignment. An effective control system encompasses flexibility, precision, and timeliness. Existing reinforcement learning algorithms excel at learning to select an action given a state. However, executing an optimal action at an appropriate time remains challenging. Building a reinforcement learning agent which can learn an optimal time to act along with an optimal action can address this challenge. Neural networks in which timing relies on dynamic changes in the activity of population neurons have been shown to be a more effective representation of time. In this work, we trained a reinforcement learning agent to create its representation of time using a neural network with a population of recurrently connected nonlinear firing rate neurons. Trained using a reward-based recursive least square algorithm, the agent learned to produce a neural trajectory that peaks at the ``time-to-act''; thus, it learns ``when'' to act. A few control system applications also require the agent to temporally scale its action. We trained the agent so that it could temporally scale its action for different speed inputs. Furthermore, given one state, the agent could learn to plan multiple future actions, that is, multiple times to act without needing to observe a new state.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MY5LIM6Y/Akella_Lin_2021_Time and Action Co-Training in Reinforcement Learning Agents.pdf}
}

@article{al-darabsahImpactSmallTime2021,
  title = {The {{Impact}} of {{Small Time Delays}} on the {{Onset}} of {{Oscillations}} and {{Synchrony}} in {{Brain Networks}}},
  author = {{Al-Darabsah}, Isam and Chen, Liang and Nicola, Wilten and Campbell, Sue Ann},
  year = {2021},
  month = jul,
  journal = {Frontiers in Systems Neuroscience},
  volume = {15},
  pages = {688517},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2021.688517},
  abstract = {The human brain constitutes one of the most advanced networks produced by nature, consisting of billions of neurons communicating with each other. However, this communication is not in real-time, with different communication or time-delays occurring between neurons in different brain areas. Here, we investigate the impacts of these delays by modeling large interacting neural circuits as neural-field systems which model the bulk activity of populations of neurons. By using a Master Stability Function analysis combined with numerical simulations, we find that delays (1) may actually stabilize brain dynamics by temporarily preventing the onset to oscillatory and pathologically synchronized dynamics and (2) may enhance or diminish synchronization depending on the underlying eigenvalue spectrum of the connectivity matrix. Real eigenvalues with large magnitudes result in increased synchronizability while complex eigenvalues with large magnitudes and positive real parts yield a decrease in synchronizability in the delay vs. instantaneously coupled case. This result applies to networks with fixed, constant delays, and was robust to networks with heterogeneous delays. In the case of real brain networks, where the eigenvalues are predominantly real, owing to the nearly symmetric nature of these weight matrices, biologically plausible, small delays, are likely to increase synchronization, rather than decreasing it.},
  pmcid = {PMC8287421},
  pmid = {34290593},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BSQW96PN/Al-Darabsah et al_2021_The Impact of Small Time Delays on the Onset of Oscillations and Synchrony in.pdf}
}

@inproceedings{albert27thAnnualCSP2014,
  title = {27th Annual {{CSP}} Workshops on "Recent Developments in Computer Simulation Studies in Condensed Matter Physics", {{CSP}} 2014 the Inverse Ising Problem},
  booktitle = {Physics {{Procedia}}},
  author = {Albert, Joseph and Swendsen, Robert H.},
  year = {2014},
  doi = {10.1016/j.phpro.2014.08.140},
  abstract = {The inverse Ising problem consists of taking a set of Ising configurations generated with unknown interaction parameters, and determining reliable estimates for the values of those interaction parameters. The problem first arose in connection with the Monte Carlo renormalization group, and was solved thirty years ago. Recently, there has been renewed interest in the inverse Ising problem due to biological applications. The original solution seems to have been forgotten, as it was rediscovered in a different representation by Aurell and Ekeberg in 2012. In this paper we modify the earlier equations to solve problems that are not translationally invariant.},
  keywords = {Inverse,Ising,Monte carlo},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TQ8XXAXQ/Albert, Swendsen - 2014 - 27th annual CSP workshops on recent developments in computer simulation studies in condensed matter physics, C.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@albert27thAnnualCSP2014.md}
}

@book{alexanderOptimaAnimalsRevised1996,
  title = {Optima for {{Animals}}: {{Revised Edition}}},
  shorttitle = {Optima for {{Animals}}},
  author = {Alexander, R. McNeill},
  year = {1996},
  publisher = {{Princeton University Press}},
  doi = {10.2307/j.ctv173f0gj},
  abstract = {Optimization theory is designed to find the best ways of doing things. The structures of animals, their movements, their behavior, and their life histories have all been shaped by the optimizing processes of evolution or of learning by trial and error. In this revised edition of R. McNeill Alexander's widely acclaimed  \emph{Optima for Animals} , we see how extraordinarily diverse branches of biology are illuminated by the powerful methods of optimization theory. What is the best strength for a bone? Too weak a bone will probably break but an excessively stout one will be cumbersome. At what speed should humans change from walking to running? Should a bird take only big juicy worms or should it eat every worm it finds, and do birds make the best choices? Why do the males of some species of fishes and the females of others look after the young, while the young of others are looked after by both parents or neither? Is it possible that all these policies can be optimal, in different circumstances? This book shows how these and many other questions can be answered. The mathematics involved is explained very simply, with biology students in mind, but the book is not just for them. It is also for professionals, ranging from teachers to researchers.},
  isbn = {978-0-691-02798-2}
}

@book{amariNeuralInformationProcessing2017,
  title = {Neural {{Information Processing}}},
  author = {AMARI, Shun-ichi},
  year = {2017},
  journal = {Journal of the Society of Mechanical Engineers},
  volume = {90},
  pages = {759},
  doi = {10.1299/jsmemag.90.823_758},
  abstract = {This paper discusses the directional properties of human hand force perception during maintained arm posture in operation of a robotic device by means of robotic and psychological techniques. A series of perception experiments is carried out using an impedance-controlled robot depending on force magnitudes and directions in the different arm postures. Experimental results demonstrate that human hand force perception is much affected by the stimulus direction and can be expressed with an ellipse. Finally, the relationship between hand force perception properties and hand force manipulability is analyzed by means of human force manipulability ellipse.},
  isbn = {978-3-319-26560-5},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BTX994XN/AMARI - 2017 - Neural Information Processing.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N9J8DBM6/AMARI - 2017 - Neural Information Processing(2).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@amariNeuralInformationProcessing2017.md}
}

@book{amit1992modeling,
  title = {Modeling Brain Function: {{The}} World of Attractor Neural Networks},
  author = {Amit, Daniel J and Amit, Daniel J},
  year = {1992},
  publisher = {{Cambridge university press}},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@amit1992modeling.md}
}

@article{anandkumarHighDimensionalGaussianGraphical2012,
  title = {High-{{Dimensional}} Gaussian Graphical Model Selection: {{Walk}} Summability and Local Separation Criterion},
  author = {Anandkumar, Animashree and Tan, Vincent Y.F. and Huang, Furong and Willsky, Alan S.},
  year = {2012},
  journal = {Journal of Machine Learning Research},
  volume = {13},
  pages = {2293--2337},
  abstract = {We consider the problem of high-dimensional Gaussian graphical model selection. We identify a set of graphs for which an efficient estimation algorithm exists, and this algorithm is based on thresholding of empirical conditional covariances. Under a set of transparent conditions, we establish structural consistency (or sparsistency) for the proposed algorithm, when the number of samples n = {$\omega$}(J-2min log p), where p is the number of variables and Jmin is the minimum (absolute) edge potential of the graphical model. The sufficient conditions for sparsistency are based on the notion of walk-summability of the model and the presence of sparse local vertex separators in the underlying graph. We also derive novel non-asymptotic necessary conditions on the number of samples required for sparsistency. \textcopyright{} 2012 Animashree Anandkumar, Vincent Tan, Furong Huang and Alan Willsky.},
  keywords = {Gaussian graphical model selection,High-dimensional learning,Local-separation property,Necessary conditions for model selection,Walk-summability},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/D8WZH8T6/anandkumar12a.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@anandkumarHighDimensionalGaussianGraphical2012.md}
}

@article{anandkumarHighdimensionalStructureEstimation2012,
  title = {High-Dimensional Structure Estimation in Ising Models: {{Local}} Separation Criterion},
  author = {Anandkumar, Animashree and Tan, Vincent Y.F. and Huang, Furong and Willsky, Alan S.},
  year = {2012},
  journal = {Annals of Statistics},
  volume = {40},
  number = {3},
  pages = {1346--1375},
  doi = {10.1214/12-AOS1009},
  abstract = {We consider the problem of high-dimensional Ising (graphical) model selection. We propose a simple algorithm for structure estimation based on the thresholding of the empirical conditional variation distances. We introduce a novel criterion for tractable graph families, where this method is efficient, based on the presence of sparse local separators between node pairs in the underlying graph. For such graphs, the proposed algorithm has a sample complexity of n = {$\omega$}(J-2 min log p), where p is the number of variables, and Jmin is the minimum (absolute) edge potential in the model.We also establish nonasymptotic necessary and sufficient conditions for structure estimation. \textcopyright{} Institute of Mathematical Statistics, 2012.},
  keywords = {Graphical model selection,Ising models,Local-separation property},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/K75MRA4X/euclid.aos.1344610586.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@anandkumarHighdimensionalStructureEstimation2012.md}
}

@article{anandkumarMethodMomentsMixture2012,
  title = {A Method of Moments for Mixture Models and Hidden Markov Models},
  author = {Anandkumar, Animashree and Hsu, Daniel and Kakade, Sham M.},
  year = {2012},
  journal = {Journal of Machine Learning Research},
  volume = {23},
  pages = {1--34},
  abstract = {Mixture models are a fundamental tool in applied statistics and machine learning for treating data taken from multiple subpopulations. The current practice for estimating the parameters of such models relies on local search heuristics (e.g., the EM algorithm) which are prone to failure, and existing consistent methods are unfavorable due to their high computational and sample complexity which typically scale exponentially with the number of mixture components. This work develops an efficient method of moments approach to parameter estimation for a broad class of high-dimensional mixture models with many components, including multi-view mixtures of Gaussians (such as mixtures of axis-aligned Gaussians) and hidden Markov models. The new method leads to rigorous unsupervised learning results for mixture models that were not achieved by previous works; and, because of its simplicity, it offers a viable alternative to EM for practical deployment. \textcopyright{} 2012 A. Anandkumar, D. Hsu \& S.M. Kakade.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FWCRWKGT/anandkumar12.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@anandkumarMethodMomentsMixture2012.md}
}

@article{anandkumarSpectralAlgorithmLatent2012,
  title = {A Spectral Algorithm for {{Latent Dirichlet Allocation}}},
  author = {Anandkumar, Anima and Foster, Dean P. and Hsu, Daniel and Kakade, Sham M. and Liu, Yi Kai},
  year = {2012},
  journal = {Advances in Neural Information Processing Systems},
  volume = {2},
  pages = {917--925},
  issn = {9781627480031},
  abstract = {Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by multiple latent factors (topics), as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden. This work provides a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (i.e., third order moments, which may be estimated with documents containing just three words). The method, called Excess Correlation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on {$\kappa$} \texttimes{} {$\kappa$} matrices, where {$\kappa$} is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GPXNUELJ/4637-a-spectral-algorithm-for-latent-dirichlet-allocation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@anandkumarSpectralAlgorithmLatent2012.md}
}

@book{anastasioTutorialNeuralSystems2010,
  title = {Tutorial on {{Neural Systems Modeling}}},
  author = {Anastasio, Thomas J.},
  year = {2010},
  month = mar,
  publisher = {{Sinauer}},
  abstract = {For students of neuroscience and cognitive science who wish to explore the functioning of the brain further, but lack an extensive background in computer programming or maths, this new book makes neural systems modelling truly accessible. Short, simple MATLAB computer programs give readers all the experience necessary to run their own simulations.},
  googlebooks = {3lddPgAACAAJ},
  isbn = {978-0-87893-339-6},
  langid = {english},
  keywords = {Medical / General,Medical / Neuroscience,Science / Life Sciences / Neuroscience},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PBR9M8LD/Anastasio_2010_Tutorial on Neural Systems Modeling.pdf}
}

@article{anastassiouEphapticCouplingCortical2011,
  title = {Ephaptic Coupling of Cortical Neurons},
  author = {Anastassiou, Costas A and Perin, Rodrigo and Markram, Henry and Koch, Christof},
  year = {2011},
  month = feb,
  journal = {Nature Neuroscience},
  volume = {14},
  number = {2},
  pages = {217--223},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nn.2727},
  abstract = {Ephaptic coupling is the feedback of extracellular fields onto the electrical potential across the neuronal membrane, independent of synapses. Here, the authors report that, under physiological conditions, endogenous brain activity can affect neural function through field effects.},
  keywords = {Computational neuroscience},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GRH7F5S5/Anastassiou et al. - 2011 - Ephaptic coupling of cortical neurons.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@anastassiouEphapticCouplingCortical2011.md}
}

@techreport{anfinsenPrinciplesThatGovern1973,
  title = {Principles That {{Govern}} the {{Folding}} of {{Protein Chains}}},
  author = {Anfinsen, Christian B},
  year = {1973},
  journal = {New Series},
  volume = {181},
  number = {4096},
  pages = {223--230},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MDF6XUA6/Anfinsen - 1973 - Principles that Govern the Folding of Protein Chains.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@anfinsenPrinciplesThatGovern1973.md}
}

@article{anselmiComputationalModelGrid2019,
  title = {A Computational Model for Grid Maps in Neural Populations},
  author = {Anselmi, Fabio and Franceschiello, Benedetta and Murray, Micah M. and Rosasco, Lorenzo},
  year = {2019},
  month = feb,
  doi = {10.13140/RG.2.2.16838.93765},
  abstract = {Grid cells in the entorhinal cortex, together with place, speed and border cells, are major contributors to the organization of spatial representations in the brain. In this contribution we introduce a novel theoretical and algorithmic framework able to explain the emergence of hexagonal grid-like response patterns from the statistics of the input stimuli. We show that this pattern is a result of minimal variance encoding of neurons. The novelty lies into the formulation of the encoding problem through the modern Frame Theory language, specifically that of equiangular Frames, providing new insights about the optimality of hexagonal grid receptive fields. The model proposed overcomes some crucial limitations of the current attractor and oscillatory models. It is based on the well-accepted and tested hypothesis of Hebbian learning, providing a simplified cortical-based framework that does not require the presence of theta velocity-driven oscillations (oscillatory model) or translational symmetries in the synaptic connections (attractor model). We moreover demonstrate that the proposed encoding mechanism naturally maps shifts, rotations and scaling of the stimuli onto the shape of grid cells' receptive fields, giving a straightforward explanation of the experimental evidence of grid cells remapping under transformations of environmental cues.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X7H622SS/Anselmi et al. - 2019 - A computational model for grid maps in neural populations.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@anselmiComputationalModelGrid2019.md}
}

@incollection{ARNABOLDI20159,
  title = {Chapter 2 - Human Social Networks},
  booktitle = {Online Social Networks},
  author = {Arnaboldi, Valerio and Passarella, Andrea and Conti, Marco and Dunbar, Robin I.M.},
  editor = {Arnaboldi, Valerio and Passarella, Andrea and Conti, Marco and Dunbar, Robin I.M.},
  year = {2015},
  series = {Computer Science Reviews and Trends},
  pages = {9--35},
  publisher = {{Elsevier}},
  address = {{Boston}},
  doi = {10.1016/B978-0-12-803023-3.00002-3},
  isbn = {978-0-12-803023-3},
  keywords = {Ego networks,Social networks,Tie strength},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ARNABOLDI20159.md}
}

@article{aronovMappingNonspatialDimension2017,
  title = {Mapping of a Non-Spatial Dimension by the Hippocampal-Entorhinal Circuit},
  author = {Aronov, Dmitriy and Nevers, Rhino and Tank, David W.},
  year = {2017},
  month = mar,
  journal = {Nature},
  volume = {543},
  number = {7647},
  pages = {719--722},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nature21692},
  abstract = {During spatial navigation, neural activity in the hippocampus and the medial entorhinal cortex (MEC) is correlated to navigational variables such as location, head direction, speed, and proximity to boundaries. These activity patterns are thought to provide a map-like representation of physical space. However, the hippocampal-entorhinal circuit is involved not only in spatial navigation, but also in a variety of memory-guided behaviours. The relationship between this general function and the specialized spatial activity patterns is unclear. A conceptual framework reconciling these views is that spatial representation is just one example of a more general mechanism for encoding continuous, task-relevant variables. Here we tested this idea by recording from hippocampal and entorhinal neurons during a task that required rats to use a joystick to manipulate sound along a continuous frequency axis. We found neural representation of the entire behavioural task, including activity that formed discrete firing fields at particular sound frequencies. Neurons involved in this representation overlapped with the known spatial cell types in the circuit, such as place cells and grid cells. These results suggest that common circuit mechanisms in the hippocampal-entorhinal system are used to represent diverse behavioural tasks, possibly supporting cognitive processes beyond spatial navigation.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@aronovMappingNonspatialDimension2017.md}
}

@article{artalejoSISSIRStochastic2011,
  title = {The {{SIS}} and {{SIR}} Stochastic Epidemic Models: {{A}} Maximum Entropy Approach},
  author = {Artalejo, J. R. and {Lopez-Herrero}, M. J.},
  year = {2011},
  month = dec,
  journal = {Theoretical Population Biology},
  volume = {80},
  number = {4},
  pages = {256--264},
  publisher = {{Academic Press}},
  doi = {10.1016/j.tpb.2011.09.005},
  abstract = {We analyze the dynamics of infectious disease spread by formulating the maximum entropy (ME) solutions of the susceptible-infected-susceptible (SIS) and the susceptible-infected-removed (SIR) stochastic models. Several scenarios providing helpful insight into the use of the ME formalism for epidemic modeling are identified. The ME results are illustrated with respect to several descriptors, including the number of recovered individuals and the time to extinction. An application to infectious data from outbreaks of extended spectrum beta lactamase (ESBL) in a hospital is also considered. \textcopyright{} 2011 Elsevier Inc.},
  keywords = {Epidemiology,Final size,Markov chains,Maximum entropy,Time to extinction},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CEGFX5AJ/Artalejo, Lopez-Herrero - 2011 - The SIS and SIR stochastic epidemic models A maximum entropy approach.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@artalejoSISSIRStochastic2011.md}
}

@article{arulkumaranDeepReinforcementLearning2017,
  title = {Deep {{Reinforcement Learning}}: {{A Brief Survey}}},
  shorttitle = {Deep {{Reinforcement Learning}}},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year = {2017},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  pages = {26--38},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2743240},
  abstract = {Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higher-level understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.},
  keywords = {Artificial intelligence,Learning (artificial intelligence),Machine learning,Neural networks,Signal processing algorithms,Visualization},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/I9LX2RHC/Arulkumaran et al_2017_Deep Reinforcement Learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3MD8FPY6/8103164.html}
}

@article{asabukiInteractiveReservoirComputing2018,
  title = {Interactive Reservoir Computing for Chunking Information Streams},
  author = {Asabuki, Toshitake and Hiratani, Naoki and Fukai, Tomoki},
  year = {2018},
  month = oct,
  journal = {PLOS Computational Biology},
  volume = {14},
  number = {10},
  pages = {e1006400},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006400},
  abstract = {Chunking is the process by which frequently repeated segments of temporal inputs are concatenated into single units that are easy to process. Such a process is fundamental to time-series analysis in biological and artificial information processing systems. The brain efficiently acquires chunks from various information streams in an unsupervised manner; however, the underlying mechanisms of this process remain elusive. A widely-adopted statistical method for chunking consists of predicting frequently repeated contiguous elements in an input sequence based on unequal transition probabilities over sequence elements. However, recent experimental findings suggest that the brain is unlikely to adopt this method, as human subjects can chunk sequences with uniform transition probabilities. In this study, we propose a novel conceptual framework to overcome this limitation. In this process, neural networks learn to predict dynamical response patterns to sequence input rather than to directly learn transition patterns. Using a mutually supervising pair of reservoir computing modules, we demonstrate how this mechanism works in chunking sequences of letters or visual images with variable regularity and complexity. In addition, we demonstrate that background noise plays a crucial role in correctly learning chunks in this model. In particular, the model can successfully chunk sequences that conventional statistical approaches fail to chunk due to uniform transition probabilities. In addition, the neural responses of the model exhibit an interesting similarity to those of the basal ganglia observed after motor habit formation.},
  langid = {english},
  keywords = {Cell signaling structures,Community structure,Learning,Neural networks,Neurons,Recurrent neural networks,Signaling networks,Teachers},
  file = {C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@asabukiInteractiveReservoirComputing2018.md;C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Zotero\\storage\\Asabuki et al_2018_Interactive reservoir computing for chunking information streams.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/25M3AYZP/article.html}
}

@article{aurellDynamicsPerformanceSusceptibility2010,
  title = {Dynamics and Performance of Susceptibility Propagation on Synthetic Data},
  author = {Aurell, E. and Ollion, C. and Roudi, Y.},
  year = {2010},
  month = oct,
  journal = {The European Physical Journal B},
  volume = {77},
  number = {4},
  pages = {587--595},
  publisher = {{Springer-Verlag}},
  doi = {10.1140/epjb/e2010-00277-0},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CE3TVYKX/Aurell, Ollion, Roudi - 2010 - Dynamics and performance of susceptibility propagation on synthetic data.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@aurellDynamicsPerformanceSusceptibility2010.md}
}

@article{aurellInverseIsingInference2012,
  title = {Inverse {{Ising Inference Using All}} the {{Data}}},
  author = {Aurell, Erik and Ekeberg, Magnus},
  year = {2012},
  doi = {10.1103/PhysRevLett.108.090201},
  abstract = {We show that a method based on logistic regression, using all the data, solves the inverse Ising problem far better than mean-field calculations relying only on sample pairwise correlation functions, while still computationally feasible for hundreds of nodes. The largest improvement in reconstruction occurs for strong interactions. Using two examples, a diluted Sherrington-Kirkpatrick model and a two-dimensional lattice, we also show that interaction topologies can be recovered from few samples with good accuracy and that the use of l 1 regularization is beneficial in this process, pushing inference abilities further into low-temperature regimes.},
  keywords = {(),0510Àa,7510Nr,numbers: 0250Tt},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SHA66JZP/Aurell, Ekeberg - 2012 - Inverse Ising inference using all the data.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Y5JTEFA4/Aurell, Ekeberg - Unknown - Inverse Ising Inference Using All the Data.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@aurellInverseIsingInference2012.md}
}

@article{avena-koenigsbergerCommunicationDynamicsComplex2017,
  title = {Communication Dynamics in Complex Brain Networks},
  author = {{Avena-Koenigsberger}, Andrea and Misic, Bratislav and Sporns, Olaf},
  year = {2017},
  month = dec,
  journal = {Nature Reviews. Neuroscience},
  volume = {19},
  number = {1},
  pages = {17--33},
  issn = {1471-0048},
  doi = {10.1038/nrn.2017.149},
  abstract = {Neuronal signalling and communication underpin virtually all aspects of brain activity and function. Network science approaches to modelling and analysing the dynamics of communication on networks have proved useful for simulating functional brain connectivity and predicting emergent network states. This Review surveys important aspects of communication dynamics in brain networks. We begin by sketching a conceptual framework that views communication dynamics as a necessary link between the empirical domains of structural and functional connectivity. We then consider how different local and global topological attributes of structural networks support potential patterns of network communication, and how the interactions between network topology and dynamic models can provide additional insights and constraints. We end by proposing that communication dynamics may act as potential generative models of effective connectivity and can offer insight into the mechanisms by which brain networks transform and process information.},
  langid = {english},
  pmid = {29238085},
  keywords = {Brain,Humans,Mental Processes,Models; Neurological,Neural Pathways}
}

@techreport{ayyashUniversalLanguageNetwork2021,
  type = {Preprint},
  title = {The Universal Language Network: {{A}} Cross-Linguistic Investigation Spanning 45 Languages and 11 Language Families},
  shorttitle = {The Universal Language Network},
  author = {Ayyash, Dima and {Malik-Moraleda}, Saima and Gall{\'e}e, Jeanne and Affourtit, Josef and Hoffman, Malte and Mineroff, Zachary and Jouravlev, Olessia and Fedorenko, Evelina},
  year = {2021},
  month = jul,
  institution = {{Neuroscience}},
  doi = {10.1101/2021.07.28.454040},
  abstract = {To understand the architecture of human language, it is critical to examine diverse languages; yet most cognitive neuroscience research has focused on a handful of primarily Indo-European languages. Here, we report a large-scale investigation of the fronto-temporal language network across 45 languages and establish the cross-linguistic generality of its key functional properties, including general topography, left-lateralization, strong functional integration among its brain regions, and functional selectivity for language processing.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AYCLCPYH/Ayyash et al. - 2021 - The universal language network A cross-linguistic.pdf}
}

@misc{ayyashUniversalLanguageNetwork2021a,
  title = {The Universal Language Network: {{A}} Cross-Linguistic Investigation Spanning 45 Languages and 11 Language Families},
  shorttitle = {The Universal Language Network},
  author = {Ayyash, Dima and {Malik-Moraleda}, Saima and Gall{\'e}e, Jeanne and Affourtit, Josef and Hoffman, Malte and Mineroff, Zachary and Jouravlev, Olessia and Fedorenko, Evelina},
  year = {2021},
  month = jul,
  pages = {2021.07.28.454040},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.07.28.454040},
  abstract = {To understand the architecture of human language, it is critical to examine diverse languages; yet most cognitive neuroscience research has focused on a handful of primarily Indo-European languages. Here, we report a large-scale investigation of the fronto-temporal language network across 45 languages and establish the cross-linguistic generality of its key functional properties, including general topography, left-lateralization, strong functional integration among its brain regions, and functional selectivity for language processing.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Ayyash et al_2021_The universal language network.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/R9QJYIU5/2021.07.28.html}
}

@article{azeredodasilveiraGeometryInformationCoding2021,
  title = {The {{Geometry}} of {{Information Coding}} in {{Correlated Neural Populations}}},
  author = {{Azeredo da Silveira}, Rava and Rieke, Fred},
  year = {2021},
  journal = {Annual Review of Neuroscience},
  volume = {44},
  number = {1},
  pages = {403--424},
  doi = {10.1146/annurev-neuro-120320-082744},
  abstract = {Neurons in the brain represent information in their collective activity. The fidelity of this neural population code depends on whether and how variability in the response of one neuron is shared with other neurons. Two decades of studies have investigated the influence of these noise correlations on the properties of neural coding. We provide an overview of the theoretical developments on the topic. Using simple, qualitative, and general arguments, we discuss, categorize, and relate the various published results. We emphasize the relevance of the fine structure of noise correlation, and we present a new approach to the issue. Throughout this review, we emphasize a geometrical picture of how noise correlations impact the neural code.},
  pmid = {33863252},
  keywords = {correlations,neural coding,neural computation},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-neuro-120320-082744},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Azeredo da Silveira_Rieke_2021_The Geometry of Information Coding in Correlated Neural Populations.pdf}
}

@article{baarsArchitecturalModelConscious2007,
  title = {An Architectural Model of Conscious and Unconscious Brain Functions: {{Global Workspace Theory}} and {{IDA}}},
  author = {Baars, Bernard J. and Franklin, Stan},
  year = {2007},
  month = nov,
  journal = {Neural Networks},
  volume = {20},
  number = {9},
  pages = {955--961},
  doi = {10.1016/j.neunet.2007.09.013},
  abstract = {While neural net models have been developed to a high degree of sophistication, they have some drawbacks at a more integrative, "architectural" level of analysis. We describe a "hybrid" cognitive architecture that is implementable in neuronal nets, and which has uniform brainlike features, including activation-passing and highly distributed "codelets," implementable as small-scale neural nets. Empirically, this cognitive architecture accounts qualitatively for the data described by Baars' Global Workspace Theory (GWT), and Franklin's LIDA architecture, including state-of-the-art models of conscious contents in action-planning, Baddeley-style Working Memory, and working models of episodic and semantic longterm memory. These terms are defined both conceptually and empirically for the current theoretical domain. The resulting architecture meets four desirable goals for a unified theory of cognition: practical workability, autonomous agency, a plausible role for conscious cognition, and translatability into plausible neural terms. It also generates testable predictions, both empirical and computational. \textcopyright{} 2007.},
  keywords = {Cognitive architecture,Conscious cognition,Global workspace theory,LIDA architecture},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JYW65VA9/Baars, Franklin - 2007 - An architectural model of conscious and unconscious brain functions Global Workspace Theory and IDA.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@baarsArchitecturalModelConscious2007.md}
}

@techreport{baarsCONSCIOUSNESSCOMPUTATIONALLIDA,
  title = {{{CONSCIOUSNESS IS COMPUTATIONAL}}: {{THE LIDA MODEL OF GLOBAL WORKSPACE THEORY}}},
  author = {Baars, Bernard J and Franklin, Stan},
  abstract = {We argue that the functions of consciousness are implemented in a bio-computational manner. That is to say, the conscious as well as the non-conscious aspects of human thinking, planning, and perception are produced by adaptive, biological algorithms. We propose that machine consciousness may be produced by similar adaptive algorithms running on the machine Global Workspace Theory is currently the most empirically supported and widely discussed theory of consciousness. It provides a high-level description of such algorithms, based on a large body of psychological and brain evidence. LIDA provides an explicit implementation of much of GWT, which can be shown to perform human-like tasks, such as the interactive assignment of naval jobs to sailors. Here we provide brief descriptions of both GWT and LIDA in relation to the scientific evidence bearing on consciousness in the brain. A companion article explores how this approach could lead to machine consciousness (Franklin et al 2009 to appear). We also discuss the important distinction between volition and consciously mediated action selection, and describe an operational definition of consciousness via verifiable reportability. These are issues that may well bear on the possibility of machine consciousness.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/F79FGCIA/Baars, Franklin - Unknown - CONSCIOUSNESS IS COMPUTATIONAL THE LIDA MODEL OF GLOBAL WORKSPACE THEORY.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@baarsCONSCIOUSNESSCOMPUTATIONALLIDA.md}
}

@techreport{baarsTheaterConsciousnessWorkspace,
  title = {In the {{Theater}} of {{Consciousness The Workspace}} of the {{Mind}}},
  author = {Baars, Bernard J},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BS9T2JG5/Baars - Unknown - In the Theater of Consciousness The Workspace of the Mind.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@baarsTheaterConsciousnessWorkspace.md}
}

@article{babaPartialCorrelationConditional2004,
  title = {Partial Correlation and Conditional Correlation as Measures of Conditional Independence},
  author = {Baba, Kunihiro and Shibata, Ritei and Sibuya, Masaaki},
  year = {2004},
  month = dec,
  journal = {Australian and New Zealand Journal of Statistics},
  volume = {46},
  number = {4},
  pages = {657--664},
  publisher = {{Blackwell Publishing Ltd}},
  doi = {10.1111/j.1467-842X.2004.00360.x},
  abstract = {This paper investigates the roles of partial correlation and conditional correlation as measures of the conditional independence of two random variables. It first establishes a sufficient condition for the coincidence of the partial correlation with the conditional correlation. The condition is satisfied not only for multivariate normal but also for elliptical, multivariate hypergeometric, multivariate negative hypergeometric, multinomial and Dirichlet distributions. Such families of distributions are characterized by a semigroup property as a parametric family of distributions. A necessary and sufficient condition for the coincidence of the partial covariance with the conditional covariance is also derived. However, a known family of multivariate distributions which satisfies this condition cannot be found, except for the multivariate normal. The paper also shows that conditional independence has no close ties with zero partial correlation except in the case of the multivariate normal distribution; it has rather close ties to the zero conditional correlation. It shows that the equivalence between zero conditional covariance and conditional independence for normal variables is retained by any monotone transformation of each variable. The results suggest that care must be taken when using such correlations as measures of conditional independence unless the joint distribution is known to be normal. Otherwise a new concept of conditional independence may need to be introduced in place of conditional independence through zero conditional correlation or other statistics.},
  keywords = {Elliptical distribution,Exchangeability,Graphical modelling,Monotone transformation},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@babaPartialCorrelationConditional2004.md}
}

@article{babiloniFundamentalsElectroencefalographyMagnetoencefalography2009,
  title = {Fundamentals of Electroencefalography, Magnetoencefalography, and Functional Magnetic Resonance Imaging},
  author = {Babiloni, Claudio and Pizzella, Vittorio and Gratta, Cosimo Del and Ferretti, Antonio and Romani, Gian Luca},
  year = {2009},
  journal = {International Review of Neurobiology},
  volume = {86},
  pages = {67--80},
  issn = {0074-7742},
  doi = {10.1016/S0074-7742(09)86005-4},
  abstract = {This review introduces readers to fundamentals of electroencephalography (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (fMRI). EEG and MEG signals are mainly produced by postsynaptic ionic currents of synchronically active pyramidal cortical neurons. These signals reflect the integrative information processing of neurons representing the output of cortical neural modules. EEG and MEG signals have a high temporal resolution ({$<$}1ms) ideal to investigate an emerging propriety of brain physiology, namely the brain rhythms. A background spontaneous oscillatory activity of brain neurons at about 10Hz generates dominant alpha rhythms of resting-state EEG and MEG activity. This background activity is blocked during sensory and cognitive-motor events. Standard EEG shows a low spatial resolution (5-9cm), which partially improves by high-resolution EEG including 64-128 channels and source estimation techniques (1-3cm); source estimation of MEG data shows a better spatial resolution (0.5-2cm). fMRI is an indirect measurement of regional brain activity based on the ratio between deoxyhemoglobin and oxyhemoglobin blood (BOLD) during events referenced to baseline conditions. Event-related BOLD response has low temporal resolution ({$>$}1s) and quite high spatial resolution ({$<$}1cm), and is especially suitable to investigate spatial details of both cortical and subcortical activation.},
  langid = {english},
  pmid = {19607991},
  keywords = {Animals,Brain,Brain Mapping,Electroencephalography,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Magnetoencephalography,Oxygen},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@babiloniFundamentalsElectroencefalographyMagnetoencefalography2009.md}
}

@inproceedings{baccalaGeneralizedPartialDirected2007,
  title = {Generalized {{Partial Directed Coherence}}},
  booktitle = {2007 15th {{International Conference}} on {{Digital Signal Processing}}},
  author = {Baccala, L. A. and Sameshima, K. and Takahashi, D. Y.},
  year = {2007},
  month = jul,
  pages = {163--166},
  issn = {2165-3577},
  doi = {10.1109/ICDSP.2007.4288544},
  abstract = {This paper re-examines the definition of partial directed coherence (PDC) which was recently introduced as a linear frequency-domain quantifier of the multivariate relationship between simultaneously observed time series for application in functional connectivity inference in neuroscience. The present reappraisal aims at improving PDC's performance under scenarios that involve severely unbalanced predictive modelling errors (innovations noise). The present modification turns out to be more robust in estimating imprecisions associated with finite time series samples.},
  keywords = {Coherence,Data analysis,Frequency domain analysis,Granger Causality,hypothesis testing,Neural connectivity,Neuroscience,Noise robustness,partial directed coherence,Predictive models,Rhythm,Technological innovation,Testing,Time series analysis,variance stabilization},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NT9W6AFV/Baccala et al. - 2007 - Generalized Partial Directed Coherence.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@baccalaGeneralizedPartialDirected2007.md;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\baccalaGeneralizedPartialDirected2007-zotero.md;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\baccalaGeneralizedPartialDirected2007.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PTZPFUC4/4288544.html}
}

@article{bailly-bechetInferenceSparseCombinatorialcontrol2010,
  title = {Inference of Sparse Combinatorial-Control Networks from Gene-Expression Data: A Message Passing Approach},
  author = {{Bailly-Bechet}, Marc and Braunstein, Alfredo and Pagnani, Andrea and Weigt, Martin and Zecchina, Riccardo},
  year = {2010},
  month = jun,
  journal = {BMC Bioinformatics},
  volume = {11},
  number = {1},
  pages = {355--355},
  publisher = {{Springer Nature}},
  doi = {10.1186/1471-2105-11-355},
  abstract = {BACKGROUND Transcriptional gene regulation is one of the most important mechanisms in controlling many essential cellular processes, including cell development, cell-cycle control, and the cellular response to variations in environmental conditions. Genes are regulated by transcription factors and other genes/proteins via a complex interconnection network. Such regulatory links may be predicted using microarray expression data, but most regulation models suppose transcription factor independence, which leads to spurious links when many genes have highly correlated expression levels. RESULTS We propose a new algorithm to infer combinatorial control networks from gene-expression data. Based on a simple model of combinatorial gene regulation, it includes a message-passing approach which avoids explicit sampling over putative gene-regulatory networks. This algorithm is shown to recover the structure of a simple artificial cell-cycle network model for baker's yeast. It is then applied to a large-scale yeast gene expression dataset in order to identify combinatorial regulations, and to a data set of direct medical interest, namely the Pleiotropic Drug Resistance (PDR) network. CONCLUSIONS The algorithm we designed is able to recover biologically meaningful interactions, as shown by recent experimental results 1. Moreover, new cases of combinatorial control are predicted, showing how simple models taking this phenomenon into account can lead to informative predictions and allow to extract more putative regulatory interactions from microarray databases.},
  keywords = {Algorithms,Bioinformatics,Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Microarrays},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4NQBIIRS/Bailly-Bechet et al. - 2010 - Inference of sparse combinatorial-control networks from gene-expression data a message passing approach.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bailly-bechetInferenceSparseCombinatorialcontrol2010.md}
}

@book{balasubramanianGeometricFormulationOccam1996,
  title = {A {{Geometric Formulation}} of {{Occam}}'s {{Razor For Inference}} of {{Parametric Distributions}}},
  author = {Balasubramanian, Vijay},
  year = {1996},
  abstract = {I define a natural measure of the complexity of a parametric distribution relative to a given true distribution called the razor of a model family. The Minimum Description Length principle (MDL) and Bayesian inference are shown to give empirical approximations of the razor via an analysis that significantly extends existing results on the asymptotics of Bayesian model selection. I treat parametric families as manifolds embedded in the space of distributions and derive a canonical metric and a measure on the parameter manifold by appealing to the classical theory of hypothesis testing. I find that the Fisher information is the natural measure of distance, and give a novel justification for a choice of Jeffreys prior for Bayesian inference. The results of this paper suggest corrections to MDL that can be important for model selection with a small amount of data. These corrections are interpreted as natural measures of the simplicity of a model family. I show that in a certain sense the logarithm of the Bayesian posterior converges to the logarithm of the razor of a model family as defined here. Close connections with known results on density estimation and ``information geometry'' are discussed as they arise.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MJMHV555/Balasubramanian - 1996 - A Geometric Formulation of Occam’s Razor For Infer.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@balasubramanianGeometricFormulationOccam1996.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4APT3CC4/summary.html}
}

@techreport{balasubramanianGeometricFormulationOccam1996a,
  title = {A {{Geometric Formulation}} of {{Occam}}'s {{Razor For Inference}} of {{Parametric Distributions}}},
  author = {Balasubramanian, Vijay},
  year = {1996},
  abstract = {I define a natural measure of the complexity of a parametric distribution relative to a given true distribution called the razor of a model family. The Minimum Description Length principle (MDL) and Bayesian inference are shown to give empirical approximations of the razor via an analysis that significantly extends existing results on the asymptotics of Bayesian model selection. I treat parametric families as manifolds embedded in the space of distributions and derive a canonical metric and a measure on the parameter manifold by appealing to the classical theory of hypothesis testing. I find that the Fisher information is the natural measure of distance, and give a novel justification for a choice of Jeffreys prior for Bayesian inference. The results of this paper suggest corrections to MDL that can be important for model selection with a small amount of data. These corrections are interpreted as natural measures of the simplicity of a model family. I show that in a certain sense the logarithm of the Bayesian posterior converges to the logarithm of the razor of a model family as defined here. Close connections with known results on density estimation and "information geometry" are discussed as they arise.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3D385IZC/Balasubramanian - 1996 - A Geometric Formulation of Occam's Razor For Inference of Parametric Distributions.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@balasubramanianGeometricFormulationOccam1996a.md}
}

@misc{balasubramanianStatisticalInferenceOccam1996,
  title = {Statistical {{Inference}}, {{Occam}}'s {{Razor}} and {{Statistical Mechanics}} on {{The Space}} of {{Probability Distributions}}},
  author = {Balasubramanian, Vijay},
  year = {1996},
  journal = {Neural Computation},
  doi = {10.1162/neco.1997.9.2.349},
  abstract = {The task of parametric model selection is cast in terms of a statistical mechanics on the space of probability distributions. Using the techniques of low-temperature expansions, we arrive at a systematic series for the Bayesian posterior probability of a model family that significantly extends known results in the literature. In particular, we arrive at a precise understanding of how Occam's Razor, the principle that simpler models should be preferred until the data justifies more complex models, is automatically embodied by probability theory. These results require a measure on the space of model parameters and we derive and discuss an interpretation of Jeffreys' prior distribution as a uniform prior over the distributions indexed by a family. Finally, we derive a theoretical index of the complexity of a parametric family relative to some true distribution that we call the \{\textbackslash it razor\} of the model. The form of the razor immediately suggests several interesting questions in the theory of learning that can be studied using the techniques of statistical mechanics.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9THBBVTC/Balasubramanian - 1996 - Statistical Inference, Occam's Razor and Statistical Mechanics on The Space of Probability Distributions(2).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FRNUMAQ6/Balasubramanian - 1996 - Statistical Inference, Occam's Razor and Statistical Mechanics on The Space of Probability Distributions.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@balasubramanianStatisticalInferenceOccam1996.md}
}

@article{balasubramanianStatisticalInferenceOccam1997,
  title = {Statistical Inference, {{Occam}}'s Razor, and Statistical Mechanics on the Space of Probability Distributions},
  author = {Balasubramanian, Vijay},
  year = {1997},
  month = feb,
  journal = {Neural Computation},
  volume = {9},
  number = {2},
  pages = {349--368},
  publisher = {{MIT Press Journals}},
  doi = {10.1162/neco.1997.9.2.349},
  abstract = {The task of parametric model selection is cast in terms of a statistical mechanics on the space of probability distributions. Using the techniques of low-temperature expansions, I arrive at a systematic series for the Bayesian posterior probability of a model family that significantly extends known results in the literature. In particular, I arrive at a precise understanding of how Occam's razor, the principle that simpler models should be preferred until the data justify more complex models, is automatically embodied by probability theory. These results require a measure on the space of model parameters and I derive and discuss an interpretation of Jeffreys' prior distribution as a uniform prior over the distributions indexed by a family. Finally, I derive a theoretical index of the complexity of a parametric family relative to some true distribution that I call the razor of the model. The form of the razor immediately suggests several interesting questions in the theory of learning that can be studied using the techniques of statistical mechanics.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@balasubramanianStatisticalInferenceOccam1997.md}
}

@article{balcerekRetrosplenialCortexSpatial2021,
  title = {Retrosplenial Cortex in Spatial Memory: Focus on Immediate Early Genes Mapping},
  shorttitle = {Retrosplenial Cortex in Spatial Memory},
  author = {Balcerek, Edyta and W{\l}odkowska, Urszula and Czajkowski, Rafa{\l}},
  year = {2021},
  month = dec,
  journal = {Molecular Brain},
  volume = {14},
  number = {1},
  pages = {172},
  issn = {1756-6606},
  doi = {10.1186/s13041-021-00880-w},
  abstract = {The ability to form, retrieve and update autobiographical memories is one of the most fascinating features of human behavior. Spatial memory, the ability to remember the layout of the external environment and to navigate within its boundaries, is closely related to the autobiographical memory domain. It is served by an overlapping brain circuit, centered around the hippocampus (HPC) where the cognitive map index is stored. Apart from the hippocampus, several cortical structures participate in this process. Their relative contribution is a subject of intense research in both humans and animal models. One of the most widely studied regions is the retrosplenial cortex (RSC), an area in the parietal lobe densely interconnected with the hippocampal formation. Several methodological approaches have been established over decades in order to investigate the cortical aspects of memory. One of the most successful techniques is based on the analysis of brain expression patterns of the immediate early genes (IEGs). The common feature of this diverse group of genes is fast upregulation of their mRNA translation upon physiologically relevant stimulus. In the central nervous system they are rapidly triggered by neuronal activity and plasticity during learning. There is a widely accepted consensus that their expression level corresponds to the engagement of individual neurons in the formation of memory trace. Imaging of the IEGs might therefore provide a picture of an emerging memory engram. In this review we present the overview of IEG mapping studies of retrosplenial cortex in rodent models. We begin with classical techniques, immunohistochemical detection of protein and fluorescent in situ hybridization of mRNA. We then proceed to advanced methods where fluorescent genetically encoded IEG reporters are chronically followed in vivo during memory formation. We end with a combination of genetic IEG labelling and optogenetic approach, where the activity of the entire engram is manipulated. We finally present a hypothesis that attempts to unify our current state of knowledge about the function of RSC.},
  keywords = {Arc,c-Fos,Homer,Immediate early genes,Retrosplenial cortex,Spatial memory},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WXCPSEGJ/Balcerek et al. - 2021 - Retrosplenial cortex in spatial memory focus on i.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UV72QSBK/s13041-021-00880-w.html}
}

@article{balciPeakIntervalProcedure2020,
  title = {The {{Peak Interval Procedure}} in {{Rodents}}: {{A Tool}} for {{Studying}} the {{Neurobiological Basis}} of {{Interval Timing}} and {{Its Alterations}} in {{Models}} of {{Human Disease}}},
  shorttitle = {The {{Peak Interval Procedure}} in {{Rodents}}},
  author = {Balc{\i}, Fuat and Freestone, David},
  year = {2020},
  month = sep,
  journal = {Bio-protocol},
  volume = {10},
  number = {17},
  pages = {e3735},
  issn = {2331-8325},
  doi = {10.21769/BioProtoc.3735},
  abstract = {Animals keep track of time intervals in the seconds to minutes range with, on average, high accuracy but substantial trial-to-trial variability. The ability to detect the statistical signatures of such timing behavior is an indispensable feature of a good and theoretically-tractable testing procedure. A widely used interval timing procedure is the peak interval (PI) procedure, where animals learn to anticipate rewards that become available after a fixed delay. After learning, they cluster their responses around that reward-availability time. The in-depth analysis of such timed anticipatory responses leads to the understanding of an internal timing mechanism, that is, the processing dynamics and systematic biases of the brain's clock. This protocol explains in detail how the PI procedure can be implemented in rodents, from training through testing to analysis. We showcase both trial-by-trial and trial-averaged analytical methods as a window into these internal processes. This protocol has the advantages of capturing timing behavior in its full-complexity in a fashion that allows for a theoretical treatment of the data.},
  pmcid = {PMC7854006},
  pmid = {33659396},
  file = {/home/michaelt/Downloads/vivaldi/MarkDown Clips/The Peak Interval Procedure in Rodents A Tool for Studying the Neurobiological Basis of Interval Timing and Its Alterations in Models of Human Disease - 29.06.22.md}
}

@article{balleriniInteractionRulingAnimal2008,
  title = {Interaction Ruling Animal Collective Behavior Depends on Topological Rather than Metric Distance: {{Evidence}} from a Field Study},
  author = {Ballerini, M. and Cabibbo, N. and Candelier, R. and Cavagna, A. and Cisbani, E. and Giardina, I. and Lecomte, V. and Orlandi, A. and Parisi, G. and Procaccini, A. and Viale, M. and Zdravkovic, V.},
  year = {2008},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {105},
  number = {4},
  pages = {1232--1237},
  publisher = {{National Academy of Sciences}},
  doi = {10.1073/pnas.0711437105},
  abstract = {Numerical models indicate that collective animal behavior may emerge from simple local rules of interaction among the individuals. However, very little is known about the nature of such interaction, so that models and theories mostly rely on aprioristic assumptions. By reconstructing the three-dimensional positions of individual birds in airborne flocks of a few thousand members, we show that the interaction does not depend on the metric distance, as most current models and theories assume, but rather on the topological distance. In fact, we discovered that each bird interacts on average with a fixed number of neighbors (six to seven), rather than with all neighbors within a fixed metric distance. We argue that a topological interaction is indispensable to maintain a flock's cohesion against the large density changes caused by external perturbations, typically predation. We support this hypothesis by numerical simulations, showing that a topological interaction grants significantly higher cohesion of the aggregation compared with a standard metric one. \textcopyright{} 2008 by The National Academy of Sciences of the USA.},
  keywords = {Animal groups,Behavioral rules,Flocking,Self-organization},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4D8QDYII/Ballerini et al. - 2008 - Interaction ruling animal collective behavior depends on topological rather than metric distance Evidence from.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@balleriniInteractionRulingAnimal2008.md}
}

@article{banavarApplicationsPrincipleMaximum2010,
  title = {Applications of the Principle of Maximum Entropy: {{From}} Physics to Ecology},
  author = {Banavar, Jayanth R. and Maritan, Amos and Volkov, Igor},
  year = {2010},
  journal = {Journal of Physics Condensed Matter},
  doi = {10.1088/0953-8984/22/6/063101},
  abstract = {There are numerous situations in physics and other disciplines which can be described at different levels of detail in terms of probability distributions. Such descriptions arise either intrinsically as in quantum mechanics, or because of the vast amount of details necessary for a complete description as, for example, in Brownian motion and in many-body systems. We show that an application of the principle of maximum entropy for estimating the underlying probability distribution can depend on the variables used for describing the system. The choice of characterization of the system carries with it implicit assumptions about fundamental attributes such as whether the system is classical or quantum mechanical or equivalently whether the individuals are distinguishable or indistinguishable. We show that the correct procedure entails the maximization of the relative entropy subject to known constraints and, additionally, requires knowledge of the behavior of the system in the absence of these constraints. We present an application of the principle of maximum entropy to understanding species diversity in ecology and introduce a new statistical ensemble corresponding to the distribution of a variable population of individuals into a set of species not defined a priori.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MV56KA74/Banavar, Maritan, Volkov - 2010 - Applications of the principle of maximum entropy From physics to ecology.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@banavarApplicationsPrincipleMaximum2010.md}
}

@article{bar-josephStudyingModellingDynamic2012,
  title = {Studying and Modelling Dynamic Biological Processes Using Time-Series Gene Expression Data},
  author = {{Bar-Joseph}, Ziv and Gitter, Anthony and Simon, Itamar},
  year = {2012},
  month = jul,
  journal = {Nature Reviews. Genetics},
  volume = {13},
  number = {8},
  pages = {552--564},
  issn = {1471-0064},
  doi = {10.1038/nrg3244},
  abstract = {Biological processes are often dynamic, thus researchers must monitor their activity at multiple time points. The most abundant source of information regarding such dynamic activity is time-series gene expression data. These data are used to identify the complete set of activated genes in a biological process, to infer their rates of change, their order and their causal effects and to model dynamic systems in the cell. In this Review we discuss the basic patterns that have been observed in time-series experiments, how these patterns are combined to form expression programs, and the computational analysis, visualization and integration of these data to infer models of dynamic biological systems.},
  langid = {english},
  pmid = {22805708},
  keywords = {Animals,Data Interpretation; Statistical,Epigenesis; Genetic,Gene Expression,Gene Expression Profiling,Humans,Mice,Models; Genetic,Signal Transduction},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@bar-josephStudyingModellingDynamic2012.md}
}

@article{barronMinimumDescriptionLength1998,
  title = {The {{Minimum Description Length Principle}} in {{Coding}} and {{Modeling}}},
  author = {Barron, Andrew and Rissanen, Jorma and Yu, Bin},
  year = {1998},
  journal = {IEEE Transactions on Information Theory},
  volume = {44},
  number = {6},
  pages = {2743--2760},
  doi = {10.1109/18.720554},
  abstract = {We review the principles of Minimum Description Length and Stochastic Complexity as used in data compression and statistical modeling. Stochastic complexity is formulated as the solution to optimum universal coding problems extending Shannon's basic source coding theorem. The normalized maximized likelihood, mixture, and predictive codings are each shown to achieve the stochastic complexity to within asymptotically vanishing terms. We assess the performance of the minimum description length criterion both from the vantage point of quality of data compression and accuracy of statistical inference. Context tree modeling, density estimation, and model selection in Gaussian linear regression serve as examples. \textcopyright{} 1998 IEEE.},
  keywords = {Complexity,Compression,Estimation,Inference,Universal modeling},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BLQW3X6K/00720554.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MISYHMEC/00720554(1).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@barronMinimumDescriptionLength1998.md}
}

@article{barryModelsGridCells2012,
  title = {Models of Grid Cells and Theta Oscillations},
  author = {Barry, Caswell and Bush, Daniel and O'keefe, John and Burgess, Neil},
  year = {2012},
  journal = {Nature},
  volume = {488},
  number = {7409},
  pages = {E1-E2},
  doi = {10.1038/nature11276},
  abstract = {Arising from M. M.Yartsev, M. P. Witter \& N. Ulanovsky Nature 479,\textbackslash n103\textendash 107 (2011) Grid cells recorded in the medial entorhinal cortex\textbackslash n(\{MEC\}) of freely moving rodents show a markedly regular spatial\textbackslash nfiring pattern whose underlying mechanism has been the subject of\textbackslash nintense interest. Yartsev et al. report that the firing of grid cells\textbackslash nin crawling bats does not show theta rhythmicity ``causally disproving\textbackslash na major class of computational models'' of grid cell firing that rely\textbackslash non oscillatory interference. However, their data may be consistent\textbackslash nwith these models, with the apparent lack of theta rhythmicity reflecting\textbackslash nslow movement speeds and low firing rates. Thus, the conclusion of\textbackslash nYartsev et al. is not supported by their data.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JQUSPY93/Barry et al. - 2012 - Models of grid cells and theta oscillations.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@barryModelsGridCells2012.md}
}

@article{bartonIsingModelsNeural2013,
  title = {Ising Models for Neural Activity Inferred via Selective Cluster Expansion: {{Structural}} and Coding Properties},
  author = {Barton, John and Cocco, Simona},
  year = {2013},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2013},
  number = {3},
  doi = {10.1088/1742-5468/2013/03/P03002},
  abstract = {We describe the selective cluster expansion (SCE) of the entropy, a method for inferring an Ising model which describes the correlated activity of populations of neurons. We re-analyze data obtained from multielectrode recordings performed in vitro on the retina and in vivo on the prefrontal cortex. Recorded population sizes N range from N = 37 to 117 neurons. We compare the SCE method with the simplest mean field methods (corresponding to a Gaussian model) and with regularizations which favor sparse networks (L 1 norm) or penalize large couplings (L2 norm). The network of the strongest interactions inferred via mean field methods generally agree with those obtained from SCE. Reconstruction of the sampled moments of the distributions, corresponding to neuron spiking frequencies and pairwise correlations, and the prediction of higher moments including three-cell correlations and multi-neuron firing frequencies, is more difficult than determining the large-scale structure of the interaction network, and, apart from a cortical recording in which the measured correlation indices are small, these goals are achieved with the SCE but not with mean field approaches. We also find differences in the inferred structure of retinal and cortical networks: inferred interactions tend to be more irregular and sparse for cortical data than for retinal data. This result may reflect the structure of the recording. As a consequence, the SCE is more effective for retinal data when expanding the entropy with respect to a mean field reference S - S MF, while expansions of the entropy S alone perform better for cortical data. \textcopyright{} 2013 IOP Publishing Ltd and SISSA Medialab srl.},
  keywords = {neuronal networks (theory),statistical inference},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/E5L44YDQ/barton-neural.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bartonIsingModelsNeural2013.md}
}

@article{bartonLargePseudocounts2norm,
  title = {Large Pseudocounts and {{L}} 2-Norm Penalties Are Necessary for the Mean-Field Inference of {{Ising}} and {{Potts}} Models},
  author = {Barton, J P and Cocco, S and De Leonardis, E and Monasson, R},
  doi = {10.1103/PhysRevE.90.012132},
  abstract = {The mean-field (MF) approximation offers a simple, fast way to infer direct interactions between elements in a network of correlated variables, a common, computationally challenging problem with practical applications in fields ranging from physics and biology to the social sciences. However, MF methods achieve their best performance with strong regularization, well beyond Bayesian expectations, an empirical fact that is poorly understood. In this work, we study the influence of pseudocount and L 2-norm regularization schemes on the quality of inferred Ising or Potts interaction networks from correlation data within the MF approximation. We argue, based on the analysis of small systems, that the optimal value of the regularization strength remains finite even if the sampling noise tends to zero, in order to correct for systematic biases introduced by the MF approximation. Our claim is corroborated by extensive numerical studies of diverse model systems and by the analytical study of the m-component spin model for large but finite m. Additionally, we find that pseudocount regularization is robust against sampling noise and often outperforms L 2-norm regularization, particularly when the underlying network of interactions is strongly heterogeneous. Much better performances are generally obtained for the Ising model than for the Potts model, for which only couplings incoming onto medium-frequency symbols are reliably inferred.},
  keywords = {0510−a,0550+q,8710Mn,number(s): 0250Tt},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Q8F5VRVU/Barton et al. - Unknown - Large pseudocounts and L 2-norm penalties are necessary for the mean-field inference of Ising and Potts models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bartonLargePseudocounts2norm.md}
}

@article{bassettEmergingFrontiersNeuroengineering2017,
  title = {Emerging {{Frontiers}} of {{Neuroengineering}}: {{A Network Science}} of {{Brain Connectivity}}},
  shorttitle = {Emerging {{Frontiers}} of {{Neuroengineering}}},
  author = {Bassett, Danielle S. and Khambhati, Ankit N. and Grafton, Scott T.},
  year = {2017},
  journal = {Annual Review of Biomedical Engineering},
  volume = {19},
  number = {1},
  pages = {327--352},
  doi = {10.1146/annurev-bioeng-071516-044511},
  abstract = {Neuroengineering is faced with unique challenges in repairing or replacing complex neural systems that are composed of many interacting parts. These interactions form intricate patterns over large spatiotemporal scales and produce emergent behaviors that are difficult to predict from individual elements. Network science provides a particularly appropriate framework in which to study and intervene in such systems by treating neural elements (cells, volumes) as nodes in a graph and neural interactions (synapses, white matter tracts) as edges in that graph. Here, we review the emerging discipline of network neuroscience, which uses and develops tools from graph theory to better understand and manipulate neural systems from micro- to macroscales. We present examples of how human brain imaging data are being modeled with network analysis and underscore potential pitfalls. We then highlight current computational and theoretical frontiers and emphasize their utility in informing diagnosis and monitoring, brain\textendash machine interfaces, and brain stimulation. A flexible and rapidly evolving enterprise, network neuroscience provides a set of powerful approaches and fundamental insights that are critical for the neuroengineer's tool kit.},
  pmid = {28375650},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-bioeng-071516-044511},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FW89V5X8/Bassett et al. - 2017 - Emerging Frontiers of Neuroengineering A Network .pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bassettEmergingFrontiersNeuroengineering2017.md}
}

@article{bassettSmallWorldBrainNetworks2017,
  title = {Small-{{World Brain Networks Revisited}}},
  author = {Bassett, Danielle S. and Bullmore, Edward T.},
  year = {2017},
  month = oct,
  journal = {The Neuroscientist},
  volume = {23},
  number = {5},
  pages = {499--516},
  publisher = {{SAGE Publications Inc STM}},
  issn = {1073-8584},
  doi = {10.1177/1073858416667720},
  abstract = {It is nearly 20 years since the concept of a small-world network was first quantitatively defined, by a combination of high clustering and short path length; and about 10 years since this metric of complex network topology began to be widely applied to analysis of neuroimaging and other neuroscience data as part of the rapid growth of the new field of connectomics. Here, we review briefly the foundational concepts of graph theoretical estimation and generation of small-world networks. We take stock of some of the key developments in the field in the past decade and we consider in some detail the implications of recent studies using high-resolution tract-tracing methods to map the anatomical networks of the macaque and the mouse. In doing so, we draw attention to the important methodological distinction between topological analysis of binary or unweighted graphs, which have provided a popular but simple approach to brain network analysis in the past, and the topology of weighted graphs, which retain more biologically relevant information and are more appropriate to the increasingly sophisticated data on brain connectivity emerging from contemporary tract-tracing and other imaging studies. We conclude by highlighting some possible future trends in the further development of weighted small-worldness as part of a deeper and broader understanding of the topology and the functional value of the strong and weak links between areas of mammalian cortex.},
  langid = {english},
  keywords = {connectomics,graph theory,network neuroscience,small-world network,small-world propensity},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/C559B78T/Bassett and Bullmore - 2017 - Small-World Brain Networks Revisited.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bassettSmallWorldBrainNetworks2017.md}
}

@article{battagliaAttractorNeuralNetworks1998,
  title = {Attractor Neural Networks Storing Multiple Space Representations: {{A}} Model for Hippocampal Place Fields},
  author = {Battaglia, F. P. and Treves, A.},
  year = {1998},
  journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
  volume = {58},
  number = {6},
  pages = {7738--7753},
  issn = {1063-651X},
  doi = {10.1103/PhysRevE.58.7738},
  abstract = {A recurrent neural network model storing multiple spatial maps, or ``charts'', is analyzed. A network of this type has been suggested as a model for the origin of place cells in the hippocampus of rodents. The extremely diluted and fully connected limits are studied, and the storage capacity and the information capacity are found. The important parameters determining the performance of the network are the sparsity of the spatial representations and the degree of connectivity, as found already for the storage of individual memory patterns in the general theory of auto-associative networks. Such results suggest a quantitative parallel between theories of hippocampal function in different animal species, such as primates (episodic memory) and rodents (memory for space).},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YCPL569C/Battaglia, Treves - 1998 - Attractor neural networks storing multiple space representations A model for hippocampal place fields.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@battagliaAttractorNeuralNetworks1998.md}
}

@techreport{battistinBeliefPropagationReplicasInference2014,
  title = {Belief-{{Propagation}} and Replicas for Inference and Learning in a Kinetic {{Ising}} Model with Hidden Spins},
  author = {Battistin, C and Hertz, J and Tyrcha, J and Roudi, Y},
  year = {2014},
  abstract = {We propose a new algorithm for inferring the state of hidden spins and reconstructing the connections in a synchronous kinetic Ising model, given the observed history. Focusing on the case in which the hidden spins are conditionally independent of each other given the state of observable spins, we show that calculating the likelihood of the data can be simplified by introducing a set of replicated auxiliary spins. Belief Propagation (BP) and Susceptibility Propagation (SusP) can then be used to infer the states of hidden variables and learn the couplings. We study the convergence and performance of this algorithm for networks with both Gaussian-distributed and binary bonds. We also study how the algorithm behaves as the fraction of hidden nodes and the amount of data are changed, showing that it outperforms the TAP equations for reconstructing the connections.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2T968X8N/Battistin et al. - 2014 - Belief-Propagation and replicas for inference and learning in a kinetic Ising model with hidden spins.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@battistinBeliefPropagationReplicasInference2014.md}
}

@article{battistinLearningUnknownsAnalyzing2017,
  title = {Learning with Unknowns: {{Analyzing}} Biological Data in the Presence of Hidden Variables},
  author = {Battistin, Claudia and Dunn, Benjamin and Roudi, Yasser},
  year = {2017},
  journal = {Current Opinion in Systems Biology},
  volume = {1},
  pages = {122--128},
  publisher = {{Elsevier Ltd}},
  doi = {10.1016/j.coisb.2016.12.010},
  abstract = {Despite our improved ability to probe biological systems at a higher spatio-temporal resolution, the high dimensionality of the biological systems often prevents sufficient sampling of the state space. Even with large scale datasets, such as gene microarrays or multi-neuronal recording techniques, the variables we are recording from are typically only a small subset, if wisely chosen, representing the most relevant degrees of freedom. The remaining variables, or the so called hidden variables, are most likely coupled to the observed ones, and affect their statistics and consequently our inference about the function of the system and the way it performs this function. Two important questions then arise in this context: which variables should we choose to observe and collect data from? and how much can we learn from data in the presence of hidden variables? In this paper we suggest that recent algorithmic developments rooting in the statistical physics of complex systems constitute a promising set of tools to extract relevant features from high-throughput data and a fruitful avenue of research for coming years.},
  keywords = {Generalized linear models,Hidden nodes,Ising model,Maximum entropy,Statistical models},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AV9V5XAG/Battistin, Dunn, Roudi - 2017 - Learning with unknowns Analyzing biological data in the presence of hidden variables.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CZB8GYW2/Battistin, Dunn, Roudi - 2017 - Learning with unknowns Analyzing biological data in the presence of hidden variables.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@battistinLearningUnknownsAnalyzing2017.md}
}

@article{bauerModellingFunctionalHuman2019,
  title = {Modelling Functional Human Neuromuscular Junctions in a Differentially-Perturbable Microfluidic Environment, Validated through Recombinant Monosynaptic Pseudotyped {{$\Delta$G-rabies}} Virus Tracing},
  author = {Bauer, Ulrich Stefan and {van de Wijdeven}, Rosanne and Raveendran, Rajeevkumar Nair and Fiskum, Vegard and Kentros, Clifford and Sandvig, Ioanna and Sandvig, Axel},
  year = {2019},
  month = aug,
  journal = {bioRxiv},
  pages = {745513--745513},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/745513},
  abstract = {Sweden Graphical abstract Functional neuromuscular junction in a microfluidic chip (a) Overview of microfluidic chip. Human iPS cell-derived motor neuron aggregates (spheroids indicated by black arrows) are seeded in the three lateral compartments of the chip, while human myotubes (white arrows) are seeded in the middle compartment. (b) Directed connectivity and retrograde virus tracing. Outgrowing axons (yellow arrow) from the motor neuron aggregate enter the directional axon tunnels (grey rectangles) and form connections with the myotubes (white arrow) within the opposite compartment. Addition of a designer monosynaptic pseudotyped {$\Delta$}G-rabies virus to the myotube compartment, infects the myotubes (green) expressing an exogenous receptor (TVA) and rabies glycoprotein (G), subsequently making infectious viruses that are retrogradely transported through the motor neuron axons (green arrow) back to the neuronal cell bodies within the aggregate, validating neuromuscular junction functionality. Abstract Compartmentalized microfluidic culture systems provide new perspectives in in vitro disease modelling as they enable co-culture of different relevant cell types in interconnected but fluidically isolated microenvironments. Such systems are thus particularly interesting in the context of in vitro modelling of mechanistic aspects of neurodegenerative diseases such as amyotrophic lateral sclerosis, which progressively affect the function of neuromuscular junctions, as they enable the co-culture of motor neurons and muscle cells in separate, but interconnected compartments. In combination with cell reprogramming technologies for the generation of human (including patient-specific) motor neurons, microfluidic platforms can thus become important research tools in preclinical studies. In this study, we present the application of a microfluidic chip with a differentially-perturbable microenvironment as a platform for establishing functional neuromuscular junctions using human induced pluripotent stem cell derived motor neurons and human myotubes. As a novel approach, we demonstrate the functionality of the platform using a designer pseudotyped {$\Delta$}G-rabies virus for retrograde monosynaptic tracing.},
  keywords = {amyotrophic lateral sclerosis,cell reprogramming,in vitro modelling,motor neurons,neuroengineering},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HN4JTBNA/Bauer et al. - 2019 - Modelling functional human neuromuscular junctions in a differentially-perturbable microfluidic environment, valid.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/R74ZZDNW/Bauer et al. - 2019 - Modelling functional human neuromuscular junctions in a differentially-perturbable microfluidic environment, valid.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bauerModellingFunctionalHuman2019.md}
}

@misc{BehaviorrelatedGeneRegulatory,
  title = {Behavior-Related Gene Regulatory Networks: {{A}} New Level of Organization in the Brain | {{PNAS}}},
  howpublished = {https://www.pnas.org/content/117/38/23270},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TAV26G9E/23270.html}
}

@techreport{belghaziMutualInformationNeural2018,
  title = {Mutual {{Information Neural Estimation}}},
  author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  year = {2018},
  abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck , applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KZXJVWBH/Belghazi et al. - 2018 - Mutual Information Neural Estimation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@belghaziMutualInformationNeural2018.md}
}

@article{bellecEligibilityTracesProvide2019,
  title = {Eligibility Traces Provide a Data-Inspired Alternative to Backpropagation through Time},
  author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  year = {2019},
  month = sep,
  abstract = {We present eligibility propagation an alternative to BPTT that is compatible with experimental data on synaptic plasticity and competes with BPTT on machine learning benchmarks.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KG44NWQH/Bellec et al_2019_Eligibility traces provide a data-inspired alternative to backpropagation.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VAF5BPQ7/forum.html}
}

@article{bellecSolutionLearningDilemma2020,
  title = {A Solution to the Learning Dilemma for Recurrent Networks of Spiking Neurons},
  author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  year = {2020},
  month = jul,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {3625},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17236-y},
  abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations~remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method\textendash called e-prop\textendash approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Electrical and electronic engineering,Learning algorithms,Network models,Neuroscience,Synaptic plasticity},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/V7R6KZFX/Bellec et al_2020_A solution to the learning dilemma for recurrent networks of spiking neurons.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UGZVBK9J/s41467-020-17236-y.html}
}

@article{bellecSolutionLearningDilemma2020a,
  title = {A Solution to the Learning Dilemma for Recurrent Networks of Spiking Neurons},
  author = {Bellec, G. and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, R. and Maass, W.},
  year = {2020},
  journal = {Nature Communications},
  doi = {10.1038/s41467-020-17236-y},
  abstract = {This learning method\textendash called e-prop\textendash approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning and suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence. Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method\textendash called e-prop\textendash approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence. Bellec et al. present a mathematically founded approximation for gradient descent training of recurrent neural networks without backwards propagation in time. This enables biologically plausible training of spike-based neural network models with working memory and supports on-chip training of neuromorphic hardware.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MAYUMGQ3/Bellec et al_2020_A solution to the learning dilemma for recurrent networks of spiking neurons.pdf}
}

@article{bengioBiologicallyPlausibleDeep2016,
  title = {Towards {{Biologically Plausible Deep Learning}}},
  author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
  year = {2016},
  month = aug,
  journal = {arXiv:1502.04156 [cs]},
  eprint = {1502.04156},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IC9HU7BZ/Bengio et al_2016_Towards Biologically Plausible Deep Learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XGFCIM8A/1502.html}
}

@article{bengioSTDPPresynapticActivity2016,
  title = {{{STDP}} as Presynaptic Activity Times Rate of Change of Postsynaptic Activity},
  author = {Bengio, Yoshua and Mesnard, Thomas and Fischer, Asja and Zhang, Saizheng and Wu, Yuhuai},
  year = {2016},
  month = mar,
  journal = {arXiv:1509.05936 [cs, q-bio]},
  eprint = {1509.05936},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {We introduce a weight update formula that is expressed only in terms of firing rates and their derivatives and that results in changes consistent with those associated with spike-timing dependent plasticity (STDP) rules and biological observations, even though the explicit timing of spikes is not needed. The new rule changes a synaptic weight in proportion to the product of the presynaptic firing rate and the temporal rate of change of activity on the postsynaptic side. These quantities are interesting for studying theoretical explanation for synaptic changes from a machine learning perspective. In particular, if neural dynamics moved neural activity towards reducing some objective function, then this STDP rule would correspond to stochastic gradient descent on that objective function.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/94BJRV3E/Bengio et al_2016_STDP as presynaptic activity times rate of change of postsynaptic activity.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7ICPCBXV/1509.html}
}

@techreport{bentoWhichGraphicalModels,
  title = {Which Graphical Models Are Difficult to Learn?},
  author = {Bento, Jos{\'e} and Montanari, Andrea},
  abstract = {We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity algorithms systematically fail when the Markov random field develops long-range correlations. More precisely, this phenomenon appears to be related to the Ising model phase transition (although it does not coincide with it).},
  note = {Not really going to use this one in any form but return to some time in order to teest our methods against difficul topologies},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WKCM8XMH/Bento, Montanari - Unknown - Which graphical models are difficult to learn.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bentoWhichGraphicalModels.md}
}

@article{berettaStochasticComplexitySpin2018,
  title = {The {{Stochastic Complexity}} of {{Spin Models}}: {{Are Pairwise Models Really Simple}}?},
  shorttitle = {The {{Stochastic Complexity}} of {{Spin Models}}},
  author = {Beretta, Alberto and Battistin, Claudia and {de Mulatier}, Cl{\'e}lia and Mastromatteo, Iacopo and Marsili, Matteo},
  year = {2018},
  month = sep,
  journal = {Entropy},
  volume = {20},
  number = {10},
  pages = {739},
  issn = {1099-4300},
  doi = {10.3390/e20100739},
  abstract = {Models can be simple for different reasons: because they yield a simple and computationally efficient interpretation of a generic dataset (e.g., in terms of pairwise dependencies)\textemdash as in statistical learning\textemdash or because they capture the laws of a specific phenomenon\textemdash as e.g., in physics\textemdash leading to non-trivial falsifiable predictions. In information theory, the simplicity of a model is quantified by the stochastic complexity, which measures the number of bits needed to encode its parameters. In order to understand how simple models look like, we study the stochastic complexity of spin models with interactions of arbitrary order. We show that bijections within the space of possible interactions preserve the stochastic complexity, which allows to partition the space of all models into equivalence classes. We thus found that the simplicity of a model is not determined by the order of the interactions, but rather by their mutual arrangements. Models where statistical dependencies are localized on non-overlapping groups of few variables are simple, affording predictions on independencies that are easy to falsify. On the contrary, fully connected pairwise models, which are often used in statistical learning, appear to be highly complex, because of their extended set of interactions, and they are hard to falsify.},
  pmcid = {PMC7512302},
  pmid = {33265828},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Beretta et al_2018_The Stochastic Complexity of Spin Models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@berettaStochasticComplexitySpin2018.md}
}

@article{berkerRenormalisationgroupCalculationsFinite1979,
  title = {Renormalisation-Group Calculations of Finite Systems: {{Order}} Parameter and Specific Heat for Epitaxial Ordering},
  author = {Berker, A. N. and Ostlund, S.},
  year = {1979},
  journal = {Journal of Physics C: Solid State Physics},
  volume = {12},
  number = {22},
  pages = {4961--4975},
  doi = {10.1088/0022-3719/12/22/035},
  abstract = {Renormalisation theory can be used to calculate the statistical properties of systems with arbitrary finite size. This method is demonstrated with the order parameter is and specific heat for epitaxial ordering. The system is renormalised until the unit length is of the order of the total length of the original system. Then, the renormalised partition function is directly evaluated with an appropriate distribution of boundary conditions. Resulting curves of the rounded transition are compared with experimental data. Underlying the finite-size calculation, Migdal-Kadanoff-type recursion relations for q-state Potts models are adjusted by varying q to yield the expected specific-heat exponent in the infinite-system limit. Additionally, it is noted that, with no adjustment, these recursion relations are self-consistently correct for q to infinity on the triangular lattice, and give the first-order transition at the exact temperature and with probably the exact latent heat.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MIQ9Z5K2/Berker, Ostlund - 1979 - Renormalisation-group calculations of finite systems Order parameter and specific heat for epitaxial ordering.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@berkerRenormalisationgroupCalculationsFinite1979.md}
}

@incollection{berlinskyCayleyTree2019,
  title = {The {{Cayley Tree}}},
  author = {Berlinsky, A. J. and Harris, A. B.},
  year = {2019},
  pages = {345--370},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-030-28187-8_14},
  abstract = {In the previous chapter, we saw that the MF approximation becomes correct in the limit of infinite spatial dimensionality. In this limit, fluctuations in the field sensed by one spin (in the Ising model) becomes small and our neglect of correlated fluctuations seems appropriate. In this chapter, we consider the construction of exact solutions to statistical problems on the Cayley tree. As we will see in a moment, the infinite Cayley tree (which is sometimes called a "Bethe lattice") provides a realization of infinite spatial dimensionality. The Cayley tree is a lattice in the form of a tree (i.e., it has no loops) which is recursively constructed as follows. One designates a central or seed site as the zeroth generation of the lattice. The first generation of the lattice consists of z sites which are neighboring to the seed site. Each first-generation site also has z nearest neighbors: one already present in the zeroth generation and {$\sigma$} {$\equiv$} z - 1 new sites added in the second generation of the lattice. The third generation of sites consists of the {$\sigma$} new sites neighboring to the second-generation sites. There are thus z first-generation sites and z{$\sigma$} (k-1) kth generation sites. In Fig. 14.1 we show four generations of a Cayley tree with z = 3. Note that a tree with z = 2 is just a linear chain. We have previously remarked that mean-field theory should become progressively more accurate as the coordination number increases. As we will see below, a more precise statement is that for systems with short-range interactions, the critical exponents become equal to their mean-field values at high spatial dimensionality. It is therefore of interest to determine the spatial dimensionality of the Cayley tree. There are at least two ways to address this question. One way is to study how the number N (r) of lattice points in a spherical volume of radius r increases for large r. If we consider a tree with k generations the number of sites is N (k) = 1 + z + z{$\sigma$} + z{$\sigma$} 2 +. .. z{$\sigma$} k-1 = 1 + z {$\sigma$} k - 1 {$\sigma$} - 1. (14.1)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IZIQR9H2/Berlinsky, Harris - 2019 - The Cayley Tree.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@berlinskyCayleyTree2019.md}
}

@article{berridgeDissectingComponentsReward2009,
  title = {Dissecting Components of Reward: `Liking', `Wanting', and Learning},
  shorttitle = {Dissecting Components of Reward},
  author = {Berridge, Kent C and Robinson, Terry E and Aldridge, J Wayne},
  year = {2009},
  month = feb,
  journal = {Current opinion in pharmacology},
  volume = {9},
  number = {1},
  pages = {65--73},
  issn = {1471-4892},
  doi = {10.1016/j.coph.2008.12.014},
  abstract = {In recent years significant progress has been made delineating the psychological components of reward and their underlying neural mechanisms. Here we briefly highlight findings on three dissociable psychological components of reward: `liking' (hedonic impact), `wanting' (incentive salience), and learning (predictive associations and cognitions). A better understanding of the components of reward, and their neurobiological substrates, may help in devising improved treatments for disorders of mood and motivation, ranging from depression to eating disorders, drug addiction, and related compulsive pursuits of rewards.},
  pmcid = {PMC2756052},
  pmid = {19162544},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4JQYVRSD/Berridge et al_2009_Dissecting components of reward.pdf}
}

@article{berridgeDissectingComponentsReward2009a,
  title = {Dissecting Components of Reward: `Liking', `Wanting', and Learning},
  shorttitle = {Dissecting Components of Reward},
  author = {Berridge, Kent C and Robinson, Terry E and Aldridge, J Wayne},
  year = {2009},
  month = feb,
  journal = {Current opinion in pharmacology},
  volume = {9},
  number = {1},
  pages = {65--73},
  issn = {1471-4892},
  doi = {10.1016/j.coph.2008.12.014},
  abstract = {In recent years significant progress has been made delineating the psychological components of reward and their underlying neural mechanisms. Here we briefly highlight findings on three dissociable psychological components of reward: `liking' (hedonic impact), `wanting' (incentive salience), and learning (predictive associations and cognitions). A better understanding of the components of reward, and their neurobiological substrates, may help in devising improved treatments for disorders of mood and motivation, ranging from depression to eating disorders, drug addiction, and related compulsive pursuits of rewards.},
  pmcid = {PMC2756052},
  pmid = {19162544},
  keywords = {desire},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HBZBM2GB/Berridge et al_2009_Dissecting components of reward.pdf}
}

@article{besagNearestNeighbourSystemsAutoLogistic1972,
  title = {Nearest-{{Neighbour Systems}} and the {{Auto-Logistic Model}} for {{Binary Data}}},
  author = {Besag, J. E.},
  year = {1972},
  month = sep,
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {34},
  number = {1},
  pages = {75--83},
  publisher = {{Wiley}},
  doi = {10.1111/j.2517-6161.1972.tb00889.x},
  abstract = {Bartlett (1966) and Whittle (1963), respectively, have proposed alternative, non-equivalent definitions of nearest-neighbour systems. The former, conditional probability definition, whilst the more intuitively attractive, presents several basic problems, not least in the identification of available models. In this paper, conditional probability nearest-neighbour systems for interacting random variables on a two-dimensional rectangular lattice are examined. It is shown that, in the case of 0, 1 variables and a homogeneous system, the only possibility is a logistic-type model but in which the explanatory variables at a point are the surrounding array variables themselves. A spatial-temporal approach leading to the same model is also suggested. The final section deals with linear nearest-neighbour systems, especially for continuous variables. The results of the paper may easily be extended to three or more dimensions.},
  keywords = {auto‐logistic model,multidimensional binary data,nearest‐neighbour systems},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@besagNearestNeighbourSystemsAutoLogistic1972.md}
}

@techreport{besagSpatialInteractionStatistical1974,
  title = {Spatial {{Interaction}} and the {{Statistical Analysis}} of {{Lattice Systems}}},
  author = {Besag, Julian},
  year = {1974},
  journal = {Source: Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {36},
  number = {2},
  pages = {192--236},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RV79BTQC/Besag - 1974 - Spatial Interaction and the Statistical Analysis of Lattice Systems.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@besagSpatialInteractionStatistical1974.md}
}

@techreport{besagStatisticalAnalysisDirty1986,
  title = {On the {{Statistical Analysis}} of {{Dirty Pictures}}},
  author = {Besag, Julian},
  year = {1986},
  journal = {Source: Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {48},
  number = {3},
  pages = {259--302},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/I2G4EDT5/Besag - 1986 - On the Statistical Analysis of Dirty Pictures.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@besagStatisticalAnalysisDirty1986.md}
}

@article{betzelMultiscaleBrainNetworks2017,
  title = {Multi-Scale Brain Networks},
  author = {Betzel, Richard F. and Bassett, Danielle S.},
  year = {2017},
  month = oct,
  journal = {NeuroImage},
  series = {Functional {{Architecture}} of the {{Brain}}},
  volume = {160},
  pages = {73--83},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2016.11.006},
  abstract = {The network architecture of the human brain has become a feature of increasing interest to the neuroscientific community, largely because of its potential to illuminate human cognition, its variation over development and aging, and its alteration in disease or injury. Traditional tools and approaches to study this architecture have largely focused on single scales\textemdash of topology, time, and space. Expanding beyond this narrow view, we focus this review on pertinent questions and novel methodological advances for the multi-scale brain. We separate our exposition into content related to multi-scale topological structure, multi-scale temporal structure, and multi-scale spatial structure. In each case, we recount empirical evidence for such structures, survey network-based methodological approaches to reveal these structures, and outline current frontiers and open questions. Although predominantly peppered with examples from human neuroimaging, we hope that this account will offer an accessible guide to any neuroscientist aiming to measure, characterize, and understand the full richness of the brain's multiscale network structure\textemdash irrespective of species, imaging modality, or spatial resolution.},
  langid = {english},
  keywords = {Brain networks,Complex networks,Graph theory,Multi-layer,Multi-resolution,Multi-scale,Network neuroscience},
  note = {\textbf{Extracted Annotations (6/19/2021, 5:49:42 PM)}
\par
\emph{another test note

(\href{zotero://open-pdf/library/items/PRG35WAH?page=1}{note on p.73})}
\par
\emph{Test note

(\href{zotero://open-pdf/library/items/PRG35WAH?page=1}{note on p.73})}},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PRG35WAH/Betzel and Bassett - 2017 - Multi-scale brain networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@betzelMultiscaleBrainNetworks2017.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ERWJHYIG/S1053811916306152.html}
}

@article{bialekRediscoveringPowerPairwise2007,
  title = {Rediscovering the Power of Pairwise Interactions},
  author = {Bialek, William and Ranganathan, Rama},
  year = {2007},
  abstract = {Two recent streams of work suggest that pairwise interactions may be sufficient to capture the complexity of biological systems ranging from protein structure to networks of neurons. In one approach, possible amino acid sequences in a family of proteins are generated by Monte Carlo annealing of a "Hamiltonian" that forces pairwise correlations among amino acid substitutions to be close to the observed correlations. In the other approach, the observed correlations among pairs of neurons are used to construct a maximum entropy model for the states of the network as a whole. We show that, in certain limits, these two approaches are mathematically equivalent, and we comment on open problems suggested by this framework},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XW5MTV2H/Bialek, Ranganathan - 2007 - Rediscovering the power of pairwise interactions.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bialekRediscoveringPowerPairwise2007.md}
}

@article{bianconiEstimationNumberCells2013,
  title = {An Estimation of the Number of Cells in the Human Body},
  author = {Bianconi, Eva and Piovesan, Allison and Facchin, Federica and Beraudi, Alina and Casadei, Raffaella and Frabetti, Flavia and Vitale, Lorenza and Pelleri, Maria Chiara and Tassani, Simone and Piva, Francesco and {Perez-Amodio}, Soledad and Strippoli, Pierluigi and Canaider, Silvia},
  year = {2013},
  month = nov,
  journal = {Annals of Human Biology},
  volume = {40},
  number = {6},
  pages = {463--471},
  publisher = {{Taylor \& Francis}},
  doi = {10.3109/03014460.2013.807878},
  abstract = {Background: All living organisms are made of individual and identifiable cells, whose number, together with their size and type, ultimately defines the structure and functions of an organism. While the total cell number of lower organisms is often known, it has not yet been defined in higher organisms. In particular, the reported total cell number of a human being ranges between 1012 and 1016 and it is widely mentioned without a proper reference. Aim: To study and discuss the theoretical issue of the total number of cells that compose the standard human adult organism. Subjects and methods: A systematic calculation of the total cell number of the whole human body and of the single organs was carried out using bibliographical and/or mathematical approaches. Results: A current estimation of human total cell number calculated for a variety of organs and cell types is presented. These partial data correspond to a total number of 3.72\texttimes 1013. Conclusions: Knowing the total cell number of the human body as well as of individual organs is important from a cultural, biological, medical and comparative modelling point of view. The presented cell count could be a starting point for a common effort to complete the total calculation. \textcopyright{} 2013 Informa UK Ltd.},
  keywords = {Cell size,Human cell number,Organ,Theoretical issue,Total cell count},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bianconiEstimationNumberCells2013.md}
}

@article{bieriSlowFastGamma2014,
  title = {Slow and {{Fast Gamma Rhythms Coordinate Different Spatial Coding Modes}} in {{Hippocampal Place Cells}}},
  author = {Bieri, Kevin~Wood and Bobbitt, Katelyn~N. and Colgin, Laura~Lee},
  year = {2014},
  month = may,
  journal = {Neuron},
  volume = {82},
  number = {3},
  pages = {670--681},
  publisher = {{Cell Press}},
  doi = {10.1016/J.NEURON.2014.03.013},
  abstract = {Previous work has hinted that prospective and retrospective coding modes exist in hippocampus. Prospective coding is believed to reflect memory retrieval processes, whereas retrospective coding is thought to be important for memory encoding. Here, we show in rats that separate prospective and retrospective modes exist in hippocampal subfield CA1 and that slow and fast gamma rhythms differentially coordinate place cells during the two modes. Slow gamma power and phase locking of spikes increased during prospective coding; fast gamma power and phase locking increased during retrospective coding. Additionally, slow gamma spikes occurred earlier in place fields than fast gamma spikes, and cell ensembles retrieved upcoming positions during slow gamma and encoded past positions during fast gamma. These results imply that alternating slow and fast gamma states allow the hippocampus to switch between prospective and retrospective modes, possibly to prevent interference between memory retrieval and encoding.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NHN3AEQZ/Bieri, Bobbitt, Colgin - 2014 - Slow and Fast Gamma Rhythms Coordinate Different Spatial Coding Modes in Hippocampal Place Cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bieriSlowFastGamma2014.md}
}

@article{billehSystematicIntegrationStructural2020,
  title = {Systematic {{Integration}} of {{Structural}} and {{Functional Data}} into {{Multi-scale Models}} of {{Mouse Primary Visual Cortex}}},
  author = {Billeh, Yazan N. and Cai, Binghuang and Gratiy, Sergey L. and Dai, Kael and Iyer, Ramakrishnan and Gouwens, Nathan W. and {Abbasi-Asl}, Reza and Jia, Xiaoxuan and Siegle, Joshua H. and Olsen, Shawn R. and Koch, Christof and Mihalas, Stefan and Arkhipov, Anton},
  year = {2020},
  month = may,
  journal = {Neuron},
  volume = {106},
  number = {3},
  pages = {388-403.e18},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2020.01.040},
  abstract = {Structural rules underlying functional properties of~cortical circuits are poorly understood. To explore these rules systematically, we integrated information from extensive literature curation and large-scale experimental surveys into a data-driven, biologically realistic simulation of the awake mouse primary visual cortex. The model was constructed at two levels of granularity, using either biophysically detailed or point neurons. Both variants have identical network connectivity and were compared to each other and to experimental recordings of visual-driven neural activity. While tuning these networks to recapitulate experimental data, we identified rules governing cell-class-specific connectivity and synaptic strengths. These structural constraints constitute hypotheses that can be tested experimentally. Despite their distinct single-cell abstraction, both spatially extended and point models perform similarly at the level of firing rate distributions for the questions we investigated. All data and models are freely available as a resource for the community.},
  langid = {english},
  file = {C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@billehSystematicIntegrationStructural2020.md;C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Zotero\\storage\\Billeh et al_2020_Systematic Integration of Structural and Functional Data into Multi-scale.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EQTD3P77/S0896627320300672.html}
}

@article{bingIndirectDirectTraining2020,
  title = {Indirect and Direct Training of Spiking Neural Networks for End-to-End Control of a Lane-Keeping Vehicle},
  author = {Bing, Zhenshan and Meschede, Claus and Chen, Guang and Knoll, Alois and Huang, Kai},
  year = {2020},
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  pages = {21--36},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.05.019},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GIKNQD9D/Bing et al_2020_Indirect and direct training of spiking neural networks for end-to-end control.pdf}
}

@techreport{BiologicalSequenceAnalysis,
  title = {Biological Sequence Analysis: {{Probabilistic}} Models of Proteins and Nucleic Acids},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HWIKSA7Z/Unknown - Unknown - Biological sequence analysis Probabilistic models of proteins and nucleic acids.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@BiologicalSequenceAnalysis.md}
}

@article{biTimeRepresentationNeural2019,
  title = {Time Representation in Neural Network Models Trained to Perform Interval Timing Tasks},
  author = {Bi, Zedong and Zhou, Changsong},
  year = {2019},
  abstract = {Fundamental principles of the neuronal coding of time that supports the brain for flexible temporal processing are disclosed and facilitate generalizable decoding of time and non-time information. To predict and maximize future rewards in this ever-changing world, animals must be able to discover the temporal structure of stimuli and then take the right action at the right time. However, we still lack a systematic understanding of the neural mechanism of how animals perceive, maintain, and use time intervals ranging from hundreds of milliseconds to multi-seconds in working memory and appropriately combine time information with spatial information processing and decision making. Here, we addressed this problem by training neural network models on four timing tasks: interval production, interval comparison, timed spatial reproduction, and timed decision making. We studied time-coding principles of the network after training, and found them consistent with existing experimental observations. We reveal that neural networks perceive time intervals through the evolution of population state along a stereotypical trajectory, maintain time intervals by line attractors along which the activities of most neurons vary monotonically with the duration of the maintained interval, and adjust the evolution speed of the state trajectory for producing or comparing time intervals. Spatial information or decision choice preserves the profiles of neuronal activities as functions of time intervals maintained in working memory or flow of time, and is coded in the amplitudes of these profiles. Decision making is combined with time perception through two firing sequences with mutual inhibition. Our work discloses fundamental principles of the neuronal coding of time that supports the brain for flexible temporal processing. These principles facilitate generalizable decoding of time and non-time information.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2QBWHNN2/Bi_Zhou_2019_Time representation in neural network models trained to perform interval timing.pdf}
}

@article{boboevaRecencyCentralTendency2022,
  title = {From Recency to the Central Tendency Bias in Working Memory: A Unifying Attractor Network Model},
  shorttitle = {From Recency to the Central Tendency Bias in Working Memory},
  author = {Boboeva, Vezha and Pezzotta, Alberto and Akrami, A. and Clopath, C.},
  year = {2022},
  journal = {bioRxiv},
  doi = {10.1101/2022.05.16.491352},
  abstract = {The dynamics of the model suggests that contraction bias can emerge as a result of a volatile working memory content which makes it susceptible to shifting to the past sensory experience, and explains both short-term history biases, as well as contraction bias towards the sensory mean for the averaged performance. The central tendency bias, or contraction bias is a phenomenon where the judgment of the magnitude of items held in working memory is biased towards the average of past observations. This phenomenon has been first described more than a century ago [1] and since then, has been replicated in various decision making tasks in humans [2\textendash 10], and rodents [11, 12]. Contraction bias is assumed to be an optimal strategy by the brain, given the noisy nature of working memory. From a Bayesian perspective [7], the progressive shift of the noisy memory towards the mean of a prior distribution built from past sensory experience helps with more accurate estimates of the memory. In this work, we propose an alternative account, via short-term history biases (serial dependence) [12\textendash 15]. Our model is motivated and inspired by recent results from an auditory delayed-discrimination task in rats, where the posterior parietal cortex (PPC) has been shown to be critical to these memory effects [12]. The dynamics of our model suggests that contraction bias can emerge as a result of a volatile working memory content which makes it susceptible to shifting to the past sensory experience. The errors, at the level of individual trials, are sampled from the full distribution of the stimuli, and are not due to a gradual shift of the memory towards the distribution's mean. Our model explains both short-term history biases, as well as contraction bias towards the sensory mean for the averaged performance. The results are consistent with the role of the PPC in encoding such sensory history biases, and provide predictions of performance across different stimulus distributions and timings, delay intervals, as well as neuronal dynamics in putative working memory areas.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IVHY4822/Boboeva et al_2022_From recency to the central tendency bias in working memory.pdf}
}

@article{boerlinPredictiveCodingDynamical2013,
  title = {Predictive {{Coding}} of {{Dynamical Variables}} in {{Balanced Spiking Networks}}},
  author = {Boerlin, Martin and Machens, Christian K. and Den{\`e}ve, Sophie},
  year = {2013},
  month = nov,
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {11},
  pages = {e1003258},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003258},
  abstract = {Two observations about the cortex have puzzled neuroscientists for a long time. First, neural responses are highly variable. Second, the level of excitation and inhibition received by each neuron is tightly balanced at all times. Here, we demonstrate that both properties are necessary consequences of neural networks that represent information efficiently in their spikes. We illustrate this insight with spiking networks that represent dynamical variables. Our approach is based on two assumptions: We assume that information about dynamical variables can be read out linearly from neural spike trains, and we assume that neurons only fire a spike if that improves the representation of the dynamical variables. Based on these assumptions, we derive a network of leaky integrate-and-fire neurons that is able to implement arbitrary linear dynamical systems. We show that the membrane voltage of the neurons is equivalent to a prediction error about a common population-level signal. Among other things, our approach allows us to construct an integrator network of spiking neurons that is robust against many perturbations. Most importantly, neural variability in our networks cannot be equated to noise. Despite exhibiting the same single unit properties as widely used population code models (e.g. tuning curves, Poisson distributed spike trains), balanced networks are orders of magnitudes more reliable. Our approach suggests that spikes do matter when considering how the brain computes, and that the reliability of cortical representations could have been strongly underestimated.},
  langid = {english},
  keywords = {Action potentials,Dynamical systems,Membrane potential,Network analysis,Neural networks,Neuronal tuning,Neurons,Sensory perception},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AQ5YIX2N/Boerlin et al_2013_Predictive Coding of Dynamical Variables in Balanced Spiking Networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5DJRCU9H/article.html}
}

@article{bohteEffectsPairwiseHigherorder2000,
  title = {The Effects of Pair-Wise and Higher-Order Correlations on the Firing Rate of a Postsynaptic Neuron},
  author = {Bohte, S. M. and Spekreijse, H. and Roelfsema, P. R.},
  year = {2000},
  month = mar,
  journal = {Neural Computation},
  volume = {12},
  number = {1},
  pages = {153--179},
  publisher = {{MIT Press Journals}},
  doi = {10.1162/089976600300015934},
  abstract = {Coincident firing of neurons projecting to a common target cell is likely to raise the probability of firing of this postsynaptic cell. Therefore, synchronized firing constitutes a significant event for postsynaptic neurons and is likely to play a role in neuronal information processing. Physiological data on synchronized firing in cortical networks are based primarily on paired recordings and cross-correlation analysis. However, pair-wise correlations among all inputs onto a postsynaptic neuron do not uniquely determine the distribution of simultaneous postsynaptic events. We develop a framework in order to calculate the amount of synchronous firing that, based on maximum entropy, should exist in a homogeneous neural network in which the neurons have known pair-wise correlations and higher-order structure is absent. According to the distribution of maximal entropy, synchronous events in which a large proportion of the neurons participates should exist even in the case of weak pair-wise correlations. Network simulations also exhibit these highly synchronous events in the case of weak pair-wise correlations. If such a group of neurons provides input to a common postsynaptic target, these network bursts may enhance the impact of this input, especially in the case of a high postsynaptic threshold. The proportion of neurons participating in synchronous bursts can be approximated by our method under restricted conditions. When these conditions are not fulfilled, the spike trains have less than maximal entropy, which is indicative of the presence of higher-order structure. In this situation, the degree of synchronicity cannot be derived from the pair-wise correlations.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VJH3TNKQ/Bohte, Spekreijse, Roelfsema - 2000 - The effects of pair-wise and higher-order correlations on the firing rate of a postsynaptic neuron.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bohteEffectsPairwiseHigherorder2000.md}
}

@article{bonnevieGridCellsRequire2013,
  title = {Grid Cells Require Excitatory Drive from the Hippocampus},
  author = {Bonnevie, Tora and Dunn, Benjamin and Fyhn, Marianne and Hafting, Torkel and Derdikman, Dori and Kubie, John L. and Roudi, Yasser and Moser, Edvard I. and Moser, May Britt},
  year = {2013},
  journal = {Nature Neuroscience},
  volume = {16},
  number = {3},
  pages = {309--317},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nn.3311},
  abstract = {To determine how hippocampal backprojections influence spatially periodic firing in grid cells, we recorded neural activity in the medial entorhinal cortex (MEC) of rats after temporary inactivation of the hippocampus. We report two major changes in entorhinal grid cells. First, hippocampal inactivation gradually and selectively extinguished the grid pattern. Second, the same grid cells that lost their grid fields acquired substantial tuning to the direction of the rat's head. This transition in firing properties was contingent on a drop in the average firing rate of the grid cells and could be replicated by the removal of an external excitatory drive in an attractor network model in which grid structure emerges by velocity-dependent translation of activity across a network with inhibitory connections. These results point to excitatory drive from the hippocampus, and possibly other regions, as one prerequisite for the formation and translocation of grid patterns in the MEC.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TZ3H8IYM/Bonnevie et al. - 2013 - Grid cells require excitatory drive from the hippocampus.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bonnevieGridCellsRequire2013.md}
}

@techreport{borgesmorenomelloMethodObtainNeuromorphic,
  title = {Method to Obtain Neuromorphic Reservoir Networks from Images of in Vitro Cortical Networks},
  author = {Borges Moreno Mello, Gustavo and {Pontes-Filho}, Sidney and Huse Ramstad, Ola and Sandvig, Ioanna and Devold Valderhaug, Vibeke and Zouganeli, Evi and Sandvig, Axel and Nichele, Stefano},
  abstract = {In the brain, the structure of a network of neurons defines how these neurons implement the computations that underlie the mind and the behavior of animals and humans.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X9E8Q77U/Borges Moreno Mello et al. - Unknown - Method to obtain neuromorphic reservoir networks from images of in vitro cortical networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XQ3JNI27/Borges Moreno Mello et al. - Unknown - Method to obtain neuromorphic reservoir networks from images of in vitro cortical networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@borgesmorenomelloMethodObtainNeuromorphic.md}
}

@article{borysovUSStockMarket2015,
  title = {{{US}} Stock Market Interaction Network as Learned by the {{Boltzmann}} Machine},
  author = {Borysov, Stanislav S and Roudi, Yasser and Balatsky, Alexander V},
  year = {2015},
  journal = {The European Physical Journal B},
  volume = {88},
  number = {12},
  pages = {321},
  publisher = {{Springer}},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@borysovUSStockMarket2015.md}
}

@article{botvinickConflictMonitoringAnterior2004,
  title = {Conflict Monitoring and Anterior Cingulate Cortex: An Update},
  author = {Botvinick, Matthew M and Cohen, Jonathan D and Carter, Cameron S},
  year = {2004},
  journal = {TRENDS in Cognitive Sciences},
  volume = {8},
  number = {12},
  doi = {10.1016/j.tics.2004.10.003},
  abstract = {One hypothesis concerning the human dorsal anterior cingulate cortex (ACC) is that it functions, in part, to signal the occurrence of conflicts in information processing , thereby triggering compensatory adjustments in cognitive control. Since this idea was first proposed, a great deal of relevant empirical evidence has accrued. This evidence has largely corroborated the conflict-monitoring hypothesis, and some very recent work has provided striking new support for the theory. At the same time, other findings have posed specific challenges , especially concerning the way the theory addresses the processing of errors. Recent research has also begun to shed light on the larger function of the ACC, suggesting some new possibilities concerning how conflict monitoring might fit into the cingulate's overall role in cognition and action. The term cognitive control refers to a set of functions serving to configure the cognitive system for the performance of specific tasks, especially in challenging and non-routine situations. A crucial question concerning these functions is: How are they recruited? One possibility is that control is recruited based, in part, on a function that detects conflicts in information processing. In a series of papers, beginning in 1998 [1-4], we and several colleagues suggested that direct evidence for a conflict-monitoring function could be discerned in data from cognitive neuroscience, and in particular work pertaining to the anterior cingulate cortex (ACC). To be more precise, we advanced two interrelated hypotheses: (1) Specific brain regions, most notably the dorsal ACC, respond to the occurrence of conflicts in information processing, for example response competition; (2) This conflict signal triggers strategic adjustments in cognitive control, which serve to prevent conflict in subsequent performance. A third proposal, also included from the outset, was that conflict monitoring might represent one aspect of a more general monitoring function, which detects internal states signaling a need to intensify or redirect attention or control. Since the conflict-monitoring hypothesis was first proposed, a wealth of new data has appeared, bearing on all three of the foregoing proposals. In many cases, such data has bolstered the account. In others, it has posed challenges. Meanwhile, new proposals have emerged concerning ACC function, modifying the context of the debate. In the present article, we summarize these recent developments, and consider their implications for the conflict-monitoring theory. A cortical response to conflict The first claim of the conflict-monitoring theory is that specific brain structures, and in particular the ACC, respond to the occurrence of conflict. This idea was originally motivated by a review of studies in which ACC activation had been observed during the performance of cognitive tasks [4,5]. In the majority of such studies, ACC engagement was associated with one of three behavioral contexts: (1) tasks that required the overriding of prepotent responses, (2) tasks that required selection among a set of equally permissible responses (under-determined responding), or (3) tasks that involved the commission of errors (Figure 1). Through a series of computational models [4,6] (see Boxes 1 and 2), we demonstrated how ACC activation in each of these contexts could be explained based on a single function-the detection of conflict. Subsequent studies have provided additional evidence concerning the involvement of the ACC in the settings of response override, underdetermined responding, and error commission. In the following, we revisit these three domains, focusing on work published within the past five years. Response override Tasks requiring the overriding of prepotent responses often involve conflict, in the form of competition between the correct response and the one being overridden. The finding of ACC engagement under such circumstances is, at this point, one of the most firmly established findings in all of cognitive neuroscience. The most frequent observation pertains to the Stroop task, where relative ACC activation has been observed in association with incon-gruent trials (Box 1), a finding that has been replicated in well over 15 studies (see [5,7] for reviews). ACC activation has also been observed in various versions of the flanker task [3,8-12] (Box 2), in the Simon task [13], in the global-local paradigm [14,15], and in the go/no-go paradigm [16-18], as well as in other response override tasks [1,18-20].},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6EER84DB/Botvinick, Cohen, Carter - 2004 - Conflict monitoring and anterior cingulate cortex an update.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@botvinickConflictMonitoringAnterior2004.md}
}

@article{botvinickDeepReinforcementLearning2020,
  title = {Deep {{Reinforcement Learning}} and Its {{Neuroscientific Implications}}},
  author = {Botvinick, Matthew and Wang, Jane X. and Dabney, Will and Miller, Kevin J. and {Kurth-Nelson}, Zeb},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.03750 [cs, q-bio]},
  eprint = {2007.03750},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {The emergence of powerful artificial intelligence is defining new research directions in neuroscience. To date, this research has focused largely on deep neural networks trained using supervised learning, in tasks such as image classification. However, there is another area of recent AI work which has so far received less attention from neuroscientists, but which may have profound neuroscientific implications: deep reinforcement learning. Deep RL offers a comprehensive framework for studying the interplay among learning, representation and decision-making, offering to the brain sciences a new set of research tools and a wide range of novel hypotheses. In the present review, we provide a high-level introduction to deep RL, discuss some of its initial applications to neuroscience, and survey its wider implications for research on brain and behavior, concluding with a list of opportunities for next-stage research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition}
}

@article{botvinickDeepReinforcementLearning2020a,
  title = {Deep {{Reinforcement Learning}} and {{Its Neuroscientific Implications}}},
  author = {Botvinick, Matthew and Wang, Jane X. and Dabney, Will and Miller, Kevin J. and {Kurth-Nelson}, Zeb},
  year = {2020},
  month = aug,
  journal = {Neuron},
  volume = {107},
  number = {4},
  pages = {603--616},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2020.06.014},
  abstract = {The emergence of powerful artificial intelligence (AI) is defining new research directions in neuroscience. To date, this research has focused largely on deep neural networks trained using supervised learning in tasks such as image classification. However, there is another area of recent AI work that has so far received less attention from neuroscientists but that may have profound neuroscientific implications: deep reinforcement learning (RL). Deep RL offers a comprehensive framework for studying the interplay among learning, representation, and decision making, offering to the brain sciences a new set of research tools and a wide range of novel hypotheses. In the present review, we provide a high-level introduction to deep RL, discuss some of its initial applications to neuroscience, and survey its wider implications for research on brain and behavior, concluding with a list of opportunities for next-stage research.},
  langid = {english},
  note = {[Sunday 3:52 PM] Anis Yazidi
\par
Quoting from a paper from Neuron: 
\par
"One important challenge, in this regard, is long-term temporal credit assignment, that is, updating behavior on the basis of rewards that may not accrue until a substantial time after the actions that were responsible for generating them. 
\par
This remains a challenge for deep RL systems. 
\par
Novel algorithms have recently been proposed (see, e.g., Hung et al., 2019), but the problem is far from solved, and a dialog with neuroscience in this area may be beneficial to both fields"},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7W9WX8S3/Botvinick et al_2020_Deep Reinforcement Learning and Its Neuroscientific Implications.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XU6Q6ZVF/S0896627320304682.html}
}

@inproceedings{bouhadjarSequenceLearningPrediction2021,
  title = {Sequence Learning, Prediction, and Replay in Networks of Spiking Neurons},
  author = {Bouhadjar, Younes and Wouters, D. and Diesmann, M. and Tetzlaff, T.},
  year = {2021},
  abstract = {A continuous-time implementation of the temporal-memory (TM) component of the HTM algorithm, which is based on a recurrent network of spiking neurons with biophysically interpretable variables and parameters, facilitates the evaluation of the TM hypothesis based on experimentally accessible quantities. Sequence learning, prediction and replay have been proposed to constitute the universal computations performed by the neocortex. The Hierarchical Temporal Memory (HTM) algorithm realizes these forms of computation. It learns sequences in an unsupervised and continuous manner using local learning rules, permits a context specific prediction of future sequence elements, and generates mismatch signals in case the predictions are not met. While the HTM algorithm accounts for a number of biological features such as topographic receptive fields, nonlinear dendritic processing, and sparse connectivity, it is based on abstract discrete-time neuron and synapse dynamics, as well as on plasticity mechanisms that can only partly be related to known biological mechanisms. Here, we devise a continuous-time implementation of the temporal-memory (TM) component of the HTM algorithm, which is based on a recurrent network of spiking neurons with biophysically interpretable variables and parameters. The model learns high-order sequences by means of a structural Hebbian synaptic plasticity mechanism supplemented with a rate-based homeostatic control. In combination with nonlinear dendritic input integration and local inhibitory feedback, this type of plasticity leads to the dynamic self-organization of narrow sequence-specific feedforward subnetworks. These subnetworks provide the substrate for a faithful propagation of sparse, synchronous activity, and, thereby, for a robust, context specific prediction of future sequence elements as well as for the autonomous replay of previously learned sequences. By strengthening the link to biology, our implementation facilitates the evaluation of the TM hypothesis based on experimentally accessible quantities. The continuous-time implementation of the TM algorithm permits, in particular, an investigation of the role of sequence timing for sequence learning, prediction and replay. We demonstrate this aspect by studying the effect of the sequence speed on the sequence learning performance and on the speed of autonomous sequence replay.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LIKM48ZI/Bouhadjar et al_2021_Sequence learning, prediction, and replay in networks of spiking neurons.pdf}
}

@article{brahlekTransportPropertiesTopological2015,
  title = {Transport Properties of Topological Insulators: {{Band}} Bending, Bulk Metal-to-Insulator Transition, and Weak Anti-Localization},
  shorttitle = {Transport Properties of Topological Insulators},
  author = {Brahlek, Matthew and Koirala, Nikesh and Bansal, Namrata and Oh, Seongshik},
  year = {2015},
  month = jul,
  journal = {Solid State Communications},
  volume = {215--216},
  pages = {54--62},
  issn = {0038-1098},
  doi = {10.1016/j.ssc.2014.10.021},
  abstract = {We reanalyze some of the critical transport experiments and provide a coherent understanding of the current generation of topological insulators (TIs). Currently TI transport studies abound with widely varying claims of the surface and bulk states, often times contradicting each other, and a proper understanding of TI transport properties is lacking. According to the simple criteria given by Mott and Ioffe\textendash Regel, even the best TIs are not true insulators in the Mott sense, and at best, are weakly-insulating bad metals. However, band-bending effects contribute significantly to the TI transport properties including Shubnikov de-Haas oscillations, and we show that utilization of this band-bending effect can lead to a Mott insulating bulk state in the thin regime. In addition, by reconsidering previous results on the weak anti-localization (WAL) effect with additional new data, we correct a misunderstanding in the literature and generate a coherent picture of the WAL effect in TIs.},
  langid = {english},
  keywords = {A. Topological insulators,D. Transport properties,review},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7CHAYW8C/Brahlek et al_2015_Transport properties of topological insulators.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NZX2UURS/S0038109814004426.html}
}

@techreport{braviExtendedPlefkaExpansion2016,
  title = {Extended {{Plefka Expansion}} for {{Stochastic Dynamics}}},
  author = {Bravi, B and Sollich, P and Opper, M},
  year = {2016},
  abstract = {We propose an extension of the Plefka expansion, which is well known for the dynamics of discrete spins, to stochastic differential equations with continuous degrees of freedom and exhibiting generic nonlinearities. The scenario is sufficiently general to allow application to e.g. biochemical networks involved in metabolism and regulation. The main feature of our approach is to constrain in the Plefka expansion not just first moments akin to magnetizations, but also second moments, specifically two-time correlations and responses for each degree of freedom. The end result is an effective equation of motion for each single degree of freedom, where couplings to other variables appear as a self-coupling to the past (i.e. memory term) and a coloured noise. This constitutes a new mean field approximation that should become exact in the thermodynamic limit of a large network, for suitably long-ranged couplings. For the analytically tractable case of linear dynamics we establish this exactness explicitly by appeal to spectral methods of Random Matrix Theory, for Gaussian couplings with arbitrary degree of symmetry.},
  keywords = {Biochemical Networks,Dynamical Functional,Mean Field,Plefka expansion,Random Matrix Theory},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8Z3YIQ9L/Bravi, Sollich, Opper - 2016 - Extended Plefka Expansion for Stochastic Dynamics.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@braviExtendedPlefkaExpansion2016.md}
}

@book{bretteHandbookNeuralActivity,
  title = {Handbook of {{Neural Activity Measurement Edited}} by {{Romain Brette}} and {{Alain Destexhe Excerpt More}} Information},
  author = {Brette, Romain and Destexhe, Alain},
  abstract = {Exterior (Na + , Cl -) + C m Figure 1.1 Equivalent electrical circuit of a patch of neuronal membrane: V is the membrane potential, g K is the conductance of ionic channels permeable to K + ions, and E K is the corresponding reversal potential. The membrane of a neuron is a bipilid layer, which is an electrical insulator (see Figure 1.1). It separates the interior and the exterior of the cell, which contain ions in different proportions: sodium (Na +) and chloride (Cl -) ions outside the cell, potassium (K +) ions inside the cell. These ions can enter or leave the cell through proteins in the membrane named ionic channels. An ionic channel forms a tiny hole in the membrane, that specific types of ions (e.g. K +) can cross. At rest, the membrane is mostly permeable to K + (and a bit less to Na + and Cl -). By diffusion, K + ions will tend to move from the interior of the cell, where the concentration is high, to the exterior of the cell, where it is lower. This phenomenon moves positive charges outside the cell, which creates an electrical field across the membrane. This field produces a movement of ions in the other direction (positive charges move to where there are fewer positive charges), and therefore an equilibrium is reached when the outward ion flux due to diffusion exactly matches the inward flux due to the electrical field. These are the basic principles of electrodiffusion. At equilibrium , there are more positive charges outside than inside the cell, and therefore the electrical potential is higher outside than inside. The membrane potential is defined as the difference V m = V in - V out , and is thus negative at rest (typically around -70 mV): the membrane is polarized. The membrane potential at equilibrium for a given ionic channel is named the equilibrium potential, the Nernst potential or the reversal potential. The latter denomination means that the equilibrium potential is the membrane potential E at which the transmembrane current I changes sign: when V m = E K no current passes through the channels (by definition), when V m {$>$} E K positive charges exit the cell and therefore I K {$>$} 0 (where the current is defined from inside to outside), and when V m {$<$} E K positive charges enter the cell,},
  isbn = {978-0-521-51622-8},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5GGWEZZN/Brette, Destexhe - Unknown - Handbook of Neural Activity Measurement Edited by Romain Brette and Alain Destexhe Excerpt More information.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TQT5BKUC/Brette, Destexhe - Unknown - Handbook of Neural Activity Measurement Edited by Romain Brette and Alain Destexhe Excerpt More information.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bretteHandbookNeuralActivity.md}
}

@article{bretteSimulationNetworksSpiking2007,
  title = {Simulation of Networks of Spiking Neurons: {{A}} Review of Tools and Strategies},
  shorttitle = {Simulation of Networks of Spiking Neurons},
  author = {Brette, Romain and Rudolph, Michelle and Carnevale, Ted and Hines, Michael and Beeman, David and Bower, James M. and Diesmann, Markus and Morrison, Abigail and Goodman, Philip H. and Harris, Frederick C. and Zirpe, Milind and Natschl{\"a}ger, Thomas and Pecevski, Dejan and Ermentrout, Bard and Djurfeldt, Mikael and Lansner, Anders and Rochel, Olivier and Vieville, Thierry and Muller, Eilif and Davison, Andrew P. and El Boustani, Sami and Destexhe, Alain},
  year = {2007},
  month = dec,
  journal = {Journal of Computational Neuroscience},
  volume = {23},
  number = {3},
  pages = {349--398},
  issn = {1573-6873},
  doi = {10.1007/s10827-007-0038-6},
  abstract = {We review different aspects of the simulation of spiking neural networks. We start by reviewing the different types of simulation strategies and algorithms that are currently implemented. We next review the precision of those simulation strategies, in particular in cases where plasticity depends on the exact timing of the spikes. We overview different simulators and simulation environments presently available (restricted to those freely available, open source and documented). For each simulation tool, its advantages and pitfalls are reviewed, with an aim to allow the reader to identify which simulator is appropriate for a given task. Finally, we provide a series of benchmark simulations of different types of networks of spiking neurons, including Hodgkin\textendash Huxley type, integrate-and-fire models, interacting with current-based or conductance-based synapses, using clock-driven or event-driven integration strategies. The same set of models are implemented on the different simulators, and the codes are made available. The ultimate goal of this review is to provide a resource to facilitate identifying the appropriate integration strategy and simulation tool to use for a given modeling problem related to spiking neural networks.},
  langid = {english},
  keywords = {Clock-driven,Event-driven,Integration strategies,Simulation tools,SNN,software,Spiking neural networks},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SFAR6F49/Brette et al_2007_Simulation of networks of spiking neurons.pdf}
}

@book{bretthorstBayesianSpectrumAnalysis1997,
  title = {Bayesian {{Spectrum Analysis}} and {{Parameter}}},
  author = {Bretthorst, G. Larry},
  year = {1997},
  isbn = {0-387-96871-7},
  keywords = {Bretthorst},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/W44ZDY9A/Bretthorst - 1997 - Bayesian Spectrum Analysis and Parameter.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bretthorstBayesianSpectrumAnalysis1997.md}
}

@article{brombergEfficientMarkovNetwork2009,
  title = {Efficient {{Markov Network Structure Discovery Using Independence Tests}}},
  author = {Bromberg, F. and Margaritis, D. and Honavar, V.},
  year = {2009},
  month = jul,
  journal = {Journal of Artificial Intelligence Research},
  volume = {35},
  pages = {449--484},
  issn = {1076-9757},
  doi = {10.1613/jair.2773},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {Pensar2017},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/B26HBHV7/Bromberg et al. - 2009 - Efficient Markov Network Structure Discovery Using.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GGYBFCEH/Bromberg et al. - 2009 - Efficient Markov Network Structure Discovery Using.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@brombergEfficientMarkovNetwork2009.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6EQJL3C7/10613.html;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LQWYFYSC/10613.html;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TKYCBEBM/10613.html;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZH7SDVWN/10613.html}
}

@article{broschReinforcementLearningLinking2015,
  title = {Reinforcement {{Learning}} of {{Linking}} and {{Tracing Contours}} in {{Recurrent Neural Networks}}},
  author = {Brosch, Tobias and Neumann, Heiko and Roelfsema, Pieter R.},
  year = {2015},
  month = oct,
  journal = {PLOS Computational Biology},
  volume = {11},
  number = {10},
  pages = {e1004489},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004489},
  abstract = {The processing of a visual stimulus can be subdivided into a number of stages. Upon stimulus presentation there is an early phase of feedforward processing where the visual information is propagated from lower to higher visual areas for the extraction of basic and complex stimulus features. This is followed by a later phase where horizontal connections within areas and feedback connections from higher areas back to lower areas come into play. In this later phase, image elements that are behaviorally relevant are grouped by Gestalt grouping rules and are labeled in the cortex with enhanced neuronal activity (object-based attention in psychology). Recent neurophysiological studies revealed that reward-based learning influences these recurrent grouping processes, but it is not well understood how rewards train recurrent circuits for perceptual organization. This paper examines the mechanisms for reward-based learning of new grouping rules. We derive a learning rule that can explain how rewards influence the information flow through feedforward, horizontal and feedback connections. We illustrate the efficiency with two tasks that have been used to study the neuronal correlates of perceptual organization in early visual cortex. The first task is called contour-integration and demands the integration of collinear contour elements into an elongated curve. We show how reward-based learning causes an enhancement of the representation of the to-be-grouped elements at early levels of a recurrent neural network, just as is observed in the visual cortex of monkeys. The second task is curve-tracing where the aim is to determine the endpoint of an elongated curve composed of connected image elements. If trained with the new learning rule, neural networks learn to propagate enhanced activity over the curve, in accordance with neurophysiological data. We close the paper with a number of model predictions that can be tested in future neurophysiological and computational studies.},
  langid = {english},
  keywords = {Eye movements,Learning,Learning curves,Monkeys,Neural networks,Recurrent neural networks,Vision,Visual cortex},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Brosch et al_2015_Reinforcement Learning of Linking and Tracing Contours in Recurrent Neural.pdf;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Brosch et al_2015_Reinforcement Learning of Linking and Tracing Contours in Recurrent Neural2.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@broschReinforcementLearningLinking2015.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z6Z9JGUX/article.html}
}

@article{brownMultipleNeuralSpike2004,
  title = {Multiple Neural Spike Train Data Analysis: State-of-the-Art and Future Challenges},
  shorttitle = {Multiple Neural Spike Train Data Analysis},
  author = {Brown, Emery N and Kass, Robert E and Mitra, Partha P},
  year = {2004},
  month = may,
  journal = {Nature Neuroscience},
  volume = {7},
  number = {5},
  pages = {456--461},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1228},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Q4AF7EX5/Brown et al. - 2004 - Multiple neural spike train data analysis state-o.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@brownMultipleNeuralSpike2004.md}
}

@article{buccinoCombiningBiophysicalModeling2018,
  title = {Combining Biophysical Modeling and Deep Learning for Multielectrode Array Neuron Localization and Classification},
  author = {Buccino, Alessio P. and Kordovan, Michael and Ness, Torbj{\o}rn V. and Merkt, Benjamin and H{\"a}fliger, Philipp D. and Fyhn, Marianne and Cauwenberghs, Gert and Rotter, Stefan and Einevoll, Gaute T.},
  year = {2018},
  journal = {Journal of Neurophysiology},
  volume = {120},
  number = {3},
  pages = {1212--1232},
  doi = {10.1152/jn.00210.2018},
  abstract = {Neural circuits typically consist of many different types of neurons, and one faces the challenge to disentangle their individual contributions in measured neural activity. Classification of cells into inhibitory and excitatory neurons, and localization of neurons based on extracellular recordings are frequently employed procedures. Current approaches, however, need a lot of human intervention, which makes them slow, biased, and unreliable. Based on recent advances in deep learning techniques and exploiting the availability of neuron models with quasi-realistic 3D morphology and physiological properties, we present a framework for automatized and objective classification and localization of cells based on the spatiotemporal profiles of the extracellular action potential recorded by multi-electrode arrays. We train Convolutional Neural Networks (CNNs) on simulated signals from a large set of cell models, and show that our framework can predict the position of neurons with high accuracy, more precisely than current state-of-the-art methods. Our method is also able to classify whether a neuron is excitatory or inhibitory with very high accuracy, substantially improving on commonly used clustering techniques. Furthermore, our new method seems to have the potential to separate certain subtypes of excitatory and inhibitory neurons. The possibility to automatically localize and classify all neurons recorded with large high-density extracellular electrodes contributes to a more accurate and more reliable mapping of neural circuits.},
  note = {THis whole thing May be of use, but it doesn't have anything to do with the model selection I am working on. More to do with in vivo experiments and probes},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XFSVEXBG/Buccino et al. - 2018 - Combining biophysical modeling and deep learning for multielectrode array neuron localization and classification.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@buccinoCombiningBiophysicalModeling2018.md}
}

@article{buccinoDeepLearningApproach2018,
  title = {A {{Deep Learning Approach}} for the {{Classification}} of {{Neuronal Cell Types}}},
  author = {Buccino, Alessio P. and Ness, Torbjorn V. and Einevoll, Gaute T. and Cauwenberghs, Gert and Hafliger, Philipp D.},
  year = {2018},
  journal = {Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference},
  volume = {2018},
  number = {C},
  pages = {999--1002},
  issn = {9781538636466},
  doi = {10.1109/EMBC.2018.8512498},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PKHDL899/Buccino et al. - 2018 - A Deep Learning Approach for the Classification of Neuronal Cell Types.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@buccinoDeepLearningApproach2018.md}
}

@article{buccinoIndependentComponentAnalysis2018,
  title = {Independent {{Component Analysis}} for {{Fully Automated Multi-Electrode Array Spike Sorting}}},
  author = {Buccino, Alessio P. and Hagen, Espen and Einevoll, Gaute T. and Hafliger, Philipp D. and Cauwenbergh, Gert},
  year = {2018},
  journal = {Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference},
  volume = {2018},
  pages = {2627--2630},
  issn = {9781538636466},
  doi = {10.1109/EMBC.2018.8512788},
  keywords = {Pensar2017},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GIN93ITL/Buccino et al. - 2018 - Independent Component Analysis for Fully Automated Multi-Electrode Array Spike Sorting.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@buccinoIndependentComponentAnalysis2018.md}
}

@article{buchlerSchemesCombinatorialTranscription2003,
  title = {On Schemes of Combinatorial Transcription Logic},
  author = {Buchler, Nicolas E. and Gerland, Ulrich and Hwa, Terence},
  year = {2003},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {100},
  number = {9},
  pages = {5136--5141},
  publisher = {{National Academy of Sciences}},
  doi = {10.1073/pnas.0930314100},
  abstract = {Cells receive a wide variety of cellular and environmental signals, which are often processed combinatorially to generate specific genetic responses. Here we explore theoretically the potentials and limitations of combinatorial signal integration at the level of cis-regulatory transcription control. Our analysis suggests that many complex transcription-control functions of the type encountered in higher eukaryotes are already implementable within the much simpler bacterial transcription system. Using a quantitative model of bacterial transcription and invoking only specific protein-DNA interaction and weak glue-like interaction between regulatory proteins, we show explicit schemes to implement regulatory logic functions of increasing complexity by appropriately selecting the strengths and arranging the relative positions of the relevant protein-binding DNA sequences in the cis-regulatory region. The architectures that emerge are naturally modular and evolvable. Our results suggest that the transcription regulatory apparatus is a "programmable" computing machine, belonging formally to the class of Boltzmann machines. Crucial to our results is the ability to regulate gene expression at a distance. In bacteria, this can be achieved for isolated genes via DNA looping controlled by the dimerization of DNA-bound proteins. However, if adopted extensively in the genome, long-distance interaction can cause unintentional intergenic cross talk, a detrimental side effect difficult to overcome by the known bacterial transcription-regulation systems. This may be a key factor limiting the genome-wide adoption of complex transcription control in bacteria. Implications of our findings for combinatorial transcription control in eukaryotes are discussed.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9AV76CVX/Buchler, Gerland, Hwa - 2003 - On schemes of combinatorial transcription logic.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@buchlerSchemesCombinatorialTranscription2003.md}
}

@article{buchsInductionLongtermPotentiation1996,
  title = {Induction of Long-Term Potentiation Is Associated with Major Ultrastructural Changes of Activated Synapses},
  author = {Buchs, P. A. and Muller, D.},
  year = {1996},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {93},
  number = {15},
  pages = {8040--8045},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.93.15.8040},
  abstract = {Long-term potentiation (LTP), an increase in synaptic efficacy believed to underlie learning and memory mechanisms, has been proposed to involve structural modifications of synapses. Precise identification of the morphological changes associated with LTP has however been hindered by the difficulty in distinguishing potentiated or activated from nonstimulated synapses. Here we used a cytochemical method that allowed detection in CA1 hippocampus at the electron microscopy level of a stimulation-specific, D-AP5-sensitive accumulation of calcium in postsynaptic spines and presynaptic terminals following application of high-frequency trains. Morphometric analyses carried out 30-40 min after LTP induction revealed dramatic ultrastructural differences between labeled and nonlabeled synapses. The majority of labeled synapses (60\%) exhibited perforated postsynaptic densities, whereas this proportion was only 20\% in nonlabeled synaptic contacts. Labeled synaptic profiles were also characterized by a larger apposition zone between pre- and postsynaptic structures, longer postsynaptic densities, and enlarged spine profiles. These results add strong support to the idea that ultrastructural modifications and specifically an increase in perforated synapses are associated with LTP induction in field CA1 of hippocampus and they suggest that a majority of activated contacts may exhibit such changes.},
  chapter = {Research Article},
  langid = {english},
  pmid = {8755599},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NKA3EE5X/Buchs and Muller - 1996 - Induction of long-term potentiation is associated .pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@buchsInductionLongtermPotentiation1996.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GR5KDXUG/8040.html}
}

@article{buesingNeuralDynamicsSampling2011,
  title = {Neural {{Dynamics}} as {{Sampling}}: {{A Model}} for {{Stochastic Computation}} in {{Recurrent Networks}} of {{Spiking Neurons}}},
  shorttitle = {Neural {{Dynamics}} as {{Sampling}}},
  author = {Buesing, Lars and Bill, Johannes and Nessler, Bernhard and Maass, Wolfgang},
  year = {2011},
  month = nov,
  journal = {PLOS Computational Biology},
  volume = {7},
  number = {11},
  pages = {e1002211},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002211},
  abstract = {The organization of computations in networks of spiking neurons in the brain is still largely unknown, in particular in view of the inherently stochastic features of their firing activity and the experimentally observed trial-to-trial variability of neural systems in the brain. In principle there exists a powerful computational framework for stochastic computations, probabilistic inference by sampling, which can explain a large number of macroscopic experimental data in neuroscience and cognitive science. But it has turned out to be surprisingly difficult to create a link between these abstract models for stochastic computations and more detailed models of the dynamics of networks of spiking neurons. Here we create such a link and show that under some conditions the stochastic firing activity of networks of spiking neurons can be interpreted as probabilistic inference via Markov chain Monte Carlo (MCMC) sampling. Since common methods for MCMC sampling in distributed systems, such as Gibbs sampling, are inconsistent with the dynamics of spiking neurons, we introduce a different approach based on non-reversible Markov chains that is able to reflect inherent temporal processes of spiking neuronal activity through a suitable choice of random variables. We propose a neural network model and show by a rigorous theoretical analysis that its neural activity implements MCMC sampling of a given distribution, both for the case of discrete and continuous time. This provides a step towards closing the gap between abstract functional models of cortical computation and more detailed models of networks of spiking neurons.},
  langid = {english},
  keywords = {Action potentials,Markov models,Membrane potential,Network analysis,Neural networks,Neurons,Probability distribution,Sensory perception},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LAASTPV4/Buesing et al_2011_Neural Dynamics as Sampling.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/S9MCLC5A/Neural Dynamics as Sampling A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons - 09.05.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/M3YEBFAT/article.html}
}

@article{buhusiWhatMakesUs2005,
  title = {What Makes Us Tick? {{Functional}} and Neural Mechanisms of Interval Timing},
  shorttitle = {What Makes Us Tick?},
  author = {Buhusi, Catalin V. and Meck, Warren H.},
  year = {2005},
  month = oct,
  journal = {Nature Reviews Neuroscience},
  volume = {6},
  number = {10},
  pages = {755--765},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn1764},
  abstract = {Temporal information is crucial for goal reaching, neuroeconomics, and the survival of humans and other animals, and requires multiple biological mechanisms to track time over multiple scales. In mammals, the circadian clock is located in the suprachiasmatic nucleus. Another timer, which is responsible for automatic motor control in the millisecond range, relies on the cerebellum. Finally, a general-purpose, flexible, cognitively-controlled timer that operates in the seconds-to-hours range involves the activation thalamo-cortico-striatal circuits.The hallmark of interval timing is that the error in estimating a duration is proportional to the duration to be timed, a property known as scalar timing. Scalar timing resembles Weber's law, which applies to most sensory modalities.The way that time is perceived, represented and estimated has traditionally been explained using a pacemaker\textendash accumulator model, which is not only straightforward but also surprisingly powerful in explaining behavioural and biological data. Pharmacological studies support a dissociation of the clock stage, which is affected by dopaminergic manipulations, and the memory stage, which is affected by cholinergic manipulations.Despite explaining many findings, the relevance of the pacemaker\textendash accumulator model to the brain mechanisms that are involved in interval timing is unclear. New models will require investigation of recent neurobiological evidence.An impaired ability to process time is found in patients with disorders of the dopamine system, such as Parkinson's disease, Huntington's disease and schizophrenia. By contrast, the failure of a neurological disorder \textemdash{} such as cerebellar injury \textemdash{} to affect interval timing is taken to indicate that the affected structures are not essential for temporal processing in the seconds-to-hours range.Because interval timing depends on the intact striatum, but not on the intact cerebellum, the cerebellum is usually charged with millisecond timing and the basal ganglia with interval timing. Recent findings suggest that separate timing circuits can be dissociated when continuity, motor demands and attentional set are manipulated.The basal ganglia, prefrontal cortex and posterior parietal cortex are activated in both interval-timing tasks, and tasks that require integration of somatosensory signals or quantity/number processing. Electrophysiological data are consistent with the involvement of these structures in number, sequence or magnitude representation as well as in interval timing, thereby supporting a mode-control model of counting and timing in which number and time are processed by the same neural circuits.Functional MRI shows that two clusters of foci are activated during millisecond and interval timing tasks. The 'automatic timing' cluster is activated by tasks that require repetitive movements and involve short timing intervals, and includes the supplementary motor area and primary somatosensory cortex. The 'cognitively controlled timing' cluster is activated when the durations are longer and the amount of movement required is limited, and includes the dorsolateral prefrontal cortex, intraparietal sulcus and premotor cortex. The basal ganglia and the cerebellum are not specific to either cluster.The striatal beat-frequency model describes interval timing as an emergent activity in the thalamo-cortico-striatal loops. In this model, timing is based on the coincidental activation of medium spiny neurons in the basal ganglia by cortical neural oscillators. The activity of the striatal neurons increases before the expected time of reward, and peaks at the criterion interval. The model demonstrates the scalar property, and incorporates features that would allow the integration of a number of lines of evidence into one vision of interval timing in the brain.},
  copyright = {2005 Nature Publishing Group},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/42TV6CLL/Buhusi_Meck_2005_What makes us tick.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WQQTMDKG/nrn1764.html}
}

@techreport{bulsoComplexityLogisticRegression2019,
  title = {On the Complexity of Logistic Regression Models},
  author = {Bulso, Nicola and Marsili, Matteo and Roudi, Yasser},
  year = {2019},
  pages = {1--50},
  abstract = {We investigate the complexity of logistic regression models which is defined by counting the number of indistinguishable distributions that the model can represent (Balasubramanian, 1997). We find that the complexity of logistic models with binary inputs does not only depend on the number of parameters but also on the distribution of inputs in a non-trivial way which standard treatments of complexity do not address. In particular, we observe that correlations among inputs induce effective dependencies among parameters thus constraining the model and, consequently, reducing its complexity. We derive simple relations for the upper and lower bounds of the complexity. Furthermore, we show analytically that, defining the model parameters on a finite support rather than the entire axis, decreases the complexity in a manner that critically depends on the size of the domain. Based on our findings, we propose a novel model selection criterion which takes into account the entropy of the input distribution. We test our proposal on the problem of selecting the input variables of a logistic regression model in a Bayesian Model Selection framework. In our numerical tests, we find that, while the reconstruction errors of standard model selection approaches (AIC, BIC, 1 regularization) strongly depend on the sparsity of the ground truth, the reconstruction error of our method is always close to the minimum in all conditions of sparsity, data size and strength of input correlations. Finally, we observe that, when considering categorical instead of binary inputs, in a simple and mathematically tractable case, the contribution of the alphabet size to the complexity is very small compared to that of parameter space dimension. We further explore the issue by analysing the dataset of the "13 keys to the White House" which is a method for forecasting the outcomes of US presidential elections.},
  keywords = {bayesian model selection,Bayesian Model Selection,imum description length,jeffreys prior,Jeffreys prior,logistic regression,min-,Minimum Descrip-tion Length,model complexity},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/H4QGR43U/Bulso, Marsili, Roudi - 2019 - On the complexity of logistic regression models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bulsoComplexityLogisticRegression2019.md}
}

@article{bulsoRestrictedBoltzmannMachines2021,
  title = {Restricted {{Boltzmann Machines}} as {{Models}} of {{Interacting Variables}}},
  author = {Bulso, Nicola and Roudi, Yasser},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.15917 [cond-mat, physics:physics, stat]},
  eprint = {2103.15917},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:physics, stat},
  abstract = {We study the type of distributions that Restricted Boltzmann Machines (RBMs) with different activation functions can express by investigating the effect of the activation function of the hidden nodes on the marginal distribution they impose on observed binary nodes. We report an exact expression for these marginals in the form of a model of interacting binary variables with the explicit form of the interactions depending on the hidden node activation function. We study the properties of these interactions in detail and evaluate how the accuracy with which the RBM approximates distributions over binary variables depends on the hidden node activation function and on the number of hidden nodes. When the inferred RBM parameters are weak, an intuitive pattern is found for the expression of the interaction terms which reduces substantially the differences across activation functions. We show that the weak parameter approximation is a good approximation for different RBMs trained on the MNIST dataset. Interestingly, in these cases, the mapping reveals that the inferred models are essentially low order interaction models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning},
  note = {Comment: Supplemental material is available as ancillary file and can be downloaded from a link on the right},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/Obsidian/Obsidian/Charlie Vault/Citations/@bulsoRestrictedBoltzmannMachines2021.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Bulso_Roudi_2021_Restricted Boltzmann Machines as Models of Interacting Variables.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NAGTFRGL/2103.html}
}

@article{bulsoSparseModelSelection2016,
  title = {Sparse Model Selection in the Highly Under-Sampled Regime},
  author = {Bulso, Nicola and Marsili, Matteo and Roudi, Yasser},
  year = {2016},
  month = sep,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2016},
  number = {9},
  pages = {093404--093404},
  publisher = {{IOP Publishing}},
  doi = {10.1088/1742-5468/2016/09/093404},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UASCH69A/Bulso, Marsili, Roudi - 2016 - Sparse model selection in the highly under-sampled regime.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bulsoSparseModelSelection2016.md}
}

@article{bulsoSupplementalInformationComplexity2019,
  title = {Supplemental {{Information}} to: {{On}} the Complexity of Logistic Regression Models},
  author = {Bulso, Nicola and Marsili, Matteo and Roudi, Yasser},
  year = {2019},
  month = mar,
  abstract = {We investigate the complexity of logistic regression models which is defined by counting the number of indistinguishable distributions that the model can represent (Balasubramanian, 1997). We find that the complexity of logistic models with binary inputs does not only depend on the number of parameters but also on the distribution of inputs in a non-trivial way which standard treatments of complexity do not address. In particular, we observe that correlations among inputs induce effective dependencies among parameters thus constraining the model and, consequently, reducing its complexity. We derive simple relations for the upper and lower bounds of the complexity. Furthermore, we show analytically that, defining the model parameters on a finite support rather than the entire axis, decreases the complexity in a manner that critically depends on the size of the domain. Based on our findings, we propose a novel model selection criterion which takes into account the entropy of the input distribution. We test our proposal on the problem of selecting the input variables of a logistic regression model in a Bayesian Model Selection framework. In our numerical tests, we find that, while the reconstruction errors of standard model selection approaches (AIC, BIC, \$\textbackslash ell\_1\$ regularization) strongly depend on the sparsity of the ground truth, the reconstruction error of our method is always close to the minimum in all conditions of sparsity, data size and strength of input correlations. Finally, we observe that, when considering categorical instead of binary inputs, in a simple and mathematically tractable case, the contribution of the alphabet size to the complexity is very small compared to that of parameter space dimension. We further explore the issue by analysing the dataset of the "13 keys to the White House" which is a method for forecasting the outcomes of US presidential elections.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7SHRMFD6/Bulso, Marsili, Roudi - 2019 - Supplemental Information to On the complexity of logistic regression models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bulsoSupplementalInformationComplexity2019.md}
}

@article{burakSpatialCodingAttractor2014,
  title = {Spatial Coding and Attractor Dynamics of Grid Cells in the Entorhinal Cortex},
  author = {Burak, Yoram},
  year = {2014},
  journal = {Current Opinion in Neurobiology},
  volume = {25},
  pages = {169--175},
  publisher = {{Elsevier Ltd}},
  doi = {10.1016/j.conb.2014.01.013},
  abstract = {Recent experiments support the theoretical hypothesis that recurrent connectivity plays a central role within the medial entorhinal cortex, by shaping activity of large neural populations, such that their joint activity lies within a continuous attractor. This conjecture involves dynamics within each population (module) of cells that share the same grid spacing. In addition, recent theoretical works raise a hypothesis that, taken together, grid cells from all modules maintain a sophisticated representation of position with uniquely large dynamical range, when compared with other known neural codes in the brain. To maintain such a code, activity in different modules must be coupled, within the entorhinal cortex or through the hippocampus. \textcopyright{} 2014 Elsevier Ltd.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KEXLFBW6/Burak - 2014 - Spatial coding and attractor dynamics of grid cells in the entorhinal cortex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@burakSpatialCodingAttractor2014.md}
}

@article{burakTriangularLatticeNeurons2006,
  title = {Triangular Lattice Neurons May Implement an Advanced Numeral System to Precisely Encode Rat Position over Large Ranges},
  author = {Burak, Yoram and Brookings, Ted and Fiete, Ila},
  year = {2006},
  volume = {93106},
  doi = {10.1007/s00703-017-0540-y},
  abstract = {We argue by observation of the neural data that neurons in area dMEC of rats, which fire whenever the rat is on any vertex of a regular triangular lattice that tiles 2-d space, may be using an advanced numeral system to reversibly encode rat position. We interpret measured dMEC properties within the framework of a residue number system (RNS), and describe how RNS encoding -- which breaks the non-periodic variable of rat position into a set of narrowly distributed periodic variables -- allows a small set of cells to compactly represent and efficiently update rat position with high resolution over a large range. We show that the uniquely useful properties of RNS encoding still hold when the encoded and encoding quantities are relaxed to be real numbers with built-in uncertainties, and provide a numerical and functional estimate of the range and resolution of rat positions that can be uniquely encoded in dMEC. The use of a compact, `arithmetic-friendly' numeral system to encode a metric variable, as we propose is happening in dMEC, is qualitatively different from all previously identified examples of coding in the brain. We discuss the numerous neurobiological implications and predictions of our hypothesis.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VQEZ3T9C/Burak, Brookings, Fiete - 2006 - Triangular lattice neurons may implement an advanced numeral system to precisely encode rat position ov.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@burakTriangularLatticeNeurons2006.md}
}

@article{burakWeUnderstandEmergent2006,
  title = {Do {{We Understand}} the {{Emergent Dynamics}} of {{Grid Cell Activity}}?},
  author = {Burak, Y.},
  year = {2006},
  journal = {Journal of Neuroscience},
  volume = {26},
  number = {37},
  pages = {9352--9354},
  issn = {1529-2401 (Electronic)\textbackslash n0270-6474 (Linking)},
  doi = {10.1523/JNEUROSCI.2857-06.2006},
  abstract = {We examine the qualitative and quantitative properties of continuous attractor networks in explaining the dynamics of grid cells.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UZVP8KD2/Burak - 2006 - Do We Understand the Emergent Dynamics of Grid Cell Activity.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@burakWeUnderstandEmergent2006.md}
}

@article{burgessControllingPhaseNoise2014,
  title = {Controlling {{Phase Noise}} in {{Oscillatory Interference Models}} of {{Grid Cell Firing}}},
  author = {Burgess, Christopher P. and Burgess, Neil},
  year = {2014},
  month = apr,
  journal = {Journal of Neuroscience},
  volume = {34},
  number = {18},
  pages = {6224--6232},
  publisher = {{Society for Neuroscience}},
  doi = {10.1523/JNEUROSCI.2540-12.2014},
  abstract = {Oscillatory interference models account for the spatial firing properties of grid cells in terms of neuronal oscillators with frequencies modulated by the animal's movement velocity. The phase of such a ``velocity-controlled oscillator'' (VCO) relative to a baseline (theta-band) oscillation tracks displacement along a preferred direction. Input from multiple VCOs with appropriate preferred directions causes a grid cell's grid-like firing pattern. However, accumulating phase noise causes the firing pattern to drift and become corrupted. Here we show how multiple redundant VCOs can automatically compensate for phase noise. By entraining the baseline frequency to the mean VCO frequency, VCO phases remain consistent, ensuring a coherent grid pattern and reducing its spatial drift. We show how the spatial stability of grid firing depends on the variability in VCO phases, e.g., a phase SD of 3 ms per 125 ms cycle results in stable grids for 1 min. Finally, coupling N VCOs with similar preferred directions as a ring attractor, so that their relative phases remain constant, produces grid cells with consistently offset grids, and reduces VCO phase variability of the order square root of N . The results suggest a viable functional organization of the grid cell network, and highlight the benefit of integrating displacement along multiple redundant directions for the purpose of path integration.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FLK4DPND/Burgess, Burgess - 2014 - Controlling Phase Noise in Oscillatory Interference Models of Grid Cell Firing.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@burgessControllingPhaseNoise2014.md}
}

@article{burgessGridCellsTheta2008,
  title = {Grid Cells and Theta as Oscillatory Interference: {{Theory}} and Predictions},
  author = {Burgess, Neil},
  year = {2008},
  journal = {Hippocampus},
  volume = {18},
  number = {12},
  pages = {1157--1174},
  issn = {18:11571174},
  doi = {10.1002/hipo.20518},
  abstract = {The oscillatory interference model [Burgess et al. (2007) Hippocampus 17:801-802] of grid cell firing is reviewed as an algorithmic level description of path integration and as an implementation level description of grid cells and their inputs. New analyses concern the relationships between the variables in the model and the theta rhythm, running speed, and the intrinsic firing frequencies of grid cells. New simulations concern the implementation of velocity-controlled oscillators (VCOs) with different preferred directions in different neurons. To summarize the model, the distance traveled along a specific direction is encoded by the phase of a VCO relative to a baseline frequency. Each VCO is an intrinsic membrane potential oscillation whose frequency increases from baseline as a result of depolarization by synaptic input from speed modulated head-direction cells. Grid cell firing is driven by the VCOs whose preferred directions match the current direction of motion. VCOs are phase-reset by location-specific input from place cells to prevent accumulation of error. The baseline frequency is identified with the local average of VCO frequencies, while EEG theta frequency is identified with the global average VCO frequency and comprises two components: the frequency at zero speed and a linear response to running speed. Quantitative predictions are given for the inter-relationships between a grid cell's intrinsic firing frequency and grid scale, the two components of theta frequency, and the running speed of the animal. Qualitative predictions are given for the properties of the VCOs, and the relationship between environmental novelty, the two components of theta, grid scale and place cell remapping.},
  keywords = {Computational model,Entorhinal cortex,Hippocampus,Path integration,Spatial navigation},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/24DW8484/Burgess - 2008 - Grid cells and theta as oscillatory interference Theory and predictions.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@burgessGridCellsTheta2008.md}
}

@article{burgessOscillatoryInterferenceModel2007,
  title = {An Oscillatory Interference Model of Grid Cell Firing},
  author = {Burgess, Neil and Barry, Caswell and O'Keefe, John},
  year = {2007},
  month = sep,
  journal = {Hippocampus},
  volume = {17},
  number = {9},
  pages = {801--812},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/hipo.20327},
  keywords = {dendrites,entorhinal cortex,hippocampus,place cells,stellate cells,theta rhythm},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/42W2T72J/Burgess, Barry, O'Keefe - 2007 - An oscillatory interference model of grid cell firing.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@burgessOscillatoryInterferenceModel2007.md}
}

@article{burkittReviewIntegrateandfireNeuron2006,
  title = {A Review of the Integrate-and-Fire Neuron Model: {{I}}. {{Homogeneous}} Synaptic Input},
  author = {Burkitt, A. N.},
  year = {2006},
  month = jul,
  journal = {Biological Cybernetics},
  volume = {95},
  number = {1},
  pages = {1--19},
  publisher = {{Biol Cybern}},
  issn = {03401200},
  doi = {10.1007/s00422-006-0068-6},
  abstract = {The integrate-and-fire neuron model is one of the most widely used models for analyzing the behavior of neural systems. It describes the membrane potential of a neuron in terms of the synaptic inputs and the injected current that it receives. An action potential (spike) is generated when the membrane potential reaches a threshold, but the actual changes associated with the membrane voltage and conductances driving the action potential do not form part of the model. The synaptic inputs to the neuron are considered to be stochastic and are described as a temporally homogeneous Poisson process. Methods and results for both current synapses and conductance synapses are examined in the diffusion approximation, where the individual contributions to the postsynaptic potential are small. The focus of this review is upon the mathematical techniques that give the time distribution of output spikes, namely stochastic differential equations and the Fokker-Planck equation. The integrate-and-fire neuron model has become established as a canonical model for the description of spiking neurons because it is capable of being analyzed mathematically while at the same time being sufficiently complex to capture many of the essential features of neural processing. A number of variations of the model are discussed, together with the relationship with the Hodgkin-Huxley neuron model and the comparison with electrophysiological data. A brief overview is given of two issues in neural information processing that the integrate-and-fire neuron model has contributed to - the irregular nature of spiking in cortical neurons and neural gain modulation.},
  pmid = {16622699},
  keywords = {Conductance models,Integrate-and-fire neuron,Neural models},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6MDTLTUS/Burkitt - 2006 - A review of the integrate-and-fire neuron model I. Homogeneous synaptic input.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GT97NV7F/Burkitt - 2006 - A review of the integrate-and-fire neuron model I. Homogeneous synaptic input.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@burkittReviewIntegrateandfireNeuron2006.md}
}

@article{bushChapter11GenomeWide2012a,
  title = {Chapter 11: {{Genome-Wide Association Studies}}},
  shorttitle = {Chapter 11},
  author = {Bush, William S. and Moore, Jason H.},
  year = {2012},
  month = dec,
  journal = {PLoS Computational Biology},
  volume = {8},
  number = {12},
  pages = {e1002822},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1002822},
  abstract = {Genome-wide association studies (GWAS) have evolved over the last ten years into a powerful tool for investigating the genetic architecture of human disease. In this work, we review the key concepts underlying GWAS, including the architecture of common diseases, the structure of common human genetic variation, technologies for capturing genetic information, study designs, and the statistical methods used for data analysis. We also look forward to the future beyond GWAS.},
  pmcid = {PMC3531285},
  pmid = {23300413},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Bush_Moore_2012_Chapter 11.pdf}
}

@article{bushHybridOscillatoryInterference2014,
  title = {A Hybrid Oscillatory Interference/Continuous Attractor Network Model of Grid Cell Firing.},
  author = {Bush, Daniel and Burgess, Neil},
  year = {2014},
  month = apr,
  journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
  volume = {34},
  number = {14},
  pages = {5065--79},
  publisher = {{Society for Neuroscience}},
  doi = {10.1523/JNEUROSCI.4017-13.2014},
  abstract = {Grid cells in the rodent medial entorhinal cortex exhibit remarkably regular spatial firing patterns that tessellate all environments visited by the animal. Two theoretical mechanisms that could generate this spatially periodic activity pattern have been proposed: oscillatory interference and continuous attractor dynamics. Although a variety of evidence has been cited in support of each, some aspects of the two mechanisms are complementary, suggesting that a combined model may best account for experimental data. The oscillatory interference model proposes that the grid pattern is formed from linear interference patterns or "periodic bands" in which velocity-controlled oscillators integrate self-motion to code displacement along preferred directions. However, it also allows the use of symmetric recurrent connectivity between grid cells to provide relative stability and continuous attractor dynamics. Here, we present simulations of this type of hybrid model, demonstrate that it generates intracellular membrane potential profiles that closely match those observed in vivo, addresses several criticisms aimed at pure oscillatory interference and continuous attractor models, and provides testable predictions for future empirical studies.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WEUHWCET/Bush, Burgess - 2014 - A hybrid oscillatory interferencecontinuous attractor network model of grid cell firing.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bushHybridOscillatoryInterference2014.md}
}

@article{bushUsingGridCells2015,
  title = {Using {{Grid Cells}} for {{Navigation}}},
  author = {Bush, Daniel and Barry, Caswell and Manson, Daniel and Burgess, Neil},
  year = {2015},
  journal = {Neuron},
  volume = {87},
  number = {3},
  pages = {507--520},
  publisher = {{The Authors}},
  doi = {10.1016/j.neuron.2015.07.006},
  abstract = {Mammals are able to navigate to hidden goal locations by direct routes that may traverse previously unvisited terrain. Empirical evidence suggests that this "vector navigation" relies on an internal representation of space provided by the hippocampal formation. The periodic spatial firing patterns of grid cells in the hippocampal formation offer a compact combinatorial code for location within large-scale space. Here, we consider the computational problem of how to determine the vector between start and goal locations encoded by the firing of grid cells when this vector may be much longer than the largest grid scale. First, we present an algorithmic solution to the problem, inspired by the Fourier shift theorem. Second, we describe several potential neural network implementations of this solution that combine efficiency of search and biological plausibility. Finally, we discuss the empirical predictions of these implementations and their relationship to the anatomy and electrophysiology of the hippocampal formation.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CRFEGNIK/Bush et al. - 2015 - Using Grid Cells for Navigation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@bushUsingGridCells2015.md}
}

@article{butcherReservoirComputingExtreme2013,
  title = {Reservoir Computing and Extreme Learning Machines for Non-Linear Time-Series Data Analysis},
  author = {Butcher, J. B. and Verstraeten, D. and Schrauwen, B. and Day, C. R. and Haycock, P. W.},
  year = {2013},
  month = feb,
  journal = {Neural Networks},
  volume = {38},
  pages = {76--89},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.11.011},
  abstract = {Random projection architectures such as Echo state networks (ESNs) and Extreme Learning Machines (ELMs) use a network containing a randomly connected hidden layer and train only the output weights, overcoming the problems associated with the complex and computationally demanding training algorithms traditionally used to train neural networks, particularly recurrent neural networks. In this study an ESN is shown to contain an antagonistic trade-off between the amount of non-linear mapping and short-term memory it can exhibit when applied to time-series data which are highly non-linear. To overcome this trade-off a new architecture, Reservoir with Random Static Projections (R2SP) is investigated, that is shown to offer a significant improvement in performance. A similar approach using an ELM whose input is presented through a time delay (TD-ELM) is shown to further enhance performance where it significantly outperformed the ESN and R2SP as well other architectures when applied to a novel task which allows the short-term memory and non-linearity to be varied. The hard-limiting memory of the TD-ELM appears to be best suited for the data investigated in this study, although ESN-based approaches may offer improved performance when processing data which require a longer fading memory.},
  langid = {english},
  keywords = {Extreme learning machine,Non-linearity,Reservoir computing,Reservoir with random static projections,Short-term memory,Time-series data},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VCEDZZCR/Reservoir computing and extreme learning machines for non-linear time-series.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YDA8RDVJ/Reservoir computing and extreme learning machines for non-linear time-series data analysis - 11.05.22.md}
}

@techreport{buzsakiNeuronalOscillationsCortical,
  title = {Neuronal {{Oscillations}} in {{Cortical Networks}}},
  author = {Buzs{\'a}ki, Gy{\"o}rgy and Draguhn, Andreas},
  abstract = {Clocks tick, bridges and skyscrapers vibrate, neuronal networks oscillate. Are neuronal oscillations an inevitable by-product, similar to bridge vibrations, or an essential part of the brain's design? Mammalian cortical neurons form behavior-dependent oscillating networks of various sizes, which span five orders of magnitude in frequency. These oscillations are phylogenetically preserved, suggesting that they are functionally relevant. Recent findings indicate that network oscillations bias input selection, temporally link neurons into assemblies, and facilitate synaptic plasticity, mechanisms that cooperatively support temporal representation and long-term consolidation of information.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XBV4IGZJ/Buzsáki, Draguhn - Unknown - Neuronal Oscillations in Cortical Networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@buzsakiNeuronalOscillationsCortical.md}
}

@article{caiInferringNeuronalNetwork2017,
  title = {Inferring Neuronal Network Functional Connectivity with Directed Information},
  author = {Cai, Zhiting and Neveu, Curtis L. and Baxter, Douglas A. and Byrne, John H. and Aazhang, Behnaam},
  year = {2017},
  journal = {Journal of Neurophysiology},
  volume = {118},
  number = {2},
  pages = {1055--1069},
  issn = {7133484749},
  doi = {10.1152/jn.00086.2017},
  abstract = {A major challenge in neuroscience is to develop effective tools that infer the circuit connectivity from large-scale recordings of neuronal activity patterns. In this study, context tree maximizing (CTM) was used to estimate directed information (DI), which measures causal influences among neural spike trains in order to infer putative synaptic connections. In contrast to existing methods, the method presented here is data driven and can readily identify both linear and nonlinear relations between neurons. This CTM-DI method reliably identified circuit structures underlying simulations of realistic conductance-based networks. It also inferred circuit properties from voltage-sensitive dye recordings of the buccal ganglion of Aplysia. This method can be applied to other large-scale recordings as well. It offers a systematic tool to map network connectivity and to track changes in network structure such as synaptic strengths as well as the degrees of connectivity of individual neurons, which in turn could provide insights into how modifications produced by learning are distributed in a neural network.NEW \& NOTEWORTHY This study brings together the techniques of voltage-sensitive dye recording and information theory to infer the functional connectome of the feeding central pattern generating network of Aplysia. In contrast to current statistical approaches, the inference method developed in this study is data driven and validated by conductance-based model circuits, can distinguish excitatory and inhibitory connections, is robust against synaptic plasticity, and is capable of detecting network structures that mediate motor patterns. Copyright 2017 the American Physiological Society.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6AISZRWW/Cai et al. - 2017 - Inferring neuronal network functional connectivity with directed information.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@caiInferringNeuronalNetwork2017.md}
}

@article{calabreseInconvenientTruthPrinciple2018,
  title = {Inconvenient {{Truth}} to {{Principle}} of {{Neuroscience}}},
  author = {Calabrese, Ronald L.},
  year = {2018},
  journal = {Trends in Neurosciences},
  volume = {41},
  number = {8},
  pages = {488--491},
  doi = {10.1016/j.tins.2018.05.006},
  abstract = {In 2004, Prinz et al. demonstrated that almost indistinguishable network activity can arise from widely different sets of underlying membrane and synaptic parameters, and, thus, likely arise from different cellular and network mechanisms. This now broadly accepted principle guides research into individual variation in neuronal and synaptic properties, and their homeostatic regulation.},
  keywords = {conductance parameter sets,homeostasis,network stability,neuronal models},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/53VUAX4U/Calabrese - 2018 - Inconvenient Truth to Principle of Neuroscience.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@calabreseInconvenientTruthPrinciple2018.md}
}

@article{calabreseInconvenientTruthPrinciple2018a,
  title = {Inconvenient {{Truth}} to {{Principle}} of {{Neuroscience}}},
  author = {Calabrese, Ronald L.},
  year = {2018},
  month = aug,
  journal = {Trends in Neurosciences},
  volume = {41},
  number = {8},
  pages = {488--491},
  issn = {1878-108X},
  doi = {10.1016/j.tins.2018.05.006},
  abstract = {In 2004, Prinz et al. demonstrated that almost indistinguishable network activity can arise from widely different sets of underlying membrane and synaptic parameters, and, thus, likely arise from different cellular and network mechanisms. This now broadly accepted principle guides research into individual variation in neuronal and synaptic properties, and their homeostatic regulation.},
  langid = {english},
  pmcid = {PMC6065260},
  pmid = {30053951},
  keywords = {conductance parameter sets,homeostasis,network stability,neuronal models},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Calabrese_2018_Inconvenient Truth to Principle of Neuroscience.pdf}
}

@article{calderonThunderstruckACDCModel2022,
  title = {Thunderstruck: {{The ACDC}} Model of Flexible Sequences and Rhythms in Recurrent Neural Circuits},
  shorttitle = {Thunderstruck},
  author = {Calderon, C. and Verguts, T. and Frank, M.},
  year = {2022},
  journal = {bioRxiv},
  doi = {10.1101/2021.04.07.438842},
  abstract = {A biologically plausible recurrent neural network of cortical dynamics is augmented to include a basal ganglia-thalamic module which uses reinforcement learning to dynamically modulate action and this ``associative cluster-dependent chain'' (ACDC) model modularly stores sequence and timing information in distinct loci of the network. Adaptive sequential behavior is a hallmark of human cognition. In particular, humans can learn to produce precise spatiotemporal sequences given a certain context. For instance, musicians can not only reproduce learned action sequences in a context-dependent manner, they can also quickly and flexibly reapply them in any desired tempo or rhythm without overwriting previous learning. Existing neural network models fail to account for these properties. We argue that this limitation emerges from the fact that sequence information (i.e., the position of the action) and timing (i.e., the moment of response execution) are typically stored in the same neural network weights. Here, we augment a biologically plausible recurrent neural network of cortical dynamics to include a basal ganglia-thalamic module which uses reinforcement learning to dynamically modulate action. This ``associative cluster-dependent chain'' (ACDC) model modularly stores sequence and timing information in distinct loci of the network. This feature increases computational power and allows ACDC to display a wide range of temporal properties (e.g., multiple sequences, temporal shifting, rescaling, and compositionality), while still accounting for several behavioral and neurophysiological empirical observations. Finally, we apply this ACDC network to show how it can learn the famous ``Thunderstruck'' song intro and then flexibly play it in a ``bossa nova'' rhythm without further training.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TTXPCVKU/Calderon et al_2022_Thunderstruck.pdf}
}

@article{callenNoteGreenFunctions1963,
  title = {A Note on {{Green}} Functions and the {{Ising}} Model},
  author = {Callen, H.B.},
  year = {1963},
  month = apr,
  journal = {Physics Letters},
  volume = {4},
  number = {3},
  pages = {161},
  issn = {00319163},
  doi = {10.1016/0031-9163(63)90344-5},
  langid = {english},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@callenNoteGreenFunctions1963.md}
}

@article{camachoNextGenerationMachineLearning2018,
  title = {Next-{{Generation Machine Learning}} for {{Biological Networks}}},
  author = {Camacho, Diogo M. and Collins, Katherine M. and Powers, Rani K. and Costello, James C. and Collins, James J.},
  year = {2018},
  journal = {Cell},
  volume = {173},
  number = {7},
  pages = {1581--1592},
  publisher = {{Elsevier Inc.}},
  issn = {0000000154871},
  doi = {10.1016/j.cell.2018.05.015},
  abstract = {Machine learning, a collection of data-analytical techniques aimed at building predictive models from multi-dimensional datasets, is becoming integral to modern biological research. By enabling one to generate models that learn from large datasets and make predictions on likely outcomes, machine learning can be used to study complex cellular systems such as biological networks. Here, we provide a primer on machine learning for life scientists, including an introduction to deep learning. We discuss opportunities and challenges at the intersection of machine learning and network biology, which could impact disease biology, drug discovery, microbiome research, and synthetic biology. Machine-learning approaches are essential for pulling information out of the vast datasets that are being collected across biology and biomedicine. This Review considers the opportunities and challenges at the intersection of network biology and data science.},
  keywords = {deep learning,Machine leaning,network biology,neural networks,synthetic biology,systems biology},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4HMZDK5P/Camacho et al. - 2018 - Next-Generation Machine Learning for Biological Networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@camachoNextGenerationMachineLearning2018.md}
}

@book{Cameron:1992:DGC:583293,
  title = {Designs, Graphs, Codes, and Their Links},
  author = {Cameron, Peter J. and Lint, J. H. Van},
  year = {1992},
  publisher = {{Cambridge University Press}},
  address = {{New York, NY, USA}},
  isbn = {0-521-42385-6}
}

@article{canningPartiallyConnectedModels1988,
  title = {Partially Connected Models of Neural Networks},
  author = {Canning, A. and Gardner, E.},
  year = {1988},
  month = aug,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {21},
  number = {15},
  pages = {3275--3284},
  publisher = {{IOP Publishing}},
  issn = {0305-4470},
  doi = {10.1088/0305-4470/21/15/016},
  abstract = {A partially connected Hopfield neural network model is studied under the restriction that w, the ratio of connections per site to the size of the system, remains finite as the size N to infinity with the connection structure at each site being the same. The replica symmetric mean field theory equations for the order parameters are derived. The zero-temperature forms of these equations are then solved numerically for a few different 'local' connectivity architectures showing phase transitions at different critical storage ratios, alpha c, where the states which the authors are trying to store in the network become discontinuously unstable. They show that the information capacity per connection improves for partially connected systems.},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Canning_Gardner_1988_Partially connected models of neural networks.pdf}
}

@misc{caoExplanatoryModelsNeuroscience2021,
  title = {Explanatory Models in Neuroscience: {{Part}} 1 -- Taking Mechanistic Abstraction Seriously},
  shorttitle = {Explanatory Models in Neuroscience},
  author = {Cao, Rosa and Yamins, Daniel},
  year = {2021},
  month = apr,
  number = {arXiv:2104.01490},
  eprint = {2104.01490},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2104.01490},
  abstract = {Despite the recent success of neural network models in mimicking animal performance on visual perceptual tasks, critics worry that these models fail to illuminate brain function. We take it that a central approach to explanation in systems neuroscience is that of mechanistic modeling, where understanding the system is taken to require fleshing out the parts, organization, and activities of a system, and how those give rise to behaviors of interest. However, it remains somewhat controversial what it means for a model to describe a mechanism, and whether neural network models qualify as explanatory. We argue that certain kinds of neural network models are actually good examples of mechanistic models, when the right notion of mechanistic mapping is deployed. Building on existing work on model-to-mechanism mapping (3M), we describe criteria delineating such a notion, which we call 3M++. These criteria require us, first, to identify a level of description that is both abstract but detailed enough to be "runnable", and then, to construct model-to-brain mappings using the same principles as those employed for brain-to-brain mapping across individuals. Perhaps surprisingly, the abstractions required are those already in use in experimental neuroscience, and are of the kind deployed in the construction of more familiar computational models, just as the principles of inter-brain mappings are very much in the spirit of those already employed in the collection and analysis of data across animals. In a companion paper, we address the relationship between optimization and intelligibility, in the context of functional evolutionary explanations. Taken together, mechanistic interpretations of computational models and the dependencies between form and function illuminated by optimization processes can help us to understand why brain systems are built they way they are.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NYHD2SWH/Cao_Yamins_2021_Explanatory models in neuroscience.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZXAPHLY8/2104.html}
}

@misc{caoExplanatoryModelsNeuroscience2021a,
  title = {Explanatory Models in Neuroscience: {{Part}} 2 -- Constraint-Based Intelligibility},
  shorttitle = {Explanatory Models in Neuroscience},
  author = {Cao, Rosa and Yamins, Daniel},
  year = {2021},
  month = apr,
  number = {arXiv:2104.01489},
  eprint = {2104.01489},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2104.01489},
  abstract = {Computational modeling plays an increasingly important role in neuroscience, highlighting the philosophical question of how computational models explain. In the context of neural network models for neuroscience, concerns have been raised about model intelligibility, and how they relate (if at all) to what is found in the brain. We claim that what makes a system intelligible is an understanding of the dependencies between its behavior and the factors that are causally responsible for that behavior. In biological systems, many of these dependencies are naturally "top-down": ethological imperatives interact with evolutionary and developmental constraints under natural selection. We describe how the optimization techniques used to construct NN models capture some key aspects of these dependencies, and thus help explain why brain systems are as they are -- because when a challenging ecologically-relevant goal is shared by a NN and the brain, it places tight constraints on the possible mechanisms exhibited in both kinds of systems. By combining two familiar modes of explanation -- one based on bottom-up mechanism (whose relation to neural network models we address in a companion paper) and the other on top-down constraints, these models illuminate brain function.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/R3EVMJLY/Cao_Yamins_2021_Explanatory models in neuroscience.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EB3JGM5B/2104.html}
}

@article{capizziSpikingNeuralNetworkbased2020,
  title = {A Spiking Neural Network-Based Long-Term Prediction System for Biogas Production},
  author = {Capizzi, Giacomo and Lo Sciuto, Grazia and Napoli, Christian and Wo{\'z}niak, Marcin and Susi, Gianluca},
  year = {2020},
  month = sep,
  journal = {Neural Networks},
  volume = {129},
  pages = {271--279},
  issn = {08936080},
  doi = {10.1016/j.neunet.2020.06.001},
  langid = {english}
}

@article{caponeBiologicallyPlausibleDreaming2022,
  title = {Towards Biologically Plausible {{Dreaming}} and {{Planning}}},
  author = {Capone, C. and Paolucci, P.},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2205.10044},
  abstract = {This work proposes a two-module (agent and model) neural network in which "dreaming" (living new experiences in a model-based simulated environment) significantly boosts learning, and explores "planning", an online alternative to dreaming, that shows comparable performances. Humans and animals can learn new skills after practicing for a few hours, while current reinforcement learning algorithms require a large amount of data to achieve good performances. Recent model-based approaches show promising results by reducing the number of necessary interactions with the environment to learn a desirable policy. However, these methods require biological implausible ingredients, such as the detailed storage of older experiences, and long periods of offline learning. The optimal way to learn and exploit word-models is still an open question. Taking inspiration from biology, we suggest that dreaming might be an efficient expedient to use an inner model. We propose a two-module (agent and model) neural network in which "dreaming" (living new experiences in a model-based simulated environment) significantly boosts learning. We also explore "planning", an online alternative to dreaming, that shows comparable performances. Impor-tantly, our model does not require the detailed storage of experiences, and learns online the world-model. This is a key ingredient for biological plausibility and implementability (e.g., in neuromorphic hardware). Our network is composed of spiking neurons, further increasing the energetic efficiency and the plausibility of the model. To our knowledge, there are no previous works proposing biologically plausible model-based reinforcement learning in recurrent spiking networks. Our work is a step toward building efficient neuromorphic systems for autonomous robots, capable of learning new skills in real-world environments. Even when the environment is no longer accessible, the robot optimizes learning by "reasoning" in its own "mind". These approaches are of great relevance when the acquisition from the environment is slow, expensive (robotics) or unsafe (autonomous driving).},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YHJGMY9E/Capone_Paolucci_2022_Towards biologically plausible Dreaming and Planning.pdf}
}

@inproceedings{caponeBurstdependentPlasticityDendritic2022,
  title = {Burst-Dependent Plasticity and Dendritic Amplification Support Target-Based Learning and Hierarchical Imitation Learning},
  author = {Capone, C. and Lupo, C. and Muratore, P. and Paolucci, P.},
  year = {2022},
  abstract = {A multi-compartment model of pyramidal neuron, in which bursts and dendritic input segregation give the possibility to plausibly support a biological target-based learning, is proposed, enabling the decomposition of challenging long-horizon decision-making tasks into simpler subtasks. The brain can learn to solve a wide range of tasks with high temporal and energetic efficiency. However, most biological models are composed of simple single compartment neurons and cannot achieve the state-of-art performances of artificial intelligence. We propose a multi-compartment model of pyramidal neuron, in which bursts and dendritic input segregation give the possibility to plausibly support a biological target-based learning. In target-based learning, the internal solution of a problem (a spatio temporal pattern of bursts in our case) is suggested to the network, bypassing the problems of error backpropagation and credit assignment. Finally, we show that this neuronal architecture naturally supports the orchestration of ``hierarchical imitation learning'', enabling the decomposition of challenging long-horizon decision-making tasks into simpler subtasks.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FLHEUG7P/Capone et al_2022_Burst-dependent plasticity and dendritic amplification support target-based.pdf}
}

@article{carpenterGridCellsForm2015,
  title = {Grid Cells Form a Global Representation of Connected Environments},
  author = {Carpenter, Francis and Manson, Daniel and Jeffery, Kate and Burgess, Neil and Barry, Caswell},
  year = {2015},
  journal = {Current Biology},
  volume = {25},
  number = {9},
  pages = {1176--1182},
  publisher = {{Elsevier}},
  issn = {1879-0445 (Electronic) 0960-9822 (Linking)},
  doi = {10.1016/j.cub.2015.02.037},
  abstract = {The firing patterns of grid cells in medial entorhinal cortex (mEC) and associated brain areas form triangular arrays that tessellate the environment [1, 2] and maintain constant spatial offsets to each other between environments [3, 4]. These cells are thought to provide an efficient metric for navigation in large-scale space [5-8]. However, an accurate and universal metric requires grid cell firing patterns to uniformly cover the space to be navigated, in contrast to recent demonstrations that environmental features such as boundaries can distort [9-11] and fragment [12] grid patterns. To establish whether grid firing is determined by local environmental cues, or provides a coherent global representation, we recorded mEC grid cells in rats foraging in an environment containing two perceptually identical compartments connected via a corridor. During initial exposures to the multicompartment environment, grid firing patterns were dominated by local environmental cues, replicating between the two compartments. However, with prolonged experience, grid cell firing patterns formed a single, continuous representation that spanned both compartments. Thus, we provide the first evidence that in a complex environment, grid cell firing can form the coherent global pattern necessary for them to act as a metric capable of supporting large-scale spatial navigation.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7VUVQPNC/Carpenter et al. - 2015 - Grid cells form a global representation of connected environments.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@carpenterGridCellsForm2015.md}
}

@incollection{carvalhoTemporalBisectionProcedure2019,
  title = {Temporal {{Bisection Procedure}}},
  booktitle = {Encyclopedia of {{Animal Cognition}} and {{Behavior}}},
  author = {de Carvalho, Marilia Pinheiro and Machado, Armando and Vasconcelos, Marco},
  editor = {Vonk, Jennifer and Shackelford, Todd},
  year = {2019},
  pages = {1--4},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-47829-6_2067-1},
  isbn = {978-3-319-47829-6},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7FRUARDS/Carvalho et al_2019_Temporal Bisection Procedure.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KT5K53QE/Temporal Bisection Procedure - 20.06.22.md}
}

@article{castroFeedforwardModelFormation2014,
  title = {A Feedforward Model for the Formation of a Grid Field Where Spatial Information Is Provided Solely from Place Cells},
  author = {Castro, Lu{\'i}sa and Aguiar, Paulo},
  year = {2014},
  journal = {Biological Cybernetics},
  volume = {108},
  number = {2},
  pages = {133--143},
  doi = {10.1007/s00422-013-0581-3},
  abstract = {Grid cells (GCs) in the medial entorhinal cortex (mEC) have the property of having their firing activity spatially tuned to a regular triangular lattice. Several theoretical models for grid field formation have been proposed, but most assume that place cells (PCs) are a product of the grid cell system. There is, however, an alternative possibility that is supported by various strands of experimental data. Here we present a novel model for the emergence of gridlike firing patterns that stands on two key hypotheses: (1) spatial information in GCs is provided from PC activity and (2) grid fields result from a combined synaptic plasticity mechanism involving inhibitory and excitatory neurons mediating the connections between PCs and GCs. Depending on the spatial location, each PC can contribute with excitatory or inhibitory inputs to GC activity. The nature and magnitude of the PC input is a function of the distance to the place field center, which is inferred from rate decoding. A biologically plausible learning rule drives the evolution of the connection strengths from PCs to a GC. In this model, PCs compete for GC activation, and the plasticity rule favors efficient packing of the space representation. This leads to gridlike firing patterns. In a new environment, GCs continuously recruit new PCs to cover the entire space. The model described here makes important predictions and can represent the feedforward connections from hippocampus CA1 to deeper mEC layers.},
  keywords = {Combined plasticity rule,Competition,Efficient packing,Grid cells,Inhibitory neurons plasticity,Place cells},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6YYMNCUG/Castro, Aguiar - 2014 - A feedforward model for the formation of a grid field where spatial information is provided solely from place ce.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@castroFeedforwardModelFormation2014.md}
}

@article{cavagnaNewStatisticalTools2008,
  title = {New Statistical Tools for Analyzing the Structure of Animal Groups},
  author = {Cavagna, Andrea and Cimarelli, Alessio and Giardina, Irene and Orlandi, Alberto and Parisi, Giorgio and Procaccini, Andrea and Santagati, Raffaele and Stefanini, Fabio},
  year = {2008},
  month = jul,
  journal = {Mathematical Biosciences},
  volume = {214},
  number = {1-2},
  pages = {32--37},
  publisher = {{Elsevier}},
  doi = {10.1016/j.mbs.2008.05.006},
  abstract = {The statistical characterization of the spatial structure of large animal groups has been very limited so far, mainly due to a lack of empirical data, especially in three dimensions (3D). Here we focus on the case of large flocks of starlings (Sturnus vulgaris) in the field. We reconstruct the 3D positions of individual birds within flocks of up to few thousands of elements. In this respect our data constitute a unique set. We perform a statistical analysis of flocks' structure by using two quantities that are new to the field of collective animal behaviour, namely the conditional density and the pair correlation function. These tools were originally developed in the context of condensed matter theory. We explain what is the meaning of these two quantities, how to measure them in a reliable way, and why they are useful in assessing the density fluctuations and the statistical correlations across the group. We show that the border-to-centre density gradient displayed by starling flocks gives rise to an anomalous behaviour of the conditional density. We also find that the pair correlation function has a structure incompatible with a crystalline arrangement of birds. In fact, our results suggest that flocks are somewhat intermediate between the liquid and the gas phase of physical systems. \textcopyright{} 2008 Elsevier Inc. All rights reserved.},
  keywords = {Bird flocks,Collective behaviour,Statistical analysis},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3X5UI288/Cavagna et al. - 2008 - New statistical tools for analyzing the structure of animal groups.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@cavagnaNewStatisticalTools2008.md}
}

@article{cayleyDesiderataSuggestionsNo1878,
  title = {Desiderata and {{Suggestions}}: {{No}}. 2. {{The Theory}} of {{Groups}}: {{Graphical Representation}}},
  shorttitle = {Desiderata and {{Suggestions}}},
  author = {Cayley, Professor},
  year = {1878},
  journal = {American Journal of Mathematics},
  volume = {1},
  number = {2},
  pages = {174},
  issn = {00029327},
  doi = {10.2307/2369306},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@cayleyDesiderataSuggestionsNo1878.md}
}

@misc{centerforhistoryandnewmediaZoteroQuickStart,
  title = {Zotero {{Quick Start Guide}}},
  author = {{Center for History and New Media}},
  howpublished = {http://zotero.org/support/quick\_start\_guide},
  note = {\textbf{Welcome to Zotero!}
\par
View the Quick Start Guide to learn how to begin collecting, managing, citing, and sharing your research sources.
\par
Thanks for installing Zotero.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@centerforhistoryandnewmediaZoteroQuickStart.md}
}

@article{chandler50YearsAgo2017,
  title = {From 50 {{Years Ago}}, the {{Birth}} of {{Modern Liquid-State Science}}},
  author = {Chandler, David},
  year = {2017},
  journal = {Annual Review of Physical Chemistry},
  volume = {68},
  number = {1},
  pages = {19--38},
  doi = {10.1146/annurev-physchem-052516-044941},
  abstract = {The story told in this autobiographical perspective begins 50 years ago, at the 1967 Gordon Research Conference on the Physics and Chemistry of Liquids. It traces developments in liquid-state science from that time, including contributions from the author, and especially in the study of liquid water. It emphasizes the importance of fluctuations and the challenges of far-from-equilibrium phenomena.},
  pmid = {28375691},
  keywords = {correlation functions,crystals,equilibrium,fluctuations,glasses,liquids,nonequilibrium,transitions},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-physchem-052516-044941},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HCBERCEI/Chandler_2017_From 50 Years Ago, the Birth of Modern Liquid-State Science.pdf}
}

@inproceedings{chandrasekaranLatentVariableGraphical2010,
  title = {Latent Variable Graphical Model Selection via Convex Optimization},
  booktitle = {2010 48th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}}, {{Allerton}} 2010},
  author = {Chandrasekaran, Venkat and Parrilo, Pablo A. and Willsky, Alan S.},
  year = {2010},
  pages = {1610--1613},
  doi = {10.1109/ALLERTON.2010.5707106},
  abstract = {Suppose we have samples of a subset of a collection of random variables. No additional information is provided about the number of latent variables, nor of the relationship between the latent and observed variables. Is it possible to discover the number of hidden components, and to learn a statistical model over the entire collection of variables? We address this question in the setting in which the latent and observed variables are jointly Gaussian, with the conditional statistics of the observed variables conditioned on the latent variables being specified by a graphical model. As a first step we give natural conditions under which such latent-variable Gaussian graphical models are identifiable given marginal statistics of only the observed variables. Essentially these conditions require that the conditional graphical model among the observed variables is sparse, while the effect of the latent variables is "spread out" over most of the observed variables. Next we propose a tractable convex program based on regularized maximum-likelihood for model selection in this latent-variable setting; the regularizer uses both the {$\mathscr{l}$}1 norm and the nuclear norm. Our modeling framework can be viewed as a combination of dimensionality reduction (to identify latent variables) and graphical modeling (to capture remaining statistical structure not attributable to the latent variables), and it consistently estimates both the number of hidden components and the conditional graphical model structure among the observed variables. These results are applicable in the high-dimensional setting in which the number of latent/observed variables grows with the number of samples of the observed variables. The geometric properties of the algebraic varieties of sparse matrices and of low-rank matrices play an important role in our analysis. \textcopyright 2010 IEEE.},
  isbn = {978-1-4244-8214-6},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YQLKIW25/Chandrasekaran, Parrilo, Willsky - 2010 - Latent variable graphical model selection via convex optimization.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@chandrasekaranLatentVariableGraphical2010.md}
}

@article{chauhanSaccadeVelocityDriven2019,
  title = {Saccade {{Velocity Driven Oscillatory Network Model}} of {{Grid Cells}}},
  author = {Chauhan, Ankur and Soman, Karthik and Chakravarthy, V. Srinivasa},
  year = {2019},
  journal = {Frontiers in Computational Neuroscience},
  volume = {12},
  number = {January},
  pages = {1--12},
  doi = {10.3389/fncom.2018.00107},
  keywords = {grid cells,hippocampus,oscillator,p,principal component analysis-pca,principal component analysis-PCA,saccades,salience map},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X5YZWABE/Chauhan, Soman, Chakravarthy - 2019 - Saccade Velocity Driven Oscillatory Network Model of Grid Cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@chauhanSaccadeVelocityDriven2019.md}
}

@article{chenArtificialNeuralNetworksBased2019,
  title = {Artificial {{Neural Networks-Based Machine Learning}} for {{Wireless Networks}}: {{A Tutorial}}},
  shorttitle = {Artificial {{Neural Networks-Based Machine Learning}} for {{Wireless Networks}}},
  author = {Chen, Mingzhe and Challita, Ursula and Saad, Walid and Yin, Changchuan and Debbah, M{\'e}rouane},
  year = {2019},
  journal = {IEEE Communications Surveys Tutorials},
  volume = {21},
  number = {4},
  pages = {3039--3071},
  issn = {1553-877X},
  doi = {10.1109/COMST.2019.2926625},
  abstract = {In order to effectively provide ultra reliable low latency communications and pervasive connectivity for Internet of Things (IoT) devices, next-generation wireless networks can leverage intelligent, data-driven functions enabled by the integration of machine learning (ML) notions across the wireless core and edge infrastructure. In this context, this paper provides a comprehensive tutorial that overviews how artificial neural networks (ANNs)-based ML algorithms can be employed for solving various wireless networking problems. For this purpose, we first present a detailed overview of a number of key types of ANNs that include recurrent, spiking, and deep neural networks, that are pertinent to wireless networking applications. For each type of ANN, we present the basic architecture as well as specific examples that are particularly important and relevant wireless network design. Such ANN examples include echo state networks, liquid state machine, and long short term memory. Then, we provide an in-depth overview on the variety of wireless communication problems that can be addressed using ANNs, ranging from communication using unmanned aerial vehicles to virtual reality applications over wireless networks as well as edge computing and caching. For each individual application, we present the main motivation for using ANNs along with the associated challenges while we also provide a detailed example for a use case scenario and outline future works that can be addressed using ANNs. In a nutshell, this paper constitutes the first holistic tutorial on the development of ANN-based ML techniques tailored to the needs of future wireless networks.},
  keywords = {artificial intelligence,Artificial intelligence,Artificial neural networks,communications,Machine learning,neural networks,reinforcement learning,Reinforcement learning,Tutorials,virtual reality,Virtual reality,wireless networks,Wireless networks},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SMBCTXWW/Chen et al_2019_Artificial Neural Networks-Based Machine Learning for Wireless Networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XPMMDBX5/8755300.html}
}

@article{chenDeepReinforcementLearning2022,
  title = {Deep {{Reinforcement Learning}} with {{Spiking Q-learning}}},
  author = {Chen, Ding and Peng, Peixi and Huang, Tiejun and Tian, Yonghong},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.09754 [cs]},
  eprint = {2201.09754},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combing SNNs and deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of non-spiking neurons as the representation of Q-value, which can directly learn robust policies from high-dimensional sensory inputs using end-to-end RL. Experiments conducted on 17 Atari games demonstrate the effectiveness of DSQN by outperforming the ANN-based deep Q-network (DQN) in most games. Moreover, the experimental results show superior learning stability and robustness to adversarial attacks of DSQN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {\section{Annotations\\
(5/20/2022, 11:32:06 AM)}

\par
``There are only a few existing SNN-based RL methods at present.'' (Chen et al., 2022, p. 1)
\par
``them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training.'' (Chen et al., 2022, p. 1)
\par
``The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in trainin'' (Chen et al., 2022, p. 1)
\par
``deep spiking Q-network (DSQN)'' (Chen et al., 2022, p. 1)
\par
``ents conducted on 17 Atari games demonstrate the effectiveness of DSQN by outperforming the ANN-based deep Q-network (DQN) in most games'' (Chen et al., 2022, p. 1)
\par
``. To overcome the limitations of SNN in solving high-dimensional control problems, it would be natural to combine the energyefficiency of SNN with the optimality of deep RL'' (Chen et al., 2022, p. 1)
\par
``, these methods only apply to shallow SNNs and low-dimensional control tasks, or need to tune numerous hyper-parameters for each scenario [1]'' (Chen et al., 2022, p. 1)
\par
``several methods aim to apply the surrogate gradient method [11] to train SNN in RL'' (Chen et al., 2022, p. 1)
\par
``Since SNN usually uses the firing rate as the equivalent activation value [17] which is a discrete value between 0 and 1, it is hard to represent the value function of RL which doesn't have a certain range in training.'' (Chen et al., 2022, p. 1)
\par
``several methods [15; 19] aim to convert the trained DQN to SNN for execution. In addition, several methods based on hybrid framework [21; 22] utilize SNN to model policy function (i.e., a probability distribution), and resort ANN to estimate value function for auxiliary training'' (Chen et al., 2022, p. 1)
\par
``, these methods may lose the potential biological plausibility of SNN, and limit the application of different types of RL algorithm'' (Chen et al., 2022, p. 1)
\par
``, it is necessary to develop a spike-based RL method where only SNN is used in both training and execution'' (Chen et al., 2022, p. 1)
\par
``hod where only SNN is used in both training and execution. The key issue is to design a new neuarXiv:2201.09754v1 [cs.NE] 21 Jan 202'' (Chen et al., 2022, p. 1)
\par
``ral coding method to decode the spike-train into the results of value function, realizing end-to-end spike-based RL'' (Chen et al., 2022, p. 2) End to end?
\par
``voltage in all simulation time and make the learning stable, we argue that the statistics of membrane voltage rather than the last membrane voltage should be used to represent Q-value.'' (Chen et al., 2022, p. 2)
\par
``ly enhance spike-based RL and ensure strong robustness. The main contributions of this paper is summarized as follows: 1. A novel spike-based RL method, referred to as deep spiking Q-network (DSQN), is proposed, which could represent Q-value by the membrane voltage of nonspiking neurons. 2. The method achieves end-to-end Q-learning without any additional ANN for auxiliary training, and maintains the high biological plausibility of SNN. 3. The method is evaluated on 17 Atari games, and our results outperform ANN-based DQN on most of the games. Moreover, the experimental results show superior learning stability and robustness to adversarial attacks such as FGSM [9].'' (Chen et al., 2022, p. 2)
\par
``Reward-based Learning by Three-factor Learning Rules To bridge the gap between the time scales of behavior and neuronal action potential, modern theories of synaptic plasticity assume that the co-activation of presynaptic and postsynaptic neurons sets a flag at the synapse, called eligibility trace [18]. Only if a third factor, signaling reward, punishment, surprise or novelty, exists while the flag is set, the synaptic weight will change. Although the theoretical framework of three-factor learning rules has been developed in the past few decades, experimental evidence supporting eligibility trace has only been collected in the past few years. Through the derivation of the RL rule for continuous time, the existing approaches have been able to solve the standard control tasks [7] and robot control tasks [13]'' (Chen et al., 2022, p. 2)
\par
``and lowdimensional control tasks. To solve these problems, Bellec et al. [1] propose a learning method for recurrently connected networks of spiking neurons, which is called e-prop'' (Chen et al., 2022, p. 2)
\par
``2.3 Co-learning of Hybrid Framework Tang et al. [20] first propose the hybrid framework, composed of a spiking actor network (SAN) and a deep critic network. Through the co-learning of the two networks, these methods [21; 22] avoid the problem of value estimation using SNNs. Hence, these methods only work for actor-critic structure, and the energy consumption in the training process is much higher than the pure SNN methods [10].'' (Chen et al., 2022, p. 2)
\par
``Following the surrogate gradient method [11], the spikebased backpropagation (BP) algorithm has quickly become the mainstream solution for training multi-layer SNNs [6; 5]. As shown in several open-source frameworks of SNNs [4; 16], the membrane voltage of non-spiking neurons is feasible to represent a continuous value in a spike-based BP method. However, how to use it to effectively train SNN with Q-learning has not been systematically studied and remains unsolved, which is the goal of this paper.'' (Chen et al., 2022, p. 2)
\par
``coding method in the following sections. 3.5 Deep Spiking Q-network The typical architecture of DSQN is shown in Figure 5, which consists of synaptic layers and neuronal layers. The synaptic layers include convolutional layers and fully-connected (FC) layers, each of which is followed by a neuronal layer. Except that the LI layer is used after the last FC layer, the other synaptic layers are followed by LIF layers. In DSQN, all bias parameters are omitted and all weight parameters are shared at all simulation time-steps.'' (Chen et al., 2022, p. 4)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@chenDeepReinforcementLearning2022-MDnotes.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/chenDeepReinforcementLearning2022-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LU25QILL/Chen et al_2022_Deep Reinforcement Learning with Spiking Q-learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TKDBAXKR/2201.html}
}

@article{chenNeuralOrdinaryDifferential2019,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  journal = {arXiv:1806.07366 [cs, stat]},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Chen et al_2019_Neural Ordinary Differential Equations.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TWQAUMTM/1806.html}
}

@article{chenTVMAutomatedEndtoEnd2018,
  title = {{{TVM}}: {{An Automated End-to-End Optimizing Compiler}} for {{Deep Learning}}},
  author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  year = {2018},
  month = feb,
  abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@chenTVMAutomatedEndtoEnd2018.md}
}

@article{chiemStructureinformedFunctionalConnectivity2021,
  title = {Structure-Informed Functional Connectivity Driven by Identifiable and State-Specific Control Regions},
  author = {Chi{\^e}m, Benjamin and Crevecoeur, Fr{\'e}d{\'e}ric and Delvenne, Jean-Charles},
  year = {2021},
  journal = {Network Neuroscience (Cambridge, Mass.)},
  volume = {5},
  number = {2},
  pages = {591--613},
  issn = {2472-1751},
  doi = {10.1162/netn_a_00192},
  abstract = {Describing how the brain anatomical wiring contributes to the emergence of coordinated neural activity underlying complex behavior remains challenging. Indeed, patterns of remote coactivations that adjust with the ongoing task-demand do not systematically match direct, static anatomical links. Here, we propose that observed coactivation patterns, known as functional connectivity (FC), can be explained by a controllable linear diffusion dynamics defined on the brain architecture. Our model, termed structure-informed FC, is based on the hypothesis that different sets of brain regions controlling the information flow on the anatomical wiring produce state-specific functional patterns. We thus introduce a principled framework for the identification of potential control centers in the brain. We find that well-defined, sparse, and robust sets of control regions, partially overlapping across several tasks and resting state, produce FC patterns comparable to empirical ones. Our findings suggest that controllability is a fundamental feature allowing the brain to reach different states.},
  langid = {english},
  pmcid = {PMC8233121},
  pmid = {34189379},
  keywords = {Connectome,Control regions,Controllability,Function,Structure}
}

@article{chiemStructureinformedFunctionalConnectivity2021a,
  title = {Structure-Informed Functional Connectivity Driven by Identifiable and State-Specific Control Regions},
  author = {Chi{\^e}m, Benjamin and Crevecoeur, Fr{\'e}d{\'e}ric and Delvenne, Jean-Charles},
  year = {2021},
  month = jun,
  journal = {Network Neuroscience},
  volume = {5},
  number = {2},
  pages = {591--613},
  issn = {2472-1751},
  doi = {10.1162/netn_a_00192},
  abstract = {Describing how the brain anatomical wiring contributes to the emergence of coordinated neural activity underlying complex behavior remains challenging. Indeed, patterns of remote coactivations that adjust with the ongoing task-demand do not systematically match direct, static anatomical links. Here, we propose that observed coactivation patterns, known as functional connectivity (FC), can be explained by a controllable linear diffusion dynamics defined on the brain architecture. Our model, termed structure-informed FC, is based on the hypothesis that different sets of brain regions controlling the information flow on the anatomical wiring produce state-specific functional patterns. We thus introduce a principled framework for the identification of potential control centers in the brain. We find that well-defined, sparse, and robust sets of control regions, partially overlapping across several tasks and resting state, produce FC patterns comparable to empirical ones. Our findings suggest that controllability is a fundamental feature allowing the brain to reach different states., Understanding how brain anatomy promotes particular patterns of coactivations among neural regions is a key challenge in neuroscience. This challenge can be addressed using network science and systems theory. Here, we propose that coactivations result from the diffusion of information through the network of anatomical links connecting brain regions, with certain regions controlling the dynamics. We translate this hypothesis into a model called structure-informed functional connectivity, and we introduce a framework for identifying control regions based on empirical data. We find that our model produces coactivation patterns comparable to empirical ones, and that distinct sets of control regions are associated with different functional states. These findings suggest that controllability is an important feature allowing the brain to reach different states.},
  pmcid = {PMC8233121},
  pmid = {34189379},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/E6IW4E99/Chiêm et al_2021_Structure-informed functional connectivity driven by identifiable and.pdf}
}

@inproceedings{chilkuriParallelizingLegendreMemory2021,
  title = {Parallelizing {{Legendre Memory Unit Training}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Chilkuri, Narsimha Reddy and Eliasmith, Chris},
  year = {2021},
  month = jul,
  pages = {1898--1907},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Recently, a new recurrent neural network (RNN) named the Legendre Memory Unit (LMU) was proposed and shown to achieve state-of-the-art performance on several benchmark datasets. Here we leverage the linear time-invariant (LTI) memory component of the LMU to construct a simplified variant that can be parallelized during training (and yet executed as an RNN during inference), resulting in up to 200 times faster training. We note that our efficient parallelizing scheme is general and is applicable to any deep network whose recurrent components are linear dynamical systems. We demonstrate the improved accuracy of our new architecture compared to the original LMU and a variety of published LSTM and transformer networks across seven benchmarks. For instance, our LMU sets a new state-of-the-art result on psMNIST, and uses half the parameters while outperforming DistilBERT and LSTM models on IMDB sentiment analysis.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@chilkuriParallelizingLegendreMemory2021.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/chilkuriParallelizingLegendreMemory2021-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/I6ID9IH2/Chilkuri and Eliasmith - 2021 - Parallelizing Legendre Memory Unit Training.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/S9SRRNVJ/Chilkuri_Eliasmith_2021_Parallelizing Legendre Memory Unit Training.pdf}
}

@article{chungGliaDriveSynaptic2015,
  title = {Do Glia Drive Synaptic and Cognitive Impairment in Disease?},
  author = {Chung, Won Suk and Welsh, Christina A. and Barres, Ben A. and Stevens, Beth},
  year = {2015},
  journal = {Nature Neuroscience},
  volume = {18},
  number = {11},
  pages = {1539--1545},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nn.4142},
  abstract = {Nature Neuroscience 18, 1539 (2015). doi:10.1038/nn.4142},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FE242R9V/Chung et al. - 2015 - Do glia drive synaptic and cognitive impairment in disease.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@chungGliaDriveSynaptic2015.md}
}

@article{cisekComputerMetaphorBehaviour1999,
  title = {Beyond the {{Computer Metaphor}}: {{Behaviour}} as {{Interaction}}},
  shorttitle = {Beyond the {{Computer Metaphor}}},
  author = {Cisek, Paul},
  year = {1999},
  journal = {Journal of Consciousness Studies},
  volume = {6},
  number = {11-12},
  pages = {11--12},
  publisher = {{Imprint Academic}},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YB7ANPAX/Cisek - BEYOND THE COMPUTER METAPHOR BEHAVIOR AS INTERACT.pdf;/home/Mike/Insync/m@tarlton.info/Google Drive/Obsidian/Obsidian/Charlie Vault/Citations/@cisekComputerMetaphorBehaviour1999.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2TMXUS2A/CISBTC.html}
}

@article{coccoFunctionalNetworksInverse2017,
  title = {Functional Networks from Inverse Modeling of Neural Population Activity},
  author = {Cocco, Simona and Monasson, R{\'e}mi and Posani, Lorenzo and Tavoni, Gaia},
  year = {2017},
  month = jun,
  journal = {Current Opinion in Systems Biology},
  volume = {3},
  pages = {103--110},
  publisher = {{Elsevier Ltd}},
  doi = {10.1016/j.coisb.2017.04.017},
  abstract = {The availability of large-scale neural multi-electrode or optical recordings make now possible the modelling of the simultaneous activities of tens to thousand of neurons. One promising approach relies on the inference of detailed functional connectivity between the recorded cells, that is, of an effective coupling network reproducing the correlation structure of the spiking events. Here we report some recent applications of those approaches to retinal, hippocampal, and cortical data, illustrating in particular how functional coupling networks may be useful to decode complex brain representations, and how their changes may be tracked in behaving animals, with a possible connection to behavioral learning. Statistical, theoretical, and neurobiological issues raised by the inverse modeling of population activity are discussed.},
  keywords = {Cell assemblies,Functional connectivity,Hippocampus,Inference,Ising model,Map decoding,Memory replay,Prefrontal cortex,Retina},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/67X4FRWQ/Cocco et al. - 2017 - Functional networks from inverse modeling of neural population activity.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FP5L4DTC/Cocco et al. - 2017 - Functional networks from inverse modeling of neural population activity(3).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/L98RF5TL/Cocco et al. - 2017 - Functional networks from inverse modeling of neural population activity(2).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YFM8V2C7/Posani et al. - 2017 - Functional Networks from Inverse Modeling of Neural Population Activity Functional networks from inverse mo(2017).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@coccoFunctionalNetworksInverse2017.md}
}

@article{coccoInverseStatisticalPhysics2018,
  title = {Inverse Statistical Physics of Protein Sequences: {{A}} Key Issues Review},
  author = {Cocco, Simona and Feinauer, Christoph and Figliuzzi, Matteo and Monasson, R{\'e}mi and Weigt, Martin},
  year = {2018},
  month = jan,
  journal = {Reports on Progress in Physics},
  volume = {81},
  number = {3},
  pages = {032601--032601},
  publisher = {{Institute of Physics Publishing}},
  doi = {10.1088/1361-6633/aa9965},
  abstract = {In the course of evolution, proteins undergo important changes in their amino acid sequences, while their three-dimensional folded structure and their biological function remain remarkably conserved. Thanks to modern sequencing techniques, sequence data accumulate at unprecedented pace. This provides large sets of so-called homologous, i.e. evolutionarily related protein sequences, to which methods of inverse statistical physics can be applied. Using sequence data as the basis for the inference of Boltzmann distributions from samples of microscopic configurations or observables, it is possible to extract information about evolutionary constraints and thus protein function and structure. Here we give an overview over some biologically important questions, and how statistical-mechanics inspired modeling approaches can help to answer them. Finally, we discuss some open questions, which we expect to be addressed over the next years.},
  keywords = {coevolution,inverse Ising / Potts problem,inverse problems,protein sequence analysis,protein structure prediction,protein-protein interaction,statistical inference},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X7TANDRF/Cocco et al. - 2018 - Inverse statistical physics of protein sequences A key issues review.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@coccoInverseStatisticalPhysics2018.md}
}

@article{cofreIntroductionNonEquilibriumSteady2019,
  title = {An {{Introduction}} to the {{Non-Equilibrium Steady States}} of {{Maximum Entropy Spike Trains}}},
  author = {Cofr{\'e}, Rodrigo and Videla, Leonardo and Rosas, Fernando},
  year = {2019},
  month = sep,
  journal = {Entropy},
  volume = {21},
  number = {9},
  pages = {884--884},
  publisher = {{MDPI AG}},
  doi = {10.3390/e21090884},
  abstract = {Although most biological processes are characterized by a strong temporal asymmetry, several popular mathematical models neglect this issue. Maximum entropy methods provide a principled way of addressing time irreversibility, which leverages powerful results and ideas from the literature of non-equilibrium statistical mechanics. This tutorial provides a comprehensive overview of these issues, with a focus in the case of spike train statistics. We provide a detailed account of the mathematical foundations and work out examples to illustrate the key concepts and results from non-equilibrium statistical mechanics.},
  keywords = {entropy production,equilibrium steady states,maximum entropy principle,non,spike train statistics},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IZAYU59Z/Cofré, Videla, Rosas - 2019 - An Introduction to the Non-Equilibrium Steady States of Maximum Entropy Spike Trains.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@cofreIntroductionNonEquilibriumSteady2019.md}
}

@article{colginGammaOscillationsHippocampus,
  title = {Gamma {{Oscillations}} in the {{Hippocampus}}},
  author = {Colgin, Laura Lee and Moser, Edvard I},
  doi = {10.1152/physiol.00021.2010},
  abstract = {Gamma oscillations are thought to temporally link the activity of distributed cells. We discuss mechanisms of gamma oscillations in the hippocampus and review evidence supporting a functional role for such oscillations in several key hippocampal operations, including cell grouping, dynamic routing, and memory. We propose that memory encoding and retrieval are coordinated by different frequencies of hippocampal gamma oscillations and suggest how transitions between slow and fast gamma may occur. Many cognitive operations require dynamic coordination of activity across distributed groups of neurons. Several mechanisms exist for this purpose, but one of the best understood is synchronization of neuronal activity by oscillations. Oscillations can be readily studied using local field potential (LFP) recording techniques. LFPs are measured ex-tracellularly and, for the most part, reflect the spatial integration of voltages generated by currents flowing in and out of the dendrites of many neu-rons within an area. When postsynaptic activity across many cells is periodically synchronized, oscillations appear in LFP recordings. Widespread use of LFP recording techniques over the last several decades has provided a great deal of evidence suggesting that oscillations are not merely an epi-phenomenon but are instead important for temporal coordination of neural activity on a relatively fast time scale. In the hippocampus, a brain region critically involved in encoding, storage, and retrieval of memory (75), two main types of oscillations synchronize neuronal activity during active waking behaviors. These two types of oscillations are termed theta and gamma rhythms, and their distinctive temporal properties fit well with different coordination functions. The theta rhythm is a large amplitude, relatively slow (4-12 Hz), and highly regular rhythm that plays an important role in spatial and episodic memory processing (see Ref.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9LTCQF9Z/Colgin, Moser - Unknown - Gamma Oscillations in the Hippocampus.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@colginGammaOscillationsHippocampus.md}
}

@article{colmanMechanismsOxidativeDecarboxylation1975,
  title = {Mechanisms for the Oxidative Decarboxylation of Isocitrate: Implications for Control},
  shorttitle = {Mechanisms for the Oxidative Decarboxylation of Isocitrate},
  author = {Colman, R. F.},
  year = {1975},
  journal = {Advances in Enzyme Regulation},
  volume = {13},
  pages = {413--433},
  issn = {0065-2571},
  doi = {10.1016/0065-2571(75)90028-x},
  langid = {english},
  pmid = {1977},
  keywords = {Carboxylic Acids,Decarboxylation,Hydrogen-Ion Concentration,Isocitrate Dehydrogenase,Isocitrates,Kinetics,Lysine,Methionine,Models; Chemical,NAD,NADP,Sulfhydryl Compounds},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@colmanMechanismsOxidativeDecarboxylation1975.md}
}

@book{ComputationalNetworkScience2015,
  title = {Computational {{Network Science}}},
  year = {2015},
  publisher = {{Elsevier}},
  doi = {10.1016/C2013-0-19272-0},
  isbn = {978-0-12-800891-1},
  langid = {english},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ComputationalNetworkScience2015.md}
}

@book{ComputationalPsychiatry2016,
  title = {Computational {{Psychiatry}}},
  year = {2016},
  month = nov,
  publisher = {{The MIT Press}},
  abstract = {Modern psychiatry seeks to treat disorders of the brain, the most complex and least understood organ in the human body. This complexity poses a set of challenges that make progress in psychiatric research particularly difficult, despite the development of several promising novel avenues of research. New tools that explore the neural basis of behavior have accelerated the discovery in neuroscience, yet discovery into better psychiatric treatments has not kept pace. This chapter focuses on this disconnect between the challenges and promises of psychiatric neuroscience. It highlights the need for diagnostic nosology, biomarkers, and better treatments in psychiatry, and discusses three promising conceptual advances in psychiatric neuroscience. It holds that rigorous theory is needed to address the challenges faced by psychiatrists.},
  isbn = {978-0-262-03542-2 978-0-262-33785-4},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NLKALXPL/2016 - Computational Psychiatry.pdf}
}

@article{coneLearningPreciseSpatiotemporal2021,
  title = {Learning Precise Spatiotemporal Sequences via Biophysically Realistic Learning Rules in a Modular, Spiking Network},
  author = {Cone, I. and Shouval, H.},
  year = {2021},
  journal = {eLife},
  doi = {10.7554/eLife.63751},
  abstract = {A network model that learns and recalls discrete sequences of variable order and duration is proposed, which provides a possible framework for biologically plausible sequence learning and memory. Multiple brain regions are able to learn and express temporal sequences, and this functionality is an essential component of learning and memory. We propose a substrate for such representations via a network model that learns and recalls discrete sequences of variable order and duration. The model consists of a network of spiking neurons placed in a modular microcolumn based architecture. Learning is performed via a biophysically realistic learning rule that depends on synaptic `eligibility traces'. Before training, the network contains no memory of any particular sequence. After training, presentation of only the first element in that sequence is sufficient for the network to recall an entire learned representation of the sequence. An extended version of the model also demonstrates the ability to successfully learn and recall non-Markovian sequences. This model provides a possible framework for biologically plausible sequence learning and memory, in agreement with recent experimental results.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7CSNM78B/Cone_Shouval_2021_Learning precise spatiotemporal sequences via biophysically realistic learning.pdf}
}

@article{constantinouBurstingNeuronsHippocampal2016,
  title = {Bursting {{Neurons}} in the {{Hippocampal Formation Encode Features}} of {{LFP Rhythms}}},
  author = {Constantinou, Maria and Gonzalo Cogno, Soledad and Elijah, Daniel H. and Kropff, Emilio and Gigg, John and Samengo, In{\'e}s and Montemurro, Marcelo A.},
  year = {2016},
  month = dec,
  journal = {Frontiers in Computational Neuroscience},
  volume = {10},
  pages = {133--133},
  publisher = {{Frontiers}},
  doi = {10.3389/fncom.2016.00133},
  abstract = {Burst spike patterns are common in regions of the hippocampal formation such as the subiculum and medial entorhinal cortex (MEC). Neurons in these areas are immersed in extracellular electrical potential fluctuations often recorded as the local field potential (LFP). LFP rhythms within different frequency bands are linked to different behavioral states. For example, delta rhythms are often associated with slow-wave sleep, inactivity and anesthesia; whereas theta rhythms are prominent during awake exploratory behavior and REM sleep. Recent evidence suggests that bursting neurons in the hippocampal formation can encode LFP features. We explored this hypothesis using a two-compartment model of a bursting pyramidal neuron driven by time-varying input signals containing spectral peaks at either delta or theta rhythms. The model predicted a neural code in which bursts represented the instantaneous value, phase, slope and amplitude of the driving signal both in their timing and size (spike number). To verify whether this code is employed in vivo, we examined electrophysiological recordings from the subiculum of anesthetized rats and the MEC of a behaving rat containing prevalent delta or theta rhythms, respectively. In both areas, we found bursting cells that encoded information about the instantaneous voltage, phase, slope and/or amplitude of the dominant LFP rhythm with essentially the same neural code as the simulated neurons. A fraction of the cells encoded part of the information in burst size, in agreement with model predictions. These results provide in-vivo evidence that the output of bursting neurons in the mammalian brain is tuned to features of the LFP.},
  keywords = {bursting,Entorhinal Cortex,Information Theory,local field potential,Neural coding,subiculum},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RZK5DW4J/Constantinou et al. - 2016 - Bursting Neurons in the Hippocampal Formation Encode Features of LFP Rhythms.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@constantinouBurstingNeuronsHippocampal2016.md}
}

@techreport{ConvergenceConditionTAP,
  title = {Convergence Condition of the {{TAP}} Equation for the Infinite-Ranged {{Ising}} Spin Glass Model {{Related}} Content {{Convergence}} Condition of the {{TAP}} Equation for the Infinite-Ranged {{Ising}} Spin Glass Model?},
  abstract = {A lower bound for the spin glass order parameter of the infinite-ranged Ising spin glass model T Plefka-Metastable states in spin glasses A J Bray and M A Moore-Stability conditions for the infinite-range vector spin glass V Z Vulovic-Recent citations Statistical physics of interacting proteins: Impact of dataset size and quality assessed in synthetic sequences Carlos A. Gandarilla-P\'erez et al-Giorgio Parisi et al-Empirical Bayes method for Boltzmann machines Muneki Yasuda and Tomoyuki Obuchi-This content was downloaded from IP address 129.241.229.223 on 26/03/2020 at 11:23 Abstract. It is shown that the power expansion of the Gibbs potential of the SK model up to second order in the exchange couplings leads to the TAP equation. This result remains valid for the general (including a ferromagnetic exchange) SK model. Theorems of power expansions and resolvent techniques are employed to solve the convergence problem. The convergence condition is presented for the whole temperature range and for general distributions of the local magnetisations.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/G83Q4KGS/Unknown - Unknown - Convergence condition of the TAP equation for the infinite-ranged Ising spin glass model Related content Convergence.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ConvergenceConditionTAP.md}
}

@article{cordesSequenceSpaceFolding1996,
  title = {Sequence Space, Folding and Protein Design},
  author = {Cordes, Matthew H.J. and Davidson, Alan R. and Sauer, Robert T.},
  year = {1996},
  month = feb,
  journal = {Current Opinion in Structural Biology},
  volume = {6},
  number = {1},
  pages = {3--10},
  publisher = {{Elsevier Ltd}},
  doi = {10.1016/S0959-440X(96)80088-1},
  abstract = {Protein design efforts are beginning to yield molecules with many of the properties of natural proteins. Such experiments are informed by and contribute to our understanding of the sequence determinants of protein folding and stability. The most important design elements seem to be the proper placement of hydrophobic residues along the polypeptide chain and the ability of these residues to form a well packed core. Buried polar interactions, turn and capping motifs and secondary structural propensities also contribute, although probably to a lesser extent.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/55LYVNTX/Cordes, Davidson, Sauer - 1996 - Sequence space, folding and protein design.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@cordesSequenceSpaceFolding1996.md}
}

@article{cossartStepStepCells2022,
  title = {Step by Step: Cells with Multiple Functions in Cortical Circuit Assembly},
  shorttitle = {Step by Step},
  author = {Cossart, Rosa and Garel, Sonia},
  year = {2022},
  month = apr,
  journal = {Nature Reviews Neuroscience},
  pages = {1--16},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-022-00585-6},
  abstract = {It is often thought that the construction of cortical circuits occurs as the result of an elegantly designed process that unfolds sequentially as an animal develops until adult functional networks emerge. In reality, cortical circuits are shaped by evolutionary mechanisms, changes in developmental programmes driven by neuronal activity or epigenetic mechanisms and the need to adapt to the external world, and must pass through several important phases and timely checkpoints as they form. Some cortical cell types serve multiple functions during this developmental journey and are then reused (or `recycled') to perform different functions in the adult cortex. Understanding the different stages of the cortical construction process and taking into account the ways in which cellular functions change across time and space is therefore essential if we are to build a comprehensive framework of cortical wiring in both health and disease.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Development of the nervous system,Neuroscience},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NPBB3CP7/s41583-022-00585-6.html}
}

@article{costaCoincidenceComplexNetworks2022,
  title = {Coincidence Complex Networks},
  author = {Costa, Luciano da Fontoura},
  year = {2022},
  month = mar,
  journal = {Journal of Physics: Complexity},
  volume = {3},
  number = {1},
  pages = {015012},
  publisher = {{IOP Publishing}},
  issn = {2632-072X},
  doi = {10.1088/2632-072X/ac54c3},
  abstract = {Complex networks, which constitute the main subject of network science, have been wide and extensively adopted for representing, characterizing, and modeling an ample range of structures and phenomena from both theoretical and applied perspectives. The present work describes the application of the real-valued Jaccard and real-valued coincidence similarity indices for translating generic datasets into networks. More specifically, two data elements are linked whenever the similarity between their respective features, gauged by some similarity index, is greater than a given threshold. Weighted networks can also be obtained by taking these indices as weights. It is shown that the two proposed real-valued approaches can lead to enhanced performance when compared to cosine and Pearson correlation approaches, yielding a detailed description of the specific patterns of connectivity between the nodes, with enhanced modularity. In addition, a parameter {$\alpha$} is introduced that can be used to control the contribution of positive and negative joint variations between the considered features, catering for enhanced flexibility while obtaining networks. The ability of the proposed methodology to capture detailed interconnections and emphasize the modular structure of networks is illustrated and quantified respectively to real-world networks, including handwritten letters and raisin datasets, as well as the Caenorhabditis elegans neuronal network. The reported methodology and results pave the way to a significant number of theoretical and applied developments.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Costa_2022_Coincidence complex networks.pdf}
}

@inproceedings{costaCorticalMicrocircuitsGatedrecurrent2017,
  title = {Cortical Microcircuits as Gated-Recurrent Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Costa, Rui and Assael, Ioannis Alexandros and Shillingford, Brendan and {de Freitas}, Nando and Vogels, TIm},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Cortical circuits exhibit intricate recurrent architectures that are remarkably similar across different brain areas. Such stereotyped structure suggests the existence of common computational principles. However, such principles have remained largely elusive. Inspired by gated-memory networks, namely long short-term memory networks (LSTMs), we introduce a recurrent neural network in which information is gated through inhibitory cells that are subtractive (subLSTM).  We propose a natural mapping of subLSTMs onto known canonical excitatory-inhibitory cortical microcircuits.   Our empirical evaluation across sequential image classification and language modelling tasks shows that subLSTM units can achieve similar performance to LSTM units. These results suggest that cortical circuits can be optimised to solve complex contextual problems and proposes a novel view on their computational function. Overall our work provides a step towards unifying recurrent networks as used in machine learning with their biological counterparts.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/G6W9C6JJ/Costa et al_2017_Cortical microcircuits as gated-recurrent neural networks.pdf}
}

@article{costaUnifiedPrePostsynaptic2015,
  title = {Unified Pre- and Postsynaptic Long-Term Plasticity Enables Reliable and Flexible Learning},
  author = {Costa, Rui Ponte and Froemke, Robert C and Sj{\"o}str{\"o}m, P Jesper and {van Rossum}, Mark CW},
  editor = {Nelson, Sacha B},
  year = {2015},
  month = aug,
  journal = {eLife},
  volume = {4},
  pages = {e09457},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.09457},
  abstract = {Although it is well known that long-term synaptic plasticity can be expressed both pre- and postsynaptically, the functional consequences of this arrangement have remained elusive. We show that spike-timing-dependent plasticity with both pre- and postsynaptic expression develops receptive fields with reduced variability and improved discriminability compared to postsynaptic plasticity alone. These long-term modifications in receptive field statistics match recent sensory perception experiments. Moreover, learning with this form of plasticity leaves a hidden postsynaptic memory trace that enables fast relearning of previously stored information, providing a cellular substrate for memory savings. Our results reveal essential roles for presynaptic plasticity that are missed when only postsynaptic expression of long-term plasticity is considered, and suggest an experience-dependent distribution of pre- and postsynaptic strength changes.},
  keywords = {learning rules,memory savings,pre- and postsynaptic long-term plasticity,receptive fields,spike-timing-dependent plasticity,synaptic transmission},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2VXFSMD2/Costa et al_2015_Unified pre- and postsynaptic long-term plasticity enables reliable and.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VEFBTUSI/Unified pre- and postsynaptic long-term plasticity enables reliable and flexible learning - 11.05.22.md}
}

@article{cotterillCharacterizationEarlyCortical2015,
  title = {Characterization of Early Cortical Neural Network Development in Multiwell Microelectrode Array Plates},
  author = {Cotterill, Ellese and Hall, Diana and Wallace, Kathleen and Mundy, William R. and Eglen, Stephen J. and Shafer, Timothy J.},
  year = {2015},
  journal = {Journal of Biomolecular Screening},
  volume = {21},
  number = {5},
  pages = {510--519},
  issn = {1087057116},
  doi = {10.1177/1087057116640520},
  abstract = {We examined neural network ontogeny using microelectrode array (MEA) recordings made in multiwell MEA (mwMEA) plates over the first 12 days in vitro (DIV). In primary cortical cultures, action potential spiking activity developed rapidly between DIV 5 and 12. Spiking was sporadic and unorganized at early DIV, and became progressively more organized with time, with bursting parameters, synchrony, and network bursting increasing between DIV 5 and 12. We selected 12 features to describe network activity; principal components analysis using these features demonstrated segregation of data by age at both the well and plate levels. Using random forest classifiers and support vector machines, we demonstrated that four features (coefficient of variation [CV] of within-burst interspike interval, CV of interburst interval, network spike rate, and burst rate) could predict the age of each well recording with {$>$}65\% accuracy. When restricting the classification to a binary decision, accuracy improved to as high as 95\%. Further, we present a novel resampling approach to determine the number of wells needed for comparing different treatments. Overall, these results demonstrate that network development on mwMEA plates is similar to development in single-well MEAs. The increased throughput of mwMEAs will facilitate screening drugs, chemicals, or disease states for effects on neurodevelopment.},
  keywords = {cell-based assays,membrane potential,neurological diseases,toxicology},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IB4NL922/Cotterill et al. - 2015 - Characterization of early cortical neural network development in multiwell microelectrode array plates.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@cotterillCharacterizationEarlyCortical2015.md}
}

@article{crickFrameworkConsciousness2003,
  title = {A Framework for Consciousness},
  author = {Crick, Francis and Koch, Christof},
  year = {2003},
  month = feb,
  journal = {Nature Neuroscience},
  volume = {6},
  number = {2},
  pages = {119--126},
  doi = {10.1038/nn0203-119},
  abstract = {Here we summarize our present approach to the problem of consciousness. After an introduction outlining our general strategy, we describe what is meant by the term 'framework' and set it out under ten headings. This framework offers a coherent scheme for explaining the neural correlates of (visual) consciousness in terms of competing cellular assemblies. Most of the ideas we favor have been suggested before, but their combination is original. We also outline some general experimental approaches to the problem and, finally, acknowledge some relevant aspects of the brain that have been left out of the proposed framework.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8LFSZSSY/Crick, Koch - 2003 - A framework for consciousness.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@crickFrameworkConsciousness2003.md}
}

@article{crickNeurobiologicalTheoryConsciousness1990,
  title = {Toward a Neurobiological Theory of Consciousness},
  author = {Crick, Francis and Koch, Christof},
  year = {1990},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@crickNeurobiologicalTheoryConsciousness1990.md}
}

@techreport{crispinoInformativeExtendedMallows2019,
  title = {Informative Extended {{Mallows}} Priors in the {{Bayesian Mallows}} Model},
  author = {Crispino, Marta and {Antoniano-Villalobos}, Isadora},
  year = {2019},
  abstract = {The aim of this work is to study the problem of prior elicitation for the Mallows model with Spearman's distance, a popular distance-based model for rankings or permutation data. Previous Bayesian inference for such model has been limited to the use of the uniform prior over the space of permutations. We present a novel strategy to elicit subjective prior beliefs on the location parameter of the model, discussing the interpretation of hyper-parameters and the implication of prior choices for the posterior analysis.},
  keywords = {Bayesian subjective inference,conjugate priors,Mallows model for rankings,permutations,permutohedron,ranking data},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EFI9EJIQ/Crispino, Antoniano-Villalobos - 2019 - Informative extended Mallows priors in the Bayesian Mallows model.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@crispinoInformativeExtendedMallows2019.md}
}

@article{cruzActionSuppressionReveals2022,
  title = {Action Suppression Reveals Opponent Parallel Control via Striatal Circuits},
  author = {Cruz, Bruno F. and Guiomar, Gon{\c c}alo and Soares, Sofia and Motiwala, Asma and Machens, Christian K. and Paton, Joseph J.},
  year = {2022},
  month = jul,
  journal = {Nature},
  pages = {1--6},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-022-04894-9},
  abstract = {The direct and indirect pathways of the basal ganglia are classically thought to promote and suppress action, respectively1. However, the observed co-activation of striatal direct and indirect medium spiny neurons2 (dMSNs and iMSNs, respectively) has challenged this view. Here we study these circuits in mice performing an interval categorization task that requires a series of self-initiated and cued actions and, critically, a sustained period of dynamic action suppression. Although movement produced the co-activation of iMSNs and dMSNs in the sensorimotor, dorsolateral striatum (DLS), fibre photometry and photo-identified electrophysiological recordings revealed signatures of functional opponency between the two pathways during action suppression. Notably, optogenetic inhibition showed that DLS circuits were largely engaged to suppress\textemdash and not promote\textemdash action. Specifically, iMSNs on a given hemisphere were dynamically engaged to suppress tempting contralateral action. To understand how such regionally specific circuit function arose, we constructed a computational reinforcement learning model that reproduced key features of behaviour, neural activity and optogenetic inhibition. The model predicted that parallel striatal circuits outside the DLS learned the action-promoting functions, generating the temptation to act. Consistent with this, optogenetic inhibition experiments revealed that dMSNs in the associative, dorsomedial striatum, in contrast to those in the DLS, promote contralateral actions. These data highlight how opponent interactions between multiple circuit- and region-specific basal ganglia processes can lead to behavioural control, and establish a critical role for the sensorimotor indirect pathway in the proactive suppression of tempting actions.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Basal ganglia,Learning algorithms},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4QHN75FK/Action suppression reveals opponent parallel control via striatal circuits - 12.07.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6EGD62RE/Cruz et al_2022_Action suppression reveals opponent parallel control via striatal circuits.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BS449J6Q/Scientists discover how the brain keeps the urge to act in check  Champalimaud Foundation - 12.07.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GPYVCDEV/s41586-022-04894-9.html}
}

@article{cuberoMinimumDescriptionLength2018,
  title = {Minimum {{Description Length Codes Are Critical}}},
  author = {Cubero, Ryan and Marsili, Matteo and Roudi, Yasser},
  year = {2018},
  month = oct,
  journal = {Entropy},
  volume = {20},
  number = {10},
  pages = {755--755},
  publisher = {{MDPI AG}},
  doi = {10.3390/e20100755},
  abstract = {In the Minimum Description Length (MDL) principle, learning from the data is equivalent to an optimal coding problem. We show that the codes that achieve optimal compression in MDL are critical in a very precise sense. First, when they are taken as generative models of samples, they generate samples with broad empirical distributions and with a high value of the relevance, defined as the entropy of the empirical frequencies. These results are derived for different statistical models (Dirichlet model, independent and pairwise dependent spin models, and restricted Boltzmann machines). Second, MDL codes sit precisely at a second order phase transition point where the symmetry between the sampled outcomes is spontaneously broken. The order parameter controlling the phase transition is the coding cost of the samples. The phase transition is a manifestation of the optimality of MDL codes, and it arises because codes that achieve a higher compression do not exist. These results suggest a clear interpretation of the widespread occurrence of statistical criticality as a characterization of samples which are maximally informative on the underlying generative process.},
  keywords = {Bose-Einstein condensation,Non-Additive Entropies,Superstatistics},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/36AFY4VJ/entropy-20-00755-v2.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@cuberoMinimumDescriptionLength2018.md}
}

@article{cuberoMultiscaleRelevanceInformative,
  title = {Multiscale Relevance and Informative Encoding in Neuronal Spike Trains},
  author = {Cubero, Ryan John and Marsili, Matteo and Roudi, Yasser},
  doi = {10.1007/s10827-020-00740-x},
  abstract = {Neuronal responses to complex stimuli and tasks can encompass a wide range of time scales. Understanding these responses requires measures that characterize how the information on these response patterns are represented across multiple temporal resolutions. In this paper we propose a metric-which we call multiscale relevance (MSR)-to capture the dynamical variability of the activity of single neurons across different time scales. The MSR is a non-parametric, fully featureless indicator in that it uses only the time stamps of the firing activity without resorting to any a priori covariate or invoking any specific structure in the tuning curve for neural activity. When applied to neural data from the mEC and from the ADn and PoS regions of freely-behaving rodents, we found that neurons having low MSR tend to have low mutual information and low firing sparsity across the correlates that are believed to be encoded by the region of the brain where the recordings were made. In addition, neurons with high MSR contain significant information on spatial navigation and allow to decode spatial position or head direction as efficiently as those neurons whose firing activity has high mutual information with the covariate to be decoded and significantly better than the set of neurons with high local variations in their interspike intervals. Given these results, we propose that the MSR can be used as a measure to rank and select neurons for their information content without the need to appeal to any a priori covariate.},
  keywords = {Bayesian decoding,Information theory,Multiple time scale analysis,Spike train data,Time series analysis},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UC7PHWMG/Cubero, Marsili, Roudi - Unknown - Multiscale relevance and informative encoding in neuronal spike trains.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@cuberoMultiscaleRelevanceInformative.md}
}

@techreport{cuberoMultiscaleRelevanceInformative2019,
  title = {Multiscale Relevance and Informative Encoding in Neuronal Spike Trains},
  author = {Cubero, Ryan John and Marsili, Matteo and Roudi, Yasser},
  year = {2019},
  abstract = {Neuronal responses to complex stimuli and tasks can encompass a wide range of time scales. Understanding these responses requires measures that characterize how the information on these response patterns are represented across multiple temporal resolutions. In this paper we propose a metric-which we call multiscale relevance (MSR)-to capture the dynamical variability of the activity of single neurons across different time scales. The MSR is a non-parametric, fully featureless indicator in that it uses only the time stamps of the firing activity without resorting to any a priori covariate or invoking any specific structure in the tuning curve for neural activity. When applied to neural data from the mEC and from the ADn and PoS regions of freely-behaving rodents, we found that neurons having low MSR tend to have low mutual information and low firing sparsity across the correlates that are believed to be encoded by the region of the brain where the recordings were made. In addition, neurons with high MSR contain significant information on spatial navigation and allow to decode spatial position or head direction as efficiently as those neurons whose firing activity has high mutual information with the covariate to be decoded and significantly better than the set of neurons with high local variations in their interspike intervals. Given these results, we propose that the MSR can be used as a measure to rank and select neurons for their information content without the need to appeal to any a priori covariate.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/E52E5ZEJ/Cubero, Marsili, Roudi - 2019 - Multiscale relevance and informative encoding in neuronal spike trains.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@cuberoMultiscaleRelevanceInformative2019.md}
}

@article{cuberoStatisticalCriticalityArises2019,
  title = {Statistical Criticality Arises in Most Informative Representations {{Recent}} Citations {{Multiscale}} Relevance and Informative Encoding in Neuronal Spike Trains},
  author = {Cubero, Ryan John},
  year = {2019},
  doi = {10.1088/1742-5468/ab16c8},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Y8IEZG4B/Cubero - 2019 - Statistical criticality arises in most informative representations Recent citations Multiscale relevance and informative.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@cuberoStatisticalCriticalityArises2019.md}
}

@article{cunninghamDimensionalityReductionLargescale2014,
  title = {Dimensionality Reduction for Large-Scale Neural Recordings},
  author = {Cunningham, John P and Yu, Byron M},
  year = {2014},
  volume = {17},
  number = {11},
  doi = {10.1038/nn.3776},
  abstract = {A central tenet of neuroscience is that the remarkable computational abilities of our brains arise as a result of populations of interconnected neurons. Indeed, we find ourselves at an exciting moment in the history of neuroscience, as the field is experiencing rapid growth in the quantity and complexity of the recorded neural activity. Many groups have begun to adopt multi-electrode 1 and optical 2 recording technologies that can monitor the activity of many neurons simultaneously in cortex and, in some cases, in deeper structures. Ongoing development of recording technologies promises to increase the number of simultaneously recorded neurons by orders of magnitude 3. At the same time, massive increases in computational power and algo-rithmic development have enabled advanced multivariate analyses of neural population activity, where the neurons may be recorded either sequentially or simultaneously. These technological advances have enabled researchers to reconsider the types of scientific questions that are being posed and how neural activity is analyzed, even with classical behavioral tasks and in brain areas that have been studied for decades. Indeed, many studies of neural systems are undergoing a paradigm shift from single-neuron to population-level hypotheses and analyses. We begin this review by discussing three scientific motivations for considering a neural population jointly, rather than on a single-neuron basis: single-trial hypotheses requiring statistical power, hypotheses of population response structure and exploratory analyses of large data sets. Critically, we show that there are settings in which data fundamentally cannot be understood on a single-neuron basis, whether as a result of neural spiking variability or a hypothesis about neural mechanism that depends on how the responses of multiple neurons covary. The object of this review is to focus on one class of statistical methods, dimensionality reduction, which is well-suited for analyzing neu-ral population activity. Dimensionality reduction methods produce low-dimensional representations of high-dimensional data, where the representation is chosen to preserve or highlight some feature of interest in the data. These methods have begun to reveal tantalizing evidence of the neural mechanisms underlying various phenomena, including the selection and integration of sensory input during decision making in prefrontal cortex 4 , the ability of premotor cortex to prepare movements without executing them 5 , and odor discrimination in the olfactory system 6. Dimensionality reduction has also been fruitfully applied to population recordings in other studies of decision making 7-9 , the motor system 10-12 and the olfactory system 13,14 , as well as in working memory 15,16 , visual attention 17 , the auditory system 18 , rule learning 19 , speech 20 and more. We introduce dimen-sionality reduction and bring together previous studies that have used these methods to address each of the three scientific motivations for population analyses. Because the use of dimensionality reduction is still relatively new in systems neuroscience, we then present methodological details and practical considerations. Much of this work in neuroscience has developed in the last decade: as presciently noted by Brown et al. 21 , "the future challenge is to design methods that truly allow neuroscientists to perform multivariate analyses of multiple spike train data". Dimensionality reduction is one important way in which many researchers have answered and will continue to answer this challenge. Scientific motivation of population analyses The growth in scale and resolution of recording technologies brings with it challenges for the analysis of neural activity. Consider Most sensory, cognitive and motor functions depend on the interactions of many neurons. In recent years, there has been rapid development and increasing use of technologies for recording from large numbers of neurons, either sequentially or simultaneously. A key question is what scientific insight can be gained by studying a population of recorded neurons beyond studying each neuron individually. Here, we examine three important motivations for population studies: single-trial hypotheses requiring statistical power, hypotheses of population response structure and exploratory analyses of large data sets. Many recent studies have adopted dimensionality reduction to analyze these populations and to find features that are not apparent at the level of individual neurons. We describe the dimensionality reduction methods commonly applied to population activity and offer practical advice about selecting methods and interpreting their outputs. This review is intended for experimental and computational researchers who seek to understand the role dimensionality reduction has had and can have in systems neuroscience, and who seek to apply these methods to their own data. npg},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2UVX9BES/Cunningham, Yu - 2014 - Dimensionality reduction for large-scale neural recordings.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@cunninghamDimensionalityReductionLargescale2014.md}
}

@article{dabacoGrapheneFoamBiocompatible2018,
  title = {Graphene Foam as a Biocompatible Scaffold for Culturing Human Neurons},
  author = {D'Abaco, Giovanna M. and Mattei, Cristiana and Nasr, Babak and Hudson, Emma J. and Alshawaf, Abdullah J. and Chana, Gursharan and Everall, Ian P. and Nayagam, Bryony and Dottori, Mirella and Skafidas, Efstratios},
  year = {2018},
  journal = {Royal Society Open Science},
  doi = {10.1098/rsos.171364},
  abstract = {In this study, we explore the use of electrically active graphene foam as a scaffold for the culture of human-derived neurons. Human embryonic stem cell (hESC)-derived cortical neurons fated as either glutamatergic or GABAergic neuronal phenotypes were cultured on graphene foam. We show that graphene foam is biocompatible for the culture of human neurons, capable of supporting cell viability and differentiation of hESC-derived cortical neurons. Based on the findings, we propose that graphene foam represents a suitable scaffold for engineering neuronal tissue and warrants further investigation as a model for understanding neuronal maturation, function and circuit formation.},
  keywords = {Biocompatibility,Graphene foam,Human cortical neurons,Human stem cells},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dabacoGrapheneFoamBiocompatible2018.md}
}

@article{dabneyDistributionalReinforcementLearning2017,
  title = {Distributional {{Reinforcement Learning}} with {{Quantile Regression}}},
  author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, R{\'e}mi},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.10044 [cs, stat]},
  eprint = {1710.10044},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@dabneyDistributionalReinforcementLearning2017.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/dabneyDistributionalReinforcementLearning2017-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z7ZKIDFF/Dabney et al_2017_Distributional Reinforcement Learning with Quantile Regression.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WUARQFNF/1710.html}
}

@article{daggettPresentViewMechanism2003,
  title = {The Present View of the Mechanism of Protein Folding},
  author = {Daggett, Valerie and Fersht, Alan},
  year = {2003},
  month = jun,
  journal = {Nature Reviews Molecular Cell Biology},
  volume = {4},
  number = {6},
  pages = {497--502},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nrm1126},
  abstract = {We can track the positions and movements of all the atoms in small proteins as they fold and unfold by combining experimental studies with atomic-resolution molecular dynamics simulations. General principles as to how such complex architectures form so rapidly are now emerging from in-depth studies of a few proteins.},
  keywords = {Biochemistry,Cancer Research,Cell Biology,Developmental Biology,general,Life Sciences,Stem Cells},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/J6KPWZKC/Daggett, Fersht - 2003 - The present view of the mechanism of protein folding.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@daggettPresentViewMechanism2003.md}
}

@techreport{daiConnectionsRobustPCA2018,
  title = {Connections with {{Robust PCA}} and the {{Role}} of {{Emergent Sparsity}} in {{Variational Autoencoder Models}}},
  author = {Dai, Bin and Wang, Yu and Aston, John and Wipf, David},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {19},
  pages = {1--42},
  abstract = {Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing sparse outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds of unknown dimension obscured by gross corruptions.},
  keywords = {Deep Generative Model,Robust PCA,Variational Autoencoder},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/964PTU8D/Dai et al. - 2018 - Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@daiConnectionsRobustPCA2018.md}
}

@book{dalbisSinglecellSpikingModel2017,
  title = {A Single-Cell Spiking Model for the Origin of Grid-Cell Patterns},
  author = {D'Albis, Tiziano and Kempter, Richard},
  year = {2017},
  journal = {PLoS Computational Biology},
  volume = {13},
  pages = {41},
  doi = {10.1371/journal.pcbi.1005782},
  abstract = {Spatial cognition in mammals is thought to rely on the activity of grid cells in the entorhinal cortex, yet the fundamental principles underlying the origin of grid-cell firing are still debated. Grid-like patterns could emerge via Hebbian learning and neuronal adaptation, but current computational models remained too abstract to allow direct confrontation with experimental data. Here, we propose a single-cell spiking model that generates grid firing fields via spike-rate adaptation and spike-timing dependent plasticity. Through rigorous mathematical analysis applicable in the linear limit, we quantitatively predict the requirements for grid-pattern formation, and we establish a direct link to classical pattern-forming systems of the Turing type. Our study lays the groundwork for biophysically-realistic models of grid-cell activity.},
  isbn = {1-111-11111-1},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PFDYFV8T/D’Albis, Kempter - 2017 - A single-cell spiking model for the origin of grid-cell patterns.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dalbisSinglecellSpikingModel2017.md}
}

@article{DanielAmitModelingBrain,
  title = {Daniel\_{{J}}.\_{{Amit-Modeling}}\_{{Brain}}\_{{Function}}\_\_{{The}}\_{{World}}\_of\_{{Attractor}}\_{{Neural}}\_{{Networks}}\_\_-{{Cambridge}}\_{{University}}\_{{Press}}\_(1989)[1]},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7G52AZBM/Unknown - Unknown - Daniel_J._Amit-Modeling_Brain_Function__The_World_of_Attractor_Neural_Networks__-Cambridge_University_Press_(1989)1.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@DanielAmitModelingBrain.md}
}

@book{daphnekollerProbabilisticGraphicalModels2009,
  title = {Probabilistic Graphical Models : Principles and Techniques},
  author = {{Daphne Koller}},
  year = {2009},
  journal = {MIT Press},
  pages = {1270},
  isbn = {978-0-262-01319-2},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7C78X74Z/(Adaptive Computation and Machine Learning series) Daphne Koller, Nir Friedman - Probabilistic Graphical Models_ Principles and Techniques-The MIT Press (2009).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@daphnekollerProbabilisticGraphicalModels2009.md}
}

@article{daviesLoihiNeuromorphicManycore2018,
  title = {Loihi: {{A Neuromorphic Manycore Processor}} with {{On-Chip Learning}}},
  shorttitle = {Loihi},
  author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
  year = {2018},
  month = jan,
  journal = {IEEE Micro},
  volume = {38},
  number = {1},
  pages = {82--99},
  issn = {1937-4143},
  doi = {10.1109/MM.2018.112130359},
  abstract = {Loihi is a 60-mm2 chip fabricated in Intels 14-nm process that advances the state-of-the-art modeling of spiking neural networks in silicon. It integrates a wide range of novel features for the field, such as hierarchical connectivity, dendritic compartments, synaptic delays, and, most importantly, programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorithm, Loihi can solve LASSO optimization problems with over three orders of magnitude superior energy-delay-product compared to conventional solvers running on a CPU iso-process/voltage/area. This provides an unambiguous example of spike-based computation, outperforming all known conventional solutions.},
  keywords = {Algorithm design and analysis,artificial intelligence,Biological neural networks,Computational modeling,Computer architecture,machine learning,neuromorphic computing,Neuromorphics,Neurons},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/445SSRQT/8259423.html}
}

@article{davila-velderrainModelingEpigeneticAttractors2015,
  title = {Modeling the Epigenetic Attractors Landscape: Toward a Post-Genomic Mechanistic Understanding of Development},
  shorttitle = {Modeling the Epigenetic Attractors Landscape},
  author = {{Davila-Velderrain}, Jose and {Martinez-Garcia}, Juan C. and {Alvarez-Buylla}, Elena R.},
  year = {2015},
  journal = {Frontiers in Genetics},
  volume = {6},
  pages = {160},
  issn = {1664-8021},
  doi = {10.3389/fgene.2015.00160},
  abstract = {Robust temporal and spatial patterns of cell types emerge in the course of normal development in multicellular organisms. The onset of degenerative diseases may result from altered cell fate decisions that give rise to pathological phenotypes. Complex networks of genetic and non-genetic components underlie such normal and altered morphogenetic patterns. Here we focus on the networks of regulatory interactions involved in cell-fate decisions. Such networks modeled as dynamical non-linear systems attain particular stable configurations on gene activity that have been interpreted as cell-fate states. The network structure also restricts the most probable transition patterns among such states. The so-called Epigenetic Landscape (EL), originally proposed by C. H. Waddington, was an early attempt to conceptually explain the emergence of developmental choices as the result of intrinsic constraints (regulatory interactions) shaped during evolution. Thanks to the wealth of molecular genetic and genomic studies, we are now able to postulate gene regulatory networks (GRN) grounded on experimental data, and to derive EL models for specific cases. This, in turn, has motivated several mathematical and computational modeling approaches inspired by the EL concept, that may be useful tools to understand and predict cell-fate decisions and emerging patterns. In order to distinguish between the classical metaphorical EL proposal of Waddington, we refer to the Epigenetic Attractors Landscape (EAL), a proposal that is formally framed in the context of GRNs and dynamical systems theory. In this review we discuss recent EAL modeling strategies, their conceptual basis and their application in studying the emergence of both normal and pathological developmental processes. In addition, we discuss how model predictions can shed light into rational strategies for cell fate regulation, and we point to challenges ahead.},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Davila-Velderrain et al_2015_Modeling the epigenetic attractors landscape.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@davila-velderrainModelingEpigeneticAttractors2015.md;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@davila-velderrainModelingEpigeneticAttractors2015.md}
}

@techreport{davisRelationshipPrecisionRecallROC,
  title = {The {{Relationship Between Precision-Recall}} and {{ROC Curves}}},
  author = {Davis, Jesse and Goadrich, Mark},
  abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WS7LH69D/Davis, Goadrich - Unknown - The Relationship Between Precision-Recall and ROC Curves.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@davisRelationshipPrecisionRecallROC.md}
}

@article{decelleInverseProblemsStructured2019,
  title = {Inverse Problems for Structured Datasets Using Parallel {{TAP}} Equations and {{RBM}}},
  author = {Decelle, Aur{\'e}lien and Hwang, Sungmin and Rocchi, Jacopo and Tantari, Daniele},
  year = {2019},
  pages = {1--12},
  abstract = {We propose an efficient algorithm to solve inverse problems in the presence of binary clustered datasets. We consider the paradigmatic Hopfield model in a teacher student scenario, where this situation is found in the retrieval phase. This problem has been widely analyzed through various methods such as mean-field approaches or the pseudo-likelihood optimization. Our approach is based on the estimation of the posterior using the Thouless-Anderson-Palmer (TAP) equations in a parallel updating scheme. At the difference with other methods, it allows to retrieve the exact patterns of the teacher and the parallel update makes it possible to apply it for large system sizes. We also observe that the Approximate Message Passing (AMP) equations do not reproduce the expected behavior in the direct problem, questioning the standard practice used to obtain time indexes coming from Belief Propagation (BP). We tackle the same problem using a Restricted Boltzmann Machine (RBM) and discuss the analogies between the two algorithms.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6VJG46XV/Decelle2019.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@decelleInverseProblemsStructured2019.md}
}

@article{decellePseudolikelihoodDecimationAlgorithm2014,
  title = {Pseudolikelihood {{Decimation Algorithm Improving}} the {{Inference}} of the {{Interaction Network}} in a {{General Class}} of {{Ising Models}}},
  author = {Decelle, Aur{\'e}lien and {Ricci-Tersenghi}, Federico},
  year = {2014},
  doi = {10.1103/PhysRevLett.112.070603},
  abstract = {In this Letter we propose a new method to infer the topology of the interaction network in pairwise models with Ising variables. By using the pseudolikelihood method (PLM) at high temperature, it is generally possible to distinguish between zero and nonzero couplings because a clear gap separate the two groups. However at lower temperatures the PLM is much less effective and the result depends on subjective choices, such as the value of the l 1 regularizer and that of the threshold to separate nonzero couplings from null ones. We introduce a decimation procedure based on the PLM that recursively sets to zero the less significant couplings, until the variation of the pseudolikelihood signals that relevant couplings are being removed. The new method is fully automated and does not require any subjective choice by the user. Numerical tests have been performed on a wide class of Ising models, having different topologies (from random graphs to finite dimensional lattices) and different couplings (both diluted ferromagnets in a field and spin glasses). These numerical results show that the new algorithm performs better than standard PLM. Recent years have seen a growing interest of the statistical physics community in the inverse Ising problem [1-7]. Although the problem has been known for a long time as Boltzmann machine learning [8], the renewed interest is linked to the large number of data sets coming from many different fields-e.g., biology, physics, neuro-science-that require a quantitative description. Buiglding reliable models for these data sets has become therefore a fundamental problem [9-12]. Given a data set and a model, the inverse problem aims to find the parameters of the model that best fit the data. Among all possible models, the Ising one, although very simple-it involves only pairwise interactions between discrete variables-can take into account a wide range of phenomena. It is thus natural to develop inference methods for this statistical model, as initially done by Boltzmann machine learning [8]. A common approach in Bayesian inference is to infer the model couplings by maximizing the likelihood function. Unfortunately, the likelihood depends on the partition function which is extremely difficult to compute in general. Therefore we use the pseudolikelihood approximation [13,14] to perform inference on Ising models of reasonable size. This method has at least three advantages. (i) The pseudolikelihood function (PLF) can be maximized in polynomial time, (ii) the method is known to be exact in the case of infinite sampling [2], and (iii) as we will see in this Letter, the PLF can be used as an indicator of how good the reconstruction is, allowing one to decrease accurately the number of model parameters. We begin by introducing the pseudolikelihood method (PLM) that has been already tested in inferring finite dimensional models and found to be very effective [2]. We also recall the l 1-regularization extension and the thresholding procedure for discriminating nonzero couplings. We discuss the problem of very large coupling appearing in the PLM and how we solve this issue without using the l 1 regularization. Finally, we describe the new decimation procedure to select accurately the model parameters and we show that the PLF can be used as a reliable likelihood. We conclude by showing that our new symmetrized PLM with decimation provides a very good inference of model couplings, better than standard PLM, in a wide class of Ising models. Let us consider an Ising model with probability measure P\dh s\TH{} {$\frac{1}{4}$} Z -1 exp\dh{$\beta$} P i},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/W729ZVHM/Decelle, Ricci-Tersenghi - 2014 - Pseudolikelihood Decimation Algorithm Improving the Inference of the Interaction Network in a General.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@decellePseudolikelihoodDecimationAlgorithm2014.md}
}

@article{decelleSolvingInverseIsing2016,
  title = {Solving the Inverse {{Ising}} Problem by Mean-Field Methods in a Clustered Phase Space with Many States},
  author = {Decelle, Aur{\'e}lien and {Ricci-Tersenghi}, Federico},
  year = {2016},
  month = jul,
  journal = {Physical Review E},
  volume = {94},
  number = {1},
  eprint = {1501.03034},
  eprinttype = {arxiv},
  pages = {012112},
  publisher = {{American Physical Society}},
  issn = {24700053},
  doi = {10.1103/PhysRevE.94.012112},
  abstract = {In this work we explain how to properly use mean-field methods to solve the inverse Ising problem when the phase space is clustered, that is, many states are present. The clustering of the phase space can occur for many reasons, e.g., when a system undergoes a phase transition, but also when data are collected in different regimes (e.g., quiescent and spiking regimes in neural networks). Mean-field methods for the inverse Ising problem are typically used without taking into account the eventual clustered structure of the input configurations and may lead to very poor inference (e.g., in the low-temperature phase of the Curie-Weiss model). In this work we explain how to modify mean-field approaches when the phase space is clustered and we illustrate the effectiveness of our method on different clustered structures (low-temperature phases of Curie-Weiss and Hopfield models).},
  archiveprefix = {arXiv},
  arxivid = {1501.03034},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@decelleSolvingInverseIsing2016a.md}
}

@book{dejongFlexibleTimingDelay2019,
  title = {Flexible {{Timing}} with {{Delay Networks}} - {{The Scalar Property}} and {{Neural Scaling}}},
  author = {{de Jong}, Joost and Voelker, Aaron and Rijn, Hedderik and Stewart, Terrence and Eliasmith, Chris},
  year = {2019},
  month = jul,
  abstract = {We propose a spiking recurrent neural network model of flexible human timing behavior based on the delay network. The well-known 'scalar property' of timing behavior arises from the model in a natural way, and critically depends on how many dimensions are used to represent the history of stimuli. The model also produces heterogeneous firing patterns that scale with the timed interval, consistent with available neural data. This suggests that the scalar property and neural scaling are tightly linked. Further extensions of the model are discussed that may capture additional behavior, such as continuative timing, temporal cognition, and learning how to time.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@dejongFlexibleTimingDelay2019.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/dejongFlexibleTimingDelay2019-zotero.md}
}

@misc{demontisGenomewideAnalysesADHD2022,
  title = {Genome-Wide Analyses of {{ADHD}} Identify 27 Risk Loci, Refine the Genetic Architecture and Implicate Several Cognitive Domains},
  author = {Demontis, Ditte and Walters, Bragi and Athanasiadis, Georgios and Walters, Raymond and Therrien, Karen and Farajzadeh, Leila and Voloudakis, Georgios and Bendl, Jaroslav and Zeng, Biao and Zhang, Wen and Grove, Jakob and Als, Thomas Damm and Duan, Jinjie and Satterstrom, F. Kyle and {Bybjerg-Grauholm}, Jonas and {Baekved-Hansen}, Marie and Gudmundsson, Olafur O. and Magnusson, Sigurdur and Baldursson, Gisli and Davidsdottir, Katrin and Haraldsdottir, Gyda and Nielsen, Trine Tollerup and Agerbo, Esben and Hoffman, Gabriel E. and Dalsgaard, Soeren and Martin, Joanna and Ribases, Marta and Boomsma, Dorrett and Artigas, Maria Soler and Mota, Nina Roth and Howrigan, Daniel and Medland, Sarah E. and Zayats, Tetyana and Rajagopal, Veera and Nordentoft, Merete and Mors, Ole and Hougaard, David M. and Mortensen, Preben Bo and Daly, Mark and Faraone, Stephen V. and Stefansson, Hreinn and Roussos, Panos and Franke, Barbara and Werge, Thomas and Neale, Benjamin and Stefansson, Kari and Boerglum, Anders D.},
  year = {2022},
  month = feb,
  pages = {2022.02.14.22270780},
  institution = {{medRxiv}},
  doi = {10.1101/2022.02.14.22270780},
  abstract = {Attention deficit hyperactivity disorder (ADHD) is a prevalent childhood psychiatric disorder, with a major genetic component. Here we present a GWAS meta-analysis of ADHD comprising 38,691 individuals with ADHD and 186,843 controls. We identified 27 genome-wide significant loci, which is more than twice the number previously reported. Fine-mapping risk loci highlighted 76 potential risk genes enriched in genes expressed in brain, particularly the frontal cortex, and in early brain development. Overall, ADHD was associated with several brain specific neuronal sub-types and especially midbrain dopaminergic neurons. In a subsample of 17,896 exome-sequenced individuals, we identified increased load of rare protein-truncating variants in cases for a set of risk genes enriched with likely causal common variants, suggesting implication of SORCS3 in ADHD by both common and rare variants. We found ADHD to be highly polygenic, with around seven thousand variants explaining 90\% of the SNP heritability. Bivariate gaussian mixture modeling estimated that more than 90\% of ADHD influencing variants are shared with other psychiatric disorders (autism, schizophrenia and depression) and phenotypes (e.g. educational attainment) when both concordant and discordant variants are considered. Additionally, we demonstrated that common variant ADHD risk was associated with impaired complex cognition such as verbal reasoning and a range of executive functions including attention.},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/Zotero/storage/Demontis et al_2022_Genome-wide analyses of ADHD identify 27 risk loci, refine the genetic.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SXXCD4EB/2022.02.14.html}
}

@article{demulatierStatisticalInferenceMinimally2021,
  title = {Statistical {{Inference}} of {{Minimally Complex Models}}},
  author = {{de Mulatier}, Cl{\'e}lia and Mazza, Paolo P. and Marsili, Matteo},
  year = {2021},
  month = sep,
  journal = {arXiv:2008.00520 [physics, q-bio, stat]},
  eprint = {2008.00520},
  eprinttype = {arxiv},
  primaryclass = {physics, q-bio, stat},
  abstract = {Finding the model that best describes a high dimensional dataset is a daunting task. For binary data, we show that this becomes feasible when restricting the search to a family of simple models, that we call Minimally Complex Models (MCMs). These are spin models, with interactions of arbitrary order, that are composed of independent components of minimal complexity (Beretta et al., 2018). They tend to be simple in information theoretic terms, which means that they are well-fitted to specific types of data, and are therefore easy to falsify. We show that Bayesian model selection restricted to these models is computationally feasible and has many other advantages. First, their evidence, which trades off goodness-of-fit against model complexity, can be computed easily without any parameter fitting. This allows selecting the best MCM among all, even though the number of models is astronomically large. Furthermore, MCMs can be inferred and sampled from without any computational effort. Finally, model selection among MCMs is invariant with respect to changes in the representation of the data. MCMs portray the structure of dependencies among variables in a simple way, as illustrated in several examples, and thus provide robust predictions on dependencies in the data. MCMs contain interactions of any order between variables, and thus may reveal the presence of interactions of order higher than pairwise.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Statistics Theory,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Quantitative Methods},
  note = {Comment: 18 pages, 10 figures},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/de Mulatier et al_2021_Statistical Inference of Minimally Complex Models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@demulatierStatisticalInferenceMinimally2021.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X6XXP3VU/2008.html}
}

@article{dengRethinkingPerformanceComparison2020,
  title = {Rethinking the Performance Comparison between {{SNNS}} and {{ANNS}}},
  author = {Deng, Lei and Wu, Yujie and Hu, Xing and Liang, Ling and Ding, Yufei and Li, Guoqi and Zhao, Guangshe and Li, Peng and Xie, Yuan},
  year = {2020},
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  pages = {294--307},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.09.005},
  langid = {english}
}

@article{denkerTransformingNeuralNetOutput1991,
  title = {Transforming {{Neural-Net Output Levels}} to {{Probability Distributions}}},
  author = {Denker, John S and LeCun, Yann},
  year = {1991},
  journal = {Advances in Neural Information Processing Systems 3},
  pages = {853--859},
  abstract = {(1) The outputs of a typical multi-output classification network do not satisfy the axioms of probability; probabilities should be positive and sum to one. This problem can be solved by treating the trained network as a preprocessor that produces a feature vector that can be further processed, for instance by classical statistical estimation techniques. (2) We present a method for computing the first two moments ofthe probability distribution indicating the range of outputs that are consistent with the input and the training data. It is particularly useful to combine these two ideas: we implement the ideas of section 1 using Parzen windows, where the shape and relative size of each window is computed using the ideas of section 2. This allows us to make contact between important theoretical ideas (e.g. the ensemble formalism) and practical techniques (e.g. back-prop). Our results also shed new light on and generalize the well-known "soft max" scheme.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MMGPZT3H/NIPS-1990-transforming-neural-net-output-levels-to-probability-distributions-Paper.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@denkerTransformingNeuralNetOutput1991.md}
}

@article{depannemaeckerRealisticSpikingNeural2020,
  title = {Realistic Spiking Neural Network: {{Non-synaptic}} Mechanisms Improve Convergence in Cell Assembly},
  shorttitle = {Realistic Spiking Neural Network},
  author = {Depannemaecker, Damien and Canton Santos, Luiz Eduardo and Rodrigues, Ant{\^o}nio M{\'a}rcio and Scorza, Carla Alessandra and Scorza, Fulvio Alexandre and de Almeida, Ant{\^o}nio-Carlos Guimar{\~a}es},
  year = {2020},
  month = feb,
  journal = {Neural Networks},
  volume = {122},
  pages = {420--433},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.09.038},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4BZQN2N8/Depannemaecker et al_2020_Realistic spiking neural network.pdf}
}

@article{derridaExactlySolvableAsymmetric1987,
  title = {An {{Exactly Solvable Asymmetric Neural Network Model}}},
  author = {Derrida, B. and Gardner, E. and Zippelius, A.},
  year = {1987},
  month = jul,
  journal = {Europhysics Letters (EPL)},
  volume = {4},
  number = {2},
  pages = {167--173},
  publisher = {{IOP Publishing}},
  issn = {0295-5075},
  doi = {10.1209/0295-5075/4/2/007},
  abstract = {We consider a diluted and nonsymmetric version of the Little-Hopfield model which can be solved exactly. We obtain the analytic expression of the evolution of one configuration having a finite overlap on one stored pattern. We show that even when the system remembers, two different configurations which remain close to the same pattern never become identical. Lastly, we show that when two stored patterns are correlated, there exists a regime for which the system remembers these patterns without being able to distinguish them.},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Derrida et al_1987_An Exactly Solvable Asymmetric Neural Network Model.pdf}
}

@article{deverettIntervalTimingDeep2019,
  title = {Interval Timing in Deep Reinforcement Learning Agents},
  author = {Deverett, B. and Faulkner, R. and Fortunato, Meire and Wayne, Greg and Leibo, Joel Z.},
  year = {2019},
  journal = {NeurIPS},
  abstract = {This work characterize the strategies developed by recurrent and feedforward agents, which both succeed at temporal reproduction using distinct mechanisms, some of which bear specific and intriguing similarities to biological systems. The measurement of time is central to intelligent behavior. We know that both animals and artificial agents can successfully use temporal dependencies to select actions. In artificial agents, little work has directly addressed (1) which architectural components are necessary for successful development of this ability, (2) how this timing ability comes to be represented in the units and actions of the agent, and (3) whether the resulting behavior of the system converges on solutions similar to those of biology. Here we studied interval timing abilities in deep reinforcement learning agents trained end-to-end on an interval reproduction paradigm inspired by experimental literature on mechanisms of timing. We characterize the strategies developed by recurrent and feedforward agents, which both succeed at temporal reproduction using distinct mechanisms, some of which bear specific and intriguing similarities to biological systems. These findings advance our understanding of how agents come to represent time, and they highlight the value of experimentally inspired approaches to characterizing agent abilities.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X59FSW3J/Deverett et al_2019_Interval timing in deep reinforcement learning agents.pdf}
}

@article{dewarInformationTheoryExplanation2003,
  title = {Information Theory Explanation of the Fluctuation Theorem, Maximum Entropy Production and Self-Organized Criticality in Non-Equilibrium Stationary States},
  author = {Dewar, Roderick},
  year = {2003},
  month = jan,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {36},
  number = {3},
  pages = {631--641},
  publisher = {{IOP Publishing}},
  issn = {03054470},
  doi = {10.1088/0305-4470/36/3/303},
  abstract = {Jaynes' information theory formalism of statistical mechanics is applied to the stationary states of open, non-equilibrium systems. First, it is shown that the probability distribution p{$\Gamma$} of the underlying microscopic phase space trajectories {$\Gamma$} over a time interval of length {$\tau$} satisfies p{$\Gamma$} {$\propto$} exp({$\tau\sigma$} {$\Gamma$}/2kB) where {$\sigma\Gamma$} is the time-averaged rate of entropy production of {$\Gamma$}. Three consequences of this result are then derived: (1) the fluctuation theorem, which describes the exponentially declining probability of deviations from the second law of thermodynamics as {$\tau$} \textrightarrow{} {$\infty$}; (2) the selection principle of maximum entropy production for non-equilibrium stationary states, empirical support for which has been found in studies of phenomena as diverse as the Earth's climate and crystal growth morphology; and (3) the emergence of self-organized criticality for flux-driven systems in the slowly-driven limit. The explanation of these results on general information theoretic grounds underlines their relevance to a broad class of stationary, non-equilibrium systems. In turn, the accumulating empirical evidence for these results lends support to Jaynes' formalism as a common predictive framework for equilibrium and non-equilibrium statistical mechanics.},
  keywords = {6540Gr,8970+c,numbers: 0570Ln},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/F8BK5P6X/Dewar - 2003 - Information theory explanation of the fluctuation theorem, maximum entropy production and self-organized criticality in n.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/V9P8P7XI/Dewar - 2003 - Information theory explanation of the fluctuation theorem, maximum entropy production and self-organized criticality in n.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dewarInformationTheoryExplanation2003.md}
}

@techreport{diaconisALGEBRAICALGORITHMSSAMPLING1998,
  title = {{{ALGEBRAIC ALGORITHMS FOR SAMPLING FROM CONDITIONAL DISTRIBUTIONS}}},
  author = {Diaconis, Persi and Sturmfels, Bernd},
  year = {1998},
  journal = {The Annals of Statistics},
  volume = {26},
  number = {1},
  pages = {397--397},
  abstract = {We construct Markov chain algorithms for sampling from discrete exponential families conditional on a sufficient statistic. Examples include contingency tables, logistic regression, and spectral analysis of permutation data. The algorithms involve computations in polynomial rings using Grobner bases. \textasciidieresis},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VZC2QMSU/Diaconis, Sturmfels - 1998 - ALGEBRAIC ALGORITHMS FOR SAMPLING FROM CONDITIONAL DISTRIBUTIONS.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@diaconisALGEBRAICALGORITHMSSAMPLING1998.md}
}

@misc{dibaForwardReverseHippocampal2007,
  title = {Forward and Reverse Hippocampal Place-Cell Sequences during Ripples},
  author = {Diba, Kamran and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2007},
  month = oct,
  journal = {Nature Neuroscience},
  volume = {10},
  number = {10},
  pages = {1242},
  doi = {10.1038/nn1961},
  abstract = {We report that temporal spike sequences from hippocampal place neurons of rats on an elevated track recurred in reverse order at the end of a run, but in forward order in anticipation of the run, coinciding with sharp waves. Vector distances between the place fields were reflected in the temporal structure of these sequences. This bidirectional re-enactment of temporal sequences may contribute to the establishment of higher-order associations in episodic memory. The memory of a temporal sequence is characterized by both forward and backward associations among the stored items of the sequence, with forward associations showing stronger bonds 1-3. The storage of the temporal sequence is thought to involve hippocampus-dependent mechanisms during waking and sleep 2,4-6. In the hippocampus proper, temporal sequences are observed on several timescales: (i) at the behavioral timescale, as animals run through sequences of place-tuned fields 7 , (ii) at the timescale of hippocampal theta oscillations, as cells fire with location-dependent phases in a theta cycle ('phase-precession') 8,9 and (iii) at the timescale of sharp-wave ripples, when large populations of neurons co-fire with fine temporal structure 6,10. Here we show that during waking sharp waves in rats, sequences defined at the behavioral timescale, are pre-and replayed, in forward and in reverse, at the beginning and end of a journey, respectively. Population activity in the hippocampus was recorded in three rats while they ran back and forth on a linear track for a water reward at each end 10 (Fig. 1a). During the run, each neuron's firing was tuned to a particular location along the track, which was stable from lap to lap 7. These locations define a temporal sequence of place-cell firing on the timescale of seconds (see Supplementary Methods and Supplementary Figs. 1-10 online). During immobility following the run, the same neurons fired again on the timescale of hundreds of milliseconds, but in the reverse temporal order, confirming previous findings 10 (Fig. 1a, right inset). In addition, the neurons fired in the forward temporal order during immobility prior to the run (Fig. 1b, left inset). To detect and quantify these ordered events, we treated the left and right laps on the track separately and for each created a place-cell sequence ('template') according to the temporal order of peak firing on the track, calculated over the entire session. Laps in different directions generated uniquely different templates, with only 267 out of 1,256 neurons firing {$\geq$}5 Hz in both directions 11. For each template, we identified reactivation events at both ends of the track by an increase in the population activity during immobility 10 , when {$\geq$}30\% or {$\geq$}5 of the place},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CTW4SNG4/Diba, Buzsáki - 2007 - Forward and reverse hippocampal place-cell sequences during ripples.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dibaForwardReverseHippocampal2007.md}
}

@article{diehlCodingSchemes2018,
  title = {Coding {{Schemes}}},
  author = {Diehl, Geoffrey W and Hon, Olivia J and Leutgeb, Stefan and Leutgeb, Jill K and Diego, San and Jolla, La and Diego, San and Jolla, La},
  year = {2018},
  volume = {94},
  number = {1},
  pages = {83--92},
  doi = {10.1016/j.neuron.2017.03.004.Grid},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UVJILAKH/Diehl et al. - 2018 - Coding Schemes.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@diehlCodingSchemes2018.md}
}

@article{diehlUnsupervisedLearningDigit2015,
  title = {Unsupervised Learning of Digit Recognition Using Spike-Timing-Dependent Plasticity},
  author = {Diehl, Peter and Cook, Matthew},
  year = {2015},
  journal = {Frontiers in Computational Neuroscience},
  volume = {9},
  issn = {1662-5188},
  abstract = {In order to understand how the mammalian neocortex is performing computations, two things are necessary; we need to have a good understanding of the available neuronal processing units and mechanisms, and we need to gain a better understanding of how those mechanisms are combined to build functioning systems. Therefore, in recent years there is an increasing interest in how spiking neural networks (SNN) can be used to perform complex computations or solve pattern recognition tasks. However, it remains a challenging task to design SNNs which use biologically plausible mechanisms (especially for learning new patterns), since most such SNN architectures rely on training in a rate-based network and subsequent conversion to a SNN. We present a SNN for digit recognition which is based on mechanisms with increased biological plausibility, i.e., conductance-based instead of current-based synapses, spike-timing-dependent plasticity with time-dependent weight change, lateral inhibition, and an adaptive spiking threshold. Unlike most other systems, we do not use a teaching signal and do not present any class labels to the network. Using this unsupervised learning scheme, our architecture achieves 95\% accuracy on the MNIST benchmark, which is better than previous SNN implementations without supervision. The fact that we used no domain-specific knowledge points toward the general applicability of our network design. Also, the performance of our network scales well with the number of neurons used and shows similar performance for four different learning rules, indicating robustness of the full combination of mechanisms, which suggests applicability in heterogeneous biological neural networks.},
  note = {\section{Annotations\\
(6/7/2022, 3:50:44 PM)}

\par
``be also used for reinforcement learning. On the other end of the spectrum, many models in computational'' (Diehl and Cook, 2015, p. 2)
\par
(Diehl and Cook, 2015, p. 2)
\par
(Diehl and Cook, 2015, p. 2) tEST COMMENTS},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FJZFVPLI/Diehl_Cook_2015_Unsupervised learning of digit recognition using spike-timing-dependent.pdf}
}

@article{diesmannStablePropagationSynchronous1999,
  title = {Stable Propagation of Synchronous Spiking in Cortical Neural Networks},
  author = {Diesmann, M. and Gewaltig, M. and Aertsen, A.},
  year = {1999},
  journal = {Nature},
  doi = {10.1038/990101},
  abstract = {The results indicate that a combinatorial neural code, based on rapid associations of groups of neurons co-ordinating their activity at the single spike level, is possible within a cortical-like network. The classical view of neural coding has emphasized the importance of information carried by the rate at which neurons discharge action potentials. More recent proposals that information may be carried by precise spike timing have been challenged by the assumption that these neurons operate in a noisy fashion\textemdash presumably reflecting fluctuations in synaptic input\textemdash and, thus, incapable of transmitting signals with millisecond fidelity. Here we show that precisely synchronized action potentials can propagate within a model of cortical network activity that recapitulates many of the features of biological systems. An attractor, yielding a stable spiking precision in the (sub)millisecond range, governs the dynamics of synchronization. Our results indicate that a combinatorial neural code, based on rapid associations of groups of neurons co-ordinating their activity at the single spike level, is possible within a cortical-like network.}
}

@article{digiorgioSimpleCortexModel2017,
  title = {Simple {{Cortex}}: {{A Model}} of {{Cells}} in the {{Sensory Nervous System}}},
  author = {Di Giorgio, David},
  year = {2017},
  month = oct,
  abstract = {Neuroscience research has produced many theories and computational neural models of sensory nervous systems. Notwithstanding many different perspectives towards developing intelligent machines, artificial intelligence has ultimately been influenced by neuroscience. Therefore, this paper provides an introduction to biologically inspired machine intelligence by exploring the basic principles of sensation and perception as well as the structure and behavior of biological sensory nervous systems like the neocortex. Concepts like spike timing, synaptic plasticity, inhibition, neural structure, and neural behavior are applied to a new model, Simple Cortex (SC). A software implementation of SC has been built and demonstrates fast observation, learning, and prediction of spatio-temporal sensory-motor patterns and sequences. Finally, this paper suggests future areas of improvement and growth for Simple Cortex and other related machine intelligence models.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LGBSK9JC/Di Giorgio - 2017 - Simple Cortex A Model of Cells in the Sensory Nervous System.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@digiorgioSimpleCortexModel2017.md}
}

@article{dillProteinfoldingProblem502012,
  title = {The Protein-Folding Problem, 50 Years On},
  author = {Dill, Ken A. and MacCallum, Justin L.},
  year = {2012},
  month = nov,
  journal = {Science},
  volume = {338},
  number = {6110},
  pages = {1042--1046},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.1219021},
  abstract = {The protein-folding problem was first posed about one half-century ago. The term refers to three broad questions: (i) What is the physical code by which an amino acid sequence dictates a protein's native structure? (ii) How can proteins fold so fast? (iii) Can we devise a computer algorithm to predict protein structures from their sequences? We review progress on these problems. In a few cases, computer simulations of the physical forces in chemically detailedmodels have now achieved the accurate folding of small proteins. We have learned that proteins fold rapidly because random thermal motions cause conformational changes leading energetically downhill toward the native structure, a principle that is captured in funnel-shaped energy landscapes. And thanks in part to the large Protein Data Bank of known structures, predicting protein structures is now far more successful than was thought possible in the early days. What began as three questions of basic science one half-century ago has now grown into the full-fledged research field of protein physical science.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5EB9BZAW/Dill, MacCallum - 2012 - The protein-folding problem, 50 years on.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dillProteinfoldingProblem502012.md}
}

@article{DilutionNeuralNetworks2020,
  title = {Dilution (Neural Networks)},
  year = {2020},
  month = dec,
  journal = {Wikipedia},
  abstract = {Dilution (also called Dropout) is a regularization technique for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. It is an efficient way of performing model averaging with neural networks. The term dilution refers to the thinning of the weights. The term dropout refers to randomly "dropping out", or omitting, units (both hidden and visible) during the training process of a neural network. Both the thinning of weights and dropping out units trigger the same type of regularization, and often the term dropout is used when referring to the dilution of weights.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 993904521},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@DilutionNeuralNetworks2020.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/U654W8YP/index.html}
}

@article{dingBridgingAICBIC2018,
  title = {Bridging {{AIC}} and {{BIC}}: {{A New Criterion}} for {{Autoregression}}},
  author = {Ding, Jie and Tarokh, Vahid and Yang, Yuhong},
  year = {2018},
  journal = {IEEE Transactions on Information Theory},
  volume = {64},
  number = {6},
  pages = {4024--4043},
  publisher = {{IEEE}},
  doi = {10.1109/TIT.2017.2717599},
  abstract = {To address order selection for an autoregressive model fitted to time series data, we propose a new information criterion. It has the benefits of the two well-known model selection techniques: The Akaike information criterion and the Bayesian information criterion. When the data are generated from a finite-order autoregression, the Bayesian information criterion is known to be consistent, and so is the new criterion. When the true order is infinity or suitably high with respect to the sample size, the Akaike information criterion is known to be efficient in the sense that its predictive performance is asymptotically equivalent to the best offered by the candidate models; in this case, the new criterion behaves in a similar manner. Different from the two classical criteria, the proposed criterion adaptively achieves either consistency or efficiency depending on the underlying true model. In practice, where the observed time series is given without any prior information about the model specification, the proposed order selection criterion is more flexible and reliable compared with classical approaches. Numerical results are presented, demonstrating the adaptivity of the proposed technique when applied to various data sets.},
  keywords = {Adaptivity,Akaike information criterion,Asymptotic efficiency,Bayesian information criterion,Bridge criterion,Consistency,Information criterion,Model selection,Parametricness index},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7SDWGF8T/Bridging AIC and BIC A New Criterion for Autoregression, Jie Ding,.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dingBridgingAICBIC2018.md}
}

@article{dingModelSelectionTechniques2018,
  title = {Model {{Selection Techniques}}: {{An Overview}}},
  author = {Ding, Jie and Tarokh, Vahid and Yang, Yuhong},
  year = {2018},
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {6},
  pages = {16--34},
  doi = {10.1109/MSP.2018.2867638},
  abstract = {In the era of big data, analysts usually explore various statistical models or machine-learning methods for observed data to facilitate scientific discoveries or gain predictive power. Whatever data and fitting procedures are employed, a crucial step is to select the most appropriate model or method from a set of candidates. Model selection is a key ingredient in data analysis for reliable and reproducible statistical inference or prediction, and thus it is central to scientific studies in such fields as ecology, economics, engineering, finance, political science, biology, and epidemiology. There has been a long history of model selection techniques that arise from researches in statistics, information theory, and signal processing. A considerable number of methods has been proposed, following different philosophies and exhibiting varying performances. The purpose of this article is to provide a comprehensive overview of them, in terms of their motivation, large sample performance, and applicability. We provide integrated and practically relevant discussions on theoretical properties of state-of-the-art model selection approaches. We also share our thoughts on some controversial views on the practice of model selection.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UTYKXX63/1810.09583.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dingModelSelectionTechniques2018.md}
}

@misc{dinhSharpMinimaCan2017,
  title = {Sharp {{Minima Can Generalize For Deep Nets}}},
  author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  year = {2017},
  month = may,
  number = {arXiv:1703.04933},
  eprint = {1703.04933},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1703.04933},
  abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter \& Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: 8.5 pages of main content, 2.5 of bibliography and 1 page of appendix},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AFKHKJ44/Dinh et al_2017_Sharp Minima Can Generalize For Deep Nets.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SPI65JJE/1703.html}
}

@article{dixitTrainingQuantumAnnealing2022,
  title = {Training a Quantum Annealing Based Restricted {{Boltzmann}} Machine on Cybersecurity Data},
  author = {Dixit, Vivek and Selvarajan, Raja and Aldwairi, Tamer and Koshka, Yaroslav and Novotny, Mark A. and Humble, Travis S. and Alam, Muhammad A. and Kais, Sabre},
  year = {2022},
  month = jun,
  journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume = {6},
  number = {3},
  eprint = {2011.13996},
  eprinttype = {arxiv},
  primaryclass = {quant-ph},
  pages = {417--428},
  issn = {2471-285X},
  doi = {10.1109/TETCI.2021.3074916},
  abstract = {We present a real-world application that uses a quantum computer. Specifically, we train a RBM using QA for cybersecurity applications. The D-Wave 2000Q has been used to implement QA. RBMs are trained on the ISCX data, which is a benchmark dataset for cybersecurity. For comparison, RBMs are also trained using CD. CD is a commonly used method for RBM training. Our analysis of the ISCX data shows that the dataset is imbalanced. We present two different schemes to balance the training dataset before feeding it to a classifier. The first scheme is based on the undersampling of benign instances. The imbalanced training dataset is divided into five sub-datasets that are trained separately. A majority voting is then performed to get the result. Our results show the majority vote increases the classification accuracy up from 90.24\% to 95.68\%, in the case of CD. For the case of QA, the classification accuracy increases from 74.14\% to 80.04\%. In the second scheme, a RBM is used to generate synthetic data to balance the training dataset. We show that both QA and CD-trained RBM can be used to generate useful synthetic data. Balanced training data is used to evaluate several classifiers. Among the classifiers investigated, K-Nearest Neighbor (KNN) and Neural Network (NN) perform better than other classifiers. They both show an accuracy of 93\%. Our results show a proof-of-concept that a QA-based RBM can be trained on a 64-bit binary dataset. The illustrative example suggests the possibility to migrate many practical classification problems to QA-based techniques. Further, we show that synthetic data generated from a RBM can be used to balance the original dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantum Physics},
  note = {Comment: in IEEE Transactions on Emerging Topics in Computational Intelligence},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IBQ5QFQH/Dixit et al_2022_Training a quantum annealing based restricted Boltzmann machine on.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GC4LPJ8D/2011.html}
}

@article{doborjehPersonalisedModellingSpiking2019,
  title = {Personalised Modelling with Spiking Neural Networks Integrating Temporal and Static Information},
  author = {Doborjeh, Maryam and Kasabov, Nikola and Doborjeh, Zohreh and Enayatollahi, Reza and Tu, Enmei and Gandomi, Amir H.},
  year = {2019},
  month = nov,
  journal = {Neural Networks},
  volume = {119},
  pages = {162--177},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.07.021},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NRZNVSB5/Doborjeh et al_2019_Personalised modelling with spiking neural networks integrating temporal and.pdf}
}

@article{doironMechanicsStatedependentNeural2016,
  title = {The Mechanics of State-Dependent Neural Correlations},
  author = {Doiron, Brent and {Litwin-Kumar}, Ashok and Rosenbaum, Robert and Ocker, Gabriel K. and Josi{\'c}, Kre{\v s}imir},
  year = {2016},
  month = mar,
  journal = {Nature Neuroscience},
  volume = {19},
  number = {3},
  pages = {383--393},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4242},
  abstract = {The state of the nervous system shifts constantly. Most studies focus on how state determines the average neural response, with little attention to the trial-to-trial fluctuations of brain activity. We review recent theoretical advances in modeling the physiological mechanisms responsible for state-dependent modulations in the correlated fluctuations of neuronal populations.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GQ792G99/Doiron et al. - 2016 - The mechanics of state-dependent neural correlatio.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@doironMechanicsStatedependentNeural2016.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7VJS8RN3/nn.html}
}

@article{domnisoruMembranePotentialDynamics2013,
  title = {Membrane Potential Dynamics of Grid Cells},
  author = {Domnisoru, Cristina and Kinkhabwala, Amina A. and Tank, David W.},
  year = {2013},
  month = mar,
  journal = {Nature},
  volume = {495},
  number = {7440},
  pages = {199--204},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nature11973},
  abstract = {Intracellular membrane potential changes are measured directly in mouse grid cells during navigation along linear tracks in virtual reality; the recordings reveal that slow ramps of depolarization are the sub-threshold signatures of firing fields, as in attractor network models of grid cells, whereas theta oscillations pace action potential timing.},
  keywords = {Learning and memory},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FYU2L5WE/Domnisoru, Kinkhabwala, Tank - 2013 - Membrane potential dynamics of grid cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@domnisoruMembranePotentialDynamics2013.md}
}

@article{donahueMultimodalCharacterizationNeural2018,
  title = {Multimodal {{Characterization}} of {{Neural Networks}} Using {{Highly Transparent Electrode Arrays}} 1 . {{Multimodal Characterization}} of {{Neural Networks}} Using {{Highly Transparent Electrode Arrays}} 2 . {{Neural Network Recordings}} with {{Transparent Arrays Department}} of {{Bioel}}},
  author = {Donahue, Mary J. and Kaszas, Attila and Turi, Gergely F. and R{\'o}zsa, Bal{\'a}zs and Sl{\'e}zia, Andrea and Katona, Gergely and Bernard, Christophe and Malliaras, George G. and Williamson, Adam and Donahue, Mary J. and Kaszas, Attila and Turi, Gergely F. and R{\'o}zsa, Bal{\'a}zs and Sl{\'e}zia, Andrea},
  year = {2018},
  journal = {eneuro},
  doi = {10.1523/ENEURO.0187-18.2018},
  abstract = {Transparent and flexible materials are attractive for a wide range of emerging bioelectronic applications. These include neural interfacing devices for both recording and stimulation, where low electrochemical electrode impedance is valuable. Here the conducting polymer poly(3,4-ethylenedioxythiophene): poly(styrenesulfonate) (PEDOT:PSS) is utilized to fabricate electrodes that are small enough to allow unencumbered optical access for imaging a large cell population with two-photon (2P) microscopy, yet provide low impedance for simultaneous high quality recordings of neural activity in vivo . To demonstrate this, pathophysiological activity was induced in the mouse cortex using 4-aminopyridine (4AP) and the resulting electrical activity was detected with the PEDOT:PSS-based probe while imaging calcium activity directly below the probe area. The induced calcium activity of the neuronal network as measured by the fluorescence change in the cells correlated well with the electrophysiological recordings from the cortical grid of PEDOT:PSS microelectrodes. Our approach provides a valuable vehicle for complementing classical high temporal resolution electrophysiological analysis with optical imaging. Significance statement Electrophysiological recordings, with varying degrees of invasiveness, are the traditional method for measuring neural activity, possessing the capability to measure both individual neurons and populations of neurons. Imaging methods, such as computed tomography scans and functional magnetic resonance imaging, have been developed to accomplish less invasive characterization of neuronal activity; traditionally offering good spatial or relatively high temporal resolution, yet resolution of individual neurons cannot be achieved. Two-photon imaging enables network-wide analysis with cellular resolution on a faster timescale with high spatial fidelity. This study presents a method for the combination of two-photon imaging and electrophysiological recordings with highly transparent arrays of organic electrodes, presenting a powerful tool to simultaneously acquire the electrical and optical activity of neural circuits.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MCHNJKX6/Donahue et al. - 2018 - Multimodal Characterization of Neural Networks using Highly Transparent Electrode Arrays 1 . Multimodal Characte.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@donahueMultimodalCharacterizationNeural2018.md}
}

@article{drackRecentDevelopmentsGeneral2010,
  title = {Recent Developments in General System Theory},
  author = {Drack, Manfred and Schwarz, Gregor},
  year = {2010},
  month = nov,
  journal = {Systems Research and Behavioral Science},
  volume = {27},
  number = {6},
  pages = {601--610},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/sres.1013},
  abstract = {Bertalanffy's general system theory (GST) was conceived as a unifying research program, as a science of 'wholeness' to overcome or complement mechanistic and reductionist approaches. More than 50 years since the founding of the Society for General Systems Research (SGSR), today's International Society for the Systems Sciences (ISSS), two questions arise about the developments in GST research. Are there recent contributions towards enhancing GST? Are there simplifications compared to Bertalanffy's original program? To answer these questions we analysed in detail 161 articles in peer reviewed journals that referred to GST and appeared between 1995 and 2006. We emphasized contributions made in science and philosophy. Even though the system approach was introduced in applied fields such as engineering and management, we did not include these fields in this article. \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  keywords = {General system theory,GST,Ludwig von Bertalanffy},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@drackRecentDevelopmentsGeneral2010.md}
}

@article{dragoiInternalOperationsHippocampus2013,
  title = {Internal Operations in the Hippocampus: Single Cell and Ensemble Temporal Coding},
  shorttitle = {Internal Operations in the Hippocampus},
  author = {Dragoi, George},
  year = {2013},
  month = aug,
  journal = {Frontiers in Systems Neuroscience},
  volume = {7},
  pages = {46},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2013.00046},
  pmcid = {PMC3756298},
  pmid = {24009564},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Dragoi_2013_Internal operations in the hippocampus.pdf}
}

@article{dSNPHeritabilityPolygenicity2020,
  title = {Beyond {{SNP}} Heritability: {{Polygenicity}} and Discoverability of Phenotypes Estimated with a Univariate {{Gaussian}} Mixture Model},
  shorttitle = {Beyond {{SNP}} Heritability},
  author = {D, Holland and O, Frei and R, Desikan and Cc, Fan and Aa, Shadrin and Ob, Smeland and Vs, Sundar and P, Thompson and Oa, Andreassen and Am, Dale},
  year = {2020},
  month = may,
  journal = {PLoS genetics},
  volume = {16},
  number = {5},
  publisher = {{PLoS Genet}},
  issn = {1553-7404},
  doi = {10.1371/journal.pgen.1008612},
  abstract = {Estimating the polygenicity (proportion of causally associated single nucleotide polymorphisms (SNPs)) and discoverability (effect size variance) of causal SNPs for human traits is currently of considerable interest. SNP-heritability is proportional to the product of these quantities. We present a b \ldots},
  langid = {english},
  pmid = {32427991},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/D et al_2020_Beyond SNP heritability.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TI5ZCYS3/32427991.html}
}

@article{dubinkinaAlternativeStableStates2018,
  title = {Alternative Stable States in a Model of Microbial Community Limited by Multiple Essential Nutrients},
  author = {Dubinkina, Veronika and Fridman, Yulia and Pandey, Parth Pratim and Maslov, Sergei},
  year = {2018},
  month = oct,
  journal = {bioRxiv},
  pages = {439547},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/439547},
  abstract = {{$<$}p{$>$}Microbial communities routinely have several alternative stable states observed for the same environmental parameters. Sudden and irreversible transitions between these states make external manipulation of these systems more complicated. To better understand the mechanisms and origins of multistability in microbial communities, we introduce and study a model of a microbial ecosystem colonized by multiple specialist species selected from a fixed pool. Growth of each species can be limited by essential nutrients of two types, e.g. carbon and nitrogen, each represented in the environment by multiple metabolites. We demonstrate that our model has an exponentially large number of potential stable states realized for different environmental parameters. Using game theoretical methods adapted from the stable marriage problem we predict all of these states based only on ranked lists of competitive abilities of species for each of the nutrients. We show that for every set of nutrient influxes, several mutually uninvadable stable states are generally feasible and we distinguish them based upon their dynamic stability. We further explore an intricate network of discontinuous transitions (regime shifts) between these alternative states both in the course of community assembly, or upon changes of nutrient influxes.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BDSNVDJ3/Dubinkina et al. - 2018 - Alternative stable states in a model of microbial .pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dubinkinaAlternativeStableStates2018.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Y3RTEUPE/439547v1.html}
}

@inproceedings{dudikMaximumEntropyDistribution2006,
  title = {Maximum Entropy Distribution Estimation with Generalized Regularization},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Dud{\'i}k, Miroslav and Schapire, Robert E.},
  year = {2006},
  volume = {4005 LNAI},
  pages = {123--138},
  publisher = {{Springer Verlag}},
  doi = {10.1007/11776420_12},
  abstract = {We present a unified and complete account of maximum entropy distribution estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we can easily derive performance guarantees for many known regularization types, including {$\mathscr{l}$}1, {$\mathscr{l}$}2, {$\mathscr{l}$}22 and {$\mathscr{l}$}1+ {$\mathscr{l}$}22 style regularization. Furthermore, our general approach enables us to use information about the structure of the feature space or about sample selection bias to derive entirely new regularization functions with superior guarantees. We propose an algorithm solving a large and general subclass of generalized maxent problems, including all discussed in the paper, and prove its convergence. Our approach generalizes techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. \textcopyright{} Springer-Verlag Berlin Heidelberg 2006.},
  isbn = {3-540-35294-5},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AHNWI9IQ/Dudík, Schapire - 2006 - Maximum entropy distribution estimation with generalized regularization.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dudikMaximumEntropyDistribution2006.md}
}

@inproceedings{dudikPerformanceGuaranteesRegularized2004,
  title = {Performance Guarantees for Regularized Maximum Entropy Density Estimation},
  booktitle = {Lecture {{Notes}} in {{Artificial Intelligence}} ({{Subseries}} of {{Lecture Notes}} in {{Computer Science}})},
  author = {Dudik, Miroslav and Phillips, Steven J. and Schapire, Robert E.},
  year = {2004},
  volume = {3120},
  pages = {472--486},
  publisher = {{Springer Verlag}},
  doi = {10.1007/978-3-540-27819-1_33},
  abstract = {We consider the problem of estimating an unknown probability distribution from samples using the principle of maximum entropy (maxent). To alleviate overfitting with a very large number of features, we propose applying the maxent principle with relaxed constraints on the expectations of the features. By convex duality, this turns out to be equivalent to finding the Gibbs distribution minimizing a regularized version of the empirical log loss. We prove non-asymptotic bounds showing that, with respect to the true underlying distribution, this relaxed version of maxent produces density estimates that are almost as good as the best possible. These bounds are in terms of the deviation of the feature empirical averages relative to their true expectations, a number that can be bounded using standard uniform-convergence techniques. In particular, this leads to bounds that drop quickly with the number of samples, and that depend very moderately on the number or complexity of the features. We also derive and prove convergence for both sequential-update and parallel-update algorithms. Finally, we briefly describe experiments on data relevant to the modeling of species geographical distributions.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3U9DEZNI/Dudik, Phillips, Schapire - 2004 - Performance guarantees for regularized maximum entropy density estimation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dudikPerformanceGuaranteesRegularized2004.md}
}

@article{dumontNeuralCorrelatesNavigation2015,
  title = {The Neural Correlates of Navigation beyond the Hippocampus},
  author = {Dumont, Julie R. and Taube, Jeffrey S.},
  year = {2015},
  month = jan,
  journal = {Progress in Brain Research},
  volume = {219},
  pages = {83--102},
  publisher = {{Elsevier}},
  issn = {9780444635495},
  doi = {10.1016/BS.PBR.2015.03.004},
  abstract = {Navigation is a complex cognitive process that is vital for survival. The rodent hippocampus has long been implicated in spatial memory and navigation. Following the discovery of place cells, found in the hippocampus, a variety of other spatially tuned neural correlates of navigation have been found in a widely distributed network that is both anatomically and functionally interconnected with the hippocampus. Angular head velocity, head direction, and grid cells are among some of the additional spatial neural correlates. The importance of these different cells and how they function interdependently to subserve navigation is reviewed below.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dumontNeuralCorrelatesNavigation2015.md}
}

@article{dunnCorrelationsFunctionalConnections2014,
  title = {Correlations and Functional Connections in a Population of Grid Cells},
  author = {Dunn, Benjamin and M{\o}rreaunet, Maria and Roudi, Yasser},
  year = {2014},
  month = apr,
  doi = {10.1371/journal.pcbi.1004052},
  abstract = {We study the statistics of spike trains of simultaneously recorded grid cells in freely behaving rats. We evaluate pairwise correlations between these cells and, using a generalized linear model (kinetic Ising model), study their functional connectivity. Even when we account for the covariations in firing rates due to overlapping fields, both the pairwise correlations and functional connections decay as a function of the shortest distance between the vertices of the spatial firing pattern of pairs of grid cells, i.e. their phase difference. The functional connectivity takes positive values between cells with nearby phases and approaches zero or negative values for larger phase differences. We also find similar results when, in addition to correlations due to overlapping fields, we account for correlations due to theta oscillations and head directional inputs. The inferred connections between neurons can be both negative and positive regardless of whether the cells share common spatial firing characteristics, that is, whether they belong to the same modules, or not. The mean strength of these inferred connections is close to zero, but the strongest inferred connections are found between cells of the same module. Taken together, our results suggest that grid cells in the same module do indeed form a local network of interconnected neurons with a functional connectivity that supports a role for attractor dynamics in the generation of the grid pattern.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/C8MDQTPR/Dunn, Mørreaunet, Roudi - 2014 - Correlations and functional connections in a population of grid cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@dunnCorrelationsFunctionalConnections2014.md}
}

@article{duranthonMaximalRelevanceOptimal2021,
  title = {Maximal Relevance and Optimal Learning Machines},
  author = {Duranthon, O. and Marsili, M. and Xie, R.},
  year = {2021},
  month = mar,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2021},
  number = {3},
  pages = {033409},
  publisher = {{IOP Publishing}},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/abe6ff},
  abstract = {We explore the hypothesis that learning machines extract representations of maximal relevance, where the relevance is defined as the entropy of the energy distribution of the internal representation. We show that the mutual information between the internal representation of a learning machine and the features that it extracts from the data is bounded from below by the relevance. This motivates our study of models with maximal relevance\textemdash that we call optimal learning machines\textemdash as candidates of maximally informative representations. We analyse how the maximisation of the relevance is constrained both by the architecture of the model used and by the available data, in practical cases. We find that sub-extensive features that do not affect the thermodynamics of the model, may affect significantly learning performance, and that criticality enhances learning performance, but the existence of a critical point is not a necessary condition. On specific learning tasks, we find that (i) the maximal values of the likelihood are achieved by models with maximal relevance, (ii) internal representations approach the maximal relevance that can be achieved in a finite dataset and (iii) learning is associated with a broadening of the spectrum of energy levels of the internal representation, in agreement with the maximum relevance hypothesis.},
  langid = {english},
  keywords = {relevance,Sapta,Spinor},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Duranthon et al_2021_Maximal relevance and optimal learning machines.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@duranthonMaximalRelevanceOptimal2021.md}
}

@techreport{ecksteinHoppingBetheLattice2005,
  title = {Hopping on the {{Bethe}} Lattice: {{Exact}} Results for Densities of States and Dynamical Mean-Field Theory},
  author = {Eckstein, Martin and Kollar, Marcus and Byczuk, Krzysztof and Vollhardt, Dieter},
  year = {2005},
  abstract = {We derive an operator identity which relates tight-binding Hamiltonians with arbitrary hopping on the Bethe lattice to the Hamiltonian with nearest-neighbor hopping. This provides an exact expression for the density of states (DOS) of a non-interacting quantum-mechanical particle for any hopping. We present analytic results for the DOS corresponding to hopping between nearest and next-nearest neighbors, and also for exponentially decreasing hopping amplitudes. Conversely it is possible to construct a hopping Hamiltonian on the Bethe lattice for any given DOS. These methods are based only on the so-called distance regularity of the infinite Bethe lattice, and not on the absence of loops. Results are also obtained for the triangular Husimi cactus, a recursive lattice with loops. Furthermore we derive the exact self-consistency equations arising in the context of dynamical mean-field theory, which serve as a starting point for studies of Hubbard-type models with frustration.},
  keywords = {0210Ox,0550+q,7127+a,numbers: 7110Fd},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KVUWH6GM/Eckstein et al. - 2005 - Hopping on the Bethe lattice Exact results for densities of states and dynamical mean-field theory.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ecksteinHoppingBetheLattice2005.md}
}

@article{edelmanNaturalizingConsciousnessTheoretical2003,
  title = {Naturalizing Consciousness: {{A}} Theoretical Framework},
  author = {Edelman, Gerald M.},
  year = {2003},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {100},
  number = {9},
  pages = {5520--5524},
  doi = {10.1073/pnas.0931349100},
  abstract = {Consciousness has a number of apparently disparate properties, some of which seem to be highly complex and even inaccessible to outside observation. To place these properties within a biological framework requires a theory based on a set of evolutionary and developmental principles. This paper describes such a theory, which aims to provide a unifying account of conscious phenomena.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RY3XIPFV/Edelman - 2003 - Naturalizing consciousness A theoretical framework.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TJ9PA2AI/Edelman - 2003 - Naturalizing consciousness A theoretical framework(2).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@edelmanNaturalizingConsciousnessTheoretical2003.md}
}

@techreport{eggarterPHYSICALREVIEW,
  title = {{{PHYSICAL RE VIEW B Cayley}} t Ees, the {{Ismg}} Problem, and the Thermodymtmc H\textasciitilde t},
  author = {Eggarter, T P},
  abstract = {Proofs have been given that the Bethe-Peierls approximation solves exactly the Ising problem on a Cayley tree. For a tree with coordination number y \& 2, the approximation predicts, among other things, a phase transition in zero field at T, = 2J[hi[y/(y-2)][ ', with a discontinuity in the specific heat. On the other hand, the partition function in zero field can be calculated exactly and turns out to be analytic for all T. TQs paradox is analyzed and resolved. The transition occurring on a Cayley tree is found not to be of the type usually studied in thermodynamics. In a well-known review article on cooperative phenomena, Domb gave a proof that the Bethe-Peierls approximation solves exactly the Ising problem on a Cayley tree. This is a connected lattice in which each site has the same number y of nearest neighbors, and in which there exist no closed loops. The Cayley tree has since then also received the name of "Bethe lattice. " Figure 1 represents a portion of such a lattice for the case y-1=K= 3. Domb's proof is based on an expansion in Mayer diagrams given by Rushbrooke and Scoins, and on the observation that the Bethe-Peierls approximation is obtained by summing all diagrams involving no closed configurations. More recently, Wheeler and Widom3 proved a similar result for the lattice solution with an arbitrary number of components. Because of the mathematical similarity between the Ising problem and the lattice solution, their result was a confirmation and generalization of Domb's result. The above seemed to be a well-established result, frequently quoted in the literature. It was therefore a surprise for the present author to realize that the partition function in zero magnetic field can be calculated explicitly and is analytic for all T. The calculation can be done as follows: consider the usual Ising Hamiltonian 0=-J 4 0;0\&, J \&0 (g sS) where each'0; takes the values +1, and the symbol (i,j) means that the summation is over all pairs of nearest neighbors. We consider a, Cayley tree and choose an arbitrary site 0 which we call the origin; let the corresponding spin be op. We associate with each bond \textasciitilde{} a variable 8 =-cr"cr"where r and s a,re the atoms at the end of that bond. It is clear that the (8,) are all independent (since there are no closed loops), that each 8 takes only the values a I, and that the set (os, (8,j) is a complete set of coordinates in the sense that to each configuration (o;f there corresponds a unique (oc, (8)) and vice versa. The partition function is: Nb Z(p)= Z e a=2 Z \textasciitilde " Z e r\textasciitilde se =Z II (esr+e sr\vphantom\{\} =Z (2coshpg)"s=2(2coshpg) s, (2) all states (yp 8 1 b 'o (yp where Nb is the total number of bonds on the tree. It is also easy to see that Nb = N,-1 (N, = number of sites). In fact, for each site i o, iithere is a unique self-avoiding path leading from i to 0. If we associate with each site i \textasciitilde 0, the first bond along this path, we have a one to one correspondence between sites i 0 and bonds; thus N,-1 Therefore, in the thermodynamic limit N,-\textasciitilde{} the free energy per atom is f= lim-=-lim-lnZ =-kTln(e +e \vphantom\{\} (3\vphantom\{\}},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DLFZLYHI/Eggarter - Unknown - PHYSICAL RE VIEW B Cayley t ees, the Ismg problem, and the thermodymtmc h~t.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@eggarterPHYSICALREVIEW.md}
}

@article{eggerLocalAxonalConduction2020,
  title = {Local {{Axonal Conduction Shapes}} the {{Spatiotemporal Properties}} of {{Neural Sequences}}},
  author = {Egger, R. and Tupikov, Y. and Elmaleh, M. and Katlowitz, Kalman A. and Benezra, Sam E. and Picardo, M. and Moll, Felix W. and Kornfeld, J. and Jin, D. and Long, M.},
  year = {2020},
  journal = {Cell},
  doi = {10.1016/j.cell.2020.09.019},
  abstract = {Semantic Scholar extracted view of "Local Axonal Conduction Shapes the Spatiotemporal Properties of Neural Sequences" by R. Egger et al.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TUWBDZ9Q/Egger et al_2020_Local Axonal Conduction Shapes the Spatiotemporal Properties of Neural Sequences.pdf}
}

@article{eggerNeuralCircuitModel2019,
  title = {A Neural Circuit Model for Human Sensorimotor Timing},
  author = {Egger, Seth W. and Le, N. M. and Jazayeri, M.},
  year = {2019},
  journal = {bioRxiv},
  doi = {10.1101/712141},
  abstract = {A circuit-level model is developed that provides insight into how the brain coordinates movement times with expected and unexpected temporal events in the domain of sensorimotor timing and shows how recurrent interactions in a simple and modular neural circuit could create the dynamics needed to control temporal aspects of behavior. Humans can rapidly and flexibly coordinate their movements with external stimuli. Theoretical considerations suggest that this flexibility can be understood in terms of how sensory responses reconfigure the neural circuits that control movements. However, because external stimuli can occur at unexpected times, it is unclear how the corresponding sensory inputs can be used to exert flexible control over the ongoing activity of recurrent neural circuits. Here, we tackle this problem in the domain of sensorimotor timing and develop a circuit-level model that provides insight into how the brain coordinates movement times with expected and unexpected temporal events. The model consists of two interacting modules, a motor planning module that controls movement times and a sensory anticipation module that anticipates external events. Both modules harbor a reservoir of latent dynamics and their interaction forms a control system whose output is adjusted adaptively to minimize timing errors. We show that the model's output matches human behavior in a range of tasks including time interval production, periodic production, synchronization/continuation, and Bayesian time interval reproduction. These results demonstrate how recurrent interactions in a simple and modular neural circuit could create the dynamics needed to control temporal aspects of behavior.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5P4AA3UW/Egger et al_2019_A neural circuit model for human sensorimotor timing.pdf}
}

@article{einevollReliableSpiketrainRecordings2012,
  title = {Towards Reliable Spike-Train Recordings from Thousands of Neurons with Multielectrodes},
  author = {Einevoll, Gaute T and Franke, Felix and Hagen, Espen and Pouzat, Christophe and Harris, Kenneth D},
  year = {2012},
  month = feb,
  journal = {Current Opinion in Neurobiology},
  volume = {22},
  number = {1},
  pages = {11--17},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2011.10.001},
  abstract = {The new generation of silicon-based multielectrodes comprising hundreds or more electrode contacts offers unprecedented possibilities for simultaneous recordings of spike trains from thousands of neurons. Such data will not only be invaluable for finding out how neural networks in the brain work, but will likely be important also for neural prosthesis applications. This opportunity can only be realized if efficient, accurate and validated methods for automatic spike sorting are provided. In this review we describe some of the challenges that must be met to achieve this goal, and in particular argue for the critical need of realistic model data to be used as ground truth in the validation of spike-sorting algorithms.},
  pmcid = {PMC3314330},
  pmid = {22023727},
  keywords = {mea},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IESTCDVQ/Einevoll et al_2012_Towards reliable spike-train recordings from thousands of neurons with.pdf;C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@einevollReliableSpiketrainRecordings2012.md}
}

@misc{elghaouGRAPHICALMODELSENATE,
  title = {{{GRAPHICAL MODEL OF SENATE VOTING}}. {{http://www.eecs.berkeley.edu/\textasciitilde elghaoui/}} {{StatNews}}/Ex\_senate.Html},
  author = {El Ghaou, L.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@elghaouGRAPHICALMODELSENATE.md}
}

@article{elstonCortexCognitionCell2003,
  title = {Cortex, {{Cognition}} and the {{Cell}}: {{New Insights}} into the {{Pyramidal Neuron}} and {{Prefrontal Function}}},
  author = {Elston, Guy N.},
  year = {2003},
  journal = {Cerebral Cortex},
  volume = {13},
  number = {11},
  pages = {1124--1138},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhg093},
  abstract = {Arguably the most complex cortical functions are seated in human cognition, the how and why of which have been debated for centuries by theologians, philosophers and scientists alike. In his best-selling book, An Astonishing Hypothesis: A Scientific Search for the Soul, Francis Crick refined the view that these qualities are deter-mined solely by cortical cells and circuitry. Put simply, cognition is nothing more, or less, than a biological function. Accepting this to be the case, it should be possible to identify the mechanisms that subserve cognitive processing. Since the pioneering studies of Lorent de N\'o and Hebb, and the more recent studies of Fuster, Miller and Goldman-Rakic, to mention but a few, much attention has been focused on the role of persistent neural activity in cognitive processes. Application of modern technologies and modelling tech-niques has led to new hypotheses about the mechanisms of persistent activity. Here I focus on how regional variations in the pyramidal cell phenotype may determine the complexity of cortical circuitry and, in turn, influence neural activity. Data obtained from thousands of individually injected pyramidal cells in sensory, motor, association and executive cortex reveal marked differences in the numbers of putative excitatory inputs received by these cells. Pyra-midal cells in prefrontal cortex have, on average, up to 23 times more dendritic spines than those in the primary visual area. I propose that without these specializations in the structure of pyramidal cells, and the circuits they form, human cognitive processing would not have evolved to its present state. I also present data from both New World and Old World monkeys that show varying degrees of complexity in the pyramidal cell phenotype in their prefrontal cortices, suggesting that cortical circuitry and, thus, cognitive styles are evolving inde-pendently in different species.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2THIZKPK/Elston - 2003 - Cortex, Cognition and the Cell New Insights into the Pyramidal Neuron and Prefrontal Function.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@elstonCortexCognitionCell2003.md}
}

@article{epsteinCognitiveMapHumans2017,
  title = {The Cognitive Map in Humans: Spatial Navigation and Beyond},
  shorttitle = {The Cognitive Map in Humans},
  author = {Epstein, Russell A. and Patai, Eva Zita and Julian, Joshua B. and Spiers, Hugo J.},
  year = {2017},
  month = oct,
  journal = {Nature Neuroscience},
  volume = {20},
  number = {11},
  pages = {1504--1513},
  issn = {1546-1726},
  doi = {10.1038/nn.4656},
  abstract = {The 'cognitive map' hypothesis proposes that brain builds a unified representation of the spatial environment to support memory and guide future action. Forty years of electrophysiological research in rodents suggest that cognitive maps are neurally instantiated by place, grid, border and head direction cells in the hippocampal formation and related structures. Here we review recent work that suggests a similar functional organization in the human brain and yields insights into how cognitive maps are used during spatial navigation. Specifically, these studies indicate that (i) the human hippocampus and entorhinal cortex support map-like spatial codes, (ii) posterior brain regions such as parahippocampal and retrosplenial cortices provide critical inputs that allow cognitive maps to be anchored to fixed environmental landmarks, and (iii) hippocampal and entorhinal spatial codes are used in conjunction with frontal lobe mechanisms to plan routes during navigation. We also discuss how these three basic elements of cognitive map based navigation-spatial coding, landmark anchoring and route planning-might be applied to nonspatial domains to provide the building blocks for many core elements of human thought.},
  langid = {english},
  pmcid = {PMC6028313},
  pmid = {29073650},
  keywords = {Brain,Brain Mapping,Cognition,Humans,Spatial Navigation},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Epstein et al_2017_The cognitive map in humans.pdf}
}

@article{erdosDigitalObjectIdentifier2012,
  title = {Digital {{Object Identifier}} ( {{Spectral Statistics}} of {{Erd}}\H{} Os-{{R\'enyi Graphs II}}: {{Eigenvalue Spacing}} and the {{Extreme Eigenvalues}}},
  author = {Erd{\H{}} Os, L{\'a}szl{\'o} and Knowles, Antti and Yau, Horng-Tzer and Yin, Jun},
  year = {2012},
  journal = {Commun. Math. Phys},
  volume = {314},
  pages = {587--640},
  doi = {10.1007/s00220-012-1527-7},
  abstract = {We consider the ensemble of adjacency matrices of Erd\H{} os-R\'enyi random graphs, i.e. graphs on N vertices where every edge is chosen independently and with probability p {$\equiv$} p(N). We rescale the matrix so that its bulk eigenvalues are of order one. Under the assumption pN N 2/3 , we prove the universality of eigenvalue distributions both in the bulk and at the edge of the spectrum. More precisely, we prove (1) that the eigenvalue spacing of the Erd\H{} os-R\'enyi graph in the bulk of the spectrum has the same distribution as that of the Gaussian orthogonal ensemble; and (2) that the second largest eigenvalue of the Erd\H{} os-R\'enyi graph has the same distribution as the largest eigenvalue of the Gaussian orthogonal ensemble. As an application of our method, we prove the bulk universality of generalized Wigner matrices under the assumption that the matrix entries have at least 4 + {$\epsilon$} moments.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EE825SRQ/Erd˝ Os et al. - 2012 - Digital Object Identifier ( Spectral Statistics of Erd˝ os-Rényi Graphs II Eigenvalue Spacing and the Extreme Ei.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@erdosDigitalObjectIdentifier2012.md}
}

@article{erdosRandomGraphs1959,
  title = {On Random Graphs {{I}}},
  author = {Erd{\"{\cyrchar\cyro}}s, Paul and R{\.e}nyi, Alfr{\'e}d},
  year = {1959},
  journal = {Publicationes Mathematicae},
  volume = {6},
  pages = {290--297},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7MNAZPSD/Erdӧs, Rėnyi - 1959 - On random graphs I.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@erdosRandomGraphs1959.md}
}

@misc{eshraghianIntroduction2022,
  title = {Introduction},
  author = {Eshraghian, Jason},
  year = {2022},
  month = jun,
  abstract = {Deep and online learning with spiking neural networks in Python},
  keywords = {machine-learning,neural-networks,neuron-models,neuroscience,python,pytorch,snn,spike,spiking,spiking-neural-networks},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/eshraghianIntroduction2022-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JTNGF6QG/latest.html}
}

@misc{eshraghianTrainingSpikingNeural2022,
  title = {Training {{Spiking Neural Networks Using Lessons From Deep Learning}}},
  author = {Eshraghian, Jason K. and Ward, Max and Neftci, Emre and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D.},
  year = {2022},
  month = jan,
  number = {arXiv:2109.12894},
  eprint = {2109.12894},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.12894},
  abstract = {The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This paper serves as a tutorial and perspective showing how to apply the lessons learnt from several decades of research in deep learning, gradient descent, backpropagation and neuroscience to biologically plausible spiking neural neural networks. We also explore the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to spiking neural networks; the subtle link between temporal backpropagation and spike timing dependent plasticity, and how deep learning might move towards biologically plausible online learning. Some ideas are well accepted and commonly used amongst the neuromorphic engineering community, while others are presented or justified for the first time here. A series of companion interactive tutorials complementary to this paper using our Python package, snnTorch, are also made available: https://snntorch.readthedocs.io/en/latest/tutorials/index.html},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,SNN},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IL5PZMUT/Eshraghian et al_2022_Training Spiking Neural Networks Using Lessons From Deep Learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LCCVGGW4/2109.html}
}

@misc{espeholtSEEDRLScalable2020,
  title = {{{SEED RL}}: {{Scalable}} and {{Efficient Deep-RL}} with {{Accelerated Central Inference}}},
  shorttitle = {{{SEED RL}}},
  author = {Espeholt, Lasse and Marinier, Rapha{\"e}l and Stanczyk, Piotr and Wang, Ke and Michalski, Marcin},
  year = {2020},
  month = feb,
  number = {arXiv:1910.06591},
  eprint = {1910.06591},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We present a modern scalable reinforcement learning agent called SEED (Scalable, Efficient Deep-RL). By effectively utilizing modern accelerators, we show that it is not only possible to train on millions of frames per second but also to lower the cost of experiments compared to current methods. We achieve this with a simple architecture that features centralized inference and an optimized communication layer. SEED adopts two state of the art distributed algorithms, IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), and is evaluated on Atari-57, DeepMind Lab and Google Research Football. We improve the state of the art on Football and are able to reach state of the art on Atari-57 three times faster in wall-time. For the scenarios we consider, a 40\% to 80\% cost reduction for running experiments is achieved. The implementation along with experiments is open-sourced so results can be reproduced and novel ideas tried out.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,DRL,git,Statistics - Machine Learning},
  note = {Comment: New version that includes changes made during the ICLR 2020 review process (https://openreview.net/forum?id=rkgvXlrKwH)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AREKLLDY/Espeholt et al. - 2020 - SEED RL Scalable and Efficient Deep-RL with Accel.pdf}
}

@misc{ExperiencedependentModulationVisual,
  title = {Experience-Dependent Modulation of the Visual Evoked Potential: {{Testing}} Effect Sizes, Retention over Time, and Associations with Age in 415 Healthy Individuals - {{ScienceDirect}}},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S1053811920307886},
  keywords = {\#phdapp},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/78SM3J6L/S1053811920307886.html}
}

@article{eyherabideBurstFiringNeural2008,
  title = {Burst Firing Is a Neural Code in an Insect Auditory System},
  author = {Eyherabide, Hugo G.},
  year = {2008},
  month = jul,
  journal = {Frontiers in Computational Neuroscience},
  volume = {2},
  publisher = {{Frontiers Media SA}},
  doi = {10.3389/neuro.10.003.2008},
  abstract = {Various classes of neurons alternate between high-frequency discharges and silent intervals. This phenomenon is called burst firing. To analyze burst activity in an insect system, grasshopper auditory receptor neurons were recorded in vivo for several distinct stimulus types. The experimental data show that both burst probability and burst characteristics are strongly influenced by temporal modulations of the acoustic stimulus. The tendency to burst, hence, is not only determined by cell-intrinsic processes, but also by their interaction with the stimulus time course. We study this interaction quantitatively and observe that bursts containing a certain number of spikes occur shortly after stimulus deflections of specific intensity and duration. Our findings suggest a sparse neural code where information about the stimulus is represented by the number of spikes per burst, irrespective of the detailed interspike-interval structure within a burst. This compact representation cannot be interpreted as a firing-rate code. An information-theoretical analysis reveals that the number of spikes per burst reliably conveys information about the amplitude and duration of sound transients, whereas their time of occurrence is reflected by the burst onset time. The investigated neurons encode almost half of the total transmitted information in burst activity.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4W54HHSB/Eyherabide - 2008 - Burst firing is a neural code in an insect auditory system.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@eyherabideBurstFiringNeural2008.md}
}

@article{ezakiEnergyLandscapeAnalysis2017,
  title = {Energy Landscape Analysis of Neuroimaging Data},
  author = {Ezaki, T and Watanabe, T and Ohzeki, M and Masuda, N},
  year = {2017},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {375},
  doi = {10.1098/rsta.2016.0287},
  abstract = {One contribution of 14 to a theme issue 'Mathematical methods in medicine: neuroscience, cardiology and pathology' .},
  keywords = {Boltzmann machine,Ising model,statistical physics,statistical physics Keywords: functional magnetic resonance imaging,Subject Areas: medical computing},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TKY7ULA9/Ezaki et al. - 2017 - Energy landscape analysis of neuroimaging data.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ezakiEnergyLandscapeAnalysis2017.md}
}

@article{faberComputationConcentratedRich2019,
  title = {Computation Is Concentrated in Rich Clubs of Local Cortical Networks},
  author = {Faber, Samantha P. and Timme, Nicholas M. and Beggs, John M. and Newman, Ehren L.},
  year = {2019},
  month = feb,
  journal = {Network Neuroscience},
  volume = {3},
  number = {2},
  pages = {384--404},
  issn = {2472-1751},
  doi = {10.1162/netn_a_00069},
  abstract = {To understand how neural circuits process information, it is essential to identify the relationship between computation and circuit organization. Rich clubs, highly interconnected sets of neurons, are known to propagate a disproportionate amount of information within cortical circuits. Here, we test the hypothesis that rich clubs also perform a disproportionate amount of computation. To do so, we recorded the spiking activity of on average {$\sim$}300 well-isolated individual neurons from organotypic cortical cultures. We then constructed weighted, directed networks reflecting the effective connectivity between the neurons. For each neuron, we quantified the amount of computation it performed based on its inputs. We found that rich-club neurons compute {$\sim$}160\% more information than neurons outside of the rich club. The amount of computation performed in the rich club was proportional to the amount of information propagation by the same neurons. This suggests that in these circuits, information propagation drives computation. In total, our findings indicate that rich-club organization in effective cortical circuits supports not only information propagation but also neural computation.Here we answer the question of whether rich-club organization in functional networks of cortical circuits supports neural computation. To do so, we combined network analysis with information theoretic tools to analyze the spiking activity of hundreds of neurons recorded from organotypic cultures of mouse somatosensory cortex. We found that neurons in rich clubs computed significantly more than neurons outside of rich clubs, suggesting that rich clubs do support computation in cortical circuits. Indeed, the amount of computation that we found in the rich clubs was proportional to the amount of information they propagate, suggesting that in these circuits, information propagation drives computation.},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Faber et al_2019_Computation is concentrated in rich clubs of local cortical networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XCCNZAEW/Computation-is-concentrated-in-rich-clubs-of-local.html}
}

@misc{fangwei123456SpikingJelly2022,
  title = {{{SpikingJelly}}},
  author = {{fangwei123456}},
  year = {2022},
  month = jun,
  abstract = {SpikingJelly is an open-source deep learning framework for Spiking Neural Network (SNN) based on PyTorch.},
  keywords = {deep-learning,dvs,machine-learning,pytorch,snn,spiking-neural-networks},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/fangwei123456SpikingJelly2022-zotero.md}
}

@article{ferrariRandomMaximumEntropy2017,
  title = {Random versus Maximum Entropy Models of Neural Population Activity},
  author = {Ferrari, Ulisse and Obuchi, Tomoyuki and Mora, Thierry},
  year = {2017},
  month = apr,
  journal = {Physical Review E},
  volume = {95},
  number = {4},
  pages = {042321--042321},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.95.042321},
  abstract = {The principle of maximum entropy provides a useful method for inferring statistical mechanics models from observations in correlated systems, and is widely used in a variety of fields where accurate data are available. While the assumptions underlying maximum entropy are intuitive and appealing, its adequacy for describing complex empirical data has been little studied in comparison to alternative approaches. Here, data from the collective spiking activity of retinal neurons is reanalyzed. The accuracy of the maximum entropy distribution constrained by mean firing rates and pairwise correlations is compared to a random ensemble of distributions constrained by the same observables. For most of the tested networks, maximum entropy approximates the true distribution better than the typical or mean distribution from that ensemble. This advantage improves with population size, with groups as small as eight being almost always better described by maximum entropy. Failure of maximum entropy to outperform random models is found to be associated with strong correlations in the population.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/P36LD9SI/Ferrari, Obuchi, Mora - 2017 - Random versus maximum entropy models of neural population activity.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ferrariRandomMaximumEntropy2017.md}
}

@article{figurnovImplicitReparameterizationGradients2018,
  title = {Implicit {{Reparameterization Gradients}}},
  author = {Figurnov, Michael and Mohamed, Shakir and Mnih, Andriy},
  year = {2018},
  month = may,
  abstract = {By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/R2T8MK4R/Figurnov, Mohamed, Mnih - 2018 - Implicit Reparameterization Gradients.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@figurnovImplicitReparameterizationGradients2018.md}
}

@article{fisherReinforcementDeterminesTiming2017,
  title = {Reinforcement Determines the Timing Dependence of Corticostriatal Synaptic Plasticity in Vivo},
  author = {Fisher, S. D. and Robertson, P. B. and Black, M. J. and Redgrave, P. and Sagar, M. A. and Abraham, W. C. and Reynolds, J. N. J.},
  year = {2017},
  month = aug,
  journal = {Nature Communications},
  volume = {8},
  number = {1},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  abstract = {Plasticity at synapses between the cortex and striatum is considered critical for learning novel actions. However, investigations of spike-timing-dependent plasticity (STDP) at these synapses have been performed largely in brain slice preparations, without consideration of physiological reinforcement signals. This has led to conflicting findings, and hampered the ability to relate neural plasticity to behavior. Using intracellular striatal recordings in intact rats, we show here that pairing presynaptic and postsynaptic activity induces robust Hebbian bidirectional plasticity, dependent on dopamine and adenosine signaling. Such plasticity, however, requires the arrival of a reward-conditioned sensory reinforcement signal within 2\,s of the STDP pairing, thus revealing a timing-dependent eligibility trace on which reinforcement operates. These observations are validated with both computational modeling and behavioral testing. Our results indicate that Hebbian corticostriatal plasticity can be induced by classical reinforcement learning mechanisms, and might be central to the acquisition of novel actions.},
  copyright = {cc\_by\_4},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Fisher et al_2017_Reinforcement determines the timing dependence of corticostriatal synaptic.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GLYVAVZC/137938.html}
}

@techreport{floridiFinalVersionTrends,
  title = {Final Version {{Trends}} in the {{Philosophy}} of {{Information}}},
  author = {Floridi, Luciano},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UKTZRQHD/Floridi - Unknown - Final version Trends in the Philosophy of Information.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@floridiFinalVersionTrends.md}
}

@article{fornitoConnectomicsBrainDisorders2015,
  title = {The Connectomics of Brain Disorders},
  author = {Fornito, Alex and Zalesky, Andrew and Breakspear, Michael},
  year = {2015},
  journal = {Nature Reviews Neuroscience},
  volume = {16},
  number = {3},
  pages = {159--172},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048 (Electronic)\textbackslash r1471-003X (Linking)},
  doi = {10.1038/nrn3901},
  abstract = {Pathological perturbations of the brain are rarely confined to a single locus; instead, they often spread via axonal pathways to influence other regions. Patterns of such disease propagation are constrained by the extraordinarily complex, yet highly organized, topology of the underlying neural architecture; the so-called connectome. Thus, network organization fundamentally influences brain disease, and a connectomic approach grounded in network science is integral to understanding neuropathology. Here, we consider how brain-network topology shapes neural responses to damage, highlighting key maladaptive processes (such as diaschisis, transneuronal degeneration and dedifferentiation), and the resources (including degeneracy and reserve) and processes (such as compensation) that enable adaptation. We then show how knowledge of network topology allows us not only to describe pathological processes but also to generate predictive models of the spread and functional consequences of brain disease.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XYH3RJ6T/Fornito, Zalesky, Breakspear - 2015 - The connectomics of brain disorders.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@fornitoConnectomicsBrainDisorders2015.md}
}

@article{francescasargoliniConjunctiveRepresentationPosition2006,
  title = {Conjunctive {{Representation}} of {{Position}}, {{Direction}}, and {{Velocity}} in {{Entorhinal Cortex}}},
  author = {{Francesca Sargolini} and {1} and {Marianne Fyhn} and {1} and {Torkel Hafting} and {1} and {Bruce L. McNaughton} and 1, 2 and {Menno P. Witter} and 1, 3 and {May-Britt Moser} and {1} and {Edvard I. Moser} and {1}},
  year = {2006},
  journal = {Science},
  number = {May},
  pages = {758--763},
  doi = {10.1126/science.1125572},
  abstract = {Grid cells in the medial entorhinal cortex (MEC) are part of an environment-independent spatial\textbackslash r\textbackslash ncoordinate system. To determine how information about location, direction, and distance is\textbackslash r\textbackslash nintegrated in the grid-cell network, we recorded from each principal cell layer of MEC in rats that\textbackslash r\textbackslash nexplored two-dimensional environments. Whereas layer II was predominated by grid cells, grid cells\textbackslash r\textbackslash ncolocalized with head-direction cells and conjunctive grid\textbackslash r\textbackslash n \textbackslash r\textbackslash nhead-direction cells in the deeper\textbackslash r\textbackslash nlayers. All cell types were modulated by running speed. The conjunction of positional, directional,\textbackslash r\textbackslash nand translational information in a single MEC cell type may enable grid coordinates to be updated\textbackslash r\textbackslash nduring self-motion\textendash based navigation.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LW2DT2EX/Francesca Sargolini et al. - 2006 - Conjunctive Representation of Position, Direction, and Velocity in Entorhinal Cortex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@francescasargoliniConjunctiveRepresentationPosition2006.md}
}

@article{francois-lavetIntroductionDeepReinforcement2018,
  title = {An {{Introduction}} to {{Deep Reinforcement Learning}}},
  author = {{Francois-Lavet}, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  year = {2018},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {11},
  number = {3-4},
  eprint = {1811.12560},
  eprinttype = {arxiv},
  pages = {219--354},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000071},
  abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Q36IYQIF/Francois-Lavet et al_2018_An Introduction to Deep Reinforcement Learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FTEWBCPV/1811.html}
}

@techreport{franzFastAccurateAlgorithm2019,
  title = {A Fast and Accurate Algorithm for Inferring Sparse {{Ising}} Models via Parameters Activation to Maximize the Pseudo-Likelihood},
  author = {Franz, Silvio and {Ricci-Tersenghi}, Federico and Rocchi, Jacopo},
  year = {2019},
  abstract = {We propose a new algorithm to learn the network of the interactions of pairwise Ising models. The algorithm is based on the pseudo-likelihood method (PLM), that has already been proven to efficiently solve the problem in a large variety of cases. Our present implementation is particularly suitable to address the case of sparse underlying topologies and it is based on a careful search of the most important parameters in their high dimensional space. We call this algorithm Parameters Activation to Maximize Pseudo-Likelihood (PAMPL). Numerical tests have been performed on a wide class of models such as random graphs and finite dimensional lattices with different type of couplings, both ferromagnetic and spin glasses. These tests show that PAMPL improves the performances of the fastest existing algorithms. The Ising model is a graphical model whose parameters \{J ij , h i \} can be tuned in order to describe stationary distributions of binary variables, s i , according to the weight P (s) {$\sim$} exp i},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UTBJLYD2/Franz, Ricci-Tersenghi, Rocchi - 2019 - A fast and accurate algorithm for inferring sparse Ising models via parameters activation to (2).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@franzFastAccurateAlgorithm2019.md}
}

@techreport{fredkinSOUL,
  title = {{{ON THE SOUL}} *},
  author = {Fredkin, Ed},
  abstract = {The following definition of the soul is taken from the Encyclopaedia Britannica, MDCCLXXI (1771 AD), Volume III, pages 618-619. Soul, a spiritual substance, which animates the bodies of living creatures: it is the principle of life and activity within them. Various have been the opinions of philosophers concerning the substance of the human soul. The Cartesians make thinking the essence of the soul. Others again hold, that man is endowed with three kinds of souls, viz. The rational, which is purely spiritual, and infused by the immediate inspiration of God; the irrational, or sensitive, which is common to man and brutes; and lastly, the vegetative soul, or principle of growth and nutrition. That the soul is an immaterial substance appears from hence, that its primary operations of willing and thinking have not only no connection with the known properties of body, but seem plainly inconsistent with some of its most essential qualities. For the mind discovers no relation between thinking and the motion and arrangements of parts. As to the immortality of the human soul, the arguments to prove it may be reduced to the following heads: 1. The nature of the soul itself, its desires, sense of moral good and evil, gradual increase in knowledge and perfection, etc. 2. The moral attributes of God. Under the former of these heads it is urged, that the soul, being an immaterial intelligent substance, does not depend on the body for its existence; and therefore may, nay, and must, exist after the dissolution of the body, unless annihilated by the same power which gave it a being at first. This argument, especially if the infinite capacity of the soul, its strong desire after immortality, its rational activity and advancement towards perfection, be likewise considered, will appear perfectly conclusive to men of a philosophical turn; because nature, or rather the God of nature, does nothing in vain. But arguments drawn from the latter head, viz. the moral attributes of the Deity, are not only better adapted to convince men unacquainted with abstract reasoning, but equally certain and conclusive with the former: for as the justice of God can never suffer the wicked to escape unpunished, nor the good to remain always unrewarded; therefore, arguments drawn from the manifest and constant prosperity of the wicked, and the frequent unhappiness of good men in this life, must convince every thinking person, that there is a future state wherein all will be set right, and God's attributes of wisdom, justice, and goodness, fully vindicated. We shall only add, that had the virtuous and conscientious part of mankind no hopes of a future state, they would be of all men most miserable: but as this is absolutely inconsistent with the moral character of the Deity, the certainty of such a state is clear to a demonstration. * In this paper, we use "soul" to mean the common, current definition, and "soul" when we mean the new definition as given in this paper.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PC9V3Z8L/Fredkin - Unknown - ON THE SOUL.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@fredkinSOUL.md}
}

@article{fredricksonKineticIsingModel1984,
  title = {Kinetic {{Ising}} Model of the Glass Transition},
  author = {Fredrickson, Glenn H. and Andersen, Hans C.},
  year = {1984},
  journal = {Physical Review Letters},
  volume = {53},
  number = {13},
  pages = {1244--1247},
  doi = {10.1103/PhysRevLett.53.1244},
  abstract = {A graph theory of single-spin-flip kinetic Ising models is developed and applied to a class of spin models with strongly cooperative dynamics. Self-consistent approximations for the spin time correlation function are presented. One of the dynamical models exhibits a glass transition with no underlying thermodynamic singularity. The approximation for the time correlation function predicts a critical temperature, below which small fluctuations from equilibrium in the thermodynamic limit cannot relax in a finite amount of time.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Q48ZRCP6/Fredrickson, Andersen - 1984 - Kinetic Ising model of the glass transition.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@fredricksonKineticIsingModel1984.md}
}

@article{freiBivariateCausalMixture2019,
  title = {Bivariate Causal Mixture Model Quantifies Polygenic Overlap between Complex Traits beyond Genetic Correlation},
  author = {Frei, Oleksandr and Holland, Dominic and Smeland, Olav B. and Shadrin, Alexey A. and Fan, Chun Chieh and Maeland, Steffen and O'Connell, Kevin S. and Wang, Yunpeng and Djurovic, Srdjan and Thompson, Wesley K. and Andreassen, Ole A. and Dale, Anders M.},
  year = {2019},
  month = jun,
  journal = {Nature Communications},
  volume = {10},
  pages = {2417},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-10310-0},
  abstract = {Accumulating evidence from genome wide association studies (GWAS) suggests an abundance of shared genetic influences among complex human traits and disorders, such as mental disorders. Here we introduce a statistical tool, MiXeR, which quantifies polygenic overlap irrespective of genetic correlation, using GWAS summary statistics. MiXeR results are presented as a Venn diagram of unique and shared polygenic components across traits. At 90\% of SNP-heritability explained for each phenotype, MiXeR estimates that 8.3\,K variants causally influence schizophrenia and 6.4\,K influence bipolar disorder. Among these variants, 6.2\,K are shared between the disorders, which have a high genetic correlation. Further, MiXeR uncovers polygenic overlap between schizophrenia and educational attainment. Despite a genetic correlation close to zero, the phenotypes share 8.3\,K causal variants, while 2.5\,K additional variants influence only educational attainment. By considering the polygenicity, discoverability and heritability of complex phenotypes, MiXeR analysis may improve our understanding of cross-trait genetic architectures., To better understand the phenotypic relationships of complex traits it~is also important to understand their genetic overlap. Here, Frei et al. develop MiXeR which uses GWAS summary statistics to evaluate the polygenic overlap between two traits irrespective of their genetic correlation.},
  pmcid = {PMC6547727},
  pmid = {31160569},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Frei et al_2019_Bivariate causal mixture model quantifies polygenic overlap between complex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@freiBivariateCausalMixture2019.md}
}

@article{fremauxNeuromodulatedSpikeTimingDependentPlasticity2016,
  title = {Neuromodulated {{Spike-Timing-Dependent Plasticity}}, and {{Theory}} of {{Three-Factor Learning Rules}}},
  author = {Fr{\'e}maux, Nicolas and Gerstner, Wulfram},
  year = {2016},
  journal = {Frontiers in Neural Circuits},
  volume = {9},
  pages = {85},
  issn = {1662-5110},
  doi = {10.3389/fncir.2015.00085},
  abstract = {Classical Hebbian learning puts the emphasis on joint pre- and postsynaptic activity, but neglects the potential role of neuromodulators. Since neuromodulators convey information about novelty or reward, the influence of neuromodulators on synaptic plasticity is useful not just for action learning in classical conditioning, but also to decide ``when'' to create new memories in response to a flow of sensory stimuli. In this review, we focus on timing requirements for pre- and postsynaptic activity in conjunction with one or several phasic neuromodulatory signals. While the emphasis of the text is on conceptual models and mathematical theories, we also discuss some experimental evidence for neuromodulation of Spike-Timing-Dependent Plasticity. We highlight the importance of synaptic mechanisms in bridging the temporal gap between sensory stimulation and neuromodulatory signals, and develop a framework for a class of neo-Hebbian three-factor learning rules that depend on presynaptic activity, postsynaptic variables as well as the influence of neuromodulators.},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@fremauxNeuromodulatedSpikeTimingDependentPlasticity2016.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/fremauxNeuromodulatedSpikeTimingDependentPlasticity2016-mdnotes.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Frémaux_Gerstner_2016_Neuromodulated Spike-Timing-Dependent Plasticity, and Theory of Three-Factor.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@fremauxNeuromodulatedSpikeTimingDependentPlasticity2016.md}
}

@article{freyMicroelectronicSystemHighresolution2009,
  title = {Microelectronic System for High-Resolution Mapping of Extracellular Electric Fields Applied to Brain Slices},
  author = {Frey, U. and Egert, U. and Heer, F. and Hafizovic, S. and Hierlemann, A.},
  year = {2009},
  month = mar,
  journal = {Biosensors and Bioelectronics},
  volume = {24},
  number = {7},
  pages = {2191--2198},
  issn = {0956-5663},
  doi = {10.1016/j.bios.2008.11.028},
  abstract = {There is an enduring quest for technologies that provide \textendash{} temporally and spatially \textendash{} highly resolved information on electric neuronal or cardiac activity in functional tissues or cell cultures. Here, we present a planar high-density, low-noise microelectrode system realized in microelectronics technology that features 11,011 microelectrodes (3,150 electrodes per mm2), 126 of which can be arbitrarily selected and can, via a reconfigurable routing scheme, be connected to on-chip recording and stimulation circuits. This device enables long-term extracellular electrical-activity recordings at subcellular spatial resolution and microsecond temporal resolution to capture the entire dynamics of the cellular electrical signals. To illustrate the device performance, extracellular potentials of Purkinje cells (PCs) in acute slices of the cerebellum have been analyzed. A detailed and comprehensive picture of the distribution and dynamics of action potentials (APs) in the somatic and dendritic regions of a single cell was obtained from the recordings by applying spike sorting and spike-triggered averaging methods to the collected data. An analysis of the measured local current densities revealed a reproducible sink/source pattern within a single cell during an AP. The experimental data substantiated compartmental models and can be used to extend those models to better understand extracellular single-cell potential patterns and their contributions to the population activity. The presented devices can be conveniently applied to a broad variety of biological preparations, i.e., neural or cardiac tissues, slices, or cell cultures can be grown or placed directly atop of the chips for fundamental mechanistic or pharmacological studies.},
  langid = {english},
  keywords = {Acute brain slice,CMOS-based microelectrode array,Integrated circuits,Neurochip,Purkinje cells},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DTS7TU6M/Frey et al. - 2009 - Microelectronic system for high-resolution mapping.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@freyMicroelectronicSystemHighresolution2009.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PL3IDLAH/S095656630800643X.html}
}

@article{friedmanInferringCellularNetworks2004,
  title = {Inferring {{Cellular Networks Using Probabilistic Graphical Models}}},
  author = {Friedman, Nir},
  year = {2004},
  month = feb,
  journal = {Science},
  volume = {303},
  number = {5659},
  pages = {799--805},
  publisher = {{KNAW)}},
  doi = {10.1126/science.1094068},
  abstract = {High-throughput genome-wide molecular assays, which probe cellular networks from different perspectives, have become central to molecular biology. Probabilistic graphical models are useful for extracting meaningful biological insights from the resulting data sets. These models provide a concise representation of complex cellular networks by composing simpler submodels. Procedures based on well-understood principles for inferring such models from data facilitate a model-based methodology for analysis and discovery. This methodology and its capabilities are illustrated by several recent applications to gene expression data.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZQUPPKFY/Friedman - 2004 - Inferring Cellular Networks Using Probabilistic Graphical Models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@friedmanInferringCellularNetworks2004.md}
}

@article{fristonFunctionalEffectiveConnectivity1994,
  title = {Functional and Effective Connectivity in Neuroimaging: {{A}} Synthesis},
  shorttitle = {Functional and Effective Connectivity in Neuroimaging},
  author = {Friston, Karl J.},
  year = {1994},
  journal = {Human Brain Mapping},
  volume = {2},
  number = {1-2},
  pages = {56--78},
  issn = {1097-0193},
  doi = {10.1002/hbm.460020107},
  abstract = {The brain appears to adhere to two principles of functional organization; functional segregation and functional integration. The integration within and between functionally specialized areas is mediated by functional or effective connectivity. The characterization of this sort of connectivity is an important theme in many areas of neuroscience. This article presents one approach that has been used in functional imaging. This article reviews the basic distinction between functional and effective connectivity (as the terms are used in neuroimaging) and their role in addressing several aspects of functional organization (e.g. the topography of distributed system, integration between cortical areas, time-dependent changes in connectivity and nonlinear interactions). Emphasis is placed on the points of contact between the apparently diverse applications of these concepts and in particular the central role of eigenimages or spatial modes. Although the framework that has been developed is inherently linear, it has been extended to assess nonlinear interactions among cortical areas. \textcopyright 1994 Wiley-Liss, Inc.},
  copyright = {Copyright \textcopyright{} 1994 Wiley-Liss, Inc.},
  langid = {english},
  keywords = {effective connectivity,eigenimages,fMRI,functional connectivity,modulation,multidimensional scaling,PET,spatial modes,visual,word generation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.460020107},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9ZRSQDDA/Friston - 1994 - Functional and effective connectivity in neuroimag.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@fristonFunctionalEffectiveConnectivity1994.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/B3UWP8XH/hbm.html}
}

@article{fristonStatisticalParametricMaps1994,
  title = {Statistical Parametric Maps in Functional Imaging: {{A}} General Linear Approach},
  shorttitle = {Statistical Parametric Maps in Functional Imaging},
  author = {Friston, K. J. and Holmes, A. P. and Worsley, K. J. and Poline, J.-P. and Frith, C. D. and Frackowiak, R. S. J.},
  year = {1994},
  journal = {Human Brain Mapping},
  volume = {2},
  number = {4},
  pages = {189--210},
  issn = {10659471},
  doi = {10.1002/hbm.460020402},
  langid = {english},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@fristonStatisticalParametricMaps1994.md}
}

@article{fuhsSpinGlassModel2006,
  title = {A {{Spin Glass Model}} of {{Path Integration}} in {{Rat Medial Entorhinal Cortex}}},
  author = {Fuhs, M. C.},
  year = {2006},
  journal = {Journal of Neuroscience},
  volume = {26},
  number = {16},
  pages = {4266--4276},
  issn = {1529-2401 (Electronic)\textbackslash n0270-6474 (Linking)},
  doi = {10.1523/JNEUROSCI.4353-05.2006},
  abstract = {Electrophysiological recording studies in the dorsocaudal region of medial entorhinal cortex (dMEC) of the rat reveal cells whose spatial firing fields show a remarkably regular hexagonal grid pattern (Fyhn et al., 2004; Hafting et al., 2005). We describe a symmetric, locally connected neural network, or spin glass model, that spontaneously produces a hexagonal grid of activity bumps on a two-dimensional sheet of units.Thespatial firing fields of the simulated cells closely resemble those ofdMECcells.Acollection of grids with different scales and/ororientationsformsabasis set forencodingposition. Simulationsshowthat the animal's locationcaneasilybedeterminedfromthe population activity pattern. Introducing an asymmetry in the model allows the activity bumps to be shifted in any direction, at a rate proportional to velocity, to achieve path integration. Furthermore, information about the structure of the environment can be superim- posed on the spatial position signal by modulation of the bump activity levels without significantly interfering with the hexagonal periodicity of firing fields.Ourresults support the conjecture of Hafting et al. (2005) that an attractor network indMECmaybe the source of path integration information afferent to hippocampus},
  keywords = {entorhinal cortex,head direction,hippocampus,navigation,path integration,place cells},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Y6N77N66/Fuhs - 2006 - A Spin Glass Model of Path Integration in Rat Medial Entorhinal Cortex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@fuhsSpinGlassModel2006.md}
}

@article{fujisawaBehaviordependentShorttermAssembly2008,
  title = {Behavior-Dependent Short-Term Assembly Dynamics in the Medial Prefrontal Cortex},
  author = {Fujisawa, Shigeyoshi and Amarasingham, Asohan and Harrison, Matthew T. and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2008},
  month = jul,
  journal = {Nature Neuroscience},
  volume = {11},
  number = {7},
  pages = {823--833},
  publisher = {{Nat Neurosci}},
  issn = {10976256},
  doi = {10.1038/nn.2134},
  abstract = {Although short-term plasticity is believed to play a fundamental role in cortical computation, empirical evidence bearing on its role during behavior is scarce. Here we looked for the signature of short-term plasticity in the fine-timescale spiking relationships of a simultaneously recorded population of physiologically identified pyramidal cells and interneurons, in the medial prefrontal cortex of the rat, in a working memory task. On broader timescales, sequentially organized and transiently active neurons reliably differentiated between different trajectories of the rat in the maze. On finer timescales, putative monosynaptic interactions reflected short-term plasticity in their dynamic and predictable modulation across various aspects of the task, beyond a statistical accounting for the effect of the neurons' co-varying firing rates. Seeking potential mechanisms for such effects, we found evidence for both firing pattern-dependent facilitation and depression, as well as for a supralinear effect of presynaptic coincidence on the firing of postsynaptic targets. \textcopyright{} 2008 Nature Publishing Group.},
  pmid = {18516033},
  keywords = {Action Potentials / physiology,Animal / physiology*,Animals,Asohan Amarasingham,Behavior,Biological*,doi:10.1038/nn.2134,Extramural,György Buzsáki,Interneurons / physiology,Male,MEDLINE,Memory,Models,N.I.H.,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,NCBI,Neuronal Plasticity / physiology*,Neuropsychological Tests,NIH,NLM,Non-P.H.S.,Non-U.S. Gov't,Nonlinear Dynamics*,PMC2562676,pmid:18516033,Prefrontal Cortex / cytology,Prefrontal Cortex / physiology*,Probability,PubMed Abstract,Pyramidal Cells / physiology,Rats,Research Support,Shigeyoshi Fujisawa,Short-Term / physiology,U.S. Gov't},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PSPJMDIY/Fujisawa et al. - 2008 - Behavior-dependent short-term assembly dynamics in the medial prefrontal cortex.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/T98BG9EW/Fujisawa et al. - 2008 - Behavior-dependent short-term assembly dynamics in the medial prefrontal cortex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@fujisawaBehaviordependentShorttermAssembly2008.md}
}

@article{Fujishiro,
  title = {In Vivo Neuronal Action Potential Recordings via Three-Dimensional Microscale Needle-Electrode Arrays},
  author = {Fujishiro, Akifumi and Kaneko, Hidekazu and Kawashima, Takahiro and Ishida, Makoto and Kawano, Takeshi},
  doi = {10.1038/srep04868},
  abstract = {Very fine needle-electrode arrays potentially offer both low invasiveness and high spatial resolution of electrophysiological neuronal recordings in vivo. Herein we report the penetrating and recording capabilities of silicon-growth-based three-dimensional microscale-diameter needle-electrodes arrays. The fabricated needles exhibit a circular-cone shape with a 3-mm-diameter tip and a 210-mm length. Due to the microscale diameter, our silicon needles are more flexible than other microfabricated silicon needles with larger diameters. Coating the microscale-needle-tip with platinum black results in an impedance of ,600 kV in saline with output/input signal amplitude ratios of more than 90\% at 40 Hz-10 kHz. The needles can penetrate into the whisker barrel area of a rat's cerebral cortex, and the action potentials recorded from some neurons exhibit peak-to-peak amplitudes of ,300 mV pp. These results demonstrate the feasibility of in vivo neuronal action potential recordings with a microscale needle-electrode array fabricated using silicon growth technology.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3TWD2GUS/Fujishiro et al. - Unknown - In vivo neuronal action potential recordings via three-dimensional microscale needle-electrode arrays.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9TM6DB89/Fujishiro et al. - Unknown - In vivo neuronal action potential recordings via three-dimensional microscale needle-electrode arrays.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@Fujishiro.md}
}

@article{furberSpiNNakerProject2014,
  title = {The {{SpiNNaker Project}}},
  author = {Furber, Steve B. and Galluppi, Francesco and Temple, Steve and Plana, Luis A.},
  year = {2014},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {102},
  number = {5},
  pages = {652--665},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2014.2304638},
  abstract = {The spiking neural network architecture (SpiNNaker) project aims to deliver a massively parallel million-core computer whose interconnect architecture is inspired by the connectivity characteristics of the mammalian brain, and which is suited to the modeling of large-scale spiking neural networks in biological real time. Specifically, the interconnect allows the transmission of a very large number of very small data packets, each conveying explicitly the source, and implicitly the time, of a single neural action potential or ``spike.'' In this paper, we review the current state of the project, which has already delivered systems with up to 2500 processors, and present the real-time event-driven programming model that supports flexible access to the resources of the machine and has enabled its use by a wide range of collaborators around the world.},
  keywords = {Brain modeling,Computational modeling,Computer architecture,multicast algorithms,multiprocessor interconnection networks,Multitasking,neural network hardware,Neural networks,Neuroscience,parallel programming,Parallel programming,Program processors},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LVRA4CSZ/Furber et al_2014_The SpiNNaker Project.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NPUHNSWD/6750072.html}
}

@article{fusiWhyNeuronsMix2016,
  title = {Why Neurons Mix: {{High}} Dimensionality for Higher Cognition},
  author = {Fusi, Stefano and Miller, Earl K. and Rigotti, Mattia},
  year = {2016},
  journal = {Current Opinion in Neurobiology},
  volume = {37},
  pages = {66--74},
  publisher = {{Elsevier Ltd}},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2016.01.010},
  abstract = {Neurons often respond to diverse combinations of task-relevant variables. This form of mixed selectivity plays an important computational role which is related to the dimensionality of the neural representations: high-dimensional representations with mixed selectivity allow a simple linear readout to generate a huge number of different potential responses. In contrast, neural representations based on highly specialized neurons are low dimensional and they preclude a linear readout from generating several responses that depend on multiple task-relevant variables. Here we review the conceptual and theoretical framework that explains the importance of mixed selectivity and the experimental evidence that recorded neural representations are high-dimensional. We end by discussing the implications for the design of future experiments.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IZZN39ZT/Fusi, Miller, Rigotti - 2016 - Why neurons mix High dimensionality for higher cognition.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@fusiWhyNeuronsMix2016.md}
}

@article{fytasReviewRecentDevelopments2018,
  title = {Review of Recent Developments in the Random-Field {{Ising}} Model},
  author = {Fytas, Nikolaos G. and {Martin-Mayor}, Victor and Picco, Marco and Sourlas, Nicolas},
  year = {2018},
  month = jul,
  journal = {Journal of Statistical Physics},
  volume = {172},
  number = {2},
  eprint = {1711.09597},
  eprinttype = {arxiv},
  pages = {665--672},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/s10955-018-1955-7},
  abstract = {A lot of progress has been made recently in our understanding of the random-field Ising model thanks to large-scale numerical simulations. In particular, it has been shown that, contrary to previous statements: the critical exponents for different probability distributions of the random fields and for diluted antiferromagnets in a field are the same. Therefore, critical universality, which is a perturbative renormalization-group prediction, holds beyond the validity regime of perturbation theory. Most notably, dimensional reduction is restored at five dimensions, i.e., the exponents of the random-field Ising model at five dimensions and those of the pure Ising ferromagnet at three dimensions are the same.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks},
  note = {Comment: 11 pages, 4 figures, updated and extended version, to be published in J. Stat. Phys},
  file = {C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Zotero\\storage\\Fytas et al_2018_Review of recent developments in the random-field Ising model2.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SCXW3S3A/1711.html}
}

@article{fytasReviewRecentDevelopments2018a,
  title = {Review of Recent Developments in the Random-Field {{Ising}} Model},
  author = {Fytas, Nikolaos G. and {Martin-Mayor}, Victor and Picco, Marco and Sourlas, Nicolas},
  year = {2018},
  month = jul,
  journal = {Journal of Statistical Physics},
  volume = {172},
  number = {2},
  eprint = {1711.09597},
  eprinttype = {arxiv},
  pages = {665--672},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/s10955-018-1955-7},
  abstract = {A lot of progress has been made recently in our understanding of the random-field Ising model thanks to large-scale numerical simulations. In particular, it has been shown that, contrary to previous statements: the critical exponents for different probability distributions of the random fields and for diluted antiferromagnets in a field are the same. Therefore, critical universality, which is a perturbative renormalization-group prediction, holds beyond the validity regime of perturbation theory. Most notably, dimensional reduction is restored at five dimensions, i.e., the exponents of the random-field Ising model at five dimensions and those of the pure Ising ferromagnet at three dimensions are the same.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks},
  note = {Comment: 11 pages, 4 figures, updated and extended version, to be published in J. Stat. Phys},
  file = {C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Zotero\\storage\\Fytas et al_2018_Review of recent developments in the random-field Ising model.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4RBFC64D/1711.html}
}

@article{fytasReviewRecentDevelopments2018b,
  title = {Review of Recent Developments in the Random-Field {{Ising}} Model},
  author = {Fytas, Nikolaos G. and {Martin-Mayor}, Victor and Picco, Marco and Sourlas, Nicolas},
  year = {2018},
  month = jul,
  journal = {Journal of Statistical Physics},
  volume = {172},
  number = {2},
  eprint = {1711.09597},
  eprinttype = {arxiv},
  pages = {665--672},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/s10955-018-1955-7},
  abstract = {A lot of progress has been made recently in our understanding of the random\textendash field Ising model thanks to large\textendash scale numerical simulations. In particular, it has been shown that, contrary to previous statements: the critical exponents for different probability distributions of the random fields and for diluted antiferromagnets in a field are the same. Therefore, critical universality, which is a perturbative renormalization\textendash group prediction, holds beyond the validity regime of perturbation theory. Most notably, dimensional reduction is restored at five dimensions, i.e., the exponents of the random\textendash field Ising model at five dimensions and those of the pure Ising ferromagnet at three dimensions are the same.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks},
  note = {Comment: 11 pages, 4 figures, updated and extended version, to be published in J. Stat. Phys},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JM8PIYQS/Fytas et al. - 2018 - Review of recent developments in the random-field .pdf}
}

@article{gallicchioDeepEchoState2020,
  title = {Deep {{Echo State Network}} ({{DeepESN}}): {{A Brief Survey}}},
  shorttitle = {Deep {{Echo State Network}} ({{DeepESN}})},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  year = {2020},
  month = sep,
  journal = {arXiv:1712.04323 [cs, stat]},
  eprint = {1712.04323},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The study of deep recurrent neural networks (RNNs) and, in particular, of deep Reservoir Computing (RC) is gaining an increasing research attention in the neural networks community. The recently introduced Deep Echo State Network (DeepESN) model opened the way to an extremely efficient approach for designing deep neural networks for temporal data. At the same time, the study of DeepESNs allowed to shed light on the intrinsic properties of state dynamics developed by hierarchical compositions of recurrent layers, i.e. on the bias of depth in RNNs architectural design. In this paper, we summarize the advancements in the development, analysis and applications of DeepESNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6BWHJZCD/Gallicchio_Micheli_2020_Deep Echo State Network (DeepESN).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/J25R79MC/1712.html}
}

@article{gallicchioDesignDeepEcho2018,
  title = {Design of Deep Echo State Networks},
  author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
  year = {2018},
  month = dec,
  journal = {Neural Networks},
  volume = {108},
  pages = {33--47},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.08.002},
  abstract = {In this paper, we provide a novel approach to the architectural design of deep Recurrent Neural Networks using signal frequency analysis. In particular, focusing on the Reservoir Computing framework and inspired by the principles related to the inherent effect of layering, we address a fundamental open issue in deep learning, namely the question of how to establish the number of layers in recurrent architectures in the form of deep echo state networks (DeepESNs). The proposed method is first analyzed and refined on a controlled scenario and then it is experimentally assessed on challenging real-world tasks. The achieved results also show the ability of properly designed DeepESNs to outperform RC approaches on a speech recognition task, and to compete with the state-of-the-art in time-series prediction on polyphonic music tasks.},
  langid = {english},
  keywords = {Architectural design of recurrent neural networks,Deep echo state networks,Deep recurrent neural networks,Echo state networks,Reservoir computing},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/K9EPUA8L/Gallicchio et al_2018_Design of deep echo state networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UA54DX5R/Design of deep echo state networks - 09.05.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9TDJ4YH3/S0893608018302223.html}
}

@article{gallicchioLocalLyapunovExponents2018,
  title = {Local {{Lyapunov}} Exponents of Deep Echo State Networks},
  author = {Gallicchio, Claudio and Micheli, Alessio and Silvestri, Luca},
  year = {2018},
  month = jul,
  journal = {Neurocomputing},
  volume = {298},
  pages = {34--45},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.11.073},
  abstract = {The analysis of deep Recurrent Neural Network (RNN) models represents a research area of increasing interest. In this context, the recent introduction of Deep Echo State Networks (DeepESNs) within the Reservoir Computing paradigm, enabled to study the intrinsic properties of hierarchically organized RNN architectures.In this paper we investigate the DeepESN model under a dynamical system perspective, aiming at characterizing the important aspect of stability of layered recurrent dynamics excited by external input signals.To this purpose, we develop a framework based on the study of the local Lyapunov exponents of stacked recurrent models, enabling the analysis and control of the resulting dynamical regimes. The introduced framework is demonstrated on artificial as well as real-world datasets. The results of our analysis on DeepESNs provide interesting insights on the real effect of layering in RNNs. In particular, they show that when recurrent units are organized in layers, then the resulting network intrinsically develops a richer dynamical behavior that is naturally driven closer to the edge of criticality. As confirmed by experiments on the short-term Memory Capacity task, this characterization makes the layered design effective, with respect to the shallow counterpart with the same number of units, especially in tasks that require much in terms of memory.},
  langid = {english},
  keywords = {Deep echo state network,Deep learning,Deep recurrent neural networks,Echo state network,Lyapunov exponents,Reservoir Computing,Stability analysis},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CP3C3UQ2/Local Lyapunov exponents of deep echo state networks - 09.05.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QYWBF62P/Gallicchio et al_2018_Local Lyapunov exponents of deep echo state networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Y8XC3VH9/S0925231218302157.html}
}

@article{ganmorSparseLoworderInteraction2011,
  title = {Sparse Low-Order Interaction Network Underlies a Highly Correlated and Learnable Neural Population Code},
  author = {Ganmor, Elad and Segev, Ronen and Schneidman, Elad},
  year = {2011},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {23},
  pages = {9679--9684},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1019641108},
  abstract = {Information is carried in the brain by the joint activity patterns of large groups of neurons. Understanding the structure and function of population neural codes is challenging because of the exponential number of possible activity patterns and dependencies among neurons. We report here that for groups of \textasciitilde 100 retinal neurons responding to natural stimuli, pairwise-based models, which were highly accurate for small networks, are no longer sufficient. We show that because of the sparse nature of the neural code, the higher-order interactions can be easily learned using a novel model and that a very sparse low-order interaction network underlies the code of large populations of neurons. Additionally, we show that the interaction network is organized in a hierarchical and modular manner, which hints at scalability. Our results suggest that learnability may be a key feature of the neural code.},
  chapter = {Biological Sciences},
  langid = {english},
  pmid = {21602497},
  keywords = {correlations,high-order,maximum entropy,neural networks,sparseness},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/I2N67YKK/Ganmor et al. - 2011 - Sparse low-order interaction network underlies a h.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ganmorSparseLoworderInteraction2011.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SN7Y7QSU/9679.html}
}

@article{ganmorThesaurusNeuralPopulation2015,
  title = {A Thesaurus for a Neural Population Code},
  author = {Ganmor, Elad and Segev, Ronen and Schneidman, Elad},
  editor = {Kleinfeld, David},
  year = {2015},
  month = sep,
  journal = {eLife},
  volume = {4},
  pages = {e06134},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.06134},
  abstract = {Information is carried in the brain by the joint spiking patterns of large groups of noisy, unreliable neurons. This noise limits the capacity of the neural code and determines how information can be transmitted and read-out. To accurately decode, the brain must overcome this noise and identify which patterns are semantically similar. We use models of network encoding noise to learn a thesaurus for populations of neurons in the vertebrate retina responding to artificial and natural videos, measuring the similarity between population responses to visual stimuli based on the information they carry. This thesaurus reveals that the code is organized in clusters of synonymous activity patterns that are similar in meaning but may differ considerably in their structure. This organization is highly reminiscent of the design of engineered codes. We suggest that the brain may use this structure and show how it allows accurate decoding of novel stimuli from novel spiking patterns.},
  keywords = {entropy,information,metric,natural stimuli,neural code,noise,retina},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WVSIY4AT/Ganmor et al. - 2015 - A thesaurus for a neural population code.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ganmorThesaurusNeuralPopulation2015.md}
}

@article{gaoCorticalColumnWholebrain,
  title = {Cortical Column and Whole-Brain Imaging with Molecular Contrast and Nanoscale Resolution},
  author = {Gao, Ruixuan and Asano, Shoh M and Upadhyayula, Srigokul and Pisarev, Igor and Milkie, Daniel E and Liu, Tsung-Li and Singh, Ved and Graves, Austin and Huynh, Grace H and Zhao, Yongxin and Bogovic, John and Colonell, Jennifer and Ott, Carolyn M and Zugates, Christopher and Tappan, Susan and Rodriguez, Alfredo and Mosaliganti, Kishore R and Sheu, Shu-Hsien and Pasolli, H Amalia and Pang, Song and Xu, Shan and Megason, Sean G and Hess, Harald and {Lippincott-Schwartz}, Jennifer and Hantman, Adam and Rubin, Gerald M and Kirchhausen, Tom and Saalfeld, Stephan and Aso, Yoshinori and Boyden, Edward S and Betzig, Eric},
  doi = {10.1126/science.aau8302},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FH5PJA8J/Gao et al. - Unknown - Cortical column and whole-brain imaging with molecular contrast and nanoscale resolution.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gaoCorticalColumnWholebrain.md}
}

@article{gaoNeurobiologyFeedingEnergy2007,
  title = {Neurobiology of {{Feeding}} and {{Energy Expenditure}}},
  author = {Gao, Qian and Horvath, Tamas L.},
  year = {2007},
  journal = {Annual Review of Neuroscience},
  volume = {30},
  number = {1},
  pages = {367--398},
  doi = {10.1146/annurev.neuro.30.051606.094324},
  abstract = {Significant advancements have been made in the past century regarding the neuronal control of feeding behavior and energy expenditure. The effects and mechanisms of action of various peripheral metabolic signals on the brain have become clearer. Molecular and genetic tools for visualizing and manipulating individual components of brain homeostatic systems in combination with neuroanatomical, electrophysiological, behavioral, and pharmacological techniques have begun to elucidate the molecular and neuronal mechanisms of complex feeding behavior and energy expenditure. This review highlights some of these advancements that have led to the current understanding of the brain's involvement in the acute and chronic regulation of energy homeostasis.},
  keywords = {arcuate nucleus,gaba,ghrelin,glutamate,leptin,npy,pomc},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z8D52VZK/Gao, Horvath - 2007 - Neurobiology of Feeding and Energy Expenditure.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gaoNeurobiologyFeedingEnergy2007.md}
}

@techreport{gardellaModelingCorrelatedActivity2018,
  title = {Modeling the Correlated Activity of Neural Populations: {{A}} Review},
  author = {Gardella, Christophe and Marre, Olivier and Mora, Thierry},
  year = {2018},
  abstract = {The principles of neural encoding and computations are inherently collective and usually involve large populations of interacting neurons with highly correlated activities. While theories of neural function have long recognized the importance of collective effects in populations of neurons, only in the past two decades has it become possible to record from many cells simulatenously using advanced experimental techniques with single-spike resolution, and to relate these correlations to function and behaviour. This review focuses on the modeling and inference approaches that have been recently developed to describe the correlated spiking activity of populations of neurons. We cover a variety of models describing correlations between pairs of neurons as well as between larger groups, synchronous or delayed in time, with or without the explicit influence of the stimulus, and including or not latent variables. We discuss the advantages and drawbacks or each method, as well as the computational challenges related to their application to recordings of ever larger populations.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MUXRPJNM/Gardella, Marre, Mora - 2018 - Modeling the correlated activity of neural populations A review.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gardellaModelingCorrelatedActivity2018.md}
}

@article{gardellaModelingCorrelatedActivity2018a,
  title = {Modeling the {{Correlated Activity}} of {{Neural Populations}}: {{A Review}}},
  author = {Gardella, Christophe and Marre, Olivier and Morra, Thierry},
  year = {2018},
  volume = {2733},
  pages = {2709--2733},
  doi = {10.1162/NECO},
  abstract = {The principles of neural encoding and computations are inherently col- lective and usually involve large populations of interacting neurons with highly correlated activities. While theories of neural function have long recognized the importance of collective effects in populations of neu- rons, only in the past two decades has it become possible to record from many cells simultaneously using advanced experimental techniqueswith single-spike resolution and to relate these correlations to function and behavior. This review focuses on the modeling and inference approaches that have been recently developed to describe the correlated spiking ac- tivity of populations of neurons. We cover a variety of models describ- ing correlations between pairs of neurons, as well as between larger groups, synchronous or delayed in time, with or without the explicit in- fluence of the stimulus, and including or not latent variables. We discuss the advantages and drawbacks or each method, as well as the computa- tional challenges related to their application to recordings of ever larger populations.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AZ5PSMMC/Gardella, Marre, Morra - 2018 - Modeling the Correlated Activity of Neural Populations A Review.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gardellaModelingCorrelatedActivity2018a.md}
}

@article{gardellaModelingCorrelatedActivity2019,
  title = {Modeling the Correlated Activity of Neural Populations: {{A}} Review},
  author = {Gardella, Christophe and Marre, Olivier and Mora, Thierry},
  year = {2019},
  month = feb,
  journal = {Neural Computation},
  volume = {31},
  number = {2},
  pages = {233--269},
  publisher = {{MIT Press Journals}},
  doi = {10.1162/neco_a_01154},
  abstract = {The principles of neural encoding and computations are inherently collective and usually involve large populations of interacting neurons with highly correlated activities. While theories of neural function have long recognized the importance of collective effects in populations of neurons, only in the past two decades has it become possible to record from many cells simultaneously using advanced experimental techniqueswith single-spike resolution and to relate these correlations to function and behavior. This review focuses on themodeling and inference approaches that have been recently developed to describe the correlated spiking activity of populations of neurons. We cover a variety of models describing correlations between pairs of neurons, as well as between larger groups, synchronous or delayed in time, with or without the explicit influence of the stimulus, and including or not latent variables.We discuss the advantages and drawbacks or each method, as well as the computational challenges related to their application to recordings of ever larger populations.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gardellaModelingCorrelatedActivity2019.md}
}

@article{gardellaTractableMethodDescribing2016,
  title = {A {{Tractable Method}} for {{Describing Complex Couplings}} between {{Neurons}} and {{Population Rate}}},
  author = {Gardella, Christophe and Marre, Olivier and Mora, Thierry},
  year = {2016},
  month = aug,
  journal = {eNeuro},
  volume = {3},
  number = {4},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0160-15.2016},
  abstract = {Neurons within a population are strongly correlated, but how to simply capture these correlations is still a matter of debate. Recent studies have shown that the activity of each cell is influenced by the population rate, defined as the summed activity of all neurons in the population. However, an explicit, tractable model for these interactions is still lacking. Here we build a probabilistic model of population activity that reproduces the firing rate of each cell, the distribution of the population rate, and the linear coupling between them. This model is tractable, meaning that its parameters can be learned in a few seconds on a standard computer even for large population recordings. We inferred our model for a population of 160 neurons in the salamander retina. In this population, single-cell firing rates depended in unexpected ways on the population rate. In particular, some cells had a preferred population rate at which they were most likely to fire. These complex dependencies could not be explained by a linear coupling between the cell and the population rate. We designed a more general, still tractable model that could fully account for these nonlinear dependencies. We thus provide a simple and computationally tractable way to learn models that reproduce the dependence of each neuron on the population rate.},
  pmcid = {PMC4989052},
  pmid = {27570827},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HQ8ZP9GH/Gardella et al. - 2016 - A Tractable Method for Describing Complex Coupling.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gardellaTractableMethodDescribing2016.md}
}

@article{gardinGrapheneRegenerativeMedicine2016,
  title = {Graphene in {{Regenerative Medicine}}: {{Focus}} on {{Stem Cells}} and {{Neuronal Differentiation}}},
  author = {Gardin, Chiara and Piattelli, Adriano and Zavan, Barbara},
  year = {2016},
  journal = {Trends in Biotechnology},
  doi = {10.1016/j.tibtech.2016.01.006},
  abstract = {Emerging graphene-based materials offer numerous opportunities to design novel scaffolds for neural tissue engineering. Graphene is a promising candidate due to its superior topographical, chemical, and electrical cues compared with conventional biomaterials. Here we examine the state of the art in graphene-based materials science for the neurodifferentiation of stem cells.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gardinGrapheneRegenerativeMedicine2016.md}
}

@article{gardnerOptimalBasinsAttraction1989,
  title = {Optimal Basins of Attraction in Randomly Sparse Neural Network Models},
  author = {Gardner, E.},
  year = {1989},
  month = jun,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {22},
  number = {12},
  pages = {1969--1974},
  publisher = {{IOP Publishing}},
  issn = {0305-4470},
  doi = {10.1088/0305-4470/22/12/002},
  abstract = {The size of the basin of attraction for randomly sparse neural networks with optimal interactions is calculated. For all values of the storage ratio, alpha =p/C{$<$}2, where p is the number of random uncorrelated patterns and C is the connectivity, the basin of attraction is finite, while for alpha {$<$}0.42, the basin of attraction is (almost) 100\%.},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Gardner_1989_Optimal basins of attraction in randomly sparse neural network models.pdf}
}

@article{gardnerOptimalStorageProperties1988,
  title = {Optimal Storage Properties of Neural Network Models},
  author = {Gardner, E. and Derrida, B.},
  year = {1988},
  month = jan,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {21},
  number = {1},
  pages = {271--284},
  publisher = {{IOP Publishing}},
  issn = {0305-4470},
  doi = {10.1088/0305-4470/21/1/031},
  abstract = {The authors calculate the number, p= alpha N of random N-bit patterns that an optimal neural network can store allowing a given fraction f of bit errors and with the condition that each right bit is stabilised by a local field at least equal to a parameter K. For each value of alpha and K, there is a minimum fraction fmin of wrong bits. They find a critical line, alpha c(K) with alpha c(0)=2. The minimum fraction of wrong bits vanishes for alpha {$<$} alpha c(K) and increases from zero for alpha {$>$} alpha c(K). The calculations are done using a saddle-point method and the order parameters at the saddle point are assumed to be replica symmetric. This solution is locally stable in a finite region of the K, alpha plane including the line, alpha c(K) but there is a line above which the solution becomes unstable and replica symmetry must be broken.},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Gardner_Derrida_1988_Optimal storage properties of neural network models.pdf}
}

@article{gardnerSpaceInteractionsNeural1988,
  title = {The Space of Interactions in Neural Network Models},
  author = {Gardner, E.},
  year = {1988},
  month = jan,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {21},
  number = {1},
  pages = {257--270},
  publisher = {{IOP Publishing}},
  issn = {0305-4470},
  doi = {10.1088/0305-4470/21/1/030},
  abstract = {The typical fraction of the space of interactions between each pair of N Ising spins which solve the problem of storing a given set of p random patterns as N-bit spin configurations is considered. The volume is calculated explicitly as a function of the storage ratio, alpha =p/N, of the value kappa ({$>$}0) of the product of the spin and the magnetic field at each site and of the magnetisation, m. Here m may vary between 0 (no correlation) and 1 (completely correlated). The capacity increases with the correlation between patterns from alpha =2 for correlated patterns with kappa =0 and tends to infinity as m tends to 1. The calculations use a saddle-point method and the order parameters at the saddle point are assumed to be replica symmetric. This solution is shown to be locally stable. A local iterative learning algorithm for updating the interactions is given which will converge to a solution of given kappa provided such solutions exist.},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Gardner_1988_The space of interactions in neural network models.pdf}
}

@article{gardnerThreeUnfinishedWorks1989,
  title = {Three Unfinished Works on the Optimal Storage Capacity of Networks},
  author = {Gardner, E. and Derrida, B.},
  year = {1989},
  month = jun,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {22},
  number = {12},
  pages = {1983--1994},
  publisher = {{IOP Publishing}},
  issn = {0305-4470},
  doi = {10.1088/0305-4470/22/12/004},
  abstract = {The optimal storage properties of three different neural network models are studied. For two of these models the architecture of the network is a perceptron with +or-J interactions, whereas for the third model the output can be an arbitrary function of the inputs. Analytic bounds and numerical estimates of the optimal capacities and of the minimal fraction of errors are obtained for the first two models. The third model can be solved exactly and the exact solution is compared to the bounds and to the results of numerical simulations used for the two other models.},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Gardner_Derrida_1989_Three unfinished works on the optimal storage capacity of networks.pdf}
}

@article{gargVoltageDependentSynapticPlasticity2022,
  title = {Voltage-{{Dependent Synaptic Plasticity}} ({{VDSP}}): {{Unsupervised}} Probabilistic {{Hebbian}} Plasticity Rule Based on Neurons Membrane Potential},
  shorttitle = {Voltage-{{Dependent Synaptic Plasticity}} ({{VDSP}})},
  author = {Garg, Nikhil and Balafrej, Ismael and Stewart, T. and Portal, J. and Bocquet, M. and Querlioz, D. and Drouin, D. and Rouat, J. and Beilliard, Y. and Alibart, F.},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2203.11022},
  abstract = {The proposed VDSP learning rule updates the synaptic conductance on the spike of the postsynaptic neuron only, which reduces by a factor of two the number of updates with respect to standard spike timing dependent plasticity (STDP). This study proposes voltage-dependent-synaptic plasticity (VDSP), a novel brain-inspired unsupervised local learning rule for the online implementation of Hebb's plasticity mechanism on neuromorphic hardware. The proposed VDSP learning rule updates the synaptic conductance on the spike of the postsynaptic neuron only, which reduces by a factor of two the number of updates with respect to standard spike timing dependent plasticity (STDP). This update is dependent on the},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X2NMNQUI/Garg et al_2022_Voltage-Dependent Synaptic Plasticity (VDSP).pdf}
}

@article{garofaloEvaluationPerformanceInformation2009,
  title = {Evaluation of the {{Performance}} of {{Information Theory-Based Methods}} and {{Cross-Correlation}} to {{Estimate}} the {{Functional Connectivity}} in {{Cortical Networks}}},
  author = {Garofalo, Matteo and Nieus, Thierry and Massobrio, Paolo and Martinoia, Sergio},
  editor = {Sporns, Olaf},
  year = {2009},
  month = aug,
  journal = {PLoS ONE},
  volume = {4},
  number = {8},
  pages = {e6482-e6482},
  publisher = {{Public Library of Science}},
  doi = {10.1371/journal.pone.0006482},
  abstract = {Functional connectivity of in vitro neuronal networks was estimated by applying different statistical algorithms on data collected by Micro-Electrode Arrays (MEAs). First we tested these "connectivity methods" on neuronal network models at an increasing level of complexity and evaluated the performance in terms of ROC (Receiver Operating Characteristic) and PPC (Positive Precision Curve), a new defined complementary method specifically developed for functional links identification. Then, the algorithms better estimated the actual connectivity of the network models, were used to extract functional connectivity from cultured cortical networks coupled to MEAs. Among the proposed approaches, Transfer Entropy and Joint-Entropy showed the best results suggesting those methods as good candidates to extract functional links in actual neuronal networks from multi-site recordings. \textcopyright{} 2009 Garofalo et al.},
  keywords = {Action potentials,Decision making,Electrophysiology,Entropy,Network analysis,Neural networks,Neurons,Peak values},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PXNLE98U/Garofalo et al. - 2009 - Evaluation of the Performance of Information Theory-Based Methods and Cross-Correlation to Estimate the Functio.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@garofaloEvaluationPerformanceInformation2009.md}
}

@book{gazzanigaCognitiveNeurosciences2009,
  title = {The Cognitive Neurosciences},
  editor = {Gazzaniga, Michael S.},
  year = {2009},
  edition = {4th ed},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-01341-3},
  langid = {english},
  lccn = {QP360.5 .N4986 2009},
  keywords = {Brain,Cognitive neuroscience,Mental Processes,Neurosciences,physiology},
  annotation = {OCLC: ocn297494728},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z7GUXX32/Gazzaniga - 2009 - The cognitive neurosciences.pdf}
}

@article{gemanStochasticRelaxationGibbs,
  title = {Stochastic {{Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}},
  author = {Geman, Stuart and Geman, Donald},
  pages = {21},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states ("annealing"), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel "relaxation" algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HXZCV6VD/Geman and Geman - Stochastic Relaxation, Gibbs Distributions, and th.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gemanStochasticRelaxationGibbs.md}
}

@techreport{gemanStochasticRelaxationGibbs1984,
  title = {Stochastic {{Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}},
  author = {Geman, Stuart and Geman, Donald},
  year = {1984},
  journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
  number = {6},
  pages = {721--721},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution , Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms , including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states ("annealing"), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel "relaxation" algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  keywords = {Gibbs distribution,image restoration,Index Terms-Annealing,line process,MAP estimate,Markov random field,relaxation,scene model-ing,spatial degradation},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QRUQHFZK/Geman, Geman - 1984 - Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gemanStochasticRelaxationGibbs1984.md}
}

@incollection{gennaroIntermediateLevelTheory2018,
  title = {The {{Intermediate Level Theory}} of {{Consciousness}}},
  booktitle = {The {{Routledge Handbook Of Consciousness}}},
  author = {Gennaro, Rocco J. and Barrett, David},
  year = {2018},
  month = nov,
  pages = {162--173},
  publisher = {{Routledge}},
  doi = {10.4324/9781315676982-13},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gennaroIntermediateLevelTheory2018.md}
}

@incollection{gennaroMultipleDraftsModel2018,
  title = {The {{Multiple Drafts Model}}},
  booktitle = {The {{Routledge Handbook Of Consciousness}}},
  author = {Gennaro, Rocco J. and Fallon, Francis and Brook, Andrew},
  year = {2018},
  month = nov,
  pages = {149--161},
  publisher = {{Routledge}},
  doi = {10.4324/9781315676982-12},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gennaroMultipleDraftsModel2018.md}
}

@article{gershmanReinforcementLearningEpisodic2017,
  title = {Reinforcement Learning and Episodic Memory in Humans and Animals: An Integrative Framework},
  shorttitle = {Reinforcement Learning and Episodic Memory in Humans and Animals},
  author = {Gershman, Samuel J. and Daw, Nathaniel D.},
  year = {2017},
  month = jan,
  journal = {Annual review of psychology},
  volume = {68},
  pages = {101--128},
  issn = {0066-4308},
  doi = {10.1146/annurev-psych-122414-033625},
  abstract = {We review the psychology and neuroscience of reinforcement learning (RL), which has witnessed significant progress in the last two decades, enabled by the comprehensive experimental study of simple learning and decision-making tasks. However, the simplicity of these tasks misses important aspects of reinforcement learning in the real world: (i) State spaces are high-dimensional, continuous, and partially observable; this implies that (ii) data are relatively sparse: indeed precisely the same situation may never be encountered twice; and also that (iii) rewards depend on long-term consequences of actions in ways that violate the classical assumptions that make RL tractable., A seemingly distinct challenge is that, cognitively, these theories have largely connected with procedural and semantic memory: how knowledge about action values or world models extracted gradually from many experiences can drive choice. This misses many aspects of memory related to traces of individual events, such as episodic memory. We suggest that these two gaps are related. In particular, the computational challenges can be dealt with, in part, by endowing RL systems with episodic memory, allowing them to (i) efficiently approximate value functions over complex state spaces, (ii) learn with very little data, and (iii) bridge long-term dependencies between actions and rewards. We review the computational theory underlying this proposal and the empirical evidence to support it. Our proposal suggests that the ubiquitous and diverse roles of memory in RL may function as part of an integrated learning system.},
  pmcid = {PMC5953519},
  pmid = {27618944},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MX2AZV9M/Gershman_Daw_2017_Reinforcement learning and episodic memory in humans and animals.pdf}
}

@article{gershmanTimeRepresentationReinforcement2014,
  title = {Time Representation in Reinforcement Learning Models of the Basal Ganglia},
  author = {Gershman, Samuel and Moustafa, Ahmed and Ludvig, Elliot},
  year = {2014},
  journal = {Frontiers in Computational Neuroscience},
  volume = {7},
  issn = {1662-5188},
  abstract = {Reinforcement learning (RL) models have been influential in understanding many aspects of basal ganglia function, from reward prediction to action selection. Time plays an important role in these models, but there is still no theoretical consensus about what kind of time representation is used by the basal ganglia. We review several theoretical accounts and their supporting evidence. We then discuss the relationship between RL models and the timing mechanisms that have been attributed to the basal ganglia. We hypothesize that a single computational system may underlie both RL and interval timing\textemdash the perception of duration in the range of seconds to hours. This hypothesis, which extends earlier models by incorporating a time-sensitive action selection mechanism, may have important implications for understanding disorders like Parkinson's disease in which both decision making and timing are impaired.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VKR3RKK8/Gershman et al_2014_Time representation in reinforcement learning models of the basal ganglia.pdf}
}

@article{gerstnerEligibilityTracesPlasticity2018,
  title = {Eligibility {{Traces}} and {{Plasticity}} on {{Behavioral Time Scales}}: {{Experimental Support}} of {{NeoHebbian Three-Factor Learning Rules}}},
  shorttitle = {Eligibility {{Traces}} and {{Plasticity}} on {{Behavioral Time Scales}}},
  author = {Gerstner, Wulfram and Lehmann, Marco and Liakoni, Vasiliki and Corneil, Dane and Brea, Johanni},
  year = {2018},
  journal = {Frontiers in Neural Circuits},
  volume = {12},
  issn = {1662-5110},
  abstract = {Most elementary behaviors such as moving the arm to grasp an object or walking into the next room to explore a museum evolve on the time scale of seconds; in contrast, neuronal action potentials occur on the time scale of a few milliseconds. Learning rules of the brain must therefore bridge the gap between these two different time scales. Modern theories of synaptic plasticity have postulated that the co-activation of pre- and postsynaptic neurons sets a flag at the synapse, called an eligibility trace, that leads to a weight change only if an additional factor is present while the flag is set. This third factor, signaling reward, punishment, surprise, or novelty, could be implemented by the phasic activity of neuromodulators or specific neuronal inputs signaling special events. While the theoretical framework has been developed over the last decades, experimental evidence in support of eligibility traces on the time scale of seconds has been collected only during the last few years. Here we review, in the context of three-factor rules of synaptic plasticity, four key experiments that support the role of synaptic eligibility traces in combination with a third factor as a biological implementation of neoHebbian three-factor learning rules.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KIWV4Q6T/Gerstner et al_2018_Eligibility Traces and Plasticity on Behavioral Time Scales.pdf}
}

@article{gilksCorrigendumAdaptiveRejection1997,
  title = {Corrigendum: {{Adaptive Rejection Metropolis Sampling}}},
  shorttitle = {Corrigendum},
  author = {Gilks, W. R. and Neal, R. M. and Best, N. G. and Tan, K. K. C.},
  year = {1997},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {46},
  number = {4},
  pages = {541--542},
  issn = {0035-9254, 1467-9876},
  doi = {10.1111/1467-9876.00091},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JA5S8DWR/Gilks et al. - 1997 - Corrigendum Adaptive Rejection Metropolis Samplin.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gilksCorrigendumAdaptiveRejection1997.md}
}

@techreport{gilkstAdaptiveRejectionMetropolis1995,
  title = {Adaptive {{Rejection Metropolis Sampling}} within {{Gibbs Sampling}}},
  author = {Gilkst, W R and Best, N G and Tan, K K C},
  year = {1995},
  journal = {Appl. Statist},
  volume = {44},
  number = {4},
  pages = {455--472},
  abstract = {Gibbs sampling is a powerful technique for statistical inference. It involves little more than sampling from full conditional distributions, which can be both complex and computationally expensive to evaluate. Gilks and Wild have shown that in practice full conditionals are often log-concave, and they proposed a method of adaptive rejection sampling for efficiently sampling from univariate log-concave distributions. In this paper, to deal with non-log-concave full conditional distributions, we generalize adaptive rejection sampling to include a Hastings-Metropolis algorithm step. One important field of application in which statistical models may lead to non-log-concave full conditionals is population pharmacokinetics. Here, the relationship between drug dose and blood or plasma concentration in a group of patients typically is modelled by using non-linear mixed effects models. Often, the data used for analysis are routinely collected hospital measurements, which tend to be noisy and irregular. Consequently, a robust (t-distributed) error structure is appropriate to account for outlying observations and/or patients. We propose a robust non-linear full probability model for population pharmacokinetic data. We demonstrate that our method enables Bayesian inference for this model, through an analysis of antibiotic administration in newborn babies.},
  keywords = {Bayesian computation,Gibbs sampling,Markov chain Monte Carlo method,Metropolis algorithm,Pharmacokinetic model,Random variate generation},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EQBXF5CV/Gilkst, Best, Tan - 1995 - Adaptive Rejection Metropolis Sampling within Gibbs Sampling.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gilkstAdaptiveRejectionMetropolis1995.md}
}

@article{gillettCharacteristicsSequentialActivity2020,
  title = {Characteristics of Sequential Activity in Networks with Temporally Asymmetric {{Hebbian}} Learning},
  author = {Gillett, Maxwell and Pereira, Ulises and Brunel, N.},
  year = {2020},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1918674117},
  abstract = {It is shown that nonlinearities in the learning rule can lead to sparse sequences and found that sequences maintain robust decoding but display highly labile dynamics to continuous changes in the connectivity matrix, similar to recent observations in hippocampus and parietal cortex. Significance Sequential activity is a prominent feature of many neural systems, in multiple behavioral contexts. Here, we investigate how Hebbian rules lead to storage and recall of random sequences of inputs in both rate and spiking recurrent networks. In the case of the simplest (bilinear) rule, we characterize extensively the regions in parameter space that allow sequence retrieval and compute analytically the storage capacity of the network. We show that nonlinearities in the learning rule can lead to sparse sequences and find that sequences maintain robust decoding but display highly labile dynamics to continuous changes in the connectivity matrix, similar to recent observations in hippocampus and parietal cortex. Sequential activity has been observed in multiple neuronal circuits across species, neural structures, and behaviors. It has been hypothesized that sequences could arise from learning processes. However, it is still unclear whether biologically plausible synaptic plasticity rules can organize neuronal activity to form sequences whose statistics match experimental observations. Here, we investigate temporally asymmetric Hebbian rules in sparsely connected recurrent rate networks and develop a theory of the transient sequential activity observed after learning. These rules transform a sequence of random input patterns into synaptic weight updates. After learning, recalled sequential activity is reflected in the transient correlation of network activity with each of the stored input patterns. Using mean-field theory, we derive a low-dimensional description of the network dynamics and compute the storage capacity of these networks. Multiple temporal characteristics of the recalled sequential activity are consistent with experimental observations. We find that the degree of sparseness of the recalled sequences can be controlled by nonlinearities in the learning rule. Furthermore, sequences maintain robust decoding, but display highly labile dynamics, when synaptic connectivity is continuously modified due to noise or storage of other patterns, similar to recent observations in hippocampus and parietal cortex. Finally, we demonstrate that our results also hold in recurrent networks of spiking neurons with separate excitatory and inhibitory populations.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GWATZR6T/Gillett et al_2020_Characteristics of sequential activity in networks with temporally asymmetric.pdf}
}

@article{ginzburgTheoryCorrelationsStochastic1994,
  title = {Theory of Correlations in Stochastic Neural Networks},
  author = {Ginzburg, Iris and Sompolinsky, Haim},
  year = {1994},
  month = oct,
  journal = {Physical Review E},
  volume = {50},
  number = {4},
  pages = {3171--3191},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/PhysRevE.50.3171},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6QNKA8RC/Ginzburg and Sompolinsky - 1994 - Theory of correlations in stochastic neural networ.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/V8539H2N/Ginzburg and Sompolinsky - 1994 - Theory of Correlations in Stochastic Neural Networ.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ginzburgTheoryCorrelationsStochastic1994.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FMPM5PSP/summary.html;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z3ZBDU7G/summary.html}
}

@article{giocomoComputationalModelsGrid2011,
  title = {Computational {{Models}} of {{Grid Cells}}},
  author = {Giocomo, Lisa~M. M. and Moser, May-Britt Britt and Moser, Edvard~I. I.},
  year = {2011},
  month = aug,
  journal = {Neuron},
  volume = {71},
  number = {4},
  pages = {589--603},
  publisher = {{Cell Press}},
  issn = {1097-4199 (Electronic)\textbackslash r0896-6273 (Linking)},
  doi = {10.1016/J.NEURON.2011.07.023},
  abstract = {Grid cells are space-modulated neurons with periodic firing fields. In moving animals, the multiple firing fields of an individual grid cell form a triangular pattern tiling the entire space available to the animal. Collectively, grid cells are thought to provide a context-independent metric representation of the local environment. Since~the discovery of grid cells in 2005, a number of models have been proposed to explain the formation of spatially repetitive firing patterns as well as the conversion of these signals to place signals one synapse downstream in the hippocampus. The present article reviews the most recent developments in our understanding of how grid patterns are generated, maintained, and transformed, with particular emphasis on second-generation computational models that have emerged during the past 2-3 years in response to criticism and new data.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DSW5ISQ9/Giocomo, Moser, Moser - 2011 - Computational models of grid cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@giocomoComputationalModelsGrid2011.md}
}

@article{girardeauHippocampalRipplesMemory2011,
  title = {Hippocampal Ripples and Memory Consolidation},
  author = {Girardeau, Gabrielle and Zugaro, Micha{\"e} L},
  year = {2011},
  journal = {Current Opinion in Neurobiology},
  volume = {21},
  pages = {1--8},
  doi = {10.1016/j.conb.2011.02.005},
  abstract = {During slow wave sleep and quiet wakefulness, the hippocampus generates high frequency field oscillations (ripples) during which pyramidal neurons replay previous waking activity in a temporally compressed manner. As a result, reactivated firing patterns occur within shorter time windows propitious for synaptic plasticity within the hippocampal network and in downstream neocortical structures. This is consistent with the long-held view that ripples participate in strengthening and reorganizing memory traces, possibly by mediating information transfer to neocortical areas. Recent studies have confirmed that ripples and associated neuronal reactivations play a causal role in memory consolidation during sleep and rest. However, further research will be necessary to better understand the neurophysiological mechanisms of memory consolidation, in particular the selection of reactivated assemblies, and the functional specificity of awake ripples.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WEWKMXN2/Girardeau, Zugaro - 2011 - Hippocampal ripples and memory consolidation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@girardeauHippocampalRipplesMemory2011.md}
}

@article{glauberTimedependentStatisticsIsing1963,
  title = {Time-Dependent Statistics of the {{Ising}} Model},
  author = {Glauber, Roy J.},
  year = {1963},
  month = feb,
  journal = {Journal of Mathematical Physics},
  volume = {4},
  number = {2},
  pages = {294--307},
  publisher = {{American Institute of PhysicsAIP}},
  doi = {10.1063/1.1703954},
  abstract = {The individual spins of the Ising model are assumed to interact with an external agency (e.g., a heat reservoir) which causes them to change their states randomly with time. Coupling between the spins is introduced through the assumption that the transition probabilities for any one spin depend on the values of the neighboring spins. This dependence is determined, in part, by the detailed balancing condition obeyed by the equilibrium state of the model. The Markoff process which describes the spin functions is analyzed in detail for the case of a closed N-member chain. The expectation values of the individual spins and of the products of pairs of spins, each of the pair evaluated at a different time, are found explicitly. The influence of a uniform, time-varying magnetic field upon the model is discussed, and the frequency-dependent magnetic susceptibility is found in the weak-field limit. Some fluctuation-dissipation theorems are derived which relate the susceptibility to the Fourier transform of the time-dependent correlation function of the magnetization at equilibrium.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5UHY2ASB/Glauber - 1963 - Time-dependent statistics of the Ising model.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/B8ZW9EMS/Glauber - 1963 - Time-dependent statistics of the Ising model(2).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@glauberTimedependentStatisticsIsing1963.md}
}

@inproceedings{gloverDynamicalLandscapeReservoir2021,
  title = {The {{Dynamical Landscape}} of {{Reservoir Computing}} with {{Elementary Cellular Automata}}},
  booktitle = {{{ALIFE}} 2021: {{The}} 2021 {{Conference}} on {{Artificial Life}}},
  author = {Glover, Tom Eivind and Lind, Pedro and Yazidi, Anis and Osipov, Evgeny and Nichele, Stefano},
  year = {2021},
  month = jul,
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00440},
  langid = {english},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@gloverDynamicalLandscapeReservoir2021.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/gloverDynamicalLandscapeReservoir2021-mdnotes.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Glover et al_2021_The Dynamical Landscape of Reservoir Computing with Elementary Cellular Automata.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NWQTAUBD/102888.html}
}

@article{goadsbyHHSPublicAccess2016,
  title = {{{HHS Public Access}}},
  author = {Goadsby, Peter J and Kurth, Tobias and Pressman, Alice},
  year = {2016},
  volume = {35},
  number = {14},
  pages = {1252--1260},
  issn = {3037242094},
  doi = {10.1177/0333102415576222.Is},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VN5QZ3QS/Goadsby, Kurth, Pressman - 2016 - HHS Public Access.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@goadsbyHHSPublicAccess2016.md}
}

@article{goldwasserPlantingUndetectableBackdoors2022,
  title = {Planting {{Undetectable Backdoors}} in {{Machine Learning Models}}},
  author = {Goldwasser, Shafi and Kim, Michael P. and Vaikuntanathan, Vinod and Zamir, Or},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.06974 [cs]},
  eprint = {2204.06974},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate "backdoor key", the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees. First, we show how to plant a backdoor in any model, using digital signature schemes. The construction guarantees that given black-box access to the original model and the backdoored version, it is computationally infeasible to find even a single input where they differ. This property implies that the backdoored model has generalization error comparable with the original model. Second, we demonstrate how to insert undetectable backdoors in models trained using the Random Fourier Features (RFF) learning paradigm or in Random ReLU networks. In this construction, undetectability holds against powerful white-box distinguishers: given a complete description of the network and the training data, no efficient distinguisher can guess whether the model is "clean" or contains a backdoor. Our construction of undetectable backdoors also sheds light on the related issue of robustness to adversarial examples. In particular, our construction can produce a classifier that is indistinguishable from an "adversarially robust" classifier, but where every input has an adversarial example! In summary, the existence of undetectable backdoors represent a significant theoretical roadblock to certifying adversarial robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Goldwasser et al_2022_Planting Undetectable Backdoors in Machine Learning Models.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2IA3SV9U/2204.html}
}

@article{goudarEncodingSensoryMotor2018,
  title = {Encoding Sensory and Motor Patterns as Time-Invariant Trajectories in Recurrent Neural Networks},
  author = {Goudar, Vishwa and Buonomano, Dean V},
  editor = {Druckmann, Shaul},
  year = {2018},
  month = mar,
  journal = {eLife},
  volume = {7},
  pages = {e31134},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.31134},
  abstract = {Much of the information the brain processes and stores is temporal in nature\textemdash a spoken word or a handwritten signature, for example, is defined by how it unfolds in time. However, it remains unclear how neural circuits encode complex time-varying patterns. We show that by tuning the weights of a recurrent neural network (RNN), it can recognize and then transcribe spoken digits. The model elucidates how neural dynamics in cortical networks may resolve three fundamental challenges: first, encode multiple time-varying sensory and motor patterns as stable neural trajectories; second, generalize across relevant spatial features; third, identify the same stimuli played at different speeds\textemdash we show that this temporal invariance emerges because the recurrent dynamics generate neural trajectories with appropriately modulated angular velocities. Together our results generate testable predictions as to how recurrent networks may use different mechanisms to generalize across the relevant spatial and temporal features of complex time-varying stimuli.},
  keywords = {neural dynamics,recurrent neural networks,sensorimotor,temporal scaling},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Q3WW9JVP/Goudar_Buonomano_2018_Encoding sensory and motor patterns as time-invariant trajectories in recurrent.pdf}
}

@article{grabowskiIsingbasedModelOpinion2006,
  title = {Ising-Based Model of Opinion Formation in a Complex Network of Interpersonal Interactions},
  author = {Grabowski, A. and Kosi{\'n}ski, R. A.},
  year = {2006},
  month = mar,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {361},
  number = {2},
  pages = {651--664},
  issn = {0378-4371},
  doi = {10.1016/j.physa.2005.06.102},
  abstract = {In our work the process of opinion formation in the human population, treated as a scale-free network, is modeled and investigated numerically. The individuals (nodes of the network) are characterized by their authorities, which influence the interpersonal interactions in the population. Hierarchical, two-level structures of interpersonal interactions and spatial localization of individuals are taken into account. The effect of the mass media, modeled as an external stimulation acting on the social network, on the process of opinion formation is investigated. It was found that in the time evolution of opinions of individuals critical phenomena occur. The first one is observed in the critical temperature of the system TC and is connected with the situation in the community, which may be described by such quantifiers as the economic status of people, unemployment or crime wave. Another critical phenomenon is connected with the influence of mass media on the population. As results from our computations, under certain circumstances the mass media can provoke critical rebuilding of opinions in the population.},
  langid = {english},
  keywords = {Complex networks,Freezing by heating,Ising model,Opinion formation,Scale-free networks,Small world},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2IWLCVS6/Grabowski, Kosiński - 2006 - Ising-based model of opinion formation in a complex network of interpersonal interactions.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3FEHIWG9/Grabowski and Kosiński - 2006 - Ising-based model of opinion formation in a comple.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@grabowskiIsingbasedModelOpinion2006.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UNDIDBBH/S0378437105007296.html}
}

@article{granmoSolvingStochasticNonlinear2010,
  title = {Solving {{Stochastic Nonlinear Resource Allocation Problems Using}} a {{Hierarchy}} of {{Twofold Resource Allocation Automata}}},
  author = {Granmo, Ole-Christoffer and Oommen, B. John},
  year = {2010},
  month = apr,
  journal = {IEEE Transactions on Computers},
  volume = {59},
  number = {4},
  pages = {545--560},
  issn = {1557-9956},
  doi = {10.1109/TC.2009.189},
  abstract = {In a multitude of real-world situations, resources must be allocated based on incomplete and noisy information. However, in many cases, incomplete and noisy information render traditional resource allocation techniques ineffective. The decentralized Learning Automata Knapsack Game (LAKG) was recently proposed for solving one such class of problems, namely the class of Stochastic Nonlinear Fractional Knapsack Problems. Empirically, the LAKG was shown to yield a superior performance when compared to methods which are based on traditional parameter estimation schemes. This paper presents a completely new online Learning Automata (LA) system, namely the Hierarchy of Twofold Resource Allocation Automata (H-TRAA). In terms of contributions, we first of all, note that the primitive component of the H-TRAA is a Twofold Resource Allocation Automaton (TRAA) which possesses novelty in the field of LA. Second, the paper contains a formal analysis of the TRAA, including a rigorous proof for its convergence. Third, the paper proves the convergence of the H-TRAA itself. Finally, we demonstrate empirically that the H-TRAA provides orders of magnitude faster convergence compared to the LAKG for simulated data pertaining to two-material unit-value functions. Indeed, in contrast to the LAKG, the H-TRAA scales sublinearly. Consequently, we believe that the H-TRAA opens avenues for handling demanding real-world applications such as the allocation of sampling resources in large-scale Web accessibility assessment problems. We are currently working on applying the H-TRAA solution to the web-polling and sample-size detection problems applicable to the world wide web.},
  keywords = {Data mining,hierarchical learning,learning automata,Materials,Monitoring,Noise measurement,Nonlinear knapsack problems,resource allocation.,Resource management,stochastic optimization,Stochastic processes,Web pages},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Granmo_Oommen_2010_Solving Stochastic Nonlinear Resource Allocation Problems Using a Hierarchy of.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X9SRF3L7/5374378.html}
}

@article{grievesRepresentationSpaceBrain2017,
  title = {The Representation of Space in the Brain},
  author = {Grieves, Roddy M. and Jeffery, Kate J.},
  year = {2017},
  journal = {Behavioural Processes},
  volume = {135},
  pages = {113--131},
  publisher = {{Elsevier B.V.}},
  doi = {10.1016/j.beproc.2016.12.012},
  abstract = {Animals can navigate vast distances and often display behaviours or activities that indicate a detailed, internal spatial representation of their surrounding environment or a `cognitive map'. Over a century of behavioural research on spatial navigation in humans and animals has greatly increased our understanding of how this highly complex feat is achieved. In turn this has inspired half a century of electrophysiological spatial navigation and memory research which has further advanced our understanding of the brain. In particular, three functional cell types have been suggested to underlie cognitive mapping processes; place cells, head direction cells and grid cells. However, there are numerous other spatially modulated neurons in the brain. For a more complete understanding of the electrophysiological systems and behavioural processes underlying spatial navigation we must also examine these lesser understood neurons. In this review we will briefly summarise the literature surrounding place cells, head direction cells, grid cells and the evidence that these cells collectively form the neural basis of a cognitive map. We will then review literature covering many other spatially modulated neurons in the brain that perhaps further augment this cognitive map.},
  keywords = {Grid cell,Head direction cell,Navigation,Place cell,Spatial cognition},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IE9MX99Z/Grieves, Jeffery - 2017 - The representation of space in the brain.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@grievesRepresentationSpaceBrain2017.md}
}

@article{grondinTimingTimePerception2010,
  title = {Timing and Time Perception: {{A}} Review of Recent Behavioral and Neuroscience Findings and Theoretical Directions},
  shorttitle = {Timing and Time Perception},
  author = {Grondin, S.},
  year = {2010},
  journal = {Attention, Perception, and Psychophysics},
  volume = {72},
  number = {3},
  pages = {561--582},
  issn = {1943-3921},
  doi = {10.3758/APP.72.3.561},
  abstract = {The aim of the present review article is to guide the reader through portions of the human time perception, or temporal processing, literature. After distinguishing the main contemporary issues related to time perception, the article focuses on the main findings and explanations that are available in the literature on explicit judgments about temporal intervals. The review emphasizes studies that are concerned with the processing of intervals lasting a few milliseconds to several seconds and covers studies issuing from either a behavioral or a neuroscience approach. It also discusses the question of whether there is an internal clock (pacemaker counter or oscillator device) that is dedicated to temporal processing and reports the main hypotheses regarding the involvement of biological structures in time perception. \textcopyright{} 2010 The Psychonomic Society, Inc.},
  langid = {english},
  note = {Cited By :578},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AJDFT84W/Grondin_2010_Timing and time perception.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/55ADQ9MK/display.html}
}

@techreport{grunwaldMinimumDescriptionLength2019,
  title = {Minimum {{Description Length Revisited}}},
  author = {Gr{\"u}nwald, Peter and Roos, Teemu},
  year = {2019},
  abstract = {This is an up-to-date introduction to and overview of the Minimum Description Length (MDL) Principle, a theory of inductive inference that can be applied to general problems in statistics, machine learning and pattern recognition. While MDL was originally based on data compression ideas, this introduction can be read without any knowledge thereof. It takes into account all major developments since 2007, the last time an extensive overview was written. These include new methods for model selection and averaging and hypothesis testing, as well as the first completely general definition of MDL estima-tors. Incorporating these developments, MDL can be seen as a powerful extension of both penalized likelihood and Bayesian approaches, in which penalization functions and prior distributions are replaced by more general luckiness functions, average-case methodology is replaced by a more robust worst-case approach, and in which methods classically viewed as highly distinct, such as AIC vs BIC and cross-validation vs Bayes can, to a large extent, be viewed from a unified perspective.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KYLRH3EG/Grünwald, Roos - 2019 - Minimum Description Length Revisited.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@grunwaldMinimumDescriptionLength2019.md}
}

@techreport{grunwaldTutorialIntroductionMinimum,
  title = {A {{Tutorial Introduction}} to the {{Minimum Description Length Principle}}},
  author = {Gr{\"u}nwald, Peter},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/F7GTQ4TZ/Grünwald - Unknown - A Tutorial Introduction to the Minimum Description Length Principle.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@grunwaldTutorialIntroductionMinimum.md}
}

@article{grunwaldTutorialIntroductionMinimum2004,
  title = {A Tutorial Introduction to the Minimum Description Length Principle},
  author = {Grunwald, Peter},
  year = {2004},
  month = jun,
  journal = {arXiv:math/0406077},
  eprint = {math/0406077},
  eprinttype = {arxiv},
  abstract = {This tutorial provides an overview of and introduction to Rissanen's Minimum Description Length (MDL) Principle. The first chapter provides a conceptual, entirely non-technical introduction to the subject. It serves as a basis for the technical introduction given in the second chapter, in which all the ideas of the first chapter are made mathematically precise. The main ideas are discussed in great conceptual and technical detail. This tutorial is an extended version of the first two chapters of the collection "Advances in Minimum Description Length: Theory and Application" (edited by P.Grunwald, I.J. Myung and M. Pitt, to be published by the MIT Press, Spring 2005).},
  archiveprefix = {arXiv},
  keywords = {6201,6801,68T05,68T10,9401,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory},
  note = {Comment: 80 pages 5 figures Report with 2 chapters},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6P6ZCEKQ/Grunwald - 2004 - A tutorial introduction to the minimum description.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@grunwaldTutorialIntroductionMinimum2004.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Y2GD6FFP/0406077.html}
}

@article{guanellaModelGridCells2007,
  title = {A {{Model}} of {{Grid Cells Based}} on a {{Twisted Torus Topology}}},
  author = {GUANELLA, ALEXIS and KIPER, DANIEL and VERSCHURE, PAUL},
  year = {2007},
  journal = {International Journal of Neural Systems},
  volume = {17},
  number = {04},
  pages = {231--240},
  doi = {10.1142/s0129065707001093},
  abstract = {The grid cells of the rat medial entorhinal cortex (MEC) show an increased firing frequency when the position of the animal correlates with multiple regions of the environment that are arranged in regular triangular grids. Here, we describe an artificial neural network based on a twisted torus topology, which allows for the generation of regular triangular grids. The association of the activity of pre-defined hippocampal place cells with entorhinal grid cells allows for a highly robust-to-noise calibration mechanism, suggesting a role for the hippocampal back-projections to the entorhinal cortex.},
  keywords = {entorhinal cortex,grid cells,path integration,place cells,twisted torus},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5IAJ8PX8/GUANELLA, KIPER, VERSCHURE - 2007 - a Model of Grid Cells Based on a Twisted Torus Topology.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@guanellaModelGridCells2007.md}
}

@techreport{gulrajaniImprovedTrainingWasserstein,
  title = {Improved {{Training}} of {{Wasserstein GANs Montreal Institute}} for {{Learning Algorithms}}},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms. \textdagger},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6HENXTU3/Gulrajani et al. - Unknown - Improved Training of Wasserstein GANs Montreal Institute for Learning Algorithms.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@gulrajaniImprovedTrainingWasserstein.md}
}

@article{guOscillatoryMultiplexingNeural2015,
  title = {Oscillatory Multiplexing of Neural Population Codes for Interval Timing and Working Memory},
  author = {Gu, Bon-Mi and {van Rijn}, Hedderik and Meck, Warren H.},
  year = {2015},
  month = jan,
  journal = {Neuroscience \& Biobehavioral Reviews},
  volume = {48},
  pages = {160--185},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2014.10.008},
  abstract = {Interval timing and working memory are critical components of cognition that are supported by neural oscillations in prefrontal\textendash striatal\textendash hippocampal circuits. In this review, the properties of interval timing and working memory are explored in terms of behavioral, anatomical, pharmacological, and neurophysiological findings. We then describe the various neurobiological theories that have been developed to explain these cognitive processes \textendash{} largely independent of each other. Following this, a coupled excitatory \textendash{} inhibitory oscillation (EIO) model of temporal processing is proposed to address the shared oscillatory properties of interval timing and working memory. Using this integrative approach, we describe a hybrid model explaining how interval timing and working memory can originate from the same oscillatory processes, but differ in terms of which dimension of the neural oscillation is utilized for the extraction of item, temporal order, and duration information. This extension of the striatal beat-frequency (SBF) model of interval timing (Matell and Meck, 2000, Matell and Meck, 2004) is based on prefrontal\textendash striatal\textendash hippocampal circuit dynamics and has direct relevance to the pathophysiological distortions observed in time perception and working memory in a variety of psychiatric and neurological conditions.},
  langid = {english},
  keywords = {Corticostriatal circuits,Episodic memory,Neural oscillations,Timing and time perception},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7767F8X7/Gu et al_2015_Oscillatory multiplexing of neural population codes for interval timing and.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FK7BBVBP/S0149763414002589.html}
}

@article{haberNeuralNetworkUnderlying2014,
  title = {The Neural Network Underlying Incentive-Based Learning: Implications for Interpreting Circuit Disruptions in Psychiatric Disorders.},
  author = {Haber, Suzanne~N. N and Behrens, Timothy~E.J. E J},
  year = {2014},
  month = sep,
  journal = {Neuron},
  volume = {83},
  number = {5},
  pages = {1019--39},
  publisher = {{NIH Public Access}},
  doi = {10.1016/j.neuron.2014.08.031},
  abstract = {Coupling stimuli and actions with positive or negative outcomes facilitates the selection of appropriate actions. Several brain regions are involved in the development of goal-directed behaviors and habit formation during incentive-based learning. This Review focuses on higher cognitive control of decision making and the cortical and subcortical structures and connections that attribute value to stimuli, associate that value with choices, and select an action plan. Delineating the connectivity between these areas is fundamental for understanding how brain regions work together to evaluate stimuli, develop actions plans, and modify behavior, as well as for elucidating the pathophysiology of psychiatric diseases.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XXJI43EB/Haber, Behrens - 2014 - The neural network underlying incentive-based learning implications for interpreting circuit disruptions in psyc.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@haberNeuralNetworkUnderlying2014.md}
}

@article{haftingMicrostructureSpatialMap2005,
  title = {Microstructure of a Spatial Map in the Entorhinal Cortex},
  author = {Hafting, Torkel and Fyhn, Marianne and Molden, Sturla and Moser, May Britt and Moser, Edvard I.},
  year = {2005},
  journal = {Nature},
  volume = {436},
  number = {7052},
  pages = {801--806},
  issn = {1476-4687 (Electronic)\textbackslash r0028-0836 (Linking)},
  doi = {10.1038/nature03721},
  abstract = {The ability to find one's way depends on neural algorithms that integrate information about place, distance and direction, but the implementation of these operations in cortical microcircuits is poorly understood. Here we show that the dorsocaudal medial entorhinal cortex (dMEC) contains a directionally oriented, topographically organized neural map of the spatial environment. Its key unit is the 'grid cell', which is activated whenever the animal's position coincides with any vertex of a regular grid of equilateral triangles spanning the surface of the environment. Grids of neighbouring cells share a common orientation and spacing, but their vertex locations (their phases) differ. The spacing and size of individual fields increase from dorsal to ventral dMEC. The map is anchored to external landmarks, but persists in their absence, suggesting that grid cells may be part of a generalized, path-integration-based map of the spatial environment.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IBDBU6F8/Hafting et al. - 2005 - Microstructure of a spatial map in the entorhinal cortex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@haftingMicrostructureSpatialMap2005.md}
}

@article{haimoviciCriticalityMostlyInformative2015,
  title = {Criticality of Mostly Informative Samples: {{A Bayesian}} Model Selection Approach},
  shorttitle = {Criticality of Mostly Informative Samples},
  author = {Haimovici, Ariel and Marsili, Matteo},
  year = {2015},
  month = sep,
  journal = {arXiv:1502.00356 [physics]},
  eprint = {1502.00356},
  eprinttype = {arxiv},
  primaryclass = {physics},
  doi = {10.1088/1742-5468/2015/10/P10013},
  abstract = {We discuss a Bayesian model selection approach to high dimensional data in the deep under sampling regime. The data is based on a representation of the possible discrete states \$s\$, as defined by the observer, and it consists of \$M\$ observations of the state. This approach shows that, for a given sample size \$M\$, not all states observed in the sample can be distinguished. Rather, only a partition of the sampled states \$s\$ can be resolved. Such partition defines an \{\textbackslash em emergent\} classification \$q\_s\$ of the states that becomes finer and finer as the sample size increases, through a process of \{\textbackslash em symmetry breaking\} between states. This allows us to distinguish between the \$resolution\$ of a given representation of the observer defined states \$s\$, which is given by the entropy of \$s\$, and its \$relevance\$ which is defined by the entropy of the partition \$q\_s\$. Relevance has a non-monotonic dependence on resolution, for a given sample size. In addition, we characterise most relevant samples and we show that they exhibit power law frequency distributions, generally taken as signatures of "criticality". This suggests that "criticality" reflects the relevance of a given representation of the states of a complex system, and does not necessarily require a specific mechanism of self-organisation to a critical point.},
  archiveprefix = {arXiv},
  keywords = {Physics - Data Analysis; Statistics and Probability},
  note = {Comment: 31 pages, 7 figures},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9N8B3FD5/Haimovici and Marsili - 2015 - Criticality of mostly informative samples A Bayes.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@haimoviciCriticalityMostlyInformative2015.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TAE7D46L/1502.html}
}

@article{hakimPhasecodingMemoriesMind2018,
  title = {Phase-Coding Memories in Mind},
  author = {Hakim, Nicole and Vogel, Edward K},
  year = {2018},
  issn = {1111111111},
  doi = {10.1371/journal.pbio.3000012},
  abstract = {Temporarily holding information in mind is an important part of many cognitive processes, such as reasoning and language. The amount of information that can be actively held "in mind" at any time is greatly limited-research suggests that we can only actively hold three or four pieces of information at once. A central question in cognitive neuroscience is how a system comprised of billions of neurons can actively maintain such a limited amount of information. A new study published in this issue of PLOS Biology by Bahramisharif and colleagues provides significant insights into this question.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5TK3FMQ9/Hakim, Vogel - 2018 - Phase-coding memories in mind.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hakimPhasecodingMemoriesMind2018.md}
}

@article{hallquistGraphTheoryApproaches2019,
  title = {Graph Theory Approaches to Functional Network Organization in Brain Disorders: {{A}} Critique for a Brave New Small-World},
  shorttitle = {Graph Theory Approaches to Functional Network Organization in Brain Disorders},
  author = {Hallquist, Michael N. and Hillary, Frank G.},
  year = {2019},
  month = jan,
  journal = {Network Neuroscience},
  volume = {3},
  number = {1},
  pages = {1--26},
  issn = {2472-1751},
  doi = {10.1162/netn_a_00054},
  abstract = {Over the past two decades, resting-state functional connectivity (RSFC) methods have provided new insights into the network organization of the human brain. Studies of brain disorders such as Alzheimer's disease or depression have adapted tools from graph theory to characterize differences between healthy and patient populations. Here, we conducted a review of clinical network neuroscience, summarizing methodological details from 106 RSFC studies. Although this approach is prevalent and promising, our review identified four challenges. First, the composition of networks varied remarkably in terms of region parcellation and edge definition, which are fundamental to graph analyses. Second, many studies equated the number of connections across graphs, but this is conceptually problematic in clinical populations and may induce spurious group differences. Third, few graph metrics were reported in common, precluding meta-analyses. Fourth, some studies tested hypotheses at one level of the graph without a clear neurobiological rationale or considering how findings at one level (e.g., global topology) are contextualized by another (e.g., modular structure). Based on these themes, we conducted network simulations to demonstrate the impact of specific methodological decisions on case-control comparisons. Finally, we offer suggestions for promoting convergence across clinical studies in order to facilitate progress in this important field.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5WGF7LUN/Hallquist and Hillary - 2019 - Graph theory approaches to functional network orga.pdf}
}

@article{hamedaniDeepSpikingDelayed2020,
  title = {Deep {{Spiking Delayed Feedback Reservoirs}} and {{Its Application}} in {{Spectrum Sensing}} of {{MIMO-OFDM Dynamic Spectrum Sharing}}},
  author = {Hamedani, Kian and Liu, Lingjia and Liu, Shiya and He, Haibo and Yi, Yang},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {02},
  pages = {1292--1299},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i02.5484},
  abstract = {In this paper, we introduce a deep spiking delayed feedback reservoir (DFR) model to combine DFR with spiking neuros: DFRs are a new type of recurrent neural networks (RNNs) that are able to capture the temporal correlations in time series while spiking neurons are energy-efficient and biologically plausible neurons models. The introduced deep spiking DFR model is energy-efficient and has the capability of analyzing time series signals. The corresponding field programmable gate arrays (FPGA)-based hardware implementation of such deep spiking DFR model is introduced and the underlying energy-efficiency and recourse utilization are evaluated. Various spike encoding schemes are explored and the optimal spike encoding scheme to analyze the time series has been identified. To be specific, we evaluate the performance of the introduced model using the spectrum occupancy time series data in MIMO-OFDM based cognitive radio (CR) in dynamic spectrum sharing (DSS) networks. In a MIMO-OFDM DSS system, available spectrum is very scarce and efficient utilization of spectrum is very essential. To improve the spectrum efficiency, the first step is to identify the frequency bands that are not utilized by the existing users so that a secondary user (SU) can use them for transmission. Due to the channel correlation as well as users' activities, there is a significant temporal correlation in the spectrum occupancy behavior of the frequency bands in different time slots. The introduced deep spiking DFR model is used to capture the temporal correlation of the spectrum occupancy time series and predict the idle/busy subcarriers in future time slots for potential spectrum access. Evaluation results suggest that our introduced model achieves higher area under curve (AUC) in the receiver operating characteristic (ROC) curve compared with the traditional energy detection-based strategies and the learning-based support vector machines (SVMs).},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NASXSQMS/Hamedani et al_2020_Deep Spiking Delayed Feedback Reservoirs and Its Application in Spectrum.pdf}
}

@article{hammerBuildingSentimentLexicons,
  title = {Building Sentiment {{Lexicons}} Applying Graph Theory on Information from Three {{Norwegian}} Thesauruses},
  author = {Hammer, Hugo and Bai, Aleksander and Yazidi, Anis and Engelstad, Paal},
  pages = {11},
  abstract = {Sentiment lexicons are the most used tool to automatically predict sentiment in text. To the best of our knowledge, there exist no openly available sentiment lexicons for the Norwegian language. Thus in this paper we applied two different strategies to automatically generate sentiment lexicons for the Norwegian language. The first strategy used machine translation to translate an English sentiment lexicon to Norwegian and the other strategy used information from three different thesauruses to build several sentiment lexicons. The lexicons based on thesauruses were built using the Label propagation algorithm from graph theory. The lexicons were evaluated by classifying product and movie reviews. The results show satisfying classification performances. Different sentiment lexicons perform well on product and on movie reviews. Overall the lexicon based on machine translation performed the best, showing that linguistic resources in English can be translated to Norwegian without losing significant value.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LPHI2VEP/Hammer et al. - Building sentiment Lexicons applying graph theory .pdf;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@hammerBuildingSentimentLexicons.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/hammerBuildingSentimentLexicons-mdnotes.md}
}

@article{hammerClassificationDynamicalData2018,
  title = {On the Classification of Dynamical Data Streams Using Novel ``{{Anti-Bayesian}}'' Techniques},
  author = {Hammer, Hugo Lewi and Yazidi, Anis and Oommen, B. John},
  year = {2018},
  month = apr,
  journal = {Pattern Recognition},
  volume = {76},
  pages = {108--124},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2017.10.031},
  abstract = {The classification of dynamical data streams is among the most complex problems encountered in classification. This is, firstly, because the distribution of the data streams is non-stationary, and it changes without any prior ``warning''. Secondly, the manner in which it changes is also unknown. Thirdly, and more interestingly, the model operates with the assumption that the correct classes of previously-classified patterns become available at a juncture after their appearance. This paper pioneers the use of unreported novel schemes that can classify such dynamical data streams by invoking the recently-introduced ``Anti-Bayesian'' (AB) techniques. Contrary to the Bayesian paradigm, that compare the testing sample with the distribution's central points, AB techniques are based on the information in the distant-from-the-mean samples. Most Bayesian approaches can be naturally extended to dynamical systems by dynamically tracking the mean of each class using, for example, the exponential moving average based estimator, or a sliding window estimator. The AB schemes introduced by Oommen et~al.., on the other hand, work with a radically different approach and with the non-central quantiles of the distributions. Surprisingly and counter-intuitively, the reported AB methods work equally or close-to-equally well to an optimal supervised Bayesian scheme on a host of accepted Pattern Recognition problems. This thus begs its natural extension to the unexplored arena of classification for dynamical data streams. Naturally, for such an AB classification approach, we need to track the non-stationarity of the quantiles of the classes. To achieve this, in this paper, we develop an AB approach for the online classification of data streams by applying the efficient and robust quantile estimators developed by Yazidi and Hammer [12,37]. Apart from the methodology itself, in this paper, we compare the Bayesian and AB approaches using both real-life and synthetic data. The results demonstrate the intriguing and counter-intuitive results that the AB approach, sometimes, actually outperforms the Bayesian approach for this application both with respect to the peak performance obtained, and the robustness of the choice of the respective tuning parameters. Furthermore, the AB approach is much more robust against outliers, which is an inherent property of quantile estimators [12,37], which is a property that the Bayesian approach cannot match, since it rather tracks the mean.},
  langid = {english},
  keywords = {Anti-Bayesian classification,Classification with delay,Data streams,Incremental quantile estimation},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Hammer et al_2018_On the classification of dynamical data streams using novel “Anti-Bayesian”.pdf}
}

@misc{HandsonTutorialNetwork,
  title = {A Hands-on Tutorial on Network and Topological Neuroscience | {{bioRxiv}}},
  howpublished = {https://www.biorxiv.org/content/10.1101/2021.02.15.431255v1},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/L8BDVSEB/Supplemental Code blocks 1-3.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QFG5HJNS/Sup. video Filtration in a brain network.mp4;/Users/michaejt/Insync/m@tarlton.info/Google Drive/A hands-on tutorial on network and topological neuroscience bioRxiv.pdf;/Users/michaejt/Insync/m@tarlton.info/Google Drive/A hands-on tutorial on network and topological neuroscience bioRxiv2.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8PD5KQD8/2021.02.15.html}
}

@article{hansenModelSelectionPrinciple2001,
  title = {Model Selection and the Principle of Minimum Description Length},
  author = {Hansen, Mark H. and Yu, Bin},
  year = {2001},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {96},
  number = {454},
  pages = {746--774},
  publisher = {{Taylor \& Francis}},
  doi = {10.1198/016214501753168398},
  abstract = {This article reviews the principle of minimum description length (MDL) for problems of model selection. By viewing statistical modeling as a means of generating descriptions of observed data, the MDL framework discriminates between competing models based on the complexity of each description. This approach began with Kolmogorov's theory of algorithmic complexity, matured in the literature on information theory, and has recently received renewed attention within the statistics community. Here we review both the practical and the theoretical aspects of MDL as a tool for model selection, emphasizing the rich connections between information theory and statistics. At the boundary between these two disciplines we find many interesting interpretations of popular frequentist and Bayesian procedures. As we show, MDL provides an objective umbrella under which rather disparate approaches to statistical modeling can coexist and be compared. We illustrate the MDL principle by considering problems in regression, nonparametric curve estimation, cluster analysis, and time series analysis. Because model selection in linear regression is an extremely common problem that arises in many applications, we present detailed derivations of several MDL criteria in this context and discuss their properties through a number of examples. Our emphasis is on the practical application of MDL, and hence we make extensive use of real datasets. In writing this review, we tried to make the descriptive philosophy of MDL natural to a statistics audience by examining classical problems in model selection. In the engineering literature, however, MDL is being applied to ever more exotic modeling situations. As a principle for statistical modeling in general, one strength of MDL is that it can be intuitively extended to provide useful tools for new problems. \textcopyright{} 2001 American Statistical Association.},
  keywords = {Aic,Bayes information criterion,Bayesian methods,Cluster analysis,Code length,Coding redundancy,Information theory,Model selection,Pointwise and minimax lower bounds,Regression,Time series},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/I58ETYM2/Hansen, Yu - 2001 - Model selection and the principle of minimum description length.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QBT9H236/Hansen, Yu - 2001 - Model selection and the principle of minimum description length(2).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hansenModelSelectionPrinciple2001.md}
}

@article{haoBiologicallyPlausibleSupervised2020,
  title = {A Biologically Plausible Supervised Learning Method for Spiking Neural Networks Using the Symmetric {{STDP}} Rule},
  author = {Hao, Yunzhe and Huang, Xuhui and Dong, Meng and Xu, Bo},
  year = {2020},
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  pages = {387--395},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.09.007},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HPFMBEEL/Hao et al_2020_A biologically plausible supervised learning method for spiking neural networks.pdf}
}

@article{hardcastleMultiplexedHeterogeneousAdaptive2017,
  title = {A {{Multiplexed}}, {{Heterogeneous}}, and {{Adaptive Code}} for {{Navigation}} in {{Medial Entorhinal Cortex}}},
  author = {Hardcastle, Kiah and Maheswaranathan, Niru and Ganguli, Surya and Giocomo, Lisa M.},
  year = {2017},
  journal = {Neuron},
  volume = {94},
  number = {2},
  pages = {375-387.e7},
  publisher = {{Elsevier Inc.}},
  doi = {10.1016/j.neuron.2017.03.025},
  abstract = {Medial entorhinal grid cells display strikingly symmetric spatial firing patterns. The clarity of these patterns motivated the use of specific activity pattern shapes to classify entorhinal cell types. While this approach successfully revealed cells that encode boundaries, head direction, and running speed, it left a majority of cells unclassified, and its pre-defined nature may have missed unconventional, yet important coding properties. Here, we apply an unbiased statistical approach to search for cells that encode navigationally relevant variables. This approach successfully classifies the majority of entorhinal cells and reveals unsuspected entorhinal coding principles. First, we find a high degree of mixed selectivity and heterogeneity in superficial entorhinal neurons. Second, we discover a dynamic and remarkably adaptive code for space that enables entorhinal cells to rapidly encode navigational information accurately at high running speeds. Combined, these observations advance our current understanding of the mechanistic origins and functional implications of the entorhinal code for navigation. Video Abstract.},
  keywords = {adaptive coding,computational models of spatial coding,encoding mode,entorhinal cortex,Multiplexed-coding,spatial navigation,tuning heterogeneity},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BXRWY8K6/Hardcastle et al. - 2017 - A Multiplexed, Heterogeneous, and Adaptive Code for Navigation in Medial Entorhinal Cortex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hardcastleMultiplexedHeterogeneousAdaptive2017.md}
}

@article{hardyEncodingTimeFeedforward2018,
  title = {Encoding {{Time}} in {{Feedforward Trajectories}} of a {{Recurrent Neural Network Model}}},
  author = {Hardy, N. and Buonomano, D.},
  year = {2018},
  journal = {Neural Computation},
  doi = {10.1162/neco_a_01041},
  abstract = {It is established for the first time that the same RNN can generate multiple functionally feedforward patterns of activity as a result of dynamic shifts in the E/I balance imposed by the connectome of the RNN. Brain activity evolves through time, creating trajectories of activity that underlie sensorimotor processing, behavior, and learning and memory. Therefore, understanding the temporal nature of neural dynamics is essential to understanding brain function and behavior. In vivo studies have demonstrated that sequential transient activation of neurons can encode time. However, it remains unclear whether these patterns emerge from feedforward network architectures or from recurrent networks and, furthermore, what role network structure plays in timing. We address these issues using a recurrent neural network (RNN) model with distinct populations of excitatory and inhibitory units. Consistent with experimental data, a single RNN could autonomously produce multiple functionally feedforward trajectories, thus potentially encoding multiple timed motor patterns lasting up to several seconds. Importantly, the model accounted for Weber's law, a hallmark of timing behavior. Analysis of network connectivity revealed that efficiency\textemdash a measure of network interconnectedness\textemdash decreased as the number of stored trajectories increased. Additionally, the balance of excitation (E) and inhibition (I) shifted toward excitation during each unit's activation time, generating the prediction that observed sequential activity relies on dynamic control of the E/I balance. Our results establish for the first time that the same RNN can generate multiple functionally feedforward patterns of activity as a result of dynamic shifts in the E/I balance imposed by the connectome of the RNN. We conclude that recurrent network architectures account for sequential neural activity, as well as for a fundamental signature of timing behavior: Weber's law.},
  note = {\section{Annotations\\
(6/13/2022, 2:18:08 PM)}

\par
``Brain activity evolves through time, creating trajectories of activity that underlie sensorimotor processing, behavior, and learning and memory. Therefore, understanding the temporal nature of neural dynamics is essential to understanding brain function and behavior. Invivostudies have demonstrated that sequenti'' (Hardy and Buonomano, 2018, p. 1) Testing!},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JGL3THYL/Hardy_Buonomano_2018_Encoding Time in Feedforward Trajectories of a Recurrent Neural Network Model.pdf}
}

@article{hardyModelTemporalScaling2018,
  title = {A Model of Temporal Scaling Correctly Predicts That Motor Timing Improves with Speed},
  author = {Hardy, Nicholas F. and Goudar, Vishwa and {Romero-Sosa}, Juan L. and Buonomano, Dean V.},
  year = {2018},
  month = nov,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {4732},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-07161-6},
  abstract = {Timing is fundamental to complex motor behaviors: from tying a knot to playing the piano. A general feature of motor timing is temporal scaling: the ability to produce motor patterns at different speeds. One theory of temporal processing proposes that the brain encodes time in dynamic patterns of neural activity (population clocks), here we first examine whether recurrent neural network (RNN) models can account for temporal scaling. Appropriately trained RNNs exhibit temporal scaling over a range similar to that of humans and capture a signature of motor timing, Weber's law, but predict that temporal precision improves at faster speeds. Human psychophysics experiments confirm this prediction: the variability of responses in absolute time are lower at faster speeds. These results establish that RNNs can account for temporal scaling and suggest a novel psychophysical principle: the Weber-Speed effect.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Network models},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YPKSGU3M/Hardy et al_2018_A model of temporal scaling correctly predicts that motor timing improves with.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2J2GDED7/s41467-018-07161-6.html}
}

@article{harishAsynchronousRateChaos2015,
  title = {Asynchronous {{Rate Chaos}} in {{Spiking Neuronal Circuits}}},
  author = {Harish, Omri and Hansel, David},
  year = {2015},
  month = jul,
  journal = {PLOS Computational Biology},
  volume = {11},
  number = {7},
  pages = {e1004266},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004266},
  abstract = {The brain exhibits temporally complex patterns of activity with features similar to those of chaotic systems. Theoretical studies over the last twenty years have described various computational advantages for such regimes in neuronal systems. Nevertheless, it still remains unclear whether chaos requires specific cellular properties or network architectures, or whether it is a generic property of neuronal circuits. We investigate the dynamics of networks of excitatory-inhibitory (EI) spiking neurons with random sparse connectivity operating in the regime of balance of excitation and inhibition. Combining Dynamical Mean-Field Theory with numerical simulations, we show that chaotic, asynchronous firing rate fluctuations emerge generically for sufficiently strong synapses. Two different mechanisms can lead to these chaotic fluctuations. One mechanism relies on slow I-I inhibition which gives rise to slow subthreshold voltage and rate fluctuations. The decorrelation time of these fluctuations is proportional to the time constant of the inhibition. The second mechanism relies on the recurrent E-I-E feedback loop. It requires slow excitation but the inhibition can be fast. In the corresponding dynamical regime all neurons exhibit rate fluctuations on the time scale of the excitation. Another feature of this regime is that the population-averaged firing rate is substantially smaller in the excitatory population than in the inhibitory population. This is not necessarily the case in the I-I mechanism. Finally, we discuss the neurophysiological and computational significance of our results.},
  langid = {english},
  keywords = {Action potentials,Eigenvalues,Neural networks,Neurons,Simulation and modeling,Soil perturbation,Synapses,Transfer functions},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Y6N7HIDB/Harish_Hansel_2015_Asynchronous Rate Chaos in Spiking Neuronal Circuits.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/U5EPDFWK/article.html}
}

@article{harrisCorticalConnectivitySensory2013,
  title = {Cortical Connectivity and Sensory Coding},
  author = {Harris, Kenneth D. and {Mrsic-Flogel}, Thomas D.},
  year = {2013},
  journal = {Nature},
  volume = {503},
  number = {7474},
  pages = {51--58},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687 (Electronic)\textbackslash n0028-0836 (Linking)},
  doi = {10.1038/nature12654},
  abstract = {The sensory cortex contains a wide array of neuronal types, which are connected together into complex but partially stereotyped circuits. Sensory stimuli trigger cascades of electrical activity through these circuits, causing specific features of sensory scenes to be encoded in the firing patterns of cortical populations. Recent research is beginning to reveal how the connectivity of individual neurons relates to the sensory features they encode, how differences in the connectivity patterns of different cortical cell classes enable them to encode information using different strategies, and how feedback connections from higher-order cortex allow sensory information to be integrated with behavioural context.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Q9S2DE2K/Harris, Mrsic-Flogel - 2013 - Cortical connectivity and sensory coding.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@harrisCorticalConnectivitySensory2013.md}
}

@article{harrisNeocorticalCircuitThemes2015,
  title = {The Neocortical Circuit: {{Themes}} and Variations},
  author = {Harris, Kenneth D. and Shepherd, Gordon M.G.},
  year = {2015},
  journal = {Nature Neuroscience},
  volume = {18},
  number = {2},
  pages = {170--181},
  issn = {1546-1726 (Electronic)\textbackslash r1097-6256 (Linking)},
  doi = {10.1038/nn.3917},
  abstract = {Similarities in neocortical circuit organization across areas and species suggest a common strategy to process diverse types of information, including sensation from diverse modalities, motor control and higher cognitive processes. Cortical neurons belong to a small number of main classes. The properties of these classes, including their local and long-range connectivity, developmental history, gene expression, intrinsic physiology and in vivo activity patterns, are remarkably similar across areas. Each class contains subclasses; for a rapidly growing number of these, conserved patterns of input and output connections are also becoming evident. The ensemble of circuit connections constitutes a basic circuit pattern that appears to be repeated across neocortical areas, with area- and species-specific modifications. Such 'serially homologous' organization may adapt individual neocortical regions to the type of information each must process.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9JA35582/Harris, Shepherd - 2015 - The neocortical circuit Themes and variations.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@harrisNeocorticalCircuitThemes2015.md}
}

@article{harveyIntracellularDynamicsHippocampal2009,
  title = {Intracellular Dynamics of Hippocampal Place Cells during Virtual Navigation},
  author = {Harvey, Christopher D. and Collman, Forrest and Dombeck, Daniel A. and Tank, David W.},
  year = {2009},
  journal = {Nature},
  volume = {461},
  number = {7266},
  pages = {941--946},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687 (Electronic) 0028-0836 (Linking)},
  doi = {10.1038/nature08499},
  abstract = {Hippocampal place cells encode spatial information in rate and temporal codes. To examine the mechanisms underlying hippocampal coding, here we measured the intracellular dynamics of place cells by combining in vivo whole-cell recordings with a virtual-reality system. Head-restrained mice, running on a spherical treadmill, interacted with a computer-generated visual environment to perform spatial behaviours. Robust place-cell activity was present during movement along a virtual linear track. From whole-cell recordings, we identified three subthreshold signatures of place fields: an asymmetric ramp-like depolarization of the baseline membrane potential, an increase in the amplitude of intracellular theta oscillations, and a phase precession of the intracellular theta oscillation relative to the extracellularly recorded theta rhythm. These intracellular dynamics underlie the primary features of place-cell rate and temporal codes. The virtual-reality system developed here will enable new experimental approaches to study the neural circuits underlying navigation.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DRXF65V5/Harvey et al. - 2009 - Intracellular dynamics of hippocampal place cells during virtual navigation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@harveyIntracellularDynamicsHippocampal2009.md}
}

@article{haseganMultitimescaleBiologicalLearning2021,
  title = {Multi-Timescale Biological Learning Algorithms Train Spiking Neuronal Network Motor Control},
  author = {Hasegan, Daniel and Deible, Matt and Earl, Christopher W. and D'Onofrio, David J. and Hazan, Hananel and Haroon and Anwar and Neymotin, S.},
  year = {2021},
  journal = {bioRxiv},
  doi = {10.1101/2021.11.20.469405},
  abstract = {An interleaved algorithm inspired by biological evolution that combines the EVOL and STDP-RL learning in sequence to solve the CartPole reinforcement learning (RL) control problem and produces enhanced robustness in performance. Biological learning operates at multiple interlocking timescales, from long evolutionary stretches down to the relatively short time span of an individual's life. While each process has been simulated individually as a basic learning algorithm in the context of spiking neuronal networks (SNNs), the integration of the two has remained limited. In this study, we first train SNNs separately using individual model learning using spike-timing dependent reinforcement learning (STDP-RL) and evolutionary (EVOL) learning algorithms to solve the CartPole reinforcement learning (RL) control problem. We then develop an interleaved algorithm inspired by biological evolution that combines the EVOL and STDP-RL learning in sequence. We use the NEURON simulator with NetPyNE to create an SNN interfaced with the CartPole environment from OpenAI's Gym. In CartPole, the goal is to balance a vertical pole by moving left/right on a 1-D plane. Our SNN contains multiple populations of neurons organized in three layers: sensory layer, association/hidden layer, and motor layer, where neurons are connected by excitatory (AMPA/NMDA) and inhibitory (GABA) synapses. Association and motor layers contain one excitatory (E) population and two inhibitory (I) populations with different synaptic time constants. Each neuron is an event-based integrate-and-fire model with plastic connections between excitatory neurons. In our SNN, the environment activates sensory neurons tuned to specific features of the game state. We split the motor population into subsets representing each movement choice. The subset with more spiking over an interval determines the action. During STDP-RL, we supply intermediary evaluations (reward/punishment) of each action by judging the effectiveness of a move (e.g., moving the CartPole to a balanced position). During EVOL, updates consist of adding together many random perturbations of the connection weights. Each set of random perturbations is weighted by the total episodic reward it achieves when applied independently. We evaluate the performance of each algorithm after training and through the creation of sensory/motor action maps that delineate the network's transformation of sensory inputs into higher-order representations and eventual motor decisions. Both EVOL and STDP-RL training produce SNNs capable of moving the cart left and right and keeping the pole vertical. Compared to the STDP-RL and EVOL algorithms operating on their own, our interleaved training paradigm produced enhanced robustness in performance, with different strategies revealed through analysis of the sensory/motor mappings. Analysis of synaptic weight matrices also shows distributed vs clustered representations after the EVOL and STDP-RL algorithms, respectively. These weight differences also manifest as diffuse vs synchronized firing patterns. Our modeling opens up new capabilities for SNNs in RL and could serve as a testbed for neurobiologists aiming to understand multi-timescale learning mechanisms and dynamics in neuronal circuits.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CBHF6FPR/Hasegan et al_2021_Multi-timescale biological learning algorithms train spiking neuronal network.pdf}
}

@article{hasegawaModelMultisecondTiming2015,
  title = {A Model of Multisecond Timing Behaviour under Peak-Interval Procedures},
  author = {Hasegawa, Takayuki and Sakata, Shogo},
  year = {2015},
  month = apr,
  journal = {Journal of Computational Neuroscience},
  volume = {38},
  number = {2},
  pages = {301--313},
  issn = {1573-6873},
  doi = {10.1007/s10827-014-0542-4},
  abstract = {In this study, the authors developed a fundamental theory of interval timing behaviour, inspired by the learning-to-time (LeT) model and the scalar expectancy theory (SET) model, and based on quantitative analyses of such timing behaviour. Our experiments used the peak-interval procedure with rats. The proposed model of timing behaviour comprises clocks, a regulator, a mixer, a response, and memory. Using our model, we calculated the basic clock speeds indicated by the subjects' behaviour under such peak procedures. In this model, the scalar property can be defined as a kind of transposition, which can then be measured quantitatively. The Akaike information criterion (AIC) values indicated that the current model fit the data slightly better than did the SET model. Our model may therefore provide a useful addition to SET for the analysis of timing behaviour.},
  langid = {english},
  keywords = {Akaike information criterion,Basic clock speed,Learning-to-time model,Peak procedure,Scalar expectancy theory,Scalar property},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UXGVM4TL/A model of multisecond timing behaviour under peak-interval procedures - 11.07.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X4L95JNC/Hasegawa_Sakata_2015_A model of multisecond timing behaviour under peak-interval procedures.pdf}
}

@article{hassibiOptimalBrainSurgeon1994,
  title = {Optimal {{Brain Surgeon}}: Extensions and Performance Comparisons},
  author = {Hassibi, Babak and Stork, David G and Wolff, Gregory and Watanabe, Takahiro},
  year = {1994},
  journal = {Advances in Neural Information Processing Systems 6. Proceedings of the 1993 Conference},
  pages = {263--270},
  issn = {1558603220},
  abstract = {We extend Optimal Brain Surgeon (OBS) -a second-order method for pruning networks -to allow for general error mea-sures, and explore a reduced computational and storage implemen-tation via a dominant eigenspace decomposition. Simulations on nonlinear, noisy pattern classification problems reveal that OBS does lead to improved generalization, and performs favorably in comparison with Optimal Brain Damage (OBD). We find that the required retraining steps in OBD may lead to inferior generaliza-tion, a result that can be interpreted as due to injecting noise back into the system. A common technique is to stop training of a large network at the minimum validation error. We found that the test error could be reduced even further by means of OBS (but not OBD) pruning. Our results justify the t \textasciitilde{} 0 approximation used in OBS and indicate why retraining in a highly pruned network may lead to inferior performance.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2H4FJEZ3/Hassibi et al. - Optimal Brain Surgeon Extensions and performance .pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/V7PH8LHY/NIPS-1993-optimal-brain-surgeon-extensions-and-performance-comparisons-Paper.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hassibiOptimalBrainSurgeon1994.md}
}

@article{hastingsMonteCarloSampling1970,
  title = {Monte {{Carlo}} Sampling Methods Using {{Markov}} Chains and Their Applications},
  author = {Hastings, W. K.},
  year = {1970},
  month = apr,
  journal = {Biometrika},
  volume = {57},
  number = {1},
  pages = {97--109},
  publisher = {{Oxford Academic}},
  issn = {0006-3444},
  doi = {10.1093/biomet/57.1.97},
  abstract = {Summary. A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5NCHT59P/Hastings - 1970 - Monte Carlo sampling methods using Markov chains a.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hastingsMonteCarloSampling1970.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FQ7XC7IW/284580.html;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XP36YXQ7/284580.html}
}

@article{hazanUnsupervisedLearningSelfOrganizing2018,
  title = {Unsupervised {{Learning}} with {{Self-Organizing Spiking Neural Networks}}},
  author = {Hazan, Hananel and Saunders, D. J. and Sanghavi, Darpan T. and Siegelmann, H. and Kozma, R.},
  year = {2018},
  journal = {2018 International Joint Conference on Neural Networks (IJCNN)},
  doi = {10.1109/IJCNN.2018.8489673},
  abstract = {A hybridization of self-organized map properties with spiking neural networks that retain many of the features of SOMs is presented, and using the optimal choice of parameters, this approach produces improvements over state-of-art spiking Neural networks. We present a system comprising a hybridization of self-organized map (SOM) properties with spiking neural networks (SNNs) that retain many of the features of SOMs. Networks are trained in an unsupervised manner to learn a self-organized lattice of filters via excitatory-inhibitory interactions among populations of neurons. We develop and test various inhibition strategies, such as growing with inter-neuron distance and two distinct levels of inhibition. The quality of the unsupervised learning algorithm is evaluated using examples with known labels. Several biologically-inspired classification tools are proposed and compared, including population-level confidence rating, and n-grams using spike motif algorithm. Using the optimal choice of parameters, our approach produces improvements over state-of-art spiking neural networks.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LH8R2828/Hazan et al_2018_Unsupervised Learning with Self-Organizing Spiking Neural Networks.pdf}
}

@article{hebert-dufresneNetworkOnionDivergence2022,
  title = {Network {{Onion Divergence}}: {{Network}} Representation and Comparison Using Nested Configuration Models with Fixed Connectivity, Correlation and Centrality Patterns},
  shorttitle = {Network {{Onion Divergence}}},
  author = {{H{\'e}bert-Dufresne}, Laurent and Young, Jean-Gabriel and Daniels, Alexander and Allard, Antoine},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.08444 [physics]},
  eprint = {2204.08444},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Random networks, constrained to reproduce specific features of networks, are often used to represent and analyze network data as well as their mathematical descriptions. Chief among them, the configuration model constrains random networks by their degree distribution and is foundational to many areas of network science. However, these representations are often selected based on intuition or mathematical and computational simplicity rather than on statistical evidence. To evaluate the quality of a network representation we need to consider both the amount of information required by a random network model as well as the probability of recovering the original data when using the model as a generative process. To this end, we calculate the approximate size of network ensembles generated by the popular configuration model and its generalizations that include degree-correlations and centrality layers based on the onion decomposition. We then apply minimum description length as a model selection criterion and also introduce the Network Onion Divergence: model selection and network comparison over a nested family of configuration models with differing level of structural details. Using over 100 empirical sets of network data, we find that a simple Layered Configuration Model offers the most compact representation of the majority of real networks. We hope that our results will continue to motivate the development of intricate random network models that help capture network structure beyond the simple degree distribution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Social and Information Networks,Physics - Physics and Society},
  note = {Comment: Comments welcomed at laurent.hebert-dufresne@uvm.edu},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Hébert-Dufresne et al_2022_Network Onion Divergence.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HHFIRTNT/2204.html}
}

@inproceedings{heineyAssessmentManipulationComputational2019,
  title = {Assessment and Manipulation of the Computational Capacity of in Vitro Neuronal Networks through Criticality in Neuronal Avalanches},
  booktitle = {2019 {{IEEE}} Symposium Series on Computational Intelligence, {{SSCI}} 2019},
  author = {Heiney, Kristine and Ramstad, Ola Huse and Sandvig, Ioanna and Sandvig, Axel and Nichele, Stefano},
  year = {2019},
  month = dec,
  eprint = {1907.13118},
  eprinttype = {arxiv},
  pages = {247--254},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/SSCI44817.2019.9002693},
  abstract = {In this work, we report the preliminary analysis of the electrophysiological behavior of in vitro neuronal networks to identify when the networks are in a critical state based on the size distribution of network-wide avalanches of activity. The results presented here demonstrate the importance of selecting appropriate parameters in the evaluation of the size distribution and indicate that it is possible to perturb networks showing highly synchronized - or supercritical - behavior into the critical state by increasing the level of inhibition in the network. The classification of critical versus non-critical networks is valuable in identifying networks that can be expected to perform well on computational tasks, as criticality is widely considered to be the state in which a system is best suited for computation. In addition to enabling the identification of networks that are well-suited for computation, this analysis is expected to aid in the classification of networks as perturbed or healthy. This study is part of a larger research project, the overarching aim of which is to develop computational models that are able to reproduce target behaviors observed in in vitro neuronal networks. These models will ultimately be used to aid in the realization of these behaviors in nanomagnet arrays to be used in novel computing hardwares.},
  archiveprefix = {arXiv},
  arxivid = {1907.13118},
  isbn = {978-1-72812-485-8},
  keywords = {Neuronal avalanches,self-organized criticality,unconventional computing},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LDHHZZJT/Heiney et al. - 2019 - Assessment and manipulation of the computational capacity of in vitro neuronal networks through criticality in ne.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MLJTDMIW/Heiney et al. - 2019 - Assessment and manipulation of the computational capacity of in vitro neuronal networks through criticality in ne.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@heineyAssessmentManipulationComputational2019.md}
}

@article{heineyEvaluationCriticalityVitro2019,
  title = {Evaluation of the Criticality of in Vitro Neuronal Networks: {{Toward}} an Assessment of Computational Capacity},
  author = {Heiney, Kristine and Valderhaug, Vibeke Devold and Sandvig, Ioanna and Sandvig, Axel and Tufte, Gunnar and Hammer, Hugo Lewi and Nichele, Stefano},
  year = {2019},
  month = jul,
  eprint = {1907.02351},
  eprinttype = {arxiv},
  abstract = {Novel computing hardwares are necessary to keep up with today's increasing demand for data storage and processing power. In this research project, we turn to the brain for inspiration to develop novel computing substrates that are self-learning, scalable, energy-efficient, and fault-tolerant. The overarching aim of this work is to develop computational models that are able to reproduce target behaviors observed in in vitro neuronal networks. These models will be ultimately be used to aid in the realization of these behaviors in a more engineerable substrate: an array of nanomagnets. The target behaviors will be identified by analyzing electrophysiological recordings of the neuronal networks. Preliminary analysis has been performed to identify when a network is in a critical state based on the size distribution of network-wide avalanches of activity, and the results of this analysis are reported here. This classification of critical versus non-critical networks is valuable in identifying networks that can be expected to perform well on computational tasks, as criticality is widely considered to be the state in which a system is best suited for computation. This type of analysis is expected to enable the identification of networks that are well-suited for computation and the classification of networks as perturbed or healthy.},
  archiveprefix = {arXiv},
  arxivid = {1907.02351},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BHV9V5L3/Heiney et al. - 2019 - Evaluation of the criticality of in vitro neuronal networks Toward an assessment of computational capacity.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XMPZUPP7/Heiney et al. - 2019 - Evaluation of the criticality of in vitro neuronal networks Toward an assessment of computational capacity.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@heineyEvaluationCriticalityVitro2019.md}
}

@article{heInferenceRNAStructural2019,
  title = {Inference of {{RNA}} Structural Contacts by Direct Coupling Analysis},
  author = {He, Xiaoling and Li, Shuaimin and Ou, Xiujuan and Wang, Jun and Xiao, Yi},
  year = {2019},
  journal = {Communications in Information and Systems},
  volume = {19},
  number = {3},
  pages = {279--297},
  publisher = {{International Press of Boston}},
  doi = {10.4310/cis.2019.v19.n3.a3},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@heInferenceRNAStructural2019.md}
}

@article{heliasCorrelationStructureLocal2014,
  title = {The {{Correlation Structure}} of {{Local Neuronal Networks Intrinsically Results}} from {{Recurrent Dynamics}}},
  author = {Helias, Moritz and Tetzlaff, Tom and Diesmann, Markus},
  year = {2014},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {10},
  number = {1},
  pages = {e1003428},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003428},
  abstract = {Correlated neuronal activity is a natural consequence of network connectivity and shared inputs to pairs of neurons, but the task-dependent modulation of correlations in relation to behavior also hints at a functional role. Correlations influence the gain of postsynaptic neurons, the amount of information encoded in the population activity and decoded by readout neurons, and synaptic plasticity. Further, it affects the power and spatial reach of extracellular signals like the local-field potential. A theory of correlated neuronal activity accounting for recurrent connectivity as well as fluctuating external sources is currently lacking. In particular, it is unclear how the recently found mechanism of active decorrelation by negative feedback on the population level affects the network response to externally applied correlated stimuli. Here, we present such an extension of the theory of correlations in stochastic binary networks. We show that (1) for homogeneous external input, the structure of correlations is mainly determined by the local recurrent connectivity, (2) homogeneous external inputs provide an additive, unspecific contribution to the correlations, (3) inhibitory feedback effectively decorrelates neuronal activity, even if neurons receive identical external inputs, and (4) identical synaptic input statistics to excitatory and to inhibitory cells increases intrinsically generated fluctuations and pairwise correlations. We further demonstrate how the accuracy of mean-field predictions can be improved by self-consistently including correlations. As a byproduct, we show that the cancellation of correlations between the summed inputs to pairs of neurons does not originate from the fast tracking of external input, but from the suppression of fluctuations on the population level by the local network. This suppression is a necessary constraint, but not sufficient to determine the structure of correlations; specifically, the structure observed at finite network size differs from the prediction based on perfect tracking, even though perfect tracking implies suppression of population fluctuations.},
  langid = {english},
  keywords = {Action potentials,Afferent neurons,Covariance,Eigenvalues,Neural networks,Neurons,Signaling networks,Synapses},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GRHJADKZ/Helias et al. - 2014 - The Correlation Structure of Local Neuronal Networ.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@heliasCorrelationStructureLocal2014.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5VETWZRY/article.html}
}

@article{hendersonDeepReinforcementLearning2018,
  title = {Deep {{Reinforcement Learning That Matters}}},
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {Machine Learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7GBZE5C8/Henderson et al_2018_Deep Reinforcement Learning That Matters.pdf}
}

@article{herszageModulationLearningMemory2018,
  title = {Modulation of {{Learning}} and {{Memory}}: {{A Shared Framework}} for {{Interference}} and {{Generalization}}},
  shorttitle = {Modulation of {{Learning}} and {{Memory}}},
  author = {Herszage, Jasmine and Censor, Nitzan},
  year = {2018},
  month = nov,
  journal = {Neuroscience},
  volume = {392},
  pages = {270--280},
  issn = {1873-7544},
  doi = {10.1016/j.neuroscience.2018.08.006},
  abstract = {The human brain is known by its ability to modify and update existing memories, mediated by underlying neuronal plasticity. This ability is facilitated by two main phenomena, interference and generalization. Interference occurs when a new memory harms, or is being harmed by, a different memory that was acquired in temporal proximity to it. Generalization on the other hand, refers to the case in which a learned memory is expanded beyond its specific properties. While each of these two phenomena may be well known separately, we review recent evidence primarily in perceptual and motor skill memory, spanning synaptic, neural systems-level, and behavioral research, suggesting that although the outcomes are different, the underlying neural and behavioral processes responsible for their inducements share numerous commonalities. The reviewed literature may imply a common mechanism underlying these two phenomena, and suggests a unified framework of memory and learning in the human brain.},
  langid = {english},
  pmid = {30125686},
  keywords = {Animals,Brain,generalization,Generalization; Psychological,Humans,interference,Learning,memory,Memory,Models; Neurological,motor learning,Motor Skills,Neuronal Plasticity,Perception,perceptual learning,plasticity,skill learning}
}

@article{herszageModulationLearningMemory2018a,
  title = {Modulation of {{Learning}} and {{Memory}}: {{A Shared Framework}} for {{Interference}} and {{Generalization}}},
  shorttitle = {Modulation of {{Learning}} and {{Memory}}},
  author = {Herszage, Jasmine and Censor, N.},
  year = {2018},
  journal = {Neuroscience},
  doi = {10.1016/j.neuroscience.2018.08.006},
  abstract = {Recent evidence primarily in perceptual and motor skill memory is reviewed, spanning synaptic, neural systems-level, and behavioral research, suggesting that although the outcomes are different, the underlying neural and behavioral processes responsible for their inducements share numerous commonalities. The human brain is known by its ability to modify and update existing memories, mediated by underlying neuronal plasticity. This ability is facilitated by two main phenomena, interference and generalization. Interference occurs when a new memory harms, or is being harmed by, a different memory that was acquired in temporal proximity to it. Generalization on the other hand, refers to the case in which a learned memory is expanded beyond its specific properties. While each of these two phenomena may be well known separately, we review recent evidence primarily in perceptual and motor skill memory, spanning synaptic, neural systems-level, and behavioral research, suggesting that although the outcomes are different, the underlying neural and behavioral processes responsible for their inducements share numerous commonalities. The reviewed literature may imply a common mechanism underlying these two phenomena, and suggests a unified framework of memory and learning in the human brain.}
}

@article{hertzIntroductionTheoryNeural1991,
  title = {Introduction to the {{Theory}} of {{Neural Computation}}},
  author = {Hertz, John and Krogh, Anders and Palmer, Richard G. and Horner, Heinz},
  year = {1991},
  journal = {Physics Today},
  volume = {44},
  pages = {70},
  issn = {0031-9228},
  doi = {10.1063/1.2810360},
  abstract = {Not Available},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IGKFUTD9/Hertz et al. - 1991 - Introduction to the Theory of Neural Computation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hertzIntroductionTheoryNeural1991.md}
}

@article{hertzIsingModelsInferring2011,
  title = {Ising Models for Inferring Network Structure from Spike Data},
  author = {Hertz, John and Roudi, Yasser and Tyrcha, Joanna},
  year = {2011},
  month = jun,
  eprint = {1106.1752},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  arxivid = {1106.1752},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/E248G237/Hertz, Roudi, Tyrcha - 2011 - Ising Models for Inferring Network Structure From Spike Data.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KAK7PVWG/Hertz, Roudi, Tyrcha - 2011 - Ising Models for Inferring Network Structure From Spike Data.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hertzIsingModelsInferring2011.md}
}

@article{hertzIsingModelsInferring2013,
  title = {Ising Models for Inferring Network Structure from Spike Data},
  author = {Hertz, John A. and Roudi, Yasser and Tyrcha, Joanna},
  year = {2013},
  journal = {Principles of Neural Coding},
  pages = {527--546},
  issn = {9781439853313},
  doi = {10.1201/b14756},
  abstract = {Now that spike trains from many neurons can be recorded simultaneously, there is a need for methods to decode these data to learn about the networks that these neurons are part of. One approach to this problem is to adjust the parameters of a simple model network to make its spike trains resemble the data as much as possible. The connections in the model network can then give us an idea of how the real neurons that generated the data are connected and how they influence each other. In this chapter we describe how to do this for the simplest kind of model: an Ising network. We derive algorithms for finding the best model connection strengths for fitting a given data set, as well as faster approximate algorithms based on mean field theory. We test the performance of these algorithms on data from model networks and experiments.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YJM9TCKQ/Hertz, Roudi, Tyrcha - 2011 - Ising Models for Inferring Network Structure From Spike Data.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hertzIsingModelsInferring2013.md}
}

@article{hesselRainbowCombiningImprovements2017,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Rainbow},
  author = {Hessel, Matteo and Modayil, Joseph and {van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.02298 [cs]},
  eprint = {1710.02298},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: Under review as a conference paper at AAAI 2018},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Hessel et al_2017_Rainbow.pdf;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@hesselRainbowCombiningImprovements2017.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QVY2BAGJ/1710.html}
}

@incollection{HEXMOOR20151,
  title = {Chapter 1 - Ubiquity of Networks},
  booktitle = {Computational Network Science},
  author = {Hexmoor, Henry},
  editor = {Hexmoor, Henry},
  year = {2015},
  series = {Emerging Trends in Computer Science and Applied Computing},
  pages = {1--14},
  publisher = {{Morgan Kaufmann}},
  address = {{Boston}},
  doi = {10.1016/B978-0-12-800891-1.00001-9},
  isbn = {978-0-12-800891-1},
  keywords = {mathematical network models,network generation algorithms,networks,social networks},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@HEXMOOR20151.md}
}

@article{hickeyDeepBrainStimulation2016,
  title = {Deep {{Brain Stimulation}}: {{A Paradigm Shifting Approach}} to {{Treat Parkinson}}'s {{Disease}}},
  author = {Hickey, Patrick and Stacy, Mark},
  year = {2016},
  month = apr,
  journal = {Frontiers in Neuroscience},
  volume = {10},
  pages = {173--173},
  publisher = {{Frontiers}},
  doi = {10.3389/fnins.2016.00173},
  abstract = {Parkinson disease (PD) is a chronic and progressive movement disorder classically characterized by slowed voluntary movements, resting tremor, muscle rigidity, and impaired gait and balance. Medical treatment is highly successful early on, though the majority of people experience significant complications in later stages. In advanced PD, when medications no longer adequately control motor symptoms, deep brain stimulation (DBS) offers a powerful therapeutic alternative. DBS involves the surgical implantation of one or more electrodes into specific areas of the brain, which modulate or disrupt abnormal patterns of neural signaling within the targeted region. Outcomes are often dramatic following DBS, with improvements in motor function and reductions motor complications having been repeatedly demonstrated. Given such robust responses, emerging indications for DBS are being investigated. In parallel with expansions of therapeutic scope, advancements within the areas of neurosurgical technique and the precision of stimulation delivery have recently broadened as well. This review focuses on the revolutionary addition of DBS to the therapeutic armamentarium for PD, and summarizes the technological advancements in the areas of neuroimaging and biomedical engineering intended to improve targeting, programming and overall management.},
  keywords = {Deep Brain Stimulation,Globus Pallidus,Parkinson's disease,pedunculopontine nucleus,Subthalamic Nucleus},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UWTIWTYX/Hickey, Stacy - 2016 - Deep Brain Stimulation A Paradigm Shifting Approach to Treat Parkinson's Disease.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hickeyDeepBrainStimulation2016.md}
}

@article{hintonStochasticNeighborEmbedding,
  title = {Stochastic {{Neighbor Embedding}}},
  author = {Hinton, Geoffrey and Roweis, Sam},
  pages = {8},
  abstract = {We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional ``images'' of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word ``bank'', to have versions close to the images of both ``river'' and ``finance'' without forcing the images of outdoor concepts to be located close to those of corporate concepts.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2TWJDDVB/Hinton and Roweis - Stochastic Neighbor Embedding.pdf}
}

@article{hintonStochasticNeighborEmbeddinga,
  title = {Stochastic {{Neighbor Embedding}}},
  author = {Hinton, Geoffrey and Roweis, Sam},
  pages = {8},
  abstract = {We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional ``images'' of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word ``bank'', to have versions close to the images of both ``river'' and ``finance'' without forcing the images of outdoor concepts to be located close to those of corporate concepts.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/K34FJVGH/Hinton and Roweis - Stochastic Neighbor Embedding.pdf}
}

@article{hintonStochasticNeighborEmbeddingb,
  title = {Stochastic {{Neighbor Embedding}}},
  author = {Hinton, Geoffrey and Roweis, Sam},
  pages = {8},
  abstract = {We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional ``images'' of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word ``bank'', to have versions close to the images of both ``river'' and ``finance'' without forcing the images of outdoor concepts to be located close to those of corporate concepts.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/56IFNJUD/Hinton and Roweis - Stochastic Neighbor Embedding.pdf}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FUW79CB8/Long-Short-Term-Memory.html}
}

@article{hodassmanEfficientDendriticLearning2022,
  title = {Efficient Dendritic Learning as an Alternative to Synaptic Plasticity Hypothesis},
  author = {Hodassman, Shiri and Vardi, Roni and Tugendhaft, Yael and Goldental, Amir and Kanter, Ido},
  year = {2022},
  month = apr,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {6571},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-10466-8},
  abstract = {Synaptic plasticity is a long-lasting core hypothesis of brain learning that suggests local adaptation between two connecting neurons and forms the foundation of machine learning. The main complexity of synaptic plasticity is that synapses and dendrites connect neurons in series and existing experiments cannot pinpoint the significant imprinted adaptation location. We showed efficient backpropagation and Hebbian learning on dendritic trees, inspired by experimental-based evidence, for sub-dendritic adaptation and its nonlinear amplification. It has proven to achieve success rates approaching unity for handwritten digits recognition, indicating realization of deep learning even by a single dendrite or neuron. Additionally, dendritic amplification practically generates an exponential number of input crosses, higher-order interactions, with the number of inputs, which enhance success rates. However, direct implementation of a large number of the cross weights and their exhaustive manipulation independently is beyond existing and anticipated computational power. Hence, a new type of nonlinear adaptive dendritic hardware for imitating dendritic learning and estimating the computational capability of the brain must be built.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Biophysics,Computational biology and bioinformatics}
}

@article{hollandGeneticArchitectureHuman2021a,
  title = {The Genetic Architecture of Human Complex Phenotypes Is Modulated by Linkage Disequilibrium and Heterozygosity},
  author = {Holland, Dominic and Frei, Oleksandr and Desikan, Rahul and Fan, Chun-Chieh and Shadrin, Alexey A and Smeland, Olav B and Andreassen, Ole A and Dale, Anders M},
  year = {2021},
  month = mar,
  journal = {Genetics},
  volume = {217},
  number = {3},
  issn = {1943-2631},
  doi = {10.1093/genetics/iyaa046},
  abstract = {We propose an extended Gaussian mixture model for the distribution of causal effects of common single nucleotide polymorphisms (SNPs) for human complex phenotypes that depends on linkage disequilibrium (LD) and heterozygosity (H), while also allowing for independent components for small and large effects. Using a precise methodology showing how genome-wide association studies (GWASs) summary statistics (z-scores) arise through LD with underlying causal SNPs, we applied the model to GWAS of multiple human phenotypes. Our findings indicated that causal effects are distributed with dependence on total LD and H, whereby SNPs with lower total LD and H are more likely to be causal with larger effects; this dependence is consistent with models of the influence of negative pressure from natural selection. Compared with the basic Gaussian mixture model it is built on, the extended model\textemdash primarily through quantification of selection pressure\textemdash reproduces with greater accuracy the empirical distributions of z-scores, thus providing better estimates of genetic quantities, such as polygenicity and heritability, that arise from the distribution of causal effects.},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Holland et al_2021_The genetic architecture of human complex phenotypes is modulated by linkage.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@hollandGeneticArchitectureHuman2021a.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZSUBX89Y/6106267.html}
}

@article{hopfield1985neural,
  title = {``{{Neural}}'' Computation of Decisions in Optimization Problems},
  author = {Hopfield, John J and Tank, David W},
  year = {1985},
  journal = {Biological cybernetics},
  volume = {52},
  number = {3},
  pages = {141--152},
  publisher = {{Springer}},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hopfield1985neural.md}
}

@article{hopfield1986computing,
  title = {Computing with Neural Circuits: {{A}} Model},
  author = {Hopfield, John J and Tank, David W},
  year = {1986},
  journal = {Science},
  volume = {233},
  number = {4764},
  pages = {625--633},
  publisher = {{American Association for the Advancement of Science}},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hopfield1986computing.md}
}

@article{hopfieldNeuralNetworksPhysical1982,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities.},
  author = {Hopfield, J. J. and Sporns, O. and Edelman, G. M. and Edelman, Gerald M.},
  year = {1982},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {79},
  number = {8},
  pages = {2554--2558},
  publisher = {{National Academy of Sciences}},
  doi = {10.1073/pnas.79.8.2554},
  abstract = {A recent theoretical emphasis on complex interactions within neural systems underlying consciousness has been accompanied by proposals for the quantitative characterization of these interactions. In this article, we distinguish key aspects of consciousness that are amenable to quantitative measurement from those that are not. We carry out a formal analysis of the strengths and limitations of three quantitative measures of dynamical complexity in the neural systems underlying consciousness: neural complexity, information integration, and causal density. We find that no single measure fully captures the multidimensional complexity of these systems, and all of these measures have practical limitations. Our analysis suggests guidelines for the specification of alternative measures which, in combination, may improve the quantitative characterization of conscious neural systems. Given that some aspects of consciousness are likely to resist quantification altogether, we conclude that a satisfactory theory is likely to be one that combines both qualitative and quantitative elements.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NN6ZGZBH/Hopfield et al. - 1982 - Neural networks and physical systems with emergent collective computational abilities.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hopfieldNeuralNetworksPhysical1982.md}
}

@techreport{hopfieldNeuralNetworksPhysical1982a,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities (Associative Memory/Parallel Processing/Categorization/Content-Addressable Memory/Fail-Soft Devices)},
  author = {Hopfield, J J},
  year = {1982},
  journal = {Proc. NatL Acad. Sci. USA},
  volume = {79},
  pages = {2554--2558},
  abstract = {Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems-having a large number of simple equivalent components (or neurons). The physical meaning ofcon-tent-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details ofthe modeling or the failure of individual devices. Given the dynamical electrochemical properties ofneurons and their interconnections (synapses), we readily understand schemes that use a few neurons to obtain elementary useful biological behavior (1-3). Our understanding of such simple circuits in electronics allows us to plan larger and more complex circuits which are essential to large computers. Because evolution has no such plan, it becomes relevant to ask whether the ability of large collections of neurons to perform "computational" tasks may in part be a spontaneous collective consequence of having a large number of interacting simple neurons. In physical systems made from a large number of simple elements , interactions among large numbers of elementary components yield collective phenomena such as the stable magnetic orientations and domains in a magnetic system or the vortex patterns in fluid flow. Do analogous collective phenomena in a system of simple interacting neurons have useful "computa-tional" correlates? For example, are the stability of memories, the construction of categories of generalization, or time-sequential memory also emergent properties and collective in origin? This paper examines a new modeling ofthis old and fundamental question (4-8) and shows that important computational properties spontaneously arise. All modeling is based on details, and the details of neuro-anatomy and neural function are both myriad and incompletely known (9). In many physical systems, the nature of the emergent collective properties is insensitive to the details inserted in the model (e.g., collisions are essential to generate sound waves, but any reasonable interatomic force law will yield appropriate collisions). In the same spirit, I will seek collective properties that are robust against change in the model details. The model could be readily implemented by integrated circuit hardware. The conclusions suggest the design of a delo-calized content-addressable memory or categorizer using extensive asynchronous parallel processing.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/M3XLEI2J/Monz et al. - 2001 - Y su interaccion con las plantas.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SXR823MI/Hopfield - 1982 - Neural networks and physical systems with emergent collective computational abilities (associative memoryparallel proc.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hopfieldNeuralNetworksPhysical1982a.md}
}

@article{horiuchiGridCells3D2015,
  title = {Grid Cells in 3-{{D}}: {{Reconciling}} Data and Models},
  author = {Horiuchi, Timothy K. and Moss, Cynthia F.},
  year = {2015},
  journal = {Hippocampus},
  volume = {25},
  number = {12},
  pages = {1489--1500},
  doi = {10.1002/hipo.22469},
  abstract = {It is well documented that place cells and grid cells in echolocating bats show properties similar to those described in rodents, and yet, continuous theta-frequency oscillations, proposed to play a central role in grid/place cell formation, are not present in bat recordings. These comparative neurophysiological data have raised many questions about the role of theta-frequency oscillations in spatial memory and navigation. Additionally, spatial navigation in three-dimensions poses new challenges for the representation of space in neural models. Inspired by the literature on space representation in the echolocating bat, we have developed a non-oscillatory model of 3-D grid cell creation that shares many of the features of existing oscillatory-interference models. We discuss the model in the context of current knowledge of 3-D space representation and highlight directions for future research. This article is protected by copyright. All rights reserved.},
  keywords = {Bat,Grid cells,Hippocampus,Place cells,Space representation},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DFZP2ZYA/Horiuchi, Moss - 2015 - Grid cells in 3-D Reconciling data and models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@horiuchiGridCells3D2015.md}
}

@article{horovitzCooperativeInteractionsProtein1992,
  title = {Co-Operative Interactions during Protein Folding},
  author = {Horovitz, Amnon and Fersht, Alan R.},
  year = {1992},
  month = apr,
  journal = {Journal of Molecular Biology},
  volume = {224},
  number = {3},
  pages = {733--740},
  publisher = {{Academic Press}},
  doi = {10.1016/0022-2836(92)90557-Z},
  abstract = {The theory for measuring co-operativity between interactions in proteins by protein engineering experiments is developed by introducing a procedure for analysing increasing orders of synergy in a protein with increasing numbers of residues. The (pairwise) interaction energy ({$\Delta$}2Gint) between two side-chains may be measured experimentally by a double-mutant cycle consisting of the wild-type protein, the two single mutants and the double mutant. This procedure may be extended to three residues to give a value for {$\Delta$}3Gint for a triple-mutant cube, and to higher orders using multi-dimensional mutant space. We now show that {$\Delta$}3Gint is the excess energy of adding all three chains compared with the sum of all the pairwise values of ({$\Delta$}2Gint) for each of the constituent double-mutant cycles and the sum of all the single addition energies. This physical interpretation extends to higher orders of mutation. {$\Delta$}nGint (i.e. the interaction energy for n residues), thus, reveals the layers of synergy in interactions as a protein is built up. This procedure is applied to measuring changes in synergy during the refolding of barnase for the triad of salt-linked residues Asp8, Asp12 and Arg110, which are mutated to alanine residues. The value of {$\Delta$}3Gint in the folded structure is 0.77({$\pm$}0.06) kcal mol-1 (i.e. the triad is 0.77 kcal mol-1 more stable than expected from the sum of the individual pairwise interactions and single contributions). The value of {$\Delta$}3Gint is still significant in the transition state for unfolding (0.60({$\pm$}0.07) kcal mol-1) and in the folding intermediate (0.60({$\pm$}0.13 kcal mol-1). These results show that synergistic interactions exist in barnase, in its transition state for unfolding and in a refolding intermediate. A direct measurement of the change of co-operativity between the folded state and the transition state for unfolding shows a decrease of 0.17({$\pm$}0.04) kcal mol-1, suggesting that the initial stages of protein unfolding may be accompanied by some loosening of structure in parts that still interact. The similar extent of co-operativity in the transition state for unfolding and the intermediate in refolding suggests that the intermediate is homogeneous, at least in the region of the salt-linked triad, as heterogeneity would lower the co-operativity. \textcopyright{} 1992.},
  keywords = {barnase,mutagenesis,protein engineering,protein folding,protein stability},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SRPRNS8C/Horovitz, Fersht - 1992 - Co-operative interactions during protein folding.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@horovitzCooperativeInteractionsProtein1992.md}
}

@article{hosseiniOrganicSynapticCircuit2022,
  title = {An Organic Synaptic Circuit: Towards Flexible and Biocompatible Organic Neuromorphic Processing},
  shorttitle = {An Organic Synaptic Circuit},
  author = {Hosseini, Mohammad Javad Mirshojaeian and Yang, Yi and Prendergast, Aidan J and Donati, Elisa and Faezipour, Miad and Indiveri, Giacomo and Nawrocki, Robert A},
  year = {2022},
  journal = {Neuromorphic Computing and Engineering},
  issn = {2634-4386},
  doi = {10.1088/2634-4386/ac830c},
  abstract = {In the nervous system synapses play a critical role in computation. In neuromorphic systems, biologically inspired hardware implementations of spiking neural networks, electronic synaptic circuits pass signals between silicon neurons by integrating pre-synaptic voltage pulses and converting them into post-synaptic currents, which are scaled by the synaptic weight parameter. The overwhelming majority of neuromorphic systems are implemented using inorganic, mainly silicon, technology. As such, they are physically rigid, require expensive fabrication equipment and high fabrication temperatures, are limited to small-area fabrication, and are difficult to interface with biological tissue. Organic electronics are based on electronic properties of carbon-based molecules and polymers and offer benefits including physical flexibility, low cost, low temperature, and large-area fabrication, as well as biocompatibility, all unavailable to inorganic electronics. Here, we demonstrate an organic differential-pair integrator synaptic circuit, a biologically realistic synapse model, implemented using physically flexible complementary organic electronics. The synapse is shown to convert input voltage spikes into output current traces with biologically realistic time scales. We characterize circuit's responses based on various synaptic parameters, including gain and weighting voltages, time-constant, synaptic capacitance, and circuit response due to inputs of different frequencies. Time constants comparable to those of biological synapses and the neurons are critical in processing real-world sensory signals such as speech, or bio-signals measured from the body. For processing even slower signals, e.g., on behavioral time scales, we demonstrate time constants in excess of two seconds, while biologically plausible time constants are achieved by deploying smaller synaptic capacitors. We measure the circuit synaptic response to input voltage spikes and present the circuit response properties using custom-made circuit simulations, which are in good agreement with the measured behavior.},
  langid = {english},
  keywords = {neuromorphic},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7954C8HZ/Hosseini et al_2022_An organic synaptic circuit.pdf}
}

@inproceedings{howardEfficientNeuralComputation2019,
  title = {Efficient {{Neural Computation}} in the {{Laplace Domain}}},
  booktitle = {{{CoCo}}@{{NIPS}}},
  author = {Howard, Marc W. and Shankar, Karthik H. and Tiganj, Zoran},
  year = {2019},
  month = jul,
  abstract = {Cognitive computation ought to be fast, efficient and flexible, reusing the same neural mechanisms to operate on many different forms of information. In order to develop neural models for cognitive...},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7AEF6VCY/Howard et al_2019_Efficient Neural Computation in the Laplace Domain.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZXAWE4SA/forum.html}
}

@misc{HowToRebootTurn2022,
  title = {[{{HowTo}}] Reboot / Turn off Your Frozen Computer: {{REISUB}}/{{REISUO}} - {{Contributions}} / {{Tutorials}}},
  shorttitle = {[{{HowTo}}] Reboot / Turn off Your Frozen Computer},
  year = {2022},
  month = jan,
  journal = {Manjaro Linux Forum},
  abstract = {Difficulty: \ding{72}\ding{72}\ding{73}\ding{73}\ding{73}  This tutorial was created because people are using a hard power off during freezes, so this will teach you how to turn off your computer when everything is frozen using softer means.  So whenever you see someone saying they had to ``Power Off'' or ``Hard Reset'' their system without doing a REISUB, please send them here! 😇  If you're reading this in response to a question, please click the green link above this text to bring you to the full unabridged text of the tutorial...},
  howpublished = {https://forum.manjaro.org/t/howto-reboot-turn-off-your-frozen-computer-reisub-reisuo/3855/55},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/R8THV4LZ/3855.html}
}

@article{huangStatisticalMechanicsbasedMethod2011,
  title = {Statistical Mechanics-Based Method to Extract Atomic Distance-Dependent Potentials from Protein Structures},
  author = {Huang, Sheng You and Zou, Xiaoqin},
  year = {2011},
  month = sep,
  journal = {Proteins: Structure, Function and Bioinformatics},
  volume = {79},
  number = {9},
  pages = {2648--2661},
  doi = {10.1002/prot.23086},
  abstract = {In this study, we have developed a statistical mechanics-based iterative method to extract statistical atomic interaction potentials from known, nonredundant protein structures. Our method circumvents the long-standing reference state problem in deriving traditional knowledge-based scoring functions, by using rapid iterations through a physical, global convergence function. The rapid convergence of this physics-based method, unlike other parameter optimization methods, warrants the feasibility of deriving distance-dependent, all-atom statistical potentials to keep the scoring accuracy. The derived potentials, referred to as ITScore/Pro, have been validated using three diverse benchmarks: the high-resolution decoy set, the AMBER benchmark decoy set, and the CASP8 decoy set. Significant improvement in performance has been achieved. Finally, comparisons between the potentials of our model and potentials of a knowledge-based scoring function with a randomized reference state have revealed the reason for the better performance of our scoring function, which could provide useful insight into the development of other physical scoring functions. The potentials developed in this study are generally applicable for structural selection in protein structure prediction. \textcopyright{} 2011 Wiley-Liss, Inc.},
  keywords = {Distance-dependent all-atom potentials,Knowledge-based,Protein structure prediction,Scoring function},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XARGEQ6P/Huang, Zou - 2011 - Statistical mechanics-based method to extract atomic distance-dependent potentials from protein structures.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@huangStatisticalMechanicsbasedMethod2011.md}
}

@article{humplikProbabilisticModelsNeural2017,
  title = {Probabilistic Models for Neural Populations That Naturally Capture Global Coupling and Criticality},
  author = {Humplik, Jan and Tka{\v c}ik, Ga{\v s}per},
  year = {2017},
  month = sep,
  journal = {PLoS computational biology},
  volume = {13},
  number = {9},
  pages = {e1005763},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005763},
  abstract = {Advances in multi-unit recordings pave the way for statistical modeling of activity patterns in large neural populations. Recent studies have shown that the summed activity of all neurons strongly shapes the population response. A separate recent finding has been that neural populations also exhibit criticality, an anomalously large dynamic range for the probabilities of different population activity patterns. Motivated by these two observations, we introduce a class of probabilistic models which takes into account the prior knowledge that the neural population could be globally coupled and close to critical. These models consist of an energy function which parametrizes interactions between small groups of neurons, and an arbitrary positive, strictly increasing, and twice differentiable function which maps the energy of a population pattern to its probability. We show that: 1) augmenting a pairwise Ising model with a nonlinearity yields an accurate description of the activity of retinal ganglion cells which outperforms previous models based on the summed activity of neurons; 2) prior knowledge that the population is critical translates to prior expectations about the shape of the nonlinearity; 3) the nonlinearity admits an interpretation in terms of a continuous latent variable globally coupling the system whose distribution we can infer from data. Our method is independent of the underlying system's state space; hence, it can be applied to other systems such as natural scenes or amino acid sequences of proteins which are also known to exhibit criticality.},
  langid = {english},
  pmcid = {PMC5621705},
  pmid = {28926564},
  keywords = {Animals,Computational Biology,Models; Neurological,Models; Statistical,Neurons,Retinal Neurons,Thermodynamics,Urodela},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CBC2SCGJ/Humplik and Tkačik - 2017 - Probabilistic models for neural populations that n.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@humplikProbabilisticModelsNeural2017.md}
}

@article{hungOptimizingAgentBehavior2019,
  title = {Optimizing Agent Behavior over Long Time Scales by Transporting Value},
  author = {Hung, Chia-Chun and Lillicrap, Timothy and Abramson, Josh and Wu, Yan and Mirza, Mehdi and Carnevale, Federico and Ahuja, Arun and Wayne, Greg},
  year = {2019},
  month = nov,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {5223},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13073-w},
  abstract = {Humans prolifically engage in mental time travel. We dwell on past actions and experience satisfaction or regret. More than storytelling, these recollections change how we act in the future and endow us with a computationally important ability to link actions and consequences across spans of time, which helps address the problem of long-term credit assignment: the question of how to evaluate the utility of actions within a long-duration behavioral sequence. Existing approaches to credit assignment in AI cannot solve tasks with long delays between actions and consequences. Here, we introduce a paradigm where agents use recall of specific memories to credit past actions, allowing them to solve problems that are intractable for existing algorithms. This paradigm broadens the scope of problems that can be investigated in AI and offers a mechanistic account of behaviors that may inspire models in neuroscience, psychology, and behavioral economics.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Information technology,Learning algorithms},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LPKJUMV8/Hung et al_2019_Optimizing agent behavior over long time scales by transporting value.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TD6DF2D8/s41467-019-13073-w.html}
}

@article{hutsebaut-buysseHierarchicalReinforcementLearning2022,
  title = {Hierarchical {{Reinforcement Learning}}: {{A Survey}} and {{Open Research Challenges}}},
  shorttitle = {Hierarchical {{Reinforcement Learning}}},
  author = {{Hutsebaut-Buysse}, Matthias and Mets, Kevin and Latr{\'e}, Steven},
  year = {2022},
  month = mar,
  journal = {Machine Learning and Knowledge Extraction},
  volume = {4},
  number = {1},
  pages = {172--221},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2504-4990},
  doi = {10.3390/make4010009},
  abstract = {Reinforcement learning (RL) allows an agent to solve sequential decision-making problems by interacting with an environment in a trial-and-error fashion. When these environments are very complex, pure random exploration of possible solutions often fails, or is very sample inefficient, requiring an unreasonable amount of interaction with the environment. Hierarchical reinforcement learning (HRL) utilizes forms of temporal- and state-abstractions in order to tackle these challenges, while simultaneously paving the road for behavior reuse and increased interpretability of RL systems. In this survey paper we first introduce a selection of problem-specific approaches, which provided insight in how to utilize often handcrafted abstractions in specific task settings. We then introduce the Options framework, which provides a more generic approach, allowing abstractions to be discovered and learned semi-automatically. Afterwards we introduce the goal-conditional approach, which allows sub-behaviors to be embedded in a continuous space. In order to further advance the development of HRL agents, capable of simultaneously learning abstractions and how to use them, solely from interaction with complex high dimensional environments, we also identify a set of promising research directions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep reinforcement learning,hierarchical reinforcement learning,HRL,reinforcement learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4IQ2ZVHQ/Hutsebaut-Buysse et al_2022_Hierarchical Reinforcement Learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5KY4THYA/htm.html}
}

@article{hutterFoundationsUniversalSequence2006,
  title = {On the Foundations of Universal Sequence Prediction},
  author = {Hutter, Marcus},
  year = {2006},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {3959 LNCS},
  pages = {408--420},
  issn = {3540340211},
  doi = {10.1007/11750321_39},
  abstract = {Solomonoff completed the Bayesian framework by providing a rigorous, unique, formal, and universal choice for the model class and the prior. We discuss in breadth how and in which sense universal (non-i.i.d.) sequence prediction solves various (philosophical) problems of traditional Bayesian sequence prediction. We show that Solomonoff's model possesses many desirable properties: Fast convergence and strong bounds, and in contrast to most classical continuous prior densities has no zero p(oste)rior problem, i.e. can confirm universal hypotheses, is reparametrization and regrouping invariant, and avoids the old-evidence and updating problem. It even performs well (actually better) in non-computable environments. \textcopyright{} Springer-Verlag Berlin Heidelberg 2006.},
  keywords = {bayes,cam,confirmation theory,kolmogorov complexity,metry principle,model classes,oc-,old-,philosophical issues,prediction bounds,reparametrization invariance,s razor,sequence prediction,solomonoff prior,sym-},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/A562B4LW/Hutter - 2006 - On the foundations of universal sequence prediction.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@hutterFoundationsUniversalSequence2006.md}
}

@misc{IEEEXploreFullText,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@IEEEXploreFullText.md}
}

@article{ioannidesSpatiotemporalDynamicsSpiking2022,
  title = {Spatiotemporal Dynamics in Spiking Recurrent Neural Networks Using Modified-Full-{{FORCE}} on {{EEG}} Signals},
  author = {Ioannides, Georgios and Kourouklides, Ioannis and Astolfi, Alessandro},
  year = {2022},
  month = feb,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {2896},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-06573-1},
  abstract = {Methods on modelling the human brain as a Complex System have increased remarkably in the literature as researchers seek to understand the underlying foundations behind cognition, behaviour, and perception. Computational methods, especially Graph Theory-based methods, have recently contributed significantly in understanding the wiring connectivity of the brain, modelling it as a set of nodes connected by edges. Therefore, the brain's spatiotemporal dynamics can be holistically studied by considering a network, which consists of many neurons, represented by nodes. Various models have been proposed for modelling such neurons. A recently proposed method in training such networks, called full-Force, produces networks that perform tasks with fewer neurons and greater noise robustness than previous least-squares approaches (i.e. FORCE method). In this paper, the first direct applicability of a variant of the full-Force method to biologically-motivated Spiking RNNs (SRNNs) is demonstrated. The SRNN is a graph consisting of modules. Each module is modelled as a Small-World Network (SWN), which is a specific type of a biologically-plausible graph. So, the first direct applicability of a variant of the full-Force method to modular SWNs is demonstrated, evaluated through regression and information theoretic metrics. For the first time, the aforementioned method is applied to spiking neuron models and trained on various real-life Electroencephalography (EEG) signals. To the best of the authors' knowledge, all the contributions of this paper are novel. Results show that trained SRNNs match EEG signals almost perfectly, while network dynamics can mimic the target dynamics. This demonstrates that the holistic setup of the network model and the neuron model which are both more biologically plausible than previous work, can be tuned into real biological signal dynamics.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational neuroscience,Mathematics and computing},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RQLKJD4W/Ioannides et al_2022_Spatiotemporal dynamics in spiking recurrent neural networks using.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8HTIPK6C/s41598-022-06573-1.html}
}

@techreport{jacqueskotzeIntroductionMonteCarlo2008,
  title = {Introduction to {{Monte Carlo}} Methods for an {{Ising Model}} of a {{Ferromagnet}}},
  author = {{Jacques Kotze}},
  year = {2008},
  abstract = {arXiv:0803.0217v1},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FH3G72LC/Jacques Kotze - 2008 - Introduction to Monte Carlo methods for an Ising Model of a Ferromagnet.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@jacqueskotzeIntroductionMonteCarlo2008.md}
}

@techreport{jaderbergSpatialTransformerNetworks,
  title = {Spatial {{Transformer Networks}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N98EGJ37/Jaderberg et al. - Unknown - Spatial Transformer Networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@jaderbergSpatialTransformerNetworks.md}
}

@article{jadhavAwakeHippocampalSharpWave2012,
  title = {Awake {{Hippocampal Sharp-Wave Ripples Support Spatial Memory HHS Public Access}}},
  author = {Jadhav, Shantanu P and Kemere, Caleb and German, P Walter and Frank, Loren M},
  year = {2012},
  journal = {Science},
  volume = {336},
  number = {6087},
  pages = {1454--1458},
  doi = {10.1126/science.1217230},
  abstract = {The hippocampus is critical for spatial learning and memory. Hippocampal neurons in awake animals exhibit place field activity that encodes current location, as well as sharp-wave ripple (SWR) activity during which representations based on past experiences are often replayed. The relationship between these patterns of activity and the memory functions of the hippocampus is poorly understood. We interrupted awake SWRs in animals learning a spatial alternation task. We observed a specific learning and performance deficit that persisted throughout training. This deficit was associated with awake SWR activity, as SWR interruption left place field activity and post-experience SWR reactivation intact. These results provide a link between awake SWRs and hippocampal memory processes, which suggests that awake replay of memory-related information during SWRs supports learning and memory-guided decision-making. Animals use past experience to guide decisions, an ability that requires storing memories for the events of daily life and retrieving those memories as needed. This storage and retrieval depends on the hippocampus and associated structures in the medial temporal lobe (1-5), but the specific patterns of neural activity that support these memory functions remain poorly understood. We know that during exploration, individual neurons fire in specific regions of space (5, 6) known as place fields. In contrast, during periods of slow movement, immobility, and slow-wave sleep, groups of neurons are active during sharp-wave ripple (SWR) events (7, 8). This activity frequently represents a replay of a past experience on a rapid time scale (9-13). SWRs that occur during sleep contribute to memory consolidation of preceding experiences (14-18), and both changes in place fields and the intensity of awake memory reactivation have been correlated with memory performance (19). Awake SWRs in particular can reactivate sets of place fields encoding forward and reverse paths associated with both current and past locations (9-13). This reactivation has been hypothesized to contribute to multiple functions including learning, retrieval, consolidation,},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9NHGFAYA/Jadhav et al. - 2012 - Awake Hippocampal Sharp-Wave Ripples Support Spatial Memory HHS Public Access.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@jadhavAwakeHippocampalSharpWave2012.md}
}

@article{jakobDopamineMediatesBidirectional2021,
  title = {Dopamine Mediates the Bidirectional Update of Interval Timing},
  author = {Jakob, Anthony M. V. and Mikhael, John G. and Hamilos, Allison E. and Assad, J. and Gershman, S.},
  year = {2021},
  doi = {10.1101/2021.11.02.466803},
  abstract = {Measurements of dopaminergic neurons in the substantia nigra pars compacta of mice performing a self-timed movement task were reanalyzed and it was found that trial-by-trial changes in the slope could be predicted from the timing of dopamine activity on the previous trial. The role of dopamine as a reward prediction error signal in reinforcement learning tasks has been well-established over the past decades. Recent work has shown that the reward prediction error interpretation can also account for the effects of dopamine on interval timing by controlling the speed of subjective time. According to this theory, the timing of the dopamine signal relative to reward delivery dictates whether subjective time speeds up or slows down: Early DA signals speed up subjective time and late signals slow it down. To test this bidirectional prediction, we reanalyzed measurements of dopaminergic neurons in the substantia nigra pars compacta of mice performing a self-timed movement task. Using the slope of ramping dopamine activity as a read-out of subjective time speed, we found that trial-by-trial changes in the slope could be predicted from the timing of dopamine activity on the previous trial. This result provides a key piece of evidence supporting a unified computational theory of reinforcement learning and interval timing.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9KZMT28N/Jakob et al_2021_Dopamine mediates the bidirectional update of interval timing.pdf}
}

@article{janckeGenderDifferencesCortical2004,
  title = {Gender Differences in Cortical Complexity},
  author = {Jancke, Lutz and Toga, Arthur W and Narr, Katherine L and Thompson, Paul M and Rex, David E and Luders, Eileen and Steinmetz, Helmuth},
  year = {2004},
  journal = {Nature Neuroscience},
  volume = {7},
  number = {8},
  pages = {799--800},
  doi = {10.1038/nn1277},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/75QCC9I8/Jancke et al. - 2004 - Gender differences in cortical complexity.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@janckeGenderDifferencesCortical2004.md}
}

@article{jaynesInformationTheoryStatistical1957,
  title = {Information Theory and Statistical Mechanics},
  author = {Jaynes, E. T.},
  year = {1957},
  month = may,
  journal = {Physical Review},
  volume = {106},
  number = {4},
  pages = {620--630},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRev.106.620},
  abstract = {Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting "subjective statistical mechanics," the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that could have been made on the basis of the information available. It is concluded that statistical mechanics need not be regarded as a physical theory dependent for its validity on the truth of additional assumptions not contained in the laws of mechanics (such as ergodicity, metric transitivity, equal a priori probabilities, etc.). Furthermore, it is possible to maintain a sharp distinction between its physical and statistical aspects. The former consists only of the correct enumeration of the states of a system and their properties; the latter is a straightforward example of statistical inference. \textcopyright{} 1957 The American Physical Society.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@jaynesInformationTheoryStatistical1957.md}
}

@article{jaynesRationaleMaximumentropyMethods1982,
  title = {On the Rationale of Maximum-Entropy Methods},
  author = {Jaynes, E.T.},
  year = {1982},
  journal = {Proceedings of the IEEE},
  volume = {70},
  number = {9},
  pages = {939--952},
  doi = {10.1109/PROC.1982.12425},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TN3Y4SGH/Jaynes - 1982 - On the rationale of maximum-entropy methods.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@jaynesRationaleMaximumentropyMethods1982.md}
}

@article{jazayeriNeuralMechanismSensing2015,
  title = {A {{Neural Mechanism}} for {{Sensing}} and {{Reproducing}} a {{Time Interval}}},
  author = {Jazayeri, Mehrdad and Shadlen, Michael N.},
  year = {2015},
  month = oct,
  journal = {Current biology: CB},
  volume = {25},
  number = {20},
  pages = {2599--2609},
  issn = {1879-0445},
  doi = {10.1016/j.cub.2015.08.038},
  abstract = {Timing plays a crucial role in sensorimotor function. However, the neural mechanisms that enable the brain to flexibly measure and reproduce time intervals are not known. We recorded neural activity in parietal cortex of monkeys in a time reproduction task. Monkeys were trained to measure and immediately afterward reproduce different sample intervals. While measuring an interval, neural responses had a nonlinear profile that increased with the duration of the sample interval. Activity was reset during the transition from measurement to production and was followed by a ramping activity whose slope encoded the previously measured sample interval. We found that firing rates at the end of the measurement epoch were correlated with both the slope of the ramp and the monkey's corresponding production interval on a trial-by-trial basis. Analysis of response dynamics further linked the rate of change of firing rates in the measurement epoch to the slope of the ramp in the production epoch. These observations suggest that, during time reproduction, an interval is measured prospectively in relation to the desired motor plan to reproduce that interval.},
  langid = {english},
  pmcid = {PMC4618078},
  pmid = {26455307},
  keywords = {Action Potentials,Animals,Macaca mulatta,Motor Activity,Motor Neurons,Parietal Lobe,Time Perception},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3YUV8SCY/Jazayeri_Shadlen_2015_A Neural Mechanism for Sensing and Reproducing a Time Interval.pdf}
}

@techreport{jensenComputationArtificialSpin,
  title = {Computation in Artificial Spin Ice},
  author = {Jensen, Johannes H and Folven, Erik and Tufte, Gunnar},
  abstract = {We explore artificial spin ice (ASI) as a substrate for material computation. ASI consists of large numbers of nanomag-nets arranged in a 2D lattice. Local interactions between the magnets gives rise to a range of complex collective behavior. The ferromagnets form large networks of nonlinear nodes, which in many ways resemble artificial neural networks. In this work, we investigate key computational properties of ASI through micromagnetic simulations. Our nanomagnetic system exhibits a large number of reachable stable states and a wide range of available dynamics when perturbed by an external magnetic field. Furthermore, we find that the system is able to store and process temporal input patterns. The emergent behavior is highly tunable by varying the parameters of the external field. Our findings highlight ASI as a very promising substrate for in-materio computation at the nanoscale.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GTSWXAVM/Jensen, Folven, Tufte - Unknown - Computation in artificial spin ice.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@jensenComputationArtificialSpin.md}
}

@inproceedings{jensenComputationArtificialSpin2018,
  title = {Computation in Artificial Spin Ice},
  booktitle = {The 2018 {{Conference}} on {{Artificial Life}}},
  author = {Jensen, Johannes H. and Folven, Erik and Tufte, Gunnar},
  year = {2018},
  pages = {15--22},
  publisher = {{MIT Press}},
  address = {{Tokyo, Japan}},
  doi = {10.1162/isal_a_00011},
  abstract = {We explore artificial spin ice (ASI) as a substrate for material computation. ASI consists of large numbers of nanomagnets arranged in a 2D lattice. Local interactions between the magnets gives rise to a range of complex collective behavior. The ferromagnets form large networks of nonlinear nodes, which in many ways resemble artificial neural networks. In this work, we investigate key computational properties of ASI through micromagnetic simulations. Our nanomagnetic system exhibits a large number of reachable stable states and a wide range of available dynamics when perturbed by an external magnetic field. Furthermore, we find that the system is able to store and process temporal input patterns. The emergent behavior is highly tunable by varying the parameters of the external field. Our findings highlight ASI as a very promising substrate for in-materio computation at the nanoscale.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/86474J7U/Jensen et al. - 2018 - Computation in artificial spin ice.pdf;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@jensenComputationArtificialSpin2018.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/jensenComputationArtificialSpin2018-mdnotes.md}
}

@article{jiaNeuronalPlasticityRewardPropagationImproved2021,
  title = {Neuronal-{{Plasticity}} and {{Reward-Propagation Improved Recurrent Spiking Neural Networks}}},
  author = {Jia, Shuncheng and Zhang, Tielin and Cheng, Xiang and Liu, Hongxing and Xu, Bo},
  year = {2021},
  journal = {Frontiers in Neuroscience},
  doi = {10.3389/fnins.2021.654786},
  abstract = {The historically-related adaptive threshold with two channels is highlighted as important neuronal plasticity for increasing the neuronal dynamics, and then global labels instead of errors are used as a reward for the paralleling gradient propagation. Different types of dynamics and plasticity principles found through natural neural networks have been well-applied on Spiking neural networks (SNNs) because of their biologically-plausible efficient and robust computations compared to their counterpart deep neural networks (DNNs). Here, we further propose a special Neuronal-plasticity and Reward-propagation improved Recurrent SNN (NRR-SNN). The historically-related adaptive threshold with two channels is highlighted as important neuronal plasticity for increasing the neuronal dynamics, and then global labels instead of errors are used as a reward for the paralleling gradient propagation. Besides, a recurrent loop with proper sparseness is designed for robust computation. Higher accuracy and stronger robust computation are achieved on two sequential datasets (i.e., TIDigits and TIMIT datasets), which to some extent, shows the power of the proposed NRR-SNN with biologically-plausible improvements.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UVSKEJ2Z/Jia et al_2021_Neuronal-Plasticity and Reward-Propagation Improved Recurrent Spiking Neural.pdf}
}

@article{jirsaEntropyFreeEnergy2022,
  title = {Entropy, Free Energy, Symmetry and Dynamics in the Brain},
  author = {Jirsa, Viktor and Sheheitli, Hiba},
  year = {2022},
  month = feb,
  journal = {Journal of Physics: Complexity},
  volume = {3},
  number = {1},
  pages = {015007},
  publisher = {{IOP Publishing}},
  issn = {2632-072X},
  doi = {10.1088/2632-072X/ac4bec},
  abstract = {Neuroscience is home to concepts and theories with roots in a variety of domains including information theory, dynamical systems theory, and cognitive psychology. Not all of those can be coherently linked, some concepts are incommensurable, and domain-specific language poses an obstacle to integration. Still, conceptual integration is a form of understanding that provides intuition and consolidation, without which progress remains unguided. This paper is concerned with the integration of deterministic and stochastic processes within an information theoretic framework, linking information entropy and free energy to mechanisms of emergent dynamics and self-organization in brain networks. We identify basic properties of neuronal populations leading to an equivariant matrix in a network, in which complex behaviors can naturally be represented through structured flows on manifolds establishing the internal model relevant to theories of brain function. We propose a neural mechanism for the generation of internal models from symmetry breaking in the connectivity of brain networks. The emergent perspective illustrates how free energy can be linked to internal models and how they arise from the neural substrate.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SWG7PDYU/Jirsa and Sheheitli - 2022 - Entropy, free energy, symmetry and dynamics in the.pdf}
}

@article{jirsaEntropyFreeEnergy2022a,
  title = {Entropy, Free Energy, Symmetry and Dynamics in the Brain},
  author = {Jirsa, Viktor and Sheheitli, Hiba},
  year = {2022},
  month = mar,
  journal = {Journal of Physics: Complexity},
  volume = {3},
  number = {1},
  pages = {015007},
  issn = {2632-072X},
  doi = {10.1088/2632-072X/ac4bec},
  abstract = {Neuroscience is home to concepts and theories with roots in a variety of domains including information theory, dynamical systems theory, and cognitive psychology. Not all of those can be coherently linked, some concepts are incommensurable, and domain-specific language poses an obstacle to integration. Still, conceptual integration is a form of understanding that provides intuition and consolidation, without which progress remains unguided. This paper is concerned with the integration of deterministic and stochastic processes within an information theoretic framework, linking information entropy and free energy to mechanisms of emergent dynamics and self-organization in brain networks. We identify basic properties of neuronal populations leading to an equivariant matrix in a network, in which complex behaviors can naturally be represented through structured flows on manifolds establishing the internal model relevant to theories of brain function. We propose a neural mechanism for the generation of internal models from symmetry breaking in the connectivity of brain networks. The emergent perspective illustrates how free energy can be linked to internal models and how they arise from the neural substrate.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X246UK3A/Jirsa and Sheheitli - 2022 - Entropy, free energy, symmetry and dynamics in the.pdf}
}

@article{josselynFindingEngram2015,
  title = {Finding the Engram},
  author = {Josselyn, Sheena A and K{\"o}hler, Stefan and Frankland, Paul W},
  year = {2015},
  doi = {10.1038/nrn4000},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BT9GCPS8/Josselyn, Köhler, Frankland - 2015 - Finding the engram.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@josselynFindingEngram2015.md}
}

@article{josselynHeroesEngram2017,
  title = {Heroes of the {{Engram}}},
  author = {Josselyn, Sheena A and K{\"o}hler, Stefan and Frankland, Paul W and Semon, Richard and Lashley, Karl and Hebb, Donald and Penfield, Wilder and Milner, Brenda and Mcconnell, James and Thompson, Richard},
  year = {2017},
  doi = {10.1523/JNEUROSCI.0056-17.2017},
  abstract = {In 1904, Richard Semon introduced the term "engram" to describe the neural substrate responsible for (or at least important in) storing and recalling memories (i.e., a memory trace). The recent introduction of a vast array of powerful new tools to probe and manipulate memory function at the cell and neuronal circuit level has spurred an explosion of interest in studying the engram. However, the present "engram renaissance" was not borne in isolation but rather builds on a long tradition of memory research. We believe it is important to acknowledge the debts our current generation of scientists owes to those scientists who have offered key ideas, persevered through failed experiments and made important discoveries before us. Examining the past can also offer a fresh perspective on the present state and future promise of the field. Given the large amount of empirical advances made in recent years, it seems particularly timely to look back and review the scientists who introduced the seminal terminology, concepts, methodological approaches, and initial data pertaining to engrams. Rather than simply list their many accomplishments, here we color in some details of the lives and milestone contributions of our seven personal heroes of the engram (). In reviewing their historic role, we also illustrate how their work remains relevant to today's studies.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SAIZT859/Josselyn et al. - 2017 - Heroes of the Engram.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@josselynHeroesEngram2017.md}
}

@techreport{jouppiInDatacenterPerformanceAnalysis2017,
  title = {In-{{Datacenter Performance Analysis}} of a {{Tensor Processing Unit}}\hspace{0pt} {{TM}}},
  author = {Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-Luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and Mackean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  year = {2017},
  abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC-called a \hspace{0pt} Tensor Processing Unit (TPU) \hspace{0pt}-deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, \ldots ) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X-30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X-80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
  keywords = {accelerator,CNN,domain-specific architecture,Index terms-DNN,LSTM,MLP,neural network,RNN},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GFF7BNTH/Jouppi et al. - 2017 - In-Datacenter Performance Analysis of a Tensor Processing Unit TM.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@jouppiInDatacenterPerformanceAnalysis2017.md}
}

@article{junFullyIntegratedSilicon2017,
  title = {Fully Integrated Silicon Probes for High-Density Recording of Neural Activity},
  author = {Jun, James J. and Steinmetz, Nicholas A. and Siegle, Joshua H. and Denman, Daniel J. and Bauza, Marius and Barbarits, Brian and Lee, Albert K. and Anastassiou, Costas A. and Andrei, Alexandru and Ayd{\i}n, {\c C}a{\u g}atay and Barbic, Mladen and Blanche, Timothy J. and Bonin, Vincent and Couto, Jo{\~a}o and Dutta, Barundeb and Gratiy, Sergey L. and Gutnisky, Diego A. and H{\"a}usser, Michael and Karsh, Bill and Ledochowitsch, Peter and Lopez, Carolina Mora and Mitelut, Catalin and Musa, Silke and Okun, Michael and Pachitariu, Marius and Putzeys, Jan and Rich, P. Dylan and Rossant, Cyrille and Sun, Wei-lung and Svoboda, Karel and Carandini, Matteo and Harris, Kenneth D. and Koch, Christof and O'Keefe, John and Harris, Timothy D.},
  year = {2017},
  month = nov,
  journal = {Nature},
  volume = {551},
  number = {7679},
  pages = {232--236},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature24636},
  abstract = {New silicon probes known as Neuropixels are shown to record from hundreds of neurons simultaneously in awake and freely moving rodents.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CJFC8YU6/Jun et al. - 2017 - Fully integrated silicon probes for high-density r.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@junFullyIntegratedSilicon2017.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3K3A7RBM/nature24636.html}
}

@article{kabashimaBeliefPropagationVs1998,
  title = {Belief Propagation vs. {{TAP}} for Decoding Corrupted Messages},
  author = {Kabashima, Y. and Saad, D.},
  year = {1998},
  journal = {Europhysics Letters},
  doi = {10.1209/epl/i1998-00524-7},
  abstract = {We employ two different methods, based on belief propagation and TAP, for decoding corrupted messages encoded by employing Sourlas's method, where the code word comprises products of K bits selected randomly from the original message. We show that the equations obtained by the two approaches are similar and provide the same solution as the one obtained by the replica approach in some cases ( K = 2). However, we also show that for K {$\geq$} 3 and unbiased messages the iterative solution is sensitive to the initial conditions and is likely to provide erroneous solutions; and that it is generally beneficial to use Nishimori's temperature, especially in the case of biased messages.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/92JPL7G6/Kabashima, Saad - 1998 - Belief propagation vs. TAP for decoding corrupted messages.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kabashimaBeliefPropagationVs1998.md}
}

@article{kaminskiMeasuresCouplingNeural2016,
  title = {Measures of {{Coupling}} between {{Neural Populations Based}} on {{Granger Causality Principle}}},
  author = {Kaminski, Maciej and Brzezicka, Aneta and Kaminski, Jan and Blinowska, Katarzyna J.},
  year = {2016},
  month = oct,
  journal = {Frontiers in Computational Neuroscience},
  volume = {10},
  issn = {1662-5188},
  doi = {10.3389/fncom.2016.00114},
  abstract = {This paper shortly reviews the measures used to estimate neural synchronization in experimental settings. Our focus is on multivariate measures of dependence based on the Granger causality (G-causality) principle, their applications and performance in respect of robustness to noise, volume conduction, common driving, and presence of a ``weak node.'' Application of G-causality measures to EEG, intracranial signals and fMRI time series is addressed. G-causality based measures defined in the frequency domain allow the synchronization between neural populations and the directed propagation of their electrical activity to be determined. The time-varying G-causality based measure Short-time Directed Transfer Function (SDTF) supplies information on the dynamics of synchronization and the organization of neural networks. Inspection of effective connectivity patterns indicates a modular structure of neural networks, with a stronger coupling within modules than between them. The hypothetical plausible mechanism of information processing, suggested by the identified synchronization patterns, is communication between tightly coupled modules intermitted by sparser interactions providing synchronization of distant structures.},
  pmcid = {PMC5080292},
  pmid = {27833546},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/INGJ7D5L/Kaminski et al. - 2016 - Measures of Coupling between Neural Populations Ba.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZREE6T6M/Kaminski et al. - 2016 - Measures of Coupling between Neural Populations Ba.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kaminskiMeasuresCouplingNeural2016.md}
}

@inproceedings{kaplanisContinualReinforcementLearning2018,
  title = {Continual {{Reinforcement Learning}} with {{Complex Synapses}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
  year = {2018},
  month = jul,
  pages = {2497--2506},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna \& Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/C2H45ME6/Kaplanis et al_2018_Continual Reinforcement Learning with Complex Synapses.pdf}
}

@article{kaplanisContinualReinforcementLearning2020,
  title = {{Continual reinforcement learning with memory at multiple timescales}},
  author = {Kaplanis, Christos},
  year = {2020},
  month = apr,
  publisher = {{Imperial College London}},
  doi = {10.25560/82257},
  abstract = {In the past decade, with increased availability of computational resources and several improvements in training techniques, artificial neural networks (ANNs) have been rediscovered as a powerful class of machine learning methods, featuring in several groundbreaking applications of artificial intelligence. Most of these successes have been achieved in stationary, confined domains, such as a game playing and image recognition, but, ultimately, we want to apply artificial intelligence to problems that require it to interact with the real world, which is both vast and nonstationary. Unfortunately, ANNs have long been known to suffer from the phenomenon of catastrophic forgetting, whereby, in a setting where the data distribution is changing over time, new learning can lead to an abrupt erasure of previously acquired knowledge. The resurgence of ANNs has led to an increased urgency to solve this problem and endow them with the capacity for continual learning, which refers to the ability to build on their knowledge over time in environments that are constantly evolving. The most common setting for evaluating continual learning approaches to date has been in the context of training on a number of distinct tasks in sequence, and as a result many of them use the knowledge of task boundaries to consolidate knowledge during training. In the real world, however, the changes to the distribution may occur more gradually and at times that are not known in advance.     The goal of this thesis has been to develop continual learning approaches that can cope with both discrete and continuous changes to the data distribution, without any prior knowledge of the nature or timescale of the changes. I present three new methods, all of which involve learning at multiple timescales, and evaluate them in the context of deep reinforcement learning, a paradigm that combines reinforcement learning with neural networks, which provides a natural testbed for continual learning as (i) it involves interacting with an environment, and (ii) it can feature non-stationarity at unpredictable timescales during training of a single task. The first method is inspired by the process of synaptic consolidation in the brain and involves multi-timescale memory at the level of the parameters of the network; the second extends the first by directly consolidating the agent's policy over time, rather than its individual parameters; finally, the third approach extends the experience replay database, which typically maintains a buffer of the agent's most recent experiences in order to decorrelate them during training, by enabling it to store data over multiple timescales.},
  copyright = {Creative Commons Attribution NonCommercial ShareAlike Licence},
  langid = {en-US-GB},
  annotation = {Accepted: 2020-09-08T14:51:18Z},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/H5VNUPC8/Kaplanis_2020_Continual reinforcement learning with memory at multiple timescales.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ICXTUHVU/82257.html}
}

@article{kaplanisContinualReinforcementLearning2020a,
  title = {Continual {{Reinforcement Learning}} with {{Multi-Timescale Replay}}},
  author = {Kaplanis, Christos and Clopath, Claudia and Shanahan, Murray},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.07530 [cs, stat]},
  eprint = {2004.07530},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper, we propose a multi-timescale replay (MTR) buffer for improving continual learning in RL agents faced with environments that are changing continuously over time at timescales that are unknown to the agent. The basic MTR buffer comprises a cascade of sub-buffers that accumulate experiences at different timescales, enabling the agent to improve the trade-off between adaptation to new data and retention of old knowledge. We also combine the MTR framework with invariant risk minimization, with the idea of encouraging the agent to learn a policy that is robust across the various environments it encounters over time. The MTR methods are evaluated in three different continual learning settings on two continuous control tasks and, in many cases, show improvement over the baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X56NSJQT/Kaplanis et al_2020_Continual Reinforcement Learning with Multi-Timescale Replay.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6Q5UM2XV/2004.html}
}

@techreport{kappenMeanFieldTheory2000,
  title = {Mean Field Theory for Asymmetric Neural Networks},
  author = {Kappen, H. J. and Spanjers, J. J.},
  year = {2000},
  journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
  volume = {61},
  number = {5},
  pages = {5658--5663},
  doi = {10.1103/PhysRevE.61.5658},
  abstract = {The computation of mean firing rates and correlations is intractable for large neural networks. For symmetric networks one can derive mean field approximations using the Taylor series expansion of the free energy as proposed by Plefka. In asymmetric networks, the concept of free energy is absent. Therefore, it is not immediately obvious how to extend this method to asymmetric networks. In this paper we extend Plefka's approach to asymmetric networks and in fact to arbitrary probability distributions. The method is based on an information geometric argument. The method is illustrated for asymmetric neural networks with sequential dynamics. We compare our approximate analytical results with Monte Carlo simulations for a network of 100 neurons. It is shown that the quality of the approximation for asymmetric networks is as good as for symmetric networks.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/T4NSNUBZ/Kappen, Spanjers - Unknown - Mean field theory for asymmetric neural networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z9XJGFMY/Kappen, Spanjers - 2000 - Mean field theory for asymmetric neural networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kappenMeanFieldTheory2000.md}
}

@misc{karpathyGibbsSamplingIsing,
  title = {Gibbs {{Sampling}} on {{Ising Model}}},
  author = {Karpathy, Andrej},
  journal = {Gibbs Sampling on Ising Model},
  howpublished = {https://cs.stanford.edu/people/karpathy/visml/ising\_example.html},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@karpathyGibbsSamplingIsing.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JGI6U4K8/ising_example.html}
}

@article{kasabovSpikingNeuralNetworks2019,
  title = {Spiking Neural Networks for Deep Learning and Knowledge Representation: {{Editorial}}},
  shorttitle = {Spiking Neural Networks for Deep Learning and Knowledge Representation},
  author = {Kasabov, Nikola K.},
  year = {2019},
  month = nov,
  journal = {Neural Networks},
  volume = {119},
  pages = {341--342},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.08.019},
  langid = {english}
}

@article{kaufmanExactlySolubleIsing1981,
  title = {Exactly Soluble {{Ising}} Models on Hierarchical Lattices},
  author = {Kaufman, Miron and Griffiths, Robert B.},
  year = {1981},
  journal = {Physical Review B},
  doi = {10.1103/PhysRevB.24.496},
  abstract = {It is important to seek cost-effective methods of improving the care and outcome of those with serious mental illnesses. User-held records, where the person with the illness holds all or some personal information relating to the course and care of their illness, are now the norm in some clinical settings. Their value for those with severe mental illnesses is unknown. To evaluate the effects of personalised, accessible, user-held clinical information for people with a severe mental illness (defined as psychotic illnesses). We updated previous searches by searching the Cochrane Schizophrenia Group Trials Register in August 2011. This register is compiled by systematic searches of major databases, and handsearches of journals and conference proceedings. We included all relevant randomised controlled trials (RCTs) that:i. have recruited adult participants with a diagnosis of a severe mental illness (specifically psychotic illnesses and severe mood disorders such as bipolar and depression with psychotic features); andii. compared any personalised and accessible clinical information held by the user beyond standard care to standard information routinely held such as appointment cards and generic information on diagnosis, treatment or services available. Study selection and data extraction were undertaken independently by two authors and confirmed and checked by a third. We contacted authors of trials for additional and missing data. Where possible, we calculated risk ratios (RR) and 95\% confidence intervals (CI). We used a random-effects model. We assessed risk of bias for included studies and created a 'Summary of findings' table using GRADE. Four RCTs (n = 607) of user-held records versus treatment as usual met the inclusion criteria. When the effect of user-held records on psychiatric hospital admissions was compared with treatment as usual in four studies, the pooled treatment effect showed no significant impact of the intervention and was of very low magnitude (n = 597, 4 RCTs, RR 0.99 CI 0.71 to 1.38, moderate quality evidence). Similarly, there was no significant effect of the intervention in three studies which investigated compulsory psychiatric hospital admissions (n = 507, 4 RCTs, RR 0.64 CI 0.37 to 1.10, moderate quality evidence). Other outcomes including satisfaction and mental state were investigated but pooled estimates were not obtainable due to skewed or poorly reported data, or only being investigated by one study. Two outcomes (violence and death) were not investigated by the included studies. Two important randomised studies are ongoing. The evidence gap remains regarding user-held, personalised, accessible clinical information for people with psychotic illnesses for many of the outcomes of interest. However, based on moderate quality evidence, this review suggests that there is no effect of the intervention on hospital or outpatient appointment use for individuals with psychotic disorders. The number of studies is low, however, and further evidence is required to ascertain whether these results are mediated by the type of intervention, such as involvement of a clinical team or the type of information included.[CINAHL Note: The Cochrane Collaboration systematic reviews contain interactive software that allows various calculations in the MetaView.]},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2QPVEWPZ/Kaufman, Griffiths - 1981 - Exactly soluble Ising models on hierarchical lattices.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kaufmanExactlySolubleIsing1981.md}
}

@article{kimEncoding3DHead2018,
  title = {Encoding of {{3D}} Head Direction Information in the Human Brain},
  author = {Kim, Misun and Maguire, Eleanor A.},
  year = {2018},
  journal = {Hippocampus},
  number = {November},
  pages = {1--11},
  doi = {10.1002/hipo.23060},
  keywords = {3D navigation,fMRI,retrosplenial cortex,subiculum,thalamus},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MBCFRNFE/Kim, Maguire - 2018 - Encoding of 3D head direction information in the human brain.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kimEncoding3DHead2018.md}
}

@article{kimNeuralArchitectureSearch2022,
  title = {Neural {{Architecture Search}} for {{Spiking Neural Networks}}},
  author = {Kim, Youngeun and Li, Yuhang and Park, Hyoungseob and Venkatesha, Yeshwanth and Panda, P.},
  year = {2022},
  journal = {ArXiv},
  abstract = {This paper introduces a novel Neural Architecture Search (NAS) approach for finding better SNN architectures and selects the architecture that can represent diverse spike activation patterns across different data samples without training, and shows that SNASNet achieves state-of-the-art performance with significantly lower timesteps. Spiking Neural Networks (SNNs) have gained huge attention as a potential energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their inherent high-sparsity activation. However, most prior SNN methods use ANN-like architectures (e.g., VGG-Net or ResNet), which could provide sub-optimal performance for temporal sequence processing of binary information in SNNs. To address this, in this paper, we introduce a novel Neural Architecture Search (NAS) approach for finding better SNN architectures. Inspired by recent NAS approaches that find the optimal architecture from activation patterns at initialization, we select the architecture that can represent diverse spike activation patterns across different data samples without training. Moreover, to further leverage the temporal information among the spikes, we search for feed forward connections as well as backward connections (i.e., temporal feedback connections) between layers. Interestingly, SNASNet found by our search algorithm achieves higher performance with backward connections, demonstrating the importance of designing SNN architecture for suitably using temporal information. We conduct extensive experiments on three image recognition benchmarks where we show that SNASNet achieves state-of-the-art performance with significantly lower timesteps (5 timesteps). We provide the code in Supplementary.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2Y7NXBJV/Kim et al_2022_Neural Architecture Search for Spiking Neural Networks.pdf}
}

@article{kimSimpleFrameworkConstructing2019,
  title = {Simple Framework for Constructing Functional Spiking Recurrent Neural Networks},
  author = {Kim, Robert and Li, Yinghao and Sejnowski, Terrence J.},
  year = {2019},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {45},
  pages = {22811--22820},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1905926116},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EIXPI77Z/Kim et al_2019_Simple framework for constructing functional spiking recurrent neural networks.pdf}
}

@article{kimStiffNeuralOrdinary2021,
  title = {Stiff Neural Ordinary Differential Equations},
  author = {Kim, Suyong and Ji, Weiqi and Deng, Sili and Ma, Yingbo and Rackauckas, Christopher},
  year = {2021},
  month = sep,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {31},
  number = {9},
  pages = {093122},
  publisher = {{American Institute of Physics}},
  issn = {1054-1500},
  doi = {10.1063/5.0060697},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Kim et al_2021_Stiff neural ordinary differential equations.pdf}
}

@article{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  month = dec,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YI67GD8V/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kingmaAdamMethodStochastic2014.md}
}

@article{kochNeuralCorrelatesConsciousness2016,
  title = {Neural Correlates of Consciousness: {{Progress}} and Problems},
  author = {Koch, Christof and Massimini, Marcello and Boly, Melanie and Tononi, Giulio},
  year = {2016},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {17},
  number = {5},
  pages = {307--321},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nrn.2016.22},
  abstract = {There have been a number of advances in the search for the neural correlates of consciousness-the minimum neural mechanisms sufficient for any one specific conscious percept. In this Review, we describe recent findings showing that the anatomical neural correlates of consciousness are primarily localized to a posterior cortical hot zone that includes sensory areas, rather than to a fronto-parietal network involved in task monitoring and reporting. We also discuss some candidate neurophysiological markers of consciousness that have proved illusory, and measures of differentiation and integration of neural activity that offer more promising quantitative indices of consciousness.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ISIXAEAF/Koch et al. - 2016 - Neural correlates of consciousness Progress and problems.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kochNeuralCorrelatesConsciousness2016.md}
}

@article{kohInteriorPointMethodLargeScale2007,
  title = {An {{Interior-Point Method}} for {{Large-Scale}} L1-{{Regularized Logistic Regression}}},
  author = {Koh, Kwangmoo and Kim, Seung-jean and Boyd, Stephen},
  year = {2007},
  number = {2},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VKQPIKKK/Koh, Kim, Boyd - 2007 - An Interior-Point Method for Large-Scale l1-Regularized Logistic Regression.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kohInteriorPointMethodLargeScale2007.md}
}

@article{kohMethodLargeScaleL1Regularized,
  title = {A {{Method}} for {{Large-Scale}} {$\mathscr{l}$}1-{{Regularized Logistic Regression}}},
  author = {Koh, Kwangmoo and Kim, Seung-Jean and Boyd, Stephen},
  pages = {7},
  abstract = {Logistic regression with {$\mathscr{l}$}1 regularization has been proposed as a promising method for feature selection in classification problems. Several specialized solution methods have been proposed for {$\mathscr{l}$}1-regularized logistic regression problems (LRPs). However, existing methods do not scale well to large problems that arise in many practical settings. In this paper we describe an efficient interior-point method for solving {$\mathscr{l}$}1-regularized LRPs. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC. A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve large sparse problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few tens of minutes, on a PC. Numerical experiments show that our method outperforms standard methods for solving convex optimization problems as well as other methods specifically designed for {$\mathscr{l}$}1regularized LRPs.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BYIYMZ3W/Koh et al. - A Method for Large-Scale ℓ1-Regularized Logistic R.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kohMethodLargeScaleL1Regularized.md}
}

@incollection{konkoliReservoirComputingComputational2018,
  title = {Reservoir Computing with Computational Matter},
  booktitle = {Natural Computing Series},
  author = {Konkoli, Zoran and Nichele, Stefano and Dale, Matthew and Stepney, Susan},
  year = {2018},
  pages = {269--293},
  publisher = {{Springer Verlag}},
  issn = {16197127},
  doi = {10.1007/978-3-319-65826-1_14},
  abstract = {The reservoir computing paradigm of information processing has emerged as a natural response to the problem of training recurrent neural networks. It has been realized that the training phase can be avoided provided a network has some well-defined properties, e.g. the echo state property. This idea has been generalized to arbitrary artificial dynamical systems. In principle, any dynamical system could be used for advanced information processing applications provided that such a system has the separation and the approximation property. To carry out this idea in practice, the only auxiliary equipment that is needed is a simple read-out layer that can be used to access the internal states of the system. In the following, several applications scenarios of this generic idea are discussed, together with some related engineering aspects. We cover both practical problems one might meet when trying to implement the idea, and discuss several strategies of solving such problems.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5PKUNNKM/Konkoli et al. - 2018 - Reservoir computing with computational matter.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/64K2LIER/Konkoli et al. - 2018 - Reservoir computing with computational matter.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@konkoliReservoirComputingComputational2018.md}
}

@incollection{kononowiczTimingTimePerception2018,
  title = {Timing and {{Time Perception}}},
  booktitle = {Stevens' {{Handbook}} of {{Experimental Psychology}} and {{Cognitive Neuroscience}}},
  author = {Kononowicz, Tadeusz W. and {van Rijn}, Hedderik and Meck, Warren H.},
  year = {2018},
  pages = {1--38},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781119170174.epcn114},
  abstract = {This chapter reviews recent human and nonhuman animal studies investigating neural signatures of time estimation. Investigation of the neural correlates of time estimation as measured by electrophysiology, electroencephalography, magnetoencephalography, and functional magnetic resonance imaging (fMRI) in humans and other animals has largely been focused on the to-be-timed period. Climbing neural activity (e.g., ramping) originating from the supplementary motor area has been implicated as a primary neural marker that coincides with the development of subjective experience of duration. However, it has recently been questioned whether such climbing neural activity directly reflects the neural mechanism(s) underpinning the sense of time. Given that the neural signatures recorded during the to-be-timed period are insufficient to explain various aspects of interval timing, this has led to the consideration of processes influencing timing before and after the to-be-timed period. Because many of these signatures are linked with the supplementary motor area, an extended discussion of the role of this cortical structure in timing and time perception is provided. These neural correlates are interpreted in light of contemporary theories of interval timing, such as the striatal beat frequency model. Future directions for the investigation of the functional and neural mechanisms of interval timing are also discussed.},
  isbn = {978-1-119-17017-4},
  langid = {english},
  keywords = {coincidence detection,contingent negative variation,decision making,interval timing,striatal beat frequency model},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119170174.epcn114},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RD7TKUVA/Kononowicz et al_2018_Timing and Time Perception.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DWN2J5M5/9781119170174.html}
}

@article{kontoyiannisCompressionSymmetrySmallWorld2020,
  title = {Compression and {{Symmetry}} of {{Small-World Graphs}} and {{Structures}}},
  author = {Kontoyiannis, Ioannis and Lim, Yi Heng and Papakonstantinopoulou, Katia and Szpankowski, Wojtek},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.15981 [cs, math]},
  eprint = {2007.15981},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {For various purposes and, in particular, in the context of data compression, a graph can be examined at three levels. Its structure can be described as the unlabeled version of the graph; then the labeling of its structure can be added; and finally, given then structure and labeling, the contents of the labels can be described. Determining the amount of information present at each level and quantifying the degree of dependence between them, requires the study of symmetry, graph automorphism, entropy, and graph compressibility. In this paper, we focus on a class of small-world graphs. These are geometric random graphs where vertices are first connected to their nearest neighbors on a circle and then pairs of non-neighbors are connected according to a distance-dependent probability distribution. We establish the degree distribution of this model, and use it to prove the model's asymmetry in an appropriate range of parameters. Then we derive the relevant entropy and structural entropy of these random graphs, in connection with graph compression.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Mathematics - Combinatorics,Mathematics - Probability,Small World},
  note = {Comment: 20 pages, 1 figure},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GQB86WB5/Kontoyiannis et al. - 2020 - Compression and Symmetry of Small-World Graphs and.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kontoyiannisCompressionSymmetrySmallWorld2020.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/G2MJNUAG/2007.html}
}

@article{kornijcukSimplifiedCalciumSignaling2020,
  title = {Simplified Calcium Signaling Cascade for Synaptic Plasticity},
  author = {Kornijcuk, Vladimir and Kim, Dohun and Kim, Guhyun and Jeong, Doo Seok},
  year = {2020},
  month = mar,
  journal = {Neural Networks},
  volume = {123},
  pages = {38--51},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.11.022},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BRNK4G89/Kornijcuk et al_2020_Simplified calcium signaling cascade for synaptic plasticity.pdf}
}

@article{kriegeskorteNeuralTuningRepresentational2021,
  title = {Neural Tuning and Representational Geometry},
  author = {Kriegeskorte, Nikolaus and Wei, Xue-Xin},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.09743 [q-bio]},
  eprint = {2104.09743},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  abstract = {A central goal of neuroscience is to understand the representations formed by brain activity patterns and their connection to behavior. The classical approach is to investigate how individual neurons encode the stimuli and how their tuning determines the fidelity of the neural representation. Tuning analyses often use the Fisher information to characterize the sensitivity of neural responses to small changes of the stimulus. In recent decades, measurements of large populations of neurons have motivated a complementary approach, which focuses on the information available to linear decoders. The decodable information is captured by the geometry of the representational patterns in the multivariate response space. Here we review neural tuning and representational geometry with the goal of clarifying the relationship between them. The tuning induces the geometry, but different sets of tuned neurons can induce the same geometry. The geometry determines the Fisher information, the mutual information, and the behavioral performance of an ideal observer in a range of psychophysical tasks. We argue that future studies can benefit from considering both tuning and geometry to understand neural codes and reveal the connections between stimulus, brain activity, and behavior.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Neurons and Cognition},
  note = {Comment: 42 pages, 6 figures},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Kriegeskorte_Wei_2021_Neural tuning and representational geometry.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QC6QW5YM/2104.html}
}

@techreport{krippendorffMathematicalTheoryCommunication2009,
  title = {Mathematical Theory of Communication},
  author = {Krippendorff, Klaus},
  year = {2009},
  pages = {614--618},
  institution = {{Sage}},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/B9PRVXE4/Krippendorff - 2009 - Mathematical theory of communication.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@krippendorffMathematicalTheoryCommunication2009.md}
}

@article{krubitzerCorticalEvolutionMammals2012,
  title = {Cortical Evolution in Mammals: {{The}} Bane and Beauty of Phenotypic Variability},
  author = {Krubitzer, L. A. and Seelke, A. M. H.},
  year = {2012},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {Supplement\_1},
  pages = {10647--10654},
  issn = {1064710654},
  doi = {10.1073/pnas.1201891109},
  abstract = {Abstract Evolution by natural selection, the unifying theory of all biological sciences, provides a basis for understanding how phenotypic variability is generated at all levels of organization from genes to behavior. However, it is important to distinguish what is the ...\textbackslash n},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WR43JZQL/Krubitzer, Seelke - 2012 - Cortical evolution in mammals The bane and beauty of phenotypic variability.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@krubitzerCorticalEvolutionMammals2012.md}
}

@article{krumsiekGaussianGraphicalModeling2011,
  title = {Gaussian Graphical Modeling Reconstructs Pathway Reactions from High-Throughput Metabolomics Data},
  author = {Krumsiek, Jan and Suhre, Karsten and Illig, Thomas and Adamski, Jerzy and Theis, Fabian J.},
  year = {2011},
  month = jan,
  journal = {BMC Systems Biology},
  volume = {5},
  number = {1},
  pages = {1--16},
  publisher = {{BioMed Central}},
  doi = {10.1186/1752-0509-5-21},
  abstract = {Background: With the advent of high-throughput targeted metabolic profiling techniques, the question of how to interpret and analyze the resulting vast amount of data becomes more and more important. In this work we address the reconstruction of metabolic reactions from cross-sectional metabolomics data, that is without the requirement for time-resolved measurements or specific system perturbations. Previous studies in this area mainly focused on Pearson correlation coefficients, which however are generally incapable of distinguishing between direct and indirect metabolic interactions.Results: In our new approach we propose the application of a Gaussian graphical model (GGM), an undirected probabilistic graphical model estimating the conditional dependence between variables. GGMs are based on partial correlation coefficients, that is pairwise Pearson correlation coefficients conditioned against the correlation with all other metabolites. We first demonstrate the general validity of the method and its advantages over regular correlation networks with computer-simulated reaction systems. Then we estimate a GGM on data from a large human population cohort, covering 1020 fasting blood serum samples with 151 quantified metabolites. The GGM is much sparser than the correlation network, shows a modular structure with respect to metabolite classes, and is stable to the choice of samples in the data set. On the example of human fatty acid metabolism, we demonstrate for the first time that high partial correlation coefficients generally correspond to known metabolic reactions. This feature is evaluated both manually by investigating specific pairs of high-scoring metabolites, and then systematically on a literature-curated model of fatty acid synthesis and degradation. Our method detects many known reactions along with possibly novel pathway interactions, representing candidates for further experimental examination.Conclusions: In summary, we demonstrate strong signatures of intracellular pathways in blood serum data, and provide a valuable tool for the unbiased reconstruction of metabolic reactions from large-scale metabolomics data sets. \textcopyright{} 2011 Krumsiek et al; licensee BioMed Central Ltd.},
  keywords = {Algorithms,Bioinformatics,Cellular and Medical Topics,Computational Biology/Bioinformatics,Physiological,Simulation and Modeling,Systems Biology},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ADDLVIAD/Krumsiek et al. - 2011 - Gaussian graphical modeling reconstructs pathway reactions from high-throughput metabolomics data.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@krumsiekGaussianGraphicalModeling2011.md}
}

@article{kubieSpatialFrequenciesGrid2015,
  title = {Do the Spatial Frequencies of Grid Cells Mold the Firing Fields of Place Cells?},
  author = {Kubie, John L. and Fox, Steven E.},
  year = {2015},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {13},
  pages = {3860--3861},
  doi = {10.1073/pnas.1503155112},
  abstract = {The medial entorhinal cortex, populated by grid cells, projects both directly and indirectly to the CA1 and CA3 cortices, the sites of hippocampal place cells. The study by Ormond and McNaughton (1) investigates a potential mechanism by which grid cells of the medial entorhinal cortex (MEC) exert their influence on the formation of firing fields of place cells. Specifically, the authors propose a ``Fourier hypothesis'' by which the spatial frequencies from several grid cell modules converge on the place-cell layer, molding the contours of firing fields to a composite of those several frequencies; Ormond and McNaughton tested the theory by inactivating specific input components and obtained fascinating results, largely supporting the model. The work (1) is best understood in the context of the broad ``hippocampal cognitive map'' project. In 1971, O'Keefe and Dostrovsky, while recording single neurons in the hippocampus of freely moving rats, discovered cells that fired when the animal crossed a restricted region of space; these neurons were called ``place cells'' (2). A typical place cell will fire in one location, termed the ``place field'' (Fig. 1 A ). Soon after, John O'Keefe and Lynn Nadel set the framework of the project with the publication of The Hippocampus as a Cognitive Map (3). The book set out a bold theory that the hippocampus was a map, performing cognitive functions critical for navigation and other cognitive operations including episodic memory. The book also presented an explicit neural model for the creation of place cells. As noted in the book, virtually all of the processing layers were unexplored, and the model highly speculative. The theory set out a goal of understanding how neural dynamics produces cognition (Fig. 1 B ). Fig. 1. ( A ) Firing-rate maps for a place cell recorded over 15 min from a rat in a cylinder ( Left ) and a grid cell recorded \ldots{} [{$\carriagereturn$}][1]1To whom correspondence should be addressed. Email: Jkubie\{at\}downstate.edu. [1]: \#xref-corresp-1-1},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JMGPGMV6/Kubie, Fox - 2015 - Do the spatial frequencies of grid cells mold the firing fields of place cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kubieSpatialFrequenciesGrid2015.md}
}

@article{kumarasingheDeepLearningDeep2020,
  title = {Deep Learning and Deep Knowledge Representation in {{Spiking Neural Networks}} for {{Brain-Computer Interfaces}}},
  author = {Kumarasinghe, Kaushalya and Kasabov, Nikola and Taylor, Denise},
  year = {2020},
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  pages = {169--185},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.08.029},
  langid = {english}
}

@article{kumarSpikingActivityPropagation2010,
  title = {Spiking Activity Propagation in Neuronal Networks: Reconciling Different Perspectives on Neural Coding},
  shorttitle = {Spiking Activity Propagation in Neuronal Networks},
  author = {Kumar, Arvind and Rotter, Stefan and Aertsen, Ad},
  year = {2010},
  month = sep,
  journal = {Nature Reviews Neuroscience},
  volume = {11},
  number = {9},
  pages = {615--627},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn2886},
  abstract = {One of the central problems in neuroscience is the characterization and understanding of the neural code. In 1968 Perkel and Bullock defined four key functions for a candidate neural code: stimulus representation, interpretation, transformation and transmission. Although the first three have been studied extensively, surprisingly, the fourth has been largely ignored in experiments. Yet, signal transmission is a vital functions for a neural code in ensuring communication among highly specialized brain regions.Feedforward networks with convergent or divergent connections between subsequent groups of neurons have been the model system of choice in the study of spiking-activity propagation. The simple feedforward topology captures key features of the modular architecture of the brain. Moreover, from a functional perspective, certain classes of recurrent networks can be treated as feedforward networks.Theoretical studies have identified two dominant modes for propagating spiking activity in feedforward networks: the aynchronous rate mode, in which the average spike count is propagated across the sub-networks; and the synchronous event mode, in which only synchronous volleys of spikes are propagated.Various properties of individual neurons and the structure of feedforward networks can amplify even weak correlations in spiking-activity propagation. Such amplification rapidly degenerates the fidelity of an asynchronous rate code. Thus, only feedforward networks with weak shared connectivity are suitable for propagating asynchronous firing rates. Large, shared connectivity favours the propagation of a synchrony code.Structural properties of feedforward networks, in particular connection probability and synaptic strengths, have a crucial role in determining whether asynchronous firing rates or synchronous spikes are propagated. Thus, appropriate architecture of the FFN may support stable propagation of asynchronous and synchronous neural codes simultaneously.Indirect experimental evidence suggests that neural networks in vivo may indeed induce synchrony in their propagating activity. However, a direct testing of theoretical predictions is currently lacking. Controlled stimulation of appropriately selected neural networks in vivo to generate activity patterns mimicking either asynchronous or synchronous input and monitoring of their temporal evolution downstream could provide an effective paradigm for testing these predicitions.},
  copyright = {2010 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Network models,Neural encoding,Neuronal physiology},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LJKFJ5S2/Kumar et al_2010_Spiking activity propagation in neuronal networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZKTE3ET5/nrn2886.html}
}

@article{kumarSpikingActivityPropagation2010a,
  title = {Spiking Activity Propagation in Neuronal Networks: Reconciling Different Perspectives on Neural Coding},
  shorttitle = {Spiking Activity Propagation in Neuronal Networks},
  author = {Kumar, Arvind and Rotter, Stefan and Aertsen, Ad},
  year = {2010},
  month = sep,
  journal = {Nature Reviews Neuroscience},
  volume = {11},
  number = {9},
  pages = {615--627},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn2886},
  abstract = {One of the central problems in neuroscience is the characterization and understanding of the neural code. In 1968 Perkel and Bullock defined four key functions for a candidate neural code: stimulus representation, interpretation, transformation and transmission. Although the first three have been studied extensively, surprisingly, the fourth has been largely ignored in experiments. Yet, signal transmission is a vital functions for a neural code in ensuring communication among highly specialized brain regions.Feedforward networks with convergent or divergent connections between subsequent groups of neurons have been the model system of choice in the study of spiking-activity propagation. The simple feedforward topology captures key features of the modular architecture of the brain. Moreover, from a functional perspective, certain classes of recurrent networks can be treated as feedforward networks.Theoretical studies have identified two dominant modes for propagating spiking activity in feedforward networks: the aynchronous rate mode, in which the average spike count is propagated across the sub-networks; and the synchronous event mode, in which only synchronous volleys of spikes are propagated.Various properties of individual neurons and the structure of feedforward networks can amplify even weak correlations in spiking-activity propagation. Such amplification rapidly degenerates the fidelity of an asynchronous rate code. Thus, only feedforward networks with weak shared connectivity are suitable for propagating asynchronous firing rates. Large, shared connectivity favours the propagation of a synchrony code.Structural properties of feedforward networks, in particular connection probability and synaptic strengths, have a crucial role in determining whether asynchronous firing rates or synchronous spikes are propagated. Thus, appropriate architecture of the FFN may support stable propagation of asynchronous and synchronous neural codes simultaneously.Indirect experimental evidence suggests that neural networks in vivo may indeed induce synchrony in their propagating activity. However, a direct testing of theoretical predictions is currently lacking. Controlled stimulation of appropriately selected neural networks in vivo to generate activity patterns mimicking either asynchronous or synchronous input and monitoring of their temporal evolution downstream could provide an effective paradigm for testing these predicitions.},
  copyright = {2010 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Network models,Neural encoding,Neuronal physiology},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2CTM7CGJ/Kumar et al. - 2010 - Spiking activity propagation in neuronal networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NEHUWTS6/nrn2886.html}
}

@article{kuzumTransparentFlexibleLow2014,
  title = {Transparent and Flexible Low Noise Graphene Electrodes for Simultaneous Electrophysiology and Neuroimaging},
  author = {Kuzum, Duygu and Takano, Hajime and Shim, Euijae and Reed, Jason C. and Juul, Halvor and Richardson, Andrew G. and De Vries, Julius and Bink, Hank and Dichter, Marc A. and Lucas, Timothy H. and Coulter, Douglas A. and Cubukcu, Ertugrul and Litt, Brian},
  year = {2014},
  journal = {Nature Communications},
  doi = {10.1038/ncomms6259},
  abstract = {Calcium imaging is a versatile experimental approach capable of resolving single neurons with single-cell spatial resolution in the brain. Electrophysiological recordings provide high temporal, but limited spatial resolution, because of the geometrical inaccessibility of the brain. An approach that integrates the advantages of both techniques could provide new insights into functions of neural circuits. Here, we report a transparent, flexible neural electrode technology based on graphene, which enables simultaneous optical imaging and electrophysiological recording. We demonstrate that hippocampal slices can be imaged through transparent graphene electrodes by both confocal and two-photon microscopy without causing any light-induced artefacts in the electrical recordings. Graphene electrodes record high-frequency bursting activity and slow synaptic potentials that are hard to resolve by multicellular calcium imaging. This transparent electrode technology may pave the way for high spatio-temporal resolution electro-optic mapping of the dynamic neuronal activity.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IS5HBQLT/Litt - 2014 - Graphene electrodes for neuroimaging.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KFJUAY38/Kuzum et al. - 2014 - Transparent and flexible low noise graphene electrodes for simultaneous electrophysiology and neuroimaging.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@kuzumTransparentFlexibleLow2014.md}
}

@phdthesis{langsrudInferringLearningRule2020,
  title = {Inferring the Learning Rule from Spike Train Data with Particle {{Metropolis-Hastings}}},
  author = {Langsrud, Astrid and Dunn, Benjamin Adric},
  year = {2020},
  month = jan,
  abstract = {The brain is a system of connected neurons that communicate by transmitting electrical signals to each other. Research has revealed that the way in which neural connections develop over time seem to follow some underlying patterns. These are known as learning rules, and are essential for the brain to learn and form memories. Statistical methods for inferring the learning rule from recordings of neural activity may thus give insights on basic computation- ally principles in different brain areas. Furthermore it has been hypothesized that the learning rule might be disturbed by memory related diseases, such as Alzheimer's. Therefore, being able to detect the underlying learning rule could shed light on the origin and workings of Alzheimer's disease and even have applications in medical research as well. This thesis covers the implementation of particle Metropolis-Hastings for characteriz- ing the learning rule in simulated neural spike data for one synapse, inspired by the method proposed in (Linderman et al., 2014). For our purpose we used the additive spike-timing- dependent plasticity (STDP) learning rule, and aimed at inferring its learning rule param- eters. The neural spiking was modeled as a Bernoulli process in the Generalized Linear Model (GLM) framework. By numerical experiments it was demonstrated that with enough data and sufficiently low noise level, information of the learning rule parameters could be reconstructed from the spike data by using this method. The results indicate that it could be possible to distinguish between learning rules, by analysing spike train data with particle Metropolis-Hastings.},
  school = {NTNU},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@langsrudInferringLearningRule2020.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/langsrudInferringLearningRule2020-mdnotes.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Langsrud_Dunn_2020_Inferring the learning rule from spike train data with particle.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@langsrudInferringLearningRule2020.md}
}

@article{Langton1990,
  title = {Computation at the Edge of Chaos: {{Phase}} Transitions and Emergent Computation},
  author = {Langton, Chris G.},
  year = {1990},
  journal = {Physica D: Nonlinear Phenomena},
  volume = {42},
  number = {1-3},
  pages = {12--37},
  issn = {01672789},
  doi = {10.1016/0167-2789(90)90064-V},
  abstract = {In order for computation to emerge spontaneously and become an important factor in the dynamics of a system, the material substrate must support the primitive functions required for computation: the transmission, storage, and modification of information. Under what conditions might we expect physical systems to support such computational primitives? This paper presents research on cellular automata which suggests that the optimal conditions for the support of information transmission, storage, and modification, are achieved in the vicinity of a phase transition. We observe surprising similarities between the behaviors of computations and systems near phase transitions, finding analogs of computational complexity classes and the halting problem within the phenomenology of phase transitions. We conclude that there is a fundamental connection between computation and phase transitions, especially second-order or "critical" transitions, and discuss some of the implications for our understanding of nature if such a connection is borne out. \textcopyright{} 1990.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@Langton1990.md}
}

@article{laszloerdSPECTRALSTATISTICSERD2013,
  title = {{{SPECTRAL STATISTICS OF ERD}} \H{} {{OS-R\'ENYI GRAPHS I}}: {{LOCAL SEMICIRCLE LAW}}},
  author = {L{\'a}szl{\'o} Erd, BY and Knowles, Antti and Yau, Horng-tzer and Yin, Jun},
  year = {2013},
  journal = {The Annals of Probability},
  volume = {41},
  number = {3B},
  pages = {2279--2375},
  doi = {10.1214/11-AOP734},
  abstract = {We consider the ensemble of adjacency matrices of Erd\H{} os-R\'enyi random graphs, that is, graphs on N vertices where every edge is chosen independently and with probability p {$\equiv$} p(N). We rescale the matrix so that its bulk eigenvalues are of order one. We prove that, as long as pN \textrightarrow{} {$\infty$} (with a speed at least logarithmic in N), the density of eigenvalues of the Erd\H{} os-R\'enyi ensemble is given by the Wigner semicircle law for spectral windows of length larger than N -1 (up to logarithmic corrections). As a consequence, all eigenvectors are proved to be completely delocalized in the sense that the {$\infty$}-norms of the 2-normalized eigenvectors are at most of order N -1/2 with a very high probability. The estimates in this paper will be used in the companion paper [Spectral statistics of Erd\H{} os-R\'enyi graphs II: Eigenvalue spacing and the extreme eigenvalues (2011) Preprint] to prove the universal-ity of eigenvalue distributions both in the bulk and at the spectral edges under the further restriction that pN N 2/3 .},
  keywords = {15B52,82B44,density of states,Erdos--Renyi graphs,local semicircle law},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/993JFTQV/László Erd et al. - 2013 - SPECTRAL STATISTICS OF ERD ˝ OS-RÉNYI GRAPHS I LOCAL SEMICIRCLE LAW.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@laszloerdSPECTRALSTATISTICSERD2013.md}
}

@article{latuskeHippocampalRemappingIts2017,
  title = {Hippocampal {{Remapping}} and {{Its Entorhinal Origin}}.},
  author = {Latuske, Patrick and Kornienko, Olga and Kohler, Laura and Allen, Kevin},
  year = {2017},
  journal = {Frontiers in behavioral neuroscience},
  volume = {11},
  pages = {253--253},
  publisher = {{Frontiers Media SA}},
  doi = {10.3389/fnbeh.2017.00253},
  abstract = {The activity of hippocampal cell ensembles is an accurate predictor of the position of an animal in its surrounding space. One key property of hippocampal cell ensembles is their ability to change in response to alterations in the surrounding environment, a phenomenon called remapping. In this review article, we present evidence for the distinct types of hippocampal remapping. The progressive divergence over time of cell ensembles active in different environments and the transition dynamics between pre-established maps are discussed. Finally, we review recent work demonstrating that hippocampal remapping can be triggered by neurons located in the entorhinal cortex.},
  keywords = {entorhinal cortex,grid cell,hippocampus,memory,navigation,place cells,remapping},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZVXXFMHE/Latuske et al. - 2017 - Hippocampal Remapping and Its Entorhinal Origin.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@latuskeHippocampalRemappingIts2017.md}
}

@article{lauGlutamateReceptorsNeurotoxicity2010,
  title = {Glutamate Receptors, Neurotoxicity and Neurodegeneration},
  author = {Lau, Anthony and Tymianski, Michael},
  year = {2010},
  month = jul,
  journal = {Pflugers Archiv: European Journal of Physiology},
  volume = {460},
  number = {2},
  pages = {525--542},
  issn = {1432-2013},
  doi = {10.1007/s00424-010-0809-1},
  abstract = {Glutamate excitotoxicity is a hypothesis that states excessive glutamate causes neuronal dysfunction and degeneration. As glutamate is a major excitatory neurotransmitter in the central nervous system (CNS), the implications of glutamate excitotoxicity are many and far-reaching. Acute CNS insults such as ischaemia and traumatic brain injury have traditionally been the focus of excitotoxicity research. However, glutamate excitotoxicity has also been linked to chronic neurodegenerative disorders such as amyotrophic lateral sclerosis, multiple sclerosis, Parkinson's disease and others. Despite the continued research into the mechanisms of excitotoxicity, there are currently no pharmacological interventions capable of providing significant neuroprotection in the clinical setting of brain ischaemia or injury. This review addresses the current state of excitotoxic research, focusing on the structure and physiology of glutamate receptors; molecular mechanisms underlying excitotoxic cell death pathways and their interactions with each other; the evidence for glutamate excitotoxicity in acute neurologic diseases; laboratory and clinical attempts at modulating excitotoxicity; and emerging targets for excitotoxicity research.},
  langid = {english},
  pmid = {20229265},
  keywords = {Animals,Antioxidants,Calcium,Calpain,Caspases,Cell Death,Free Radical Scavengers,Free Radicals,Glutamic Acid,Humans,Hypothalamus,Nerve Degeneration,Neurodegenerative Diseases,Neurotoxicity Syndromes,Nitric Oxide,Receptors; AMPA,Receptors; Glutamate,Receptors; Kainic Acid,Receptors; Metabotropic Glutamate,Receptors; N-Methyl-D-Aspartate,Sodium-Calcium Exchanger,Zinc}
}

@article{lauritzenGraphicalModels1996,
  title = {Graphical {{Models}}},
  author = {Lauritzen, Steffan},
  year = {1996},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SJFFA2VV/Lauritzen - 1996 - Graphical Models.djvu;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@lauritzenGraphicalModels1996.md}
}

@article{lecun1989optimal,
  title = {Optimal Brain Damage},
  author = {LeCun, Yann and Denker, John and Solla, Sara},
  year = {1989},
  journal = {Advances in neural information processing systems},
  volume = {2},
  pages = {598--605},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@lecun1989optimal.md}
}

@article{lecunOptimalBrainDamage1990,
  title = {Optimal {{Brain Damage}} ({{Pruning}})},
  author = {LeCun, Yann and Denker, John S and Solla, Sara A.},
  year = {1990},
  journal = {Advances in neural information processing systems},
  pages = {598--605},
  abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7P2FKM8L/NIPS-1989-optimal-brain-damage-Paper.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@lecunOptimalBrainDamage1990.md}
}

@article{leeYASSAnotherSpike2020,
  title = {{{YASS}}: {{Yet Another Spike Sorter}} Applied to Large-Scale Multi-Electrode Array Recordings in Primate Retina},
  shorttitle = {{{YASS}}},
  author = {Lee, JinHyung and Mitelut, Catalin and Shokri, Hooshmand and Kinsella, Ian and Dethe, Nishchal and Wu, Shenghao and Li, Kevin and Reyes, Eduardo Blancas and Turcu, Denis and Batty, Eleanor and Kim, Young Joon and Brackbill, Nora and Kling, Alexandra and Goetz, Georges and Chichilnisky, E. J. and Carlson, David and Paninski, Liam},
  year = {2020},
  month = mar,
  journal = {bioRxiv},
  pages = {2020.03.18.997924},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.03.18.997924},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Spike sorting is a critical first step in extracting neural signals from large-scale multi-electrode array (MEA) data. This manuscript presents several new techniques that make MEA spike sorting more robust and accurate. Our pipeline is based on an efficient multi-stage ``triage-then-cluster-then-pursuit'' approach that initially extracts only clean, high-quality waveforms from the electrophysiological time series by temporarily skipping noisy or ``collided'' events (representing two neurons firing synchronously). This is accomplished by developing a neural network detection and denoising method followed by efficient outlier triaging. The denoised spike waveforms are then used to infer the set of spike templates through nonparametric Bayesian clustering. We use a divide-and-conquer strategy to parallelize this clustering step. Finally, we recover collided waveforms with matching-pursuit deconvolution techniques, and perform further split-and-merge steps to estimate additional templates from the pool of recovered waveforms. We apply the new pipeline to data recorded in the primate retina, where high firing rates and highly-overlapping axonal units provide a challenging testbed for the deconvolution approach; in addition, the well-defined mosaic structure of receptive fields in this preparation provides a useful quality check on any spike sorting pipeline. We show that our pipeline improves on the state-of-the-art in spike sorting (and outperforms manual sorting) on both real and semi-simulated MEA data with \&gt; 500 electrodes; open source code can be found at https://github.com/paninski-lab/yass.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Lee et al_2020_YASS.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@leeYASSAnotherSpike2020.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HZ9JMDHW/2020.03.18.997924v1.html}
}

@article{lefeberConnectivityExcitabilityActivity2014,
  title = {Connectivity, Excitability and Activity Patterns in Neuronal Networks},
  author = {Le Feber, Joost and Stoyanova, Irina I. and Chiappalone, Michela},
  year = {2014},
  journal = {Physical Biology},
  volume = {11},
  number = {3},
  publisher = {{IOP Publishing}},
  issn = {1478-3975},
  doi = {10.1088/1478-3975/11/3/036005},
  abstract = {Extremely synchronized firing patterns such as those observed in brain diseases like epilepsy may result from excessive network excitability. Although network excitability is closely related to (excitatory) connectivity, a direct measure for network excitability remains unavailable. Several methods currently exist for estimating network connectivity, most of which are related to cross-correlation. An example is the conditional firing probability (CFP) analysis which calculates the pairwise probability (CFPi,j) that electrode j records an action potential at time t = {$\tau$}, given that electrode i recorded a spike at t = 0. However, electrode i often records multiple spikes within the analysis interval, and CFP values are biased by the on-going dynamic state of the network. Here we show that in a linear approximation this bias may be removed by deconvoluting CFPi,j with the autocorrelation of i (i.e. CFPi,i), to obtain the single pulse response (SPRi,j)-the average response at electrode j to a single spike at electrode i. Thus, in a linear system SPRs would be independent of the dynamic network state. Nonlinear components of synaptic transmission, such as facilitation and short term depression, will however still affect SPRs. Therefore SPRs provide a clean measure of network excitability. We used carbachol and ghrelin to moderately activate cultured cortical networks to affect their dynamic state. Both neuromodulators transformed the bursting firing patterns of the isolated networks into more dispersed firing. We show that the influence of the dynamic state on SPRs is much smaller than the effect on CFPs, but not zero. The remaining difference reflects the alteration in network excitability. We conclude that SPRs are less contaminated by the dynamic network state and that mild excitation may decrease network excitability, possibly through short term synaptic depression.},
  keywords = {carbachol,connectivity,crosscorrelation,cultured cortical networks,excitability,ghrelin,network bursts},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IZNLM76K/Le Feber, Stoyanova, Chiappalone - 2014 - Connectivity, excitability and activity patterns in neuronal networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@lefeberConnectivityExcitabilityActivity2014.md}
}

@article{lehmanSurprisingCreativityDigital2018,
  title = {The {{Surprising Creativity}} of {{Digital Evolution}}: {{A Collection}} of {{Anecdotes}} from the {{Evolutionary Computation}} and {{Artificial Life Research Communities}}},
  author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Chrabaszcz, Patryk and Cheney, Nick and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Fr{\'e}noy, Antoine and Gagn{\'e}, Christian and Goff, Leni Le and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Shulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, Fran{\c c}ois and Tarapore, Danesh and Thibault, Simon and Weimer, Westley and Watson, Richard and Yosinski, Jason},
  year = {2018},
  month = mar,
  abstract = {Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PYJXBH63/Lehman et al. - 2018 - The Surprising Creativity of Digital Evolution A Collection of Anecdotes from the Evolutionary Computation and Ar.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@lehmanSurprisingCreativityDigital2018.md}
}

@article{lengyelDynamicallyDetunedOscillations2003,
  title = {Dynamically Detuned Oscillations Account for the Coupled Rate and Temporal Code of Place Cell Firing},
  author = {Lengyel, M{\'a}t{\'e} and Szatm{\'a}ry, Zolt{\'a}n and {\'E}rdi, P{\'e}ter},
  year = {2003},
  journal = {Hippocampus},
  volume = {13},
  number = {6},
  pages = {700--714},
  issn = {1050-9631 (Print)},
  doi = {10.1002/hipo.10116},
  abstract = {Firing of place cells in the exploring rat conveys doubly coded spatial information: both the rate of spikes and their timing relative to the phase of the ongoing field theta oscillation are correlated with the location of the animal. Specifically, the firing rate of a place cell waxes and wanes, while the timing of spikes precesses monotonically as the animal traverses the portion of the environment preferred by the cell. We propose a mechanism for the generation of this firing pattern that can be applied for place cells in all three hippocampal subfields and that encodes spatial information in the output of the cell without relying on topographical connections or topographical input. A single pyramidal cell was modeled so that the cell received rhythmic inhibition in phase with theta field potential oscillation on the soma and was excited on the dendrite with input depending on the speed of the rat. The dendrite sustained an intrinsic membrane potential oscillation, frequency modulated by its input. Firing probability of the cell was determined jointly by somatic and dendritic oscillations. Results were obtained on different levels of abstraction: a purely analytical derivation was arrived at, corroborated by numerical simulations of rate neurons, and an extension of these simulations to spiking neurons was also performed. Realistic patterns of rate and temporal coding emerged and were found to be inseparable. These results may have implications on the robustness of information coding in place cell firing and on the ways information is processed in structures downstream to the hippocampus.},
  keywords = {Hippocampus,Membrane potential oscillation,Model,Place field,Theta phase precession},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/F88PF48E/Lengyel, Szatmáry, Érdi - 2003 - Dynamically detuned oscillations account for the coupled rate and temporal code of place cell firing.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@lengyelDynamicallyDetunedOscillations2003.md}
}

@article{lentResourceSelectionCognitive2018,
  title = {Resource {{Selection}} in {{Cognitive Networks With Spiking Neural Networks}}},
  author = {Lent, Ricardo},
  year = {2018},
  month = aug,
  journal = {IEEE transactions on cognitive communications and networking},
  volume = {4},
  number = {4},
  issn = {2332-7731},
  doi = {10.1109/TCCN.2018.2865387},
  abstract = {This paper explores the feasibility of a spiking neural network-based approach to cognitive networking, that is potentially suitable for low-power neuromorphic chips. We discuss the design of a cognitive network controller (CNC), which can dynamically optimize the selection of resources for recurrent network tasks, based on both its assigned objectives and observations of the actual performance achieved by each resource. We present a coding strategy for the action decisions based on the time-to-fire of spikes, a learning algorithm, and a regulation method to keep synapse strengths within an adequate range. To evaluate the proposed method, we apply the CNC to a challenged network environment using simulation. In this scenario, the CNC requires to optimize the average file transfer time over a multichannel space communication link, which is available only for a time window because of orbital dynamics. Compared to conventional methods, we show that the CNC achieves its objective for a broad range of offered loads. We examine the impact of key system factors that include learning and space protocol parameters. The proposed CNC potentially fosters the development of new cognitive networking applications.},
  langid = {english},
  pmcid = {PMC6839547},
  pmid = {31709274},
  keywords = {learning systems,Networks,neural networks,simulation,space vehicle communication},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CGDKYE2U/Lent_2018_Resource Selection in Cognitive Networks With Spiking Neural Networks.pdf}
}

@article{leonardoEnsembleCodingVocal2005,
  title = {Ensemble {{Coding}} of {{Vocal Control}} in {{Birdsong}}},
  author = {Leonardo, Anthony and Fee, Michale S.},
  year = {2005},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {25},
  number = {3},
  pages = {652--661},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3036-04.2005},
  abstract = {Zebra finch song is represented in the high-level motor control nucleus high vocal center (HVC) (Reiner et al., 2004) as a sparse sequence of spike bursts. In contrast, the vocal organ is driven continuously by smoothly varying muscle control signals. To investigate how the sparse HVC code is transformed into continuous vocal patterns, we recorded in the singing zebra finch from populations of neurons in the robust nucleus of arcopallium (RA), a premotor area intermediate between HVC and the motor neurons. We found that highly similar song elements are typically produced by different RA ensembles. Furthermore, although the song is modulated on a wide range of time scales (10-100 ms), patterns of neural activity in RA change only on a short time scale (5-10 ms). We suggest that song is driven by a dynamic circuit that operates on a single underlying clock, and that the large convergence of RA neurons to vocal control muscles results in a many-to-one mapping of RA activity to song structure. This permits rapidly changing RA ensembles to drive both fast and slow acoustic modulations, thereby transforming the sparse HVC code into a continuous vocal pattern.},
  chapter = {Behavioral/Systems/Cognitive},
  copyright = {Copyright \textcopyright{} 2005 Society for Neuroscience 0270-6474/05/25652-10.00/0},
  langid = {english},
  pmid = {15659602},
  keywords = {animal communication,chronic recording,motor coding,single unit,vocalization,zebra finch},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TB74T4VI/Leonardo_Fee_2005_Ensemble Coding of Vocal Control in Birdsong.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/R6MYKMT2/652.html}
}

@misc{leonardsenDeepNeuralNetworks2021,
  title = {Deep Neural Networks Learn General and Clinically Relevant Representations of the Ageing Brain},
  author = {Leonardsen, Esten H. and Peng, Han and Kaufmann, Tobias and Agartz, Ingrid and Andreassen, Ole A. and Celius, Elisabeth Gulowsen and Espeseth, Thomas and Harbo, Hanne F. and H{\o}gest{\o}l, Einar A. and de Lange, Ann-Marie and Marquand, Andre F. and {Vidal-Pi{\~n}eiro}, Didac and Roe, James M. and Selb{\ae}k, Geir and S{\o}rensen, {\O}ystein and Smith, Stephen M. and Westlye, Lars T. and Wolfers, Thomas and Wang, Yunpeng},
  year = {2021},
  month = oct,
  pages = {2021.10.29.21265645},
  institution = {{medRxiv}},
  doi = {10.1101/2021.10.29.21265645},
  abstract = {The discrepancy between chronological age and the apparent age of the brain based on neuroimaging data \textemdash{} the brain age delta \textemdash{} has emerged as a reliable marker of brain health. With an increasing wealth of data, approaches to tackle heterogeneity in data acquisition are vital. To this end, we compiled raw structural magnetic resonance images into one of the largest and most diverse datasets assembled (n=53542), and trained convolutional neural networks (CNNs) to predict age. We achieved state-of-the-art performance on unseen data from unknown scanners (n=2553), and showed that higher brain age delta is associated with diabetes, alcohol intake and smoking. Using transfer learning, the intermediate representations learned by our model complemented and partly outperformed brain age delta in predicting common brain disorders. Our work shows we can achieve generalizable and biologically plausible brain age predictions using CNNs trained on heterogeneous datasets, and transfer them to clinical use cases.},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/Zotero/storage/Leonardsen et al_2021_Deep neural networks learn general and clinically relevant representations of.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/89HI35JB/2021.10.29.html}
}

@article{leppa-ahoLearningGaussianGraphical2017,
  title = {Learning {{Gaussian}} Graphical Models with Fractional Marginal Pseudo-Likelihood},
  author = {{Lepp{\"a}-aho}, Janne and Pensar, Johan and Roos, Teemu and Corander, Jukka},
  year = {2017},
  month = apr,
  journal = {International Journal of Approximate Reasoning},
  volume = {83},
  pages = {21--42},
  publisher = {{Elsevier Inc.}},
  doi = {10.1016/j.ijar.2017.01.001},
  abstract = {We propose a Bayesian approximate inference method for learning the dependence structure of a Gaussian graphical model. Using pseudo-likelihood, we derive an analytical expression to approximate the marginal likelihood for an arbitrary graph structure without invoking any assumptions about decomposability. The majority of the existing methods for learning Gaussian graphical models are either restricted to decomposable graphs or require specification of a tuning parameter that may have a substantial impact on learned structures. By combining a simple sparsity inducing prior for the graph structures with a default reference prior for the model parameters, we obtain a fast and easily applicable scoring function that works well for even high-dimensional data. We demonstrate the favourable performance of our approach by large-scale comparisons against the leading methods for learning non-decomposable Gaussian graphical models. A theoretical justification for our method is provided by showing that it yields a consistent estimator of the graph structure.},
  keywords = {Approximate likelihood,Fractional Bayes factors,Gaussian graphical models,Model selection,Structure learning},
  note = {THis has some good reviews of the concepts but ultimately goes off in a nother direction. Still worth using a to a degree.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HPMFRLWM/Leppä-aho et al. - 2017 - Learning Gaussian graphical models with fractional marginal pseudo-likelihood.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@leppa-ahoLearningGaussianGraphical2017.md}
}

@article{levanquyenDisentanglingDynamicCore2003,
  title = {Disentangling the Dynamic Core: {{A}} Research Program for a Neurodynamics at the Large Scale},
  author = {Le Van Quyen, Michel},
  year = {2003},
  journal = {Biological Research},
  volume = {36},
  number = {1},
  pages = {67--88},
  publisher = {{Society of Biology of Chile}},
  doi = {10.4067/S0716-97602003000100006},
  abstract = {My purpose in this paper is to sketch a research direction based on Francisco Varela's pioneering work in neurodynamics (see also Rudrauf et al. 2003, in this issue). Very early on he argued that the internal coherence of every mental-cognitive state lies in the global self-organization of the brain activities at the large-scale, constituting a fundamental pole of integration called here a "dynamic core". Recent neuroimaging evidence appears to broadly support this hypothesis and suggests that a global brain dynamics emerges at the large scale level from the cooperative interactions among widely distributed neuronal populations. Despite a growing body of evidence supporting this view, our understanding of these large-scale brain processes remains hampered by the lack of a theoretical language for expressing these complex behaviors in dynamical terms. In this paper, I propose a rough cartography of a comprehensive approach that offers a conceptual and mathematical framework to analyze spatio-temporal large-scale brain phenomena. I emphasize how these nonlinear methods can be applied, what property might be inferred from neuronal signals, and where one might productively proceed for the future. This paper is dedicated, with respect and affection, to the memory of Francisco Varela.},
  keywords = {Chaos,EEG,Non-gaussian statistics,Nonlinear dynamics,Phase synchronization},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HVC9JXRU/Le Van Quyen - 2003 - Disentangling the dynamic core A research program for a neurodynamics at the large scale.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@levanquyenDisentanglingDynamicCore2003.md}
}

@incollection{leverFunctionOscillationsHippocampal2014,
  title = {The {{Function}} of {{Oscillations}} in the {{Hippocampal Formation}}},
  booktitle = {Space,{{Time}} and {{Memory}} in the {{Hippocampal Formation}}},
  author = {Lever, Colin and Kaplan, Raphael and Burgess, Neil},
  year = {2014},
  pages = {303--350},
  publisher = {{Springer Vienna}},
  address = {{Vienna}},
  doi = {10.1007/978-3-7091-1292-2_12},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@leverFunctionOscillationsHippocampal2014.md}
}

@article{lewerenzChronicGlutamateToxicity2015,
  title = {Chronic {{Glutamate Toxicity}} in {{Neurodegenerative Diseases-What}} Is the {{Evidence}}?},
  author = {Lewerenz, Jan and Maher, Pamela},
  year = {2015},
  journal = {Frontiers in Neuroscience},
  volume = {9},
  pages = {469},
  issn = {1662-4548},
  doi = {10.3389/fnins.2015.00469},
  abstract = {Together with aspartate, glutamate is the major excitatory neurotransmitter in the brain. Glutamate binds and activates both ligand-gated ion channels (ionotropic glutamate receptors) and a class of G-protein coupled receptors (metabotropic glutamate receptors). Although the intracellular glutamate concentration in the brain is in the millimolar range, the extracellular glutamate concentration is kept in the low micromolar range by the action of excitatory amino acid transporters that import glutamate and aspartate into astrocytes and neurons. Excess extracellular glutamate may lead to excitotoxicity in vitro and in vivo in acute insults like ischemic stroke via the overactivation of ionotropic glutamate receptors. In addition, chronic excitotoxicity has been hypothesized to play a role in numerous neurodegenerative diseases including amyotrophic lateral sclerosis, Alzheimer's disease and Huntington's disease. Based on this hypothesis, a good deal of effort has been devoted to develop and test drugs that either inhibit glutamate receptors or decrease extracellular glutamate. In this review, we provide an overview of the different pathways that are thought to lead to an over-activation of the glutamatergic system and glutamate toxicity in neurodegeneration. In addition, we summarize the available experimental evidence for glutamate toxicity in animal models of neurodegenerative diseases.},
  langid = {english},
  pmcid = {PMC4679930},
  pmid = {26733784},
  keywords = {Alzheimer's disease,amyotrophic lateral sclerosis,excitotoxicity,glutamate receptors,glutamate transporters,Huntington's disease,neurodegeneration,system x−c},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Lewerenz_Maher_2015_Chronic Glutamate Toxicity in Neurodegenerative Diseases-What is the Evidence.pdf}
}

@article{lezonUsingPrincipleEntropy2006,
  title = {Using the Principle of Entropy Maximization to Infer Genetic Interaction Networks from Gene Expression Patterns},
  author = {Lezon, Timothy R. and Banavar, Jayanth R. and Cieplak, Marek and Maritan, Amos and Fedoroff, Nina V.},
  year = {2006},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {103},
  number = {50},
  pages = {19033--19038},
  publisher = {{National Academy of Sciences}},
  doi = {10.1073/pnas.0609152103},
  abstract = {We describe a method based on the principle of entropy maximization to identify the gene interaction network with the highest probability of giving rise to experimentally observed transcript profiles. In its simplest form, the method yields the pairwise gene interaction network, but it can also be extended to deduce higher-order interactions. Analysis of microarray data from genes in Saccharomyces cerevisiae chemostat cultures exhibiting energy metabolic oscillations identifies a gene interaction network that reflects the intracellular communication pathways that adjust cellular metabolic activity and cell division to the limiting nutrient conditions that trigger metabolic oscillations. The success of the present approach in extracting meaningful genetic connections suggests that the maximum entropy principle is a useful concept for understanding living systems, as it is for other complex, nonequilibrium systems. \textcopyright{} 2006 by The National Academy of Sciences of the USA.},
  keywords = {Gene interactions,Metabolic oscillations,Network inference,Signaling},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3TDC53VK/Lezon et al. - 2006 - Using the principle of entropy maximization to infer genetic interaction networks from gene expression patterns.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@lezonUsingPrincipleEntropy2006.md}
}

@article{libbyRotationalDynamicsReduce2021,
  title = {Rotational Dynamics Reduce Interference between Sensory and Memory Representations},
  author = {Libby, Alexandra and Buschman, Timothy J.},
  year = {2021},
  month = may,
  journal = {Nature Neuroscience},
  volume = {24},
  number = {5},
  pages = {715--726},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-021-00821-9},
  abstract = {Cognition depends on integrating sensory percepts with the memory of recent stimuli. However, the distributed nature of neural coding can lead to interference between sensory and memory representations. Here, we show that the brain mitigates such interference by rotating sensory representations into orthogonal memory representations over time. To study how sensory inputs and memories are represented, we recorded neurons from the auditory cortex of mice as they implicitly learned sequences of sounds. We found that the neural population represented sensory inputs and the memory of recent stimuli in two orthogonal dimensions. The transformation of sensory information into a memory was facilitated by a combination of `stable' neurons, which maintained their selectivity over time, and `switching' neurons, which inverted their selectivity over time. Together, these neural responses rotated the population representation, transforming sensory inputs into memory. Theoretical modeling showed that this rotational dynamic is an efficient mechanism for generating orthogonal representations, thereby protecting memories from sensory interference.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Network models;Sensory processing;Short-term memory Subject\_term\_id: network-models;sensory-processing;short-term-memory},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Libby_Buschman_2021_Rotational dynamics reduce interference between sensory and memory.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QGY4QRR5/s41593-021-00821-9.html}
}

@article{liDeepReinforcementLearning2018,
  title = {Deep {{Reinforcement Learning}}: {{An Overview}}},
  shorttitle = {Deep {{Reinforcement Learning}}},
  author = {Li, Yuxi},
  year = {2018},
  month = nov,
  journal = {arXiv:1701.07274 [cs]},
  eprint = {1701.07274},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Li_2018_Deep Reinforcement Learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/42GB226C/1701.html}
}

@article{liLearningSynapticIntrinsic2021,
  title = {Learning the {{Synaptic}} and {{Intrinsic Membrane Dynamics Underlying Working Memory}} in {{Spiking Neural Network Models}}},
  author = {Li, Yinghao and Kim, Robert and Sejnowski, T.},
  year = {2021},
  journal = {Neural Computation},
  doi = {10.1162/neco_a_01409},
  abstract = {A method to directly train not only synaptic-related variables but also membrane-related parameters of a spiking RNN model is developed, offering a unique window into how connectivity patterns and intrinsic neuronal properties contribute to complex dynamics in neural populations. Abstract Recurrent neural network (RNN) models trained to perform cognitive tasks are a useful computational tool for understanding how cortical circuits execute complex computations. However, these models are often composed of units that interact with one another using continuous signals and overlook parameters intrinsic to spiking neurons. Here, we developed a method to directly train not only synaptic-related variables but also membrane-related parameters of a spiking RNN model. Training our model on a wide range of cognitive tasks resulted in diverse yet task-specific synaptic and membrane parameters. We also show that fast membrane time constants and slow synaptic decay dynamics naturally emerge from our model when it is trained on tasks associated with working memory (WM). Further dissecting the optimized parameters revealed that fast membrane properties are important for encoding stimuli, and slow synaptic dynamics are needed for WM maintenance. This approach offers a unique window into how connectivity patterns and intrinsic neuronal properties contribute to complex dynamics in neural populations.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/64XTED9F/Li et al_2021_Learning the Synaptic and Intrinsic Membrane Dynamics Underlying Working Memory.pdf}
}

@article{liMiceInferProbabilistic2013,
  title = {Mice Infer Probabilistic Models for Timing},
  author = {Li, Yi and Dudman, Joshua Tate},
  year = {2013},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {42},
  pages = {17154--17159},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1310666110},
  keywords = {delay reward},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RZR6UAQA/Li_Dudman_2013_Mice infer probabilistic models for timing.pdf}
}

@inproceedings{lindermanFrameworkStudyingSynaptic2014,
  title = {A Framework for Studying Synaptic Plasticity with Neural Spike Train Data},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Linderman, Scott and Stock, Christopher H and Adams, Ryan P},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@lindermanFrameworkStudyingSynaptic2014.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/lindermanFrameworkStudyingSynaptic2014-mdnotes.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Linderman et al_2014_A framework for studying synaptic plasticity with neural spike train data.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@lindermanFrameworkStudyingSynaptic2014.md}
}

@article{liThreedimensionalGrapheneFoam2013,
  title = {Three-Dimensional Graphene Foam as a Biocompatible and Conductive Scaffold for Neural Stem Cells},
  author = {Li, Ning and Zhang, Qi and Gao, Song and Song, Qin and Huang, Rong and Wang, Long and Liu, Liwei and Dai, Jianwu and Tang, Mingliang and Cheng, Guosheng},
  year = {2013},
  volume = {3},
  pages = {1604--1604},
  doi = {10.1038/srep01604},
  abstract = {Neural stem cell (NSC) based therapy provides a promising approach for neural regeneration. For the success of NSC clinical application, a scaffold is required to provide three-dimensional (3D) cell growth microenvironments and appropriate synergistic cell guidance cues. Here, we report the first utilization of graphene foam, a 3D porous structure, as a novel scaffold for NSCs in vitro. It was found that three-dimensional graphene foams (3D-GFs) can not only support NSC growth, but also keep cell at an active proliferation state with upregulation of Ki67 expression than that of two-dimensional graphene films. Meanwhile, phenotypic analysis indicated that 3D-GFs can enhance the NSC differentiation towards astrocytes and especially neurons. Furthermore, a good electrical coupling of 3D-GFs with differentiated NSCs for efficient electrical stimulation was observed. Our findings implicate 3D-GFs could offer a powerful platform for NSC research, neural tissue engineering and neural prostheses.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7YDY4L59/Li et al. - 2013 - Three-dimensional graphene foam as a biocompatible and conductive scaffold for neural stem cells(2).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/D3I4SZAX/Li et al. - 2013 - Three-dimensional graphene foam as a biocompatible and conductive scaffold for neural stem cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@liThreedimensionalGrapheneFoam2013.md}
}

@article{liuLimitedMemoryBFGS1989,
  title = {On the Limited Memory {{BFGS}} Method for Large Scale Optimization},
  author = {Liu, Dong C. and Nocedal, Jorge},
  year = {1989},
  journal = {Mathematical Programming},
  doi = {10.1007/BF01589116},
  abstract = {D. C. Liu\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash tNo contact information provided yet.\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash n\textbackslash n\textbackslash n\textbackslash n\textbackslash n                                            \textbackslash n\textbackslash n\textbackslash n\textbackslash n\textbackslash tBibliometrics:~publication history\textbackslash n\textbackslash t\textbackslash t\textbackslash n        \textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tPublication years1987-2007\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tPublication count7\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tCitation Count212\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tAvailable for download0\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n \textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tDownloads (6 Weeks)0\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tDownloads (12 Months)0\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n \textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash n\textbackslash n\textbackslash n                                        \textbackslash n                                        \textbackslash n                                                    View colleagues of D. C. Liu\textbackslash n                                            \textbackslash n                                         \textbackslash n                                        \textbackslash n                                     \textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash t\textbackslash n                                    \textbackslash n                                    \textbackslash n                                        \textbackslash n                                        \textbackslash n\textbackslash n\textbackslash n\textbackslash n\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash n\textbackslash n\textbackslash t~J. Nocedal\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash tNo contact information provided yet.\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash n\textbackslash n\textbackslash n\textbackslash n\textbackslash n                                            \textbackslash n\textbackslash n\textbackslash n\textbackslash n\textbackslash tBibliometrics:~publication history\textbackslash n\textbackslash t\textbackslash t\textbackslash n        \textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tPublication years1985-2009\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tPublication count35\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tCitation Count497\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tAvailable for download3\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n \textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tDownloads (6 Weeks)51\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash tDownloads (12 Months)477\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash t\textbackslash n \textbackslash t\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash t\textbackslash n\textbackslash n\textbackslash n\textbackslash n                                        \textbackslash n                                        \textbackslash n                                                    View colleagues of J. Nocedal},
  keywords = {conjugate gradient method,Large scale nonlinear optimization,limited memory methods,partitioned quasi-Newton method},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/G4JA3C6J/Liu, Nocedal - 1989 - On the limited memory BFGS method for large scale optimization.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@liuLimitedMemoryBFGS1989.md}
}

@article{liuMultiobjectiveReinforcementLearning2015,
  title = {Multiobjective {{Reinforcement Learning}}: {{A Comprehensive Overview}}},
  shorttitle = {Multiobjective {{Reinforcement Learning}}},
  author = {Liu, Chunming and Xu, Xin and Hu, Dewen},
  year = {2015},
  month = mar,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {45},
  number = {3},
  pages = {385--398},
  issn = {2168-2232},
  doi = {10.1109/TSMC.2014.2358639},
  abstract = {Reinforcement learning (RL) is a powerful paradigm for sequential decision-making under uncertainties, and most RL algorithms aim to maximize some numerical value which represents only one long-term objective. However, multiple long-term objectives are exhibited in many real-world decision and control systems, so recently there has been growing interest in solving multiobjective reinforcement learning (MORL) problems where there are multiple conflicting objectives. The aim of this paper is to present a comprehensive overview of MORL. The basic architecture, research topics, and na\"ive solutions of MORL are introduced at first. Then, several representative MORL approaches and some important directions of recent research are comprehensively reviewed. The relationships between MORL and other related research are also discussed, which include multiobjective optimization, hierarchical RL, and multiagent RL. Moreover, research challenges and open problems of MORL techniques are suggested.},
  keywords = {Approximation algorithms,Approximation methods,Decision making,Equations,HRL,Linear programming,Markov decision process (MDP),multiobjective reinforcement learning (MORL),Optimization,Pareto front,reinforcement learning (RL),sequential decision-making,Vectors},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@liuMultiobjectiveReinforcementLearning2015.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/liuMultiobjectiveReinforcementLearning2015-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WPT7Z82H/Liu et al_2015_Multiobjective Reinforcement Learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VDZIA7YL/6918520.html}
}

@article{llera-monteroComputationalRolesPlastic2019,
  title = {Computational Roles of Plastic Probabilistic Synapses},
  author = {{Llera-Montero}, Milton and Sacramento, Jo{\~a}o and Costa, Rui Ponte},
  year = {2019},
  month = feb,
  journal = {Current Opinion in Neurobiology},
  series = {Neurobiology of {{Learning}} and {{Plasticity}}},
  volume = {54},
  pages = {90--97},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2018.09.002},
  abstract = {The probabilistic nature of synaptic transmission has remained enigmatic. However, recent developments have started to shed light on why the brain may rely on probabilistic synapses. Here, we start out by reviewing experimental evidence on the specificity and plasticity of synaptic response statistics. Next, we overview different computational perspectives on the function of plastic probabilistic synapses for constrained, statistical and deep learning. We highlight that all of these views require some form of optimisation of probabilistic synapses, which has recently gained support from theoretical analysis of long-term synaptic plasticity experiments. Finally, we contrast these different computational views and propose avenues for future research. Overall, we argue that the time is ripe for a better understanding of the computational functions of probabilistic synapses.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/L2PUF9XW/Computational roles of plastic probabilistic synapses - 10.05.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/V6TSV82Z/Llera-Montero et al_2019_Computational roles of plastic probabilistic synapses.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/M2CT6WD7/S0959438818301028.html}
}

@article{loboExploitingStimuliEncoding2020,
  title = {Exploiting the Stimuli Encoding Scheme of Evolving {{Spiking Neural Networks}} for Stream Learning},
  author = {Lobo, Jesus L. and Oregi, Izaskun and Bifet, Albert and Del Ser, Javier},
  year = {2020},
  month = mar,
  journal = {Neural Networks},
  volume = {123},
  pages = {118--133},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.11.021},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/K2MFXMMR/Lobo et al_2020_Exploiting the stimuli encoding scheme of evolving Spiking Neural Networks for.pdf}
}

@article{loboSpikingNeuralNetworks2020,
  title = {Spiking {{Neural Networks}} and Online Learning: {{An}} Overview and Perspectives},
  shorttitle = {Spiking {{Neural Networks}} and Online Learning},
  author = {Lobo, Jesus L. and Del Ser, Javier and Bifet, Albert and Kasabov, Nikola},
  year = {2020},
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  pages = {88--100},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.09.004},
  abstract = {Applications that generate huge amounts of data in the form of fast streams are becoming increasingly prevalent, being therefore necessary to learn in an online manner. These conditions usually impose memory and processing time restrictions, and they often turn into evolving environments where a change may affect the input data distribution. Such a change causes that predictive models trained over these stream data become obsolete and do not adapt suitably to new distributions. Specially in these non-stationary scenarios, there is a pressing need for new algorithms that adapt to these changes as fast as possible, while maintaining good performance scores. Unfortunately, most off-the-shelf classification models need to be retrained if they are used in changing environments, and fail to scale properly. Spiking Neural Networks have revealed themselves as one of the most successful approaches to model the behavior and learning potential of the brain, and exploit them to undertake practical online learning tasks. Besides, some specific flavors of Spiking Neural Networks can overcome the necessity of retraining after a drift occurs. This work intends to merge both fields by serving as a comprehensive overview, motivating further developments that embrace Spiking Neural Networks for online learning scenarios, and being a friendly entry point for non-experts.},
  langid = {english},
  keywords = {Concept drift,Online learning,Spiking Neural Networks,Stream data},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/74VV7IVH/Spiking Neural Networks and online learning An overview and perspectives - 10.05.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ALGRJYNZ/Lobo et al_2020_Spiking Neural Networks and online learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PBPRFBFN/S0893608019302655.html}
}

@article{loboSpikingNeuralNetworks2020a,
  title = {Spiking {{Neural Networks}} and Online Learning: {{An}} Overview and Perspectives},
  shorttitle = {Spiking {{Neural Networks}} and Online Learning},
  author = {Lobo, Jesus L. and Del Ser, Javier and Bifet, Albert and Kasabov, Nikola},
  year = {2020},
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  pages = {88--100},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.09.004},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N3Q4TASV/Lobo et al_2020_Spiking Neural Networks and online learning.pdf}
}

@article{locasaleMaximumEntropyReconstructions2009,
  title = {Maximum Entropy Reconstructions of Dynamic Signaling Networks from Quantitative Proteomics Data},
  author = {Locasale, Jason W. and {Wolf-Yadlin}, Alejandro},
  year = {2009},
  month = aug,
  journal = {PLoS ONE},
  volume = {4},
  number = {8},
  pages = {6522--6522},
  publisher = {{Public Library of Science}},
  doi = {10.1371/journal.pone.0006522},
  abstract = {Advances in mass spectrometry among other technologies have allowed for quantitative, reproducible, proteome-wide measurements of levels of phosphorylation as signals propagate through complex networks in response to external stimuli under different conditions. However, computational approaches to infer elements of the signaling network strictly from the quantitative aspects of proteomics data are not well established. We considered a method using the principle of maximum entropy to infer a network of interacting phosphotyrosine sites from pairwise correlations in a mass spectrometry data set and derive a phosphorylation-dependent interaction network solely from quantitative proteomics data. We first investigated the applicability of this approach by using a simulation of a model biochemical signaling network whose dynamics are governed by a large set of coupled differential equations. We found that in a simulated signaling system, the method detects interactions with significant accuracy. We then analyzed a growth factor mediated signaling network in a human mammary epithelial cell line that we inferred from mass spectrometry data and observe a biologically interpretable, small-world structure of signaling nodes, as well as a catalog of predictions regarding the interactions among previously uncharacterized phosphotyrosine sites. For example, the calculation places a recently identified tumor suppressor pathway through ARHGEF7 and Scribble, in the context of growth factor signaling. Our findings suggest that maximum entropy derived network models are an important tool for interpreting quantitative proteomics data. \textcopyright{} 2009 Locasale et al.},
  keywords = {Cell signaling structures,Eigenvalues,Entropy,Network analysis,Phosphorylation,Protein interaction networks,Proteomic databases,Signaling networks},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TZUWMFCA/Locasale, Wolf-Yadlin - 2009 - Maximum entropy reconstructions of dynamic signaling networks from quantitative proteomics data.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@locasaleMaximumEntropyReconstructions2009.md}
}

@article{lokhovOptimalStructureParameter2018,
  title = {Optimal Structure and Parameter Learning of {{Ising}} Models},
  author = {Lokhov, Andrey Y. and Vuffray, Marc and Misra, Sidhant and Chertkov, Michael},
  year = {2018},
  month = mar,
  journal = {Science Advances},
  volume = {4},
  number = {3},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/sciadv.1700791},
  abstract = {Reconstruction of the structure and parameters of an Ising model from binary samples is a problem of practical importance in a variety of disciplines, ranging from statistical physics and computational biology to image processing and machine learning. The focus of the research community shifted toward developing universal reconstruction algorithms that are both computationally efficient and require the minimal amount of expensive data. We introduce a new method, interaction screening, which accurately estimates model parameters using local optimization problems. The algorithm provably achieves perfect graph structure recovery with an informationtheoretically optimal number of samples, notably in the low-Temperature regime, which is known to be the hardest for learning. The efficacy of interaction screening is assessed through extensive numerical tests on synthetic Ising models of various topologies with different types of interactions, as well as on real data produced by a D-Wave quantum computer. This study shows that the interaction screening method is an exact, tractable, and optimal technique that universally solves the inverse Ising problem.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CFHTHF34/Lokhov et al. - 2018 - Optimal structure and parameter learning of Ising models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@lokhovOptimalStructureParameter2018.md}
}

@article{lowetDistributionalReinforcementLearning2020,
  title = {Distributional {{Reinforcement Learning}} in the {{Brain}}},
  author = {Lowet, Adam S. and Zheng, Qiao and Matias, Sara and Drugowitsch, Jan and Uchida, Naoshige},
  year = {2020},
  month = dec,
  journal = {Trends in Neurosciences},
  volume = {43},
  number = {12},
  pages = {980--997},
  issn = {01662236},
  doi = {10.1016/j.tins.2020.09.004},
  langid = {english},
  keywords = {RL},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JIV3GIFM/Lowet et al. - 2020 - Distributional Reinforcement Learning in the Brain.pdf}
}

@article{lukoseviciusReservoirComputingApproaches2009,
  title = {Reservoir Computing Approaches to Recurrent Neural Network Training},
  author = {Luko{\v s}evi{\v c}ius, Mantas and Jaeger, Herbert},
  year = {2009},
  month = aug,
  journal = {Computer Science Review},
  volume = {3},
  number = {3},
  pages = {127--149},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2009.03.005},
  abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current ``brand-names'' of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed ``map'' of it.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FCRU8VEK/Reservoir computing approaches to recurrent neural network training - 09.05.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TBE4TRGL/Lukoševičius_Jaeger_2009_Reservoir computing approaches to recurrent neural network training.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JVL6LDGC/S1574013709000173.html}
}

@article{luoFunctionalConnectomeAssessed2015,
  title = {Functional Connectome Assessed Using Graph Theory in Drug-Naive {{Parkinson}}'s Disease},
  author = {Luo, Chun Yan and Guo, Xiao Yan and Song, Wei and Chen, Qin and Cao, Bei and Yang, Jing and Gong, Qi Yong and Shang, Hui-Fang},
  year = {2015},
  month = jun,
  journal = {Journal of Neurology},
  volume = {262},
  number = {6},
  pages = {1557--1567},
  issn = {1432-1459},
  doi = {10.1007/s00415-015-7750-3},
  abstract = {The purpose of this study is to investigate whether the topological organization of whole-brain functional network is disrupted in patients with Parkinson's disease (PD). We employed resting-state functional MRI (R-fMRI) and graph theory to investigate the topological organization of the functional connectome in 47 early-stage drug-na\"ive PD patients and 47 healthy control subjects. Correlations between network properties and clinical variables were tested. Both the PD and control groups showed small-world architecture in brain functional networks. However, the PD patients had lower clustering coefficient and local efficiency relative to control subjects, indicating disrupted topologic organization and a shift toward randomization in their functional brain network. At node and connection level, reduced node centralities and connectivity strength were found mainly in temporal-occipital regions and also in sensorimotor regions of PD patients. In PD patients, altered global network properties correlated with cognitive function, while motor impairment was correlated with local connection changes. This study demonstrates a disruption of whole-brain topological organization of the functional brain networks in early-stage drug-na\"ive PD patients and this disruption might contribute to preclinical changes in cognitive process in these patients.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YDTGKSY2/Luo et al. - 2015 - Functional connectome assessed using graph theory .pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@luoFunctionalConnectomeAssessed2015.md}
}

@article{luoNeuralTimingStimulus2018,
  title = {Neural Timing of Stimulus Events with Microsecond Precision},
  author = {Luo, Jinhong and Macias, Silvio and Ness, Torbj{\o}rn V. and Einevoll, Gaute T. and Zhang, Kechen and Moss, Cynthia F.},
  year = {2018},
  journal = {PLoS biology},
  volume = {16},
  number = {10},
  pages = {e2006422-e2006422},
  issn = {1111111111},
  doi = {10.1371/journal.pbio.2006422},
  abstract = {Temporal analysis of sound is fundamental to auditory processing throughout the animal kingdom. Echolocating bats are powerful models for investigating the underlying mechanisms of auditory temporal processing, as they show microsecond precision in discriminating the timing of acoustic events. However, the neural basis for microsecond auditory discrimination in bats has eluded researchers for decades. Combining extracellular recordings in the midbrain inferior colliculus (IC) and mathematical modeling, we show that microsecond precision in registering stimulus events emerges from synchronous neural firing, revealed through low-latency variability of stimulus-evoked extracellular field potentials (EFPs, 200-600 Hz). The temporal precision of the EFP increases with the number of neurons firing in synchrony. Moreover, there is a functional relationship between the temporal precision of the EFP and the spectrotemporal features of the echolocation calls. In addition, EFP can measure the time difference of simulated echolocation call-echo pairs with microsecond precision. We propose that synchronous firing of populations of neurons operates in diverse species to support temporal analysis for auditory localization and complex sound processing.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/M53ER9GB/Luo et al. - 2018 - Neural timing of stimulus events with microsecond precision.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@luoNeuralTimingStimulus2018.md}
}

@incollection{maassLiquidStateMachines2011,
  title = {Liquid {{State Machines}}: {{Motivation}}, {{Theory}}, and {{Applications}}},
  shorttitle = {Liquid {{State Machines}}},
  booktitle = {Computability in {{Context}}},
  author = {Maass, Wolfgang},
  year = {2011},
  month = feb,
  pages = {275--296},
  publisher = {{IMPERIAL COLLEGE PRESS}},
  doi = {10.1142/9781848162778_0008},
  isbn = {978-1-84816-245-7},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MX3FJQ7N/Maass_2011_Liquid State Machines.pdf}
}

@article{mackayBayesianInterpolation1992,
  title = {Bayesian {{Interpolation}}},
  author = {Mackay, David J C},
  year = {1992},
  volume = {447},
  pages = {415--447},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CP9BVU6W/MACnc92a.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mackayBayesianInterpolation1992.md}
}

@book{mackayInformationTheoryInference2008,
  title = {Information {{Theory}}, {{Inference}} and {{Learning Algorithms}}},
  author = {MacKay, David J. C.},
  year = {2008},
  journal = {Information Theory, Inference, and Learning Algorithms},
  volume = {100},
  publisher = {{Cambridge}},
  doi = {10.2277/0521642981},
  abstract = {Information theory and inference, taught together in this exciting textbook, lie at the heart of many important areas of modern technology - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics and cryptography. The book introduces theory in tandem with applications. Information theory is taught alongside practical communication systems such as arithmetic coding for data compression and sparse-graph codes for error-correction. Inference techniques, including message-passing algorithms, Monte Carlo methods and variational approximations, are developed alongside applications to clustering, convolutional codes, independent component analysis, and neural networks. Uniquely, the book covers state-of-the-art error-correcting codes, including low-density-parity-check codes, turbo codes, and digital fountain codes - the twenty-first-century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, the book is ideal for self-learning, and for undergraduate or graduate courses. It also provides an unparalleled entry point for professionals in areas as diverse as computational biology, financial engineering and machine learning.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/U3VHG6G5/MacKay - 2008 - Information Theory, Inference and Learning Algorithms.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mackayInformationTheoryInference2008.md}
}

@article{mackayPracticalBayesianFramework1992,
  title = {A {{Practical Bayesian Framework}}},
  author = {Mackay, David J C},
  year = {1992},
  volume = {472},
  number = {1},
  pages = {448--472},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FBFQNGG6/MACnc92b.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mackayPracticalBayesianFramework1992.md}
}

@article{maDesignSynapticNeuronal2021,
  title = {Design of {{Synaptic Neuronal Circuit Based}} on {{Memristors}}},
  author = {Ma, Lin and Tong, Yi and He, Lin},
  year = {2021},
  month = nov,
  journal = {Journal of Physics: Conference Series},
  volume = {2108},
  number = {1},
  pages = {012029},
  publisher = {{IOP Publishing}},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/2108/1/012029},
  abstract = {To solve the problems of poor learning efficiency and low accuracy caused by the single fixed synaptic weight in the traditional artificial neural network. On the foundation of the improved memristor model, this paper designs a synaptic neuronal circuit based on the natural memory characteristics of the memristor. This synapse is composed of six memristors. The resistance of the memristor is changed by adding a periodic square wave to update the synaptic weight. This circuit can realize signed synaptic weighting, which has certain linear characteristics. Finally, two synaptic weight update methods are proposed based on this circuit, and the validity of the design is verified through Spice simulation experiments.},
  langid = {english},
  keywords = {neuromorphic},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X2VQJWF4/Ma et al_2021_Design of Synaptic Neuronal Circuit Based on Memristors.pdf}
}

@article{maesLearningCompositionalSequences2020,
  title = {Learning Compositional Sequences with Multiple Time Scales through a Hierarchical Network of Spiking Neurons},
  author = {Maes, Amadeus and Barahona, Mauricio and Clopath, C.},
  year = {2020},
  journal = {bioRxiv},
  doi = {10.1101/2020.09.08.287748},
  abstract = {This work introduces a network model of spiking neurons with a hierarchical organisation aimed at sequence learning on multiple time scales that displays faster learning, more flexible relearning, increased capacity, and higher robustness to perturbations. Sequential behaviour is often compositional and organised across multiple time scales: a set of individual elements developing on short time scales (motifs) are combined to form longer functional sequences (syntax). Such organisation leads to a natural hierarchy that can be used advantageously for learning, since the motifs and the syntax can be acquired independently. Despite mounting experimental evidence for hierarchical structures in neuroscience, models for temporal learning based on neuronal networks have mostly focused on serial methods. Here, we introduce a network model of spiking neurons with a hierarchical organisation aimed at sequence learning on multiple time scales. Using biophysically motivated neuron dynamics and local plasticity rules, the model can learn motifs and syntax independently. Furthermore, the model can relearn sequences efficiently and store multiple sequences. Compared to serial learning, the hierarchical model displays faster learning, more flexible relearning, increased capacity, and higher robustness to perturbations. The hierarchical model redistributes the variability: it achieves high motif fidelity at the cost of higher variability in the between-motif timings.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RINR8UHZ/Maes et al_2020_Learning compositional sequences with multiple time scales through a.pdf}
}

@article{maesLearningSpatiotemporalSignals2019,
  title = {Learning Spatiotemporal Signals Using a Recurrent Spiking Network That Discretizes Time},
  author = {Maes, Amadeus and Barahona, Mauricio and Clopath, C.},
  year = {2019},
  journal = {bioRxiv},
  doi = {10.1101/693861},
  abstract = {A model using biological-plausible plasticity rules for a specific computational task: spatiotemporal sequence learning is investigated where a spiking recurrent network of excitatory and inhibitory biophysical neurons drives a read-out layer: the dynamics of the recurrent network is constrained to encode time while the read- out neurons encode space. Learning to produce spatiotemporal sequences is a common task the brain has to solve. The same neural substrate may be used by the brain to produce different sequential behaviours. The way the brain learns and encodes such tasks remains unknown as current computational models do not typically use realistic biologically-plausible learning. Here, we propose a model where a spiking recurrent network of excitatory and inhibitory biophysical neurons drives a read-out layer: the dynamics of the recurrent network is constrained to encode time while the read-out neurons encode space. Space is then linked with time through plastic synapses that follow common Hebbian learning rules. We demonstrate that the model is able to learn spatiotemporal dynamics on a timescale that is behaviourally relevant. Learned sequences are robustly replayed during a regime of spontaneous activity. Author summary The brain has the ability to learn flexible behaviours on a wide range of time scales. Previous studies have successfully build spiking network models that learn a variety of computational tasks. However, often the learning involved is not local. Here, we investigate a model using biological-plausible plasticity rules for a specific computational task: spatiotemporal sequence learning. The architecture separates time and space into two different parts and this allows learning to bind space to time. Importantly, the time component is encoded into a recurrent network which exhibits sequential dynamics on a behavioural time scale. This network is then used as an engine to drive spatial read-out neurons. We demonstrate that the model can learn complicated spatiotemporal spiking dynamics, such as the song of a bird, and replay the song robustly.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3KUCYIRU/Maes et al_2019_Learning spatiotemporal signals using a recurrent spiking network that.pdf}
}

@article{maesLongShorttermHistory2021,
  title = {Long- and Short-Term History Effects in a Spiking Network Model of Statistical Learning},
  author = {Maes, Amadeus and Barahona, Mauricio and Clopath, C.},
  year = {2021},
  journal = {bioRxiv},
  doi = {10.1101/2021.09.22.461372},
  abstract = {A model based upon learning the inverse of the cumulative distribution function is presented, which is entirely unsupervised using biophysical neurons and biologically plausible learning rules and can be accessed to compute expectations and signal surprise in downstream networks. The statistical structure of the environment is often important when making decisions. There are multiple theories of how the brain represents statistical structure. One such theory states that neural activity spontaneously samples from probability distributions. In other words, the network spends more time in states which encode high-probability stimuli. Existing spiking network models implementing sampling lack the ability to learn the statistical structure from observed stimuli and instead often hard-code a dynamics. Here, we focus on how arbitrary prior knowledge about the external world can both be learned and spontaneously recollected. We present a model based upon learning the inverse of the cumulative distribution function. Learning is entirely unsupervised using biophysical neurons and biologically plausible learning rules. We show how this prior knowledge can then be accessed to compute expectations and signal surprise in downstream networks. Sensory history effects emerge from the model as a consequence of ongoing learning.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/98K9GXUL/Maes et al_2021_Long- and short-term history effects in a spiking network model of statistical.pdf}
}

@article{magransdeabrilConnectivityInferenceNeural2018,
  title = {Connectivity Inference from Neural Recording Data: {{Challenges}}, Mathematical Bases and Research Directions},
  author = {{Magrans de Abril}, Ildefons and Yoshimoto, Junichiro and Doya, Kenji},
  year = {2018},
  month = jun,
  journal = {Neural Networks},
  volume = {102},
  pages = {120--137},
  publisher = {{Elsevier Ltd}},
  doi = {10.1016/j.neunet.2018.02.016},
  abstract = {This article presents a review of computational methods for connectivity inference from neural activity data derived from multi-electrode recordings or fluorescence imaging. We first identify biophysical and technical challenges in connectivity inference along the data processing pipeline. We then review connectivity inference methods based on two major mathematical foundations, namely, descriptive model-free approaches and generative model-based approaches. We investigate representative studies in both categories and clarify which challenges have been addressed by which method. We further identify critical open issues and possible research directions.},
  keywords = {Calcium fluorescence imaging,Connectivity inference,Effective connectivity,Functional connectivity,Multi-electrode recording},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PJ4Q2TXD/Magrans de Abril, Yoshimoto, Doya - 2018 - Connectivity inference from neural recording data Challenges, mathematical bases and research.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@magransdeabrilConnectivityInferenceNeural2018.md}
}

@article{maierGeneralizationSmallworldEffect2019,
  title = {Generalization of the Small-World Effect on a Model Approaching the {{Erd\H{o}s}}\textendash{{R\'enyi}} Random Graph},
  author = {Maier, Benjamin F.},
  year = {2019},
  month = jun,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {9268},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-45576-3},
  abstract = {The famous Watts\textendash Strogatz (WS) small-world network model does not approach the Erd\H{o}s\textendash R\'enyi (ER) random graph model in the limit of total randomization which can lead to confusion and complicates certain analyses. In this paper we discuss a simple alternative which was first introduced~by Song and Wang, where instead of rewiring, edges are drawn between pairs of nodes with a distance-based connection probability. We show that this model is simpler to analyze, approaches the true ER random graph model in the completely randomized limit, and demonstrate that the WS model and the alternative model may yield different quantitative results using the example of a random walk temporal observable. An efficient sampling algorithm for the alternative model is proposed. Analytic results regarding the degree distribution, degree variance, number of two-stars per node, number of triangles per node, clustering coefficient, and random walk mixing time are presented. Subsequently, the small-world effect is illustrated by showing that the clustering coefficient decreases much slower than an upper bound on the message delivery time with increasing long-range connection probability which generalizes the small-world effect from informed searches to random search strategies. Due to its accessibility for analytic evaluations, we propose that this modified model should be used as an alternative reference model for studying the influence of small-world topologies on dynamic systems as well as a simple model to introduce numerous topics when teaching network science.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Small world; Graphic Model},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CWJMT4FB/Maier - 2019 - Generalization of the small-world effect on a mode.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@maierGeneralizationSmallworldEffect2019.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/F8X423M4/s41598-019-45576-3.html}
}

@article{manneschiExploitingMultipleTimescales2021,
  title = {Exploiting {{Multiple Timescales}} in {{Hierarchical Echo State Networks}}},
  author = {Manneschi, Luca and Ellis, Matthew O. A. and Gigante, Guido and Lin, Andrew C. and Del Giudice, Paolo and Vasilaki, Eleni},
  year = {2021},
  journal = {Frontiers in Applied Mathematics and Statistics},
  volume = {6},
  issn = {2297-4687},
  abstract = {Echo state networks (ESNs) are a powerful form of reservoir computing that only require training of linear output weights while the internal reservoir is formed of fixed randomly connected neurons. With a correctly scaled connectivity matrix, the neurons' activity exhibits the echo-state property and responds to the input dynamics with certain timescales. Tuning the timescales of the network can be necessary for treating certain tasks, and some environments require multiple timescales for an efficient representation. Here we explore the timescales in hierarchical ESNs, where the reservoir is partitioned into two smaller linked reservoirs with distinct properties. Over three different tasks (NARMA10, a reconstruction task in a volatile environment, and psMNIST), we show that by selecting the hyper-parameters of each partition such that they focus on different timescales, we achieve a significant performance improvement over a single ESN. Through a linear analysis, and under the assumption that the timescales of the first partition are much shorter than the second's (typically corresponding to optimal operating conditions), we interpret the feedforward coupling of the partitions in terms of an effective representation of the input signal, provided by the first partition to the second, whereby the instantaneous input signal is expanded into a weighted combination of its time derivatives. Furthermore, we propose a data-driven approach to optimise the hyper-parameters through a gradient descent optimisation method that is an online approximation of backpropagation through time. We demonstrate the application of the online learning rule across all the tasks considered.},
  keywords = {ESN,hierarchial,multiple,timescales},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/97JCDS2R/Manneschi et al_2021_Exploiting Multiple Timescales in Hierarchical Echo State Networks.pdf}
}

@article{mansvelderSynapticPlasticityHuman2019,
  title = {Synaptic Plasticity in Human Cortical Circuits: Cellular Mechanisms of Learning and Memory in the Human Brain?},
  shorttitle = {Synaptic Plasticity in Human Cortical Circuits},
  author = {Mansvelder, Huibert D. and Verhoog, Matthijs B. and Goriounova, Natalia A.},
  year = {2019},
  month = feb,
  journal = {Current Opinion in Neurobiology},
  volume = {54},
  pages = {186--193},
  issn = {1873-6882},
  doi = {10.1016/j.conb.2018.06.013},
  abstract = {Synaptic plasticity is the cellular basis of learning and memory, but to what extent this holds for the adult human brain is not known. To study synaptic plasticity in human neuronal circuits poses a huge challenge, since live human neurons and synapses are not readily accessible. Despite this, various lines of research have provided insights in properties of adult human synapses and their plasticity both in vitro and in vivo, with some unexpected surprises. We first discuss the experimental approaches to study activity-dependent plasticity of adult human synapses, and then highlight rules and mechanisms of Hebbian spike timing-dependent plasticity (STDP) found in these synapses. Finally, we conclude with thoughts on how these synaptic principles can underlie human learning and memory.},
  langid = {english},
  pmid = {30017789},
  keywords = {Cerebral Cortex,Humans,Learning,Nerve Net,Neuronal Plasticity}
}

@article{marderNeuromodulationNeuronalCircuits2012,
  title = {Neuromodulation of {{Neuronal Circuits}}: {{Back}} to the {{Future}}},
  author = {Marder, Eve},
  year = {2012},
  month = oct,
  journal = {Neuron},
  volume = {76},
  number = {1},
  pages = {1--11},
  publisher = {{Cell Press}},
  doi = {10.1016/J.NEURON.2012.09.010},
  abstract = {All nervous systems are subject to neuromodulation. Neuromodulators can be delivered as local hormones, as cotransmitters in projection neurons, and through the general circulation. Because neuromodulators can transform the intrinsic firing properties of circuit neurons and alter effective synaptic strength, neuromodulatory substances reconfigure neuronal circuits, often massively altering their output. Thus, the anatomical connectome provides a minimal structure and the neuromodulatory environment constructs and specifies the functional circuits that give rise to behavior.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JCKGYIVV/Marder - 2012 - Neuromodulation of Neuronal Circuits Back to the Future.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@marderNeuromodulationNeuronalCircuits2012.md}
}

@article{mareesTutorialConductingGenome2018,
  title = {A Tutorial on Conducting Genome-wide Association Studies: {{Quality}} Control and Statistical Analysis},
  shorttitle = {A Tutorial on Conducting Genome-wide Association Studies},
  author = {Marees, Andries T. and {de Kluiver}, Hilde and Stringer, Sven and Vorspan, Florence and Curis, Emmanuel and Marie-Claire, Cynthia and Derks, Eske M.},
  year = {2018},
  month = feb,
  journal = {International Journal of Methods in Psychiatric Research},
  volume = {27},
  number = {2},
  pages = {e1608},
  issn = {1049-8931},
  doi = {10.1002/mpr.1608},
  abstract = {Objectives Genome-wide association studies (GWAS) have become increasingly popular to identify associations between single nucleotide polymorphisms (SNPs) and phenotypic traits. The GWAS method is commonly applied within the social sciences. However, statistical analyses will need to be carefully conducted and the use of dedicated genetics software will be required. This tutorial aims to provide a guideline for conducting genetic analyses. Methods We discuss and explain key concepts and illustrate how to conduct GWAS using example scripts provided through GitHub (https://github.com/MareesAT/GWA\_tutorial/ )., In addition to the illustration of standard GWAS, we will also show how to apply polygenic risk score (PRS) analysis. PRS does not aim to identify individual SNPs but aggregates information from SNPs across the genome in order to provide individual-level scores of genetic risk. Results The simulated data and scripts that will be illustrated in the current tutorial provide hands-on practice with genetic analyses. The scripts are based on PLINK, PRSice, and R, which are commonly used, freely available software tools that are accessible for novice users. Conclusions By providing theoretical background and hands-on experience, we aim to make GWAS more accessible to researchers without formal training in the field.},
  pmcid = {PMC6001694},
  pmid = {29484742},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Marees et al_2018_A tutorial on conducting genome‐wide association studies.pdf}
}

@article{markramInterneuronsNeocorticalInhibitory2004,
  title = {Interneurons of the Neocortical Inhibitory System},
  author = {Markram, Henry and {Toledo-Rodriguez}, Maria and Wang, Yun and Gupta, Anirudh and Silberberg, Gilad and Wu, Caizhi},
  year = {2004},
  journal = {Nature Reviews Neuroscience},
  volume = {5},
  number = {10},
  pages = {793--807},
  issn = {1471-003X (Print)\textbackslash r1471-003X (Linking)},
  doi = {10.1038/nrn1519},
  abstract = {Mammals adapt to a rapidly changing world because of the sophisticated cognitive functions that are supported by the neocortex. The neocortex, which forms almost 80\% of the human brain, seems to have arisen from repeated duplication of a stereotypical microcircuit template with subtle specializations for different brain regions and species. The quest to unravel the blueprint of this template started more than a century ago and has revealed an immensely intricate design. The largest obstacle is the daunting variety of inhibitory interneurons that are found in the circuit. This review focuses on the organizing principles that govern the diversity of inhibitory interneurons and their circuits.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UYANPYUE/Markram et al. - 2004 - Interneurons of the neocortical inhibitory system.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@markramInterneuronsNeocorticalInhibitory2004.md}
}

@article{marrePredictionSpatiotemporalPatterns2009,
  title = {Prediction of Spatiotemporal Patterns of Neural Activity from Pairwise Correlations},
  author = {Marre, O. and El Boustani, S. and Fr{\'e}gnac, Y. and Destexhe, A.},
  year = {2009},
  month = mar,
  journal = {Physical Review Letters},
  volume = {102},
  number = {13},
  doi = {10.1103/PhysRevLett.102.138101},
  abstract = {We designed a model-based analysis to predict the occurrence of population patterns in distributed spiking activity. Using a maximum entropy principle with a Markovian assumption, we obtain a model that accounts for both spatial and temporal pairwise correlations among neurons. This model is tested on data generated with a Glauber spin-glass system and is shown to correctly predict the occurrence probabilities of spatio-temporal patterns significantly better than Ising models taking into account only pairwise correlations. This increase of predictability was also observed on experimental data recorded in parietal cortex during slow-wave sleep. This approach can also be used to generate surrogates that reproduce the spatial and temporal correlations of a given data set.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BZWRE5FH/Marre et al. - 2009 - Prediction of spatiotemporal patterns of neural activity from pairwise correlations.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/S7FCDZEN/Marre et al. - 2008 - Prediction of Spatiotemporal Patterns of Neural Activity from Pairwise Correlations.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@marrePredictionSpatiotemporalPatterns2009.md}
}

@article{marrUnderstandingComputationUnderstanding1976,
  title = {From {{Understanding Computation}} to {{Understanding Neural Circuitry}}},
  author = {Marr, D. and Poggio, T.},
  year = {1976},
  month = may,
  abstract = {The CNS needs to be understood at four nearly independent levels of description: (1) that at which the nature of computation is expressed; (2) that at which the algorithms that implement a computation are characterized; (3) that at which an algorithm is committed to particular mechanisms; and (4) that at which the mechanisms are realized in hardware. In general, the nature of a computation is determined by the problem to be solved, the mechanisms that are used depend upon the available hardware, and the particular algorithms chosen depend on the problem and on the available mechanisms. Examples are given of theories at each level.},
  langid = {american},
  annotation = {Accepted: 2004-10-01T20:36:50Z},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NE8RZJH8/Marr_Poggio_1976_From Understanding Computation to Understanding Neural Circuitry.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UAXVEIX7/5782.html}
}

@article{marshallAnalysisPowerLaws2016,
  title = {Analysis of {{Power Laws}}, {{Shape Collapses}}, and {{Neural Complexity}}: {{New Techniques}} and {{MATLAB Support}} via the {{NCC Toolbox}}},
  shorttitle = {Analysis of {{Power Laws}}, {{Shape Collapses}}, and {{Neural Complexity}}},
  author = {Marshall, Najja and Timme, Nicholas M. and Bennett, Nicholas and Ripp, Monica and Lautzenhiser, Edward and Beggs, John M.},
  year = {2016},
  journal = {Frontiers in Physiology},
  volume = {7},
  publisher = {{Frontiers}},
  issn = {1664-042X},
  doi = {10.3389/fphys.2016.00250},
  abstract = {Neural systems include interactions that occur across many scales. Two divergent methods for characterizing such interactions have drawn on the physical analysis of critical phenomena and the mathematical study of information. Inferring criticality in neural systems has traditionally rested on fitting power laws to the property distributions of ''neural avalanches'' (contiguous bursts of activity), but the fractal nature of avalanche shapes has recently emerged as another signature of criticality. On the other hand, neural complexity, an information theoretic measure, has been used to capture the interplay between the functional localization of brain regions and their integration for higher cognitive functions. Unfortunately, treatments of all three methods - power-law fitting, avalanche shape collapse, and neural complexity \textendash{} have suffered from shortcomings. Empirical data often contain biases that introduce deviations from true power law in the tail and head of the distribution, but deviations in the tail have often been unconsidered; avalanche shape collapse has required manual parameter tuning; and the estimation of neural complexity has relied on small data sets or statistical assumptions for the sake of computational efficiency. In this paper we present technical advancements in the analysis of criticality and complexity in neural systems. We use maximum-likelihood estimation to automatically fit power laws with left and right cutoffs, present the first automated shape collapse algorithm, and describe new techniques to account for large numbers of neural variables and small data sets in the calculation of neural complexity. In order to facilitate future research in criticality and complexity, we have made the software utilized in this analysis freely available online in the MATLAB NCC (Neural Complexity and Criticality) Toolbox.},
  langid = {english},
  keywords = {Information Theory,MATLAB,neural avalanche,neural complexity,Neural Criticality,power-law,Shape Collapse},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JQXQ23WY/Marshall et al. - 2016 - Analysis of Power Laws, Shape Collapses, and Neura.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@marshallAnalysisPowerLaws2016.md}
}

@article{marsiliQuantifyingRelevanceLearning2022,
  title = {Quantifying {{Relevance}} in {{Learning}} and {{Inference}}},
  author = {Marsili, Matteo and Roudi, Yasser},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.00339 [cond-mat, physics:physics, stat]},
  eprint = {2202.00339},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:physics, stat},
  abstract = {Learning is a distinctive feature of intelligent behaviour. High-throughput experimental data and Big Data promise to open new windows on complex systems such as cells, the brain or our societies. Yet, the puzzling success of Artificial Intelligence and Machine Learning shows that we still have a poor conceptual understanding of learning. These applications push statistical inference into uncharted territories where data is high-dimensional and scarce, and prior information on "true" models is scant if not totally absent. Here we review recent progress on understanding learning, based on the notion of "relevance". The relevance, as we define it here, quantifies the amount of information that a dataset or the internal representation of a learning machine contains on the generative model of the data. This allows us to define maximally informative samples, on one hand, and optimal learning machines on the other. These are ideal limits of samples and of machines, that contain the maximal amount of information about the unknown generative process, at a given resolution (or level of compression). Both ideal limits exhibit critical features in the statistical sense: Maximally informative samples are characterised by a power-law frequency distribution (statistical criticality) and optimal learning machines by an anomalously large susceptibility. The trade-off between resolution (i.e. compression) and relevance distinguishes the regime of noisy representations from that of lossy compression. These are separated by a special point characterised by Zipf's law statistics. This identifies samples obeying Zipf's law as the most compressed loss-less representations that are optimal in the sense of maximal relevance. Criticality in optimal learning machines manifests in an exponential degeneracy of energy levels, that leads to unusual thermodynamic properties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,kavli,MSR,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning,yasser},
  note = {Comment: review article, 63 pages, 14 figures},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9QEUGLR4/Marsili and Roudi - 2022 - Quantifying Relevance in Learning and Inference.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7IRHFTMA/2202.html}
}

@article{marsiliSamplingModelingComplex2013,
  title = {On Sampling and Modeling Complex Systems},
  author = {Marsili, Matteo and Mastromatteo, Iacopo and Roudi, Yasser},
  year = {2013},
  month = sep,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2013},
  number = {09},
  pages = {P09003-P09003},
  publisher = {{IOP Publishing}},
  keywords = {clustering techniques,critical phenomena of socio-economic systems,protein function and design (theory),statistical inference ArXiv ePrint: 13013622},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/86DYFJYY/Jie Li et al. - 2013 - To cite this article Matteo Marsili et al.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8WHIDA8L/Marsili, Mastromatteo, Roudi - 2013 - On sampling and modeling complex systems.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TIS6WSNI/Haimovici, Marsili - 2013 - On sampling and modeling complex systems.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@marsiliSamplingModelingComplex2013.md}
}

@article{martinoIndependentDoublyAdaptive2015,
  title = {Independent Doubly Adaptive Rejection Metropolis Sampling within {{Gibbs}} Sampling},
  author = {Martino, Luca and Read, Jesse and Luengo, David},
  year = {2015},
  journal = {IEEE Transactions on Signal Processing},
  doi = {10.1109/TSP.2015.2420537},
  abstract = {\textcopyright{} 2015 IEEE. Bayesian methods have become very popular in signal processing lately, even though performing exact Bayesian inference is often unfeasible due to the lack of analytical expressions for optimal Bayesian estimators. In order to overcome this problem, Monte Carlo (MC) techniques are frequently used. Several classes of MC schemes have been developed, including Markov Chain Monte Carlo (MCMC) methods, particle filters and population Monte Carlo approaches. In this paper, we concentrate on the Gibbs-type approach, where automatic and fast samplers are needed to draw from univariate (full-conditional) densities. The Adaptive Rejection Metropolis Sampling (ARMS) technique is widely used within Gibbs sampling, but suffers from an important drawback: an incomplete adaptation of the proposal in some cases. In this work, we propose an alternative adaptive MCMC algorithm (IA {$<$} sup {$>$} 2 {$<$} /sup {$>$} RMS) that overcomes this limitation, speeding up the convergence of the chain to the target, allowing us to simplify the construction of the sequence of proposals, and thus reducing the computational cost of the entire algorithm. Note that, although IA {$<$} sup {$>$} 2 {$<$} /sup {$>$} RMS has been developed as an extremely efficient MCMC-within-Gibbs sampler, it also provides an excellent performance as a stand-alone algorithm when sampling from univariate distributions. In this case, the convergence of the proposal to the target is proved and a bound on the complexity of the proposal is provided. Numerical results, both for univariate (stand - alone IA {$<$} sup {$>$} 2 {$<$} /sup {$>$} RMS) and multivariate (IA {$<$} sup {$>$} 2 {$<$} /sup {$>$} RMS-within-Gibbs) distributions, show that IA {$<$} sup {$>$} 2 {$<$} /sup {$>$} RMS outperforms ARMS and other classical techniques, providing a correlation among samples close to zero.},
  keywords = {Adaptive MCMC,adaptive rejection Metropolis sampling,bayesian inference,Gibbs sampler,metropolis-hastings within Gibbs sampling,monte carlo methods},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/C8635QTN/Martino, Read, Luengo - 2015 - Independent doubly adaptive rejection metropolis sampling within Gibbs sampling.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@martinoIndependentDoublyAdaptive2015.md}
}

@article{mastromatteoCriticalityInferredModels2011,
  title = {On the Criticality of Inferred Models},
  author = {Mastromatteo, Iacopo and Marsili, Matteo},
  year = {2011},
  month = oct,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2011},
  number = {10},
  pages = {P10012},
  publisher = {{IOP Publishing}},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2011/10/P10012},
  abstract = {Advanced inference techniques allow one to reconstruct a pattern of interaction from high dimensional data sets, from probing simultaneously thousands of units of extended systems\textemdash such as cells, neural tissues and financial markets. We focus here on the statistical properties of inferred models and argue that inference procedures are likely to yield models which are close to singular values of parameters, akin to critical points in physics where phase transitions occur. These are points where the response of physical systems to external perturbations, as measured by the susceptibility, is very large and diverges in the limit of infinite size. We show that the reparameterization invariant metrics in the space of probability distributions of these models (the Fisher information) are directly related to the susceptibility of the inferred model. As a result, distinguishable models tend to accumulate close to critical points, where the susceptibility diverges in infinite systems. This region is the one where the estimate of inferred parameters is most stable. In order to illustrate these points, we discuss inference of interacting point processes with application to financial data and show that sensible choices of observation time scales naturally yield models which are close to criticality.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5F3AN49V/Mastromatteo and Marsili - 2011 - On the criticality of inferred models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mastromatteoCriticalityInferredModels2011.md}
}

@article{matellNeuropsychologicalMechanismsInterval2000,
  title = {Neuropsychological Mechanisms of Interval Timing Behavior},
  author = {Matell, Matthew S. and Meck, Warren H.},
  year = {2000},
  journal = {BioEssays},
  volume = {22},
  number = {1},
  pages = {94--103},
  issn = {1521-1878},
  doi = {10.1002/(SICI)1521-1878(200001)22:1<94::AID-BIES14>3.0.CO;2-E},
  abstract = {Interval timing in the seconds-to-minutes range is believed to underlie a variety of complex behaviors in humans and other animals. One of the more interesting problems in interval timing is trying to understand how the brain times events lasting for minutes with millisecond-based neural processes. Timing models proposing the use of coincidence-detection mechanisms (e.g., the detection of simultaneous activity across multiple neural inputs) appear to be the most compatible with known neural mechanisms. From an evolutionary perspective, coincidence detection of neuronal activity may be a fundamental mechanism of timing that is expressed across a wide variety of species. BioEssays 22:94\textendash 103, 2000. \textcopyright 2000 John Wiley \& Sons, Inc.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291521-1878\%28200001\%2922\%3A1\%3C94\%3A\%3AAID-BIES14\%3E3.0.CO\%3B2-E},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6Y5X6HSD/Matell_Meck_2000_Neuropsychological mechanisms of interval timing behavior.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VVKCFS8M/(SICI)1521-1878(200001)22194AID-BIES143.0.html}
}

@article{mateusNanoscalePatterningVitro2022,
  title = {Nanoscale {{Patterning}} of {{In Vitro Neuronal Circuits}}},
  author = {Mateus, Jos{\'e} C. and Weaver, Sean and van Swaay, Dirk and Renz, Aline F. and Hengsteler, Julian and Aguiar, Paulo and V{\"o}r{\"o}s, J{\'a}nos},
  year = {2022},
  month = apr,
  journal = {ACS Nano},
  publisher = {{American Chemical Society}},
  doi = {10.1021/acsnano.1c10750},
  abstract = {Methods for patterning neurons in vitro have gradually improved and are used to investigate questions that are difficult to address in or ex vivo. Though these techniques guide axons between groups of neurons, multiscale control of neuronal connectivity, from circuits to synapses, is yet to be achieved in vitro. As studying neuronal circuits with synaptic resolution in vivo poses significant challenges, we present an in vitro alternative to validate biophysical and computational models. In this work we use a combination of electron beam lithography and photolithography to create polydimethylsiloxane (PDMS) structures with features ranging from 150 nm to a few millimeters. Leveraging the difference between average axon and dendritic spine diameters, we restrict axon growth while allowing spines to pass through nanochannels to guide synapse formation between small groups of neurons (i.e., nodes). We show this technique can be used to generate large numbers of isolated feed-forward circuits where connections between nodes are restricted to regions connected by nanochannels. Using a genetically encoded calcium indicator in combination with fluorescently tagged postsynaptic protein, PSD-95, we demonstrate functional synapses can form in this region.},
  copyright = {\textcopyright{} 2022 American Chemical Society},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Mateus et al_2022_Nanoscale Patterning of In Vitro Neuronal Circuits2.pdf}
}

@article{mateusNanoscalePatterningVitro2022a,
  title = {Nanoscale {{Patterning}} of {{In Vitro Neuronal Circuits}}},
  author = {Mateus, Jos{\'e} C. and Weaver, Sean and {van Swaay}, Dirk and Renz, Aline F. and Hengsteler, Julian and Aguiar, Paulo and V{\"o}r{\"o}s, J{\'a}nos},
  year = {2022},
  month = apr,
  journal = {ACS Nano},
  publisher = {{American Chemical Society}},
  issn = {1936-0851},
  doi = {10.1021/acsnano.1c10750},
  abstract = {Methods for patterning neurons in vitro have gradually improved and are used to investigate questions that are difficult to address in or ex vivo. Though these techniques guide axons between groups of neurons, multiscale control of neuronal connectivity, from circuits to synapses, is yet to be achieved in vitro. As studying neuronal circuits with synaptic resolution in vivo poses significant challenges, we present an in vitro alternative to validate biophysical and computational models. In this work we use a combination of electron beam lithography and photolithography to create polydimethylsiloxane (PDMS) structures with features ranging from 150 nm to a few millimeters. Leveraging the difference between average axon and dendritic spine diameters, we restrict axon growth while allowing spines to pass through nanochannels to guide synapse formation between small groups of neurons (i.e., nodes). We show this technique can be used to generate large numbers of isolated feed-forward circuits where connections between nodes are restricted to regions connected by nanochannels. Using a genetically encoded calcium indicator in combination with fluorescently tagged postsynaptic protein, PSD-95, we demonstrate functional synapses can form in this region.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Mateus et al_2022_Nanoscale Patterning of In Vitro Neuronal Circuits.pdf}
}

@misc{MathematicalTheoryCommunication,
  title = {A Mathematical Theory of Communication - {{Nokia Bell Labs Journals}} \& {{Magazine}}},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@MathematicalTheoryCommunication.md}
}

@article{mathisMultiscaleCodesNervous2013,
  title = {Multiscale Codes in the Nervous System: {{The}} Problem of Noise Correlations and the Ambiguity of Periodic Scales},
  author = {Mathis, Alexander and Herz, Andreas V.M. M. and Stemmler, Martin B.},
  year = {2013},
  month = aug,
  journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  volume = {88},
  number = {2},
  pages = {1--10},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.88.022713},
  abstract = {Encoding information about continuous variables using noisy computational units is a challenge; nonetheless, asymptotic theory shows that combining multiple periodic scales for coding can be highly precise despite the corrupting influence of noise (Mathis et al., Phys. Rev. Lett. 2012). Indeed, cortex seems to use such stochastic multi-scale periodic `grid codes' to represent position accurately. We show here how these codes can be read out without taking the asymptotic limit; even on short time scales, the precision of neuronal grid codes scales exponentially in the number N of neurons. Does this finding also hold for neurons that are not statistically independent? To assess the extent to which biological grid codes are subject to statistical dependencies, we analyze the noise correlations between pairs of grid code neurons in behaving rodents. We find that if the grids of the two neurons align and have the same length scale, the noise correlations between the neurons can reach 0.8. For increasing mismatches between the grids of the two neurons, the noise correlations fall rapidly. Incorporating such correlations into a population coding model reveals that the correlations lessen the resolution, but the exponential scaling of resolution with N is unaffected.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/W6DZIZFF/Mathis, Herz, Stemmler - 2013 - Multiscale codes in the nervous system The problem of noise correlations and the ambiguity of periodic s.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mathisMultiscaleCodesNervous2013.md}
}

@article{mathisProbableNatureHigherdimensional2015,
  title = {Probable Nature of Higher-Dimensional Symmetries Underlying Mammalian Grid-Cell Activity Patterns Formatted {{HTML}} , {{PDF}} , and {{XML}} Versions Will Be Made Available after Technical Processing , Editing , {{Probable}} Nature of Higher-Dimensional Symmetries Underl},
  author = {Mathis, Alexander and Stemmler, Martin B and Herz, Andreas V M and Mathis, Alexander},
  year = {2015},
  journal = {eLife},
  number = {April},
  pages = {1--19},
  doi = {10.7554/eLife.05979},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/S8MJBAVP/Mathis et al. - 2015 - Probable nature of higher-dimensional symmetries underlying mammalian grid-cell activity patterns formatted HTML.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mathisProbableNatureHigherdimensional2015.md}
}

@techreport{matsumototCalculationInformationFlow1988,
  title = {Calculation of Information Flow Rate from Mutual Information},
  author = {Matsumotot, Kenji and Tsudas, Ichiro},
  year = {1988},
  journal = {J. Phys. A: Math. Gen},
  volume = {21},
  pages = {1405--1414},
  abstract = {The information flow rates in a time series and between two time series are defined. These can be calculated from mutual information curves. These flow rates enable us to trace the origin of information flowing in an element of a system. This is actually done in a coupled system of one-dimensional maps, in which a flow of information is quantitatively measured.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/42MGABTN/Matsumotot, Tsudas - 1988 - Calculation of information flow rate from mutual information.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@matsumototCalculationInformationFlow1988.md}
}

@techreport{matteiLeveragingExactLikelihood,
  title = {Leveraging the {{Exact Likelihood}} of {{Deep Latent Variable Models}}},
  author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
  abstract = {Deep latent variable models (DLVMs) combine the approximation abilities of deep neural networks and the statistical foundations of generative models. Variational methods are commonly used for inference; however, the exact likelihood of these models has been largely overlooked. The purpose of this work is to study the general properties of this quantity and to show how they can be leveraged in practice. We focus on important inferential problems that rely on the likelihood: estimation and missing data imputation. First, we investigate maximum likelihood estimation for DLVMs: in particular, we show that most unconstrained models used for continuous data have an unbounded likelihood function. This problematic behaviour is demonstrated to be a source of mode collapse. We also show how to ensure the existence of maximum likelihood estimates, and draw useful connections with nonparametric mixture models. Finally, we describe an algorithm for missing data imputation using the exact conditional likelihood of a DLVM. On several data sets, our algorithm consistently and significantly outperforms the usual imputation scheme used for DLVMs.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RLP6D9HR/Mattei, Frellsen - Unknown - Leveraging the Exact Likelihood of Deep Latent Variable Models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@matteiLeveragingExactLikelihood.md}
}

@techreport{matteiMIWAEDeepGenerative,
  title = {{{MIWAE}}: {{Deep Generative Modelling}} and {{Imputation}} of {{Incomplete Data Sets}}},
  author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
  abstract = {We consider the problem of handling missing data with deep latent variable models (DLVMs). First, we present a simple technique to train DLVMs when the training set contains missing-at-random data. Our approach, called MIWAE, is based on the importance-weighted autoencoder (IWAE), and maximises a potentially tight lower bound of the log-likelihood of the observed data. Compared to the original IWAE, our algorithm does not induce any additional computational overhead due to the missing data. We also develop Monte Carlo techniques for single and multiple imputation using a DLVM trained on an incomplete data set. We illustrate our approach by training a convolu-tional DLVM on incomplete static binarisations of MNIST. Moreover, on various continuous data sets, we show that MIWAE provides extremely accurate single imputations, and is highly competitive with state-of-the-art methods.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XTKTENKJ/Mattei, Frellsen - Unknown - MIWAE Deep Generative Modelling and Imputation of Incomplete Data Sets.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@matteiMIWAEDeepGenerative.md}
}

@article{maurerNetworkIntrinsicCellular2007,
  title = {Network and Intrinsic Cellular Mechanisms Underlying Theta Phase Precession of Hippocampal Neurons},
  author = {Maurer, Andrew P. and McNaughton, Bruce L.},
  year = {2007},
  month = jul,
  journal = {Trends in Neurosciences},
  volume = {30},
  number = {7},
  pages = {325--333},
  doi = {10.1016/j.tins.2007.05.002},
  abstract = {Hippocampal 'place cells' systematically shift their phase of firing in relation to the theta rhythm as an animal traverses the 'place field'. These dynamics imply that the neural ensemble begins each theta cycle at a point in its state-space that might 'represent' the current location of the rat, but that the ensemble 'looks ahead' during the rest of the cycle. Phase precession could result from intrinsic cellular dynamics involving interference of two oscillators of different frequencies, or from network interactions, similar to Hebb's 'phase sequence' concept, involving asymmetric synaptic connections. Both models have difficulties accounting for all of the available experimental data, however. A hybrid model, in which the look-ahead phenomenon implied by phase precession originates in superficial entorhinal cortex by some form of interference mechanism and is enhanced in the hippocampus proper by asymmetric synaptic plasticity during sequence encoding, seems to be consistent with available data, but as yet there is no fully satisfactory theoretical account of this phenomenon. This review is part of the INMED/TINS special issue Physiogenic and pathogenic oscillations: the beauty and the beast, based on presentations at the annual INMED/TINS symposium (http://inmednet.com).},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@maurerNetworkIntrinsicCellular2007.md}
}

@article{mcnaughtonPathIntegrationNeural2006,
  title = {Path Integration and the Neural Basis of the 'Cognitive Map'},
  author = {McNaughton, Bruce L. and Battaglia, Francesco P. and Jensen, Ole and Moser, Edvard I. and Moser, May Britt},
  year = {2006},
  journal = {Nature Reviews Neuroscience},
  volume = {7},
  number = {8},
  pages = {663--678},
  issn = {1471-003X (Print)\textbackslash r1471-003X (Linking)},
  doi = {10.1038/nrn1932},
  abstract = {The hippocampal formation can encode relative spatial location, without reference to external cues, by the integration of linear and angular self-motion (path integration). Theoretical studies, in conjunction with recent empirical discoveries, suggest that the medial entorhinal cortex (MEC) might perform some of the essential underlying computations by means of a unique, periodic synaptic matrix that could be self-organized in early development through a simple, symmetry-breaking operation. The scale at which space is represented increases systematically along the dorsoventral axis in both the hippocampus and the MEC, apparently because of systematic variation in the gain of a movement-speed signal. Convergence of spatially periodic input at multiple scales, from so-called grid cells in the entorhinal cortex, might result in non-periodic spatial firing patterns (place fields) in the hippocampus.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RW2MNYWL/McNaughton et al. - 2006 - Path integration and the neural basis of the 'cognitive map'.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mcnaughtonPathIntegrationNeural2006.md}
}

@article{mehonicBraininspiredComputingNeeds2022,
  title = {Brain-Inspired Computing Needs a Master Plan},
  author = {Mehonic, A. and Kenyon, A. J.},
  year = {2022},
  month = apr,
  journal = {Nature},
  volume = {604},
  number = {7905},
  pages = {255--260},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04362-w},
  abstract = {New computing technologies inspired by the brain promise fundamentally different ways to process information with extreme energy efficiency and the ability to handle the avalanche of unstructured and noisy data that we are generating at an ever-increasing rate. To realize this promise requires a brave and coordinated plan to bring together disparate research communities and to provide them with the funding, focus and support needed. We have done this in the past with digital technologies; we are in the process of doing it with quantum technologies; can we now do it for brain-inspired computing?},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Electrical and electronic engineering,Information technology},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Mehonic_Kenyon_2022_Brain-inspired computing needs a master plan.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TA3AKXYL/s41586-021-04362-w.html}
}

@article{mehtaExactMappingVariational2014,
  title = {An Exact Mapping between the {{Variational Renormalization Group}} and {{Deep Learning}}},
  author = {Mehta, Pankaj and Schwab, David J.},
  year = {2014},
  month = oct,
  abstract = {Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mehtaExactMappingVariational2014.md}
}

@article{melloDeepTEGINNDeepLearning2019,
  title = {{{DeepTEGINN}}: {{Deep}} Learning Based Tools to Extract Graphs from Images of Neural Networks},
  author = {e Mello, Gustavo Borges Moreno and Valderhaug, Vibeke Devold and {Pontes-Filho}, Sidney and Zouganeli, Evi and Sandvig, Ioanna and Nichele, Stefano},
  year = {2019},
  month = jul,
  eprint = {1907.01062},
  eprinttype = {arxiv},
  abstract = {In the brain, the structure of a network of neurons defines how these neurons implement the computations that underlie the mind and the behavior of animals and humans. Provided that we can describe the network of neurons as a graph, we can employ methods from graph theory to investigate its structure or use cellular automata to mathematically assess its function. Although, software for the analysis of graphs and cellular automata are widely available. Graph extraction from the image of networks of brain cells remains difficult. Nervous tissue is heterogeneous, and differences in anatomy may reflect relevant differences in function. Here we introduce a deep learning based toolbox to extracts graphs from images of brain tissue. This toolbox provides an easy-to-use framework allowing system neuroscientists to generate graphs based on images of brain tissue by combining methods from image processing, deep learning, and graph theory. The goals are to simplify the training and usage of deep learning methods for computer vision and facilitate its integration into graph extraction pipelines. In this way, the toolbox provides an alternative to the required laborious manual process of tracing, sorting and classifying. We expect to democratize the machine learning methods to a wider community of users beyond the computer vision experts and improve the time-efficiency of graph extraction from large brain image datasets, which may lead to further understanding of the human mind.},
  archiveprefix = {arXiv},
  arxivid = {1907.01062},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3U9CL896/Mello et al. - 2019 - DeepTEGINN Deep Learning Based Tools to Extract Graphs from Images of Neural Networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RPPEGGBP/Mello et al. - 2019 - DeepTEGINN Deep Learning Based Tools to Extract Graphs from Images of Neural Networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@melloDeepTEGINNDeepLearning2019.md}
}

@phdthesis{melloNeuralBehavioralMechanisms2016,
  title = {Neural and {{Behavioral Mechanisms}} of {{Interval Timing}} in the {{Striatum}}},
  author = {e Mello, Gustavo Borges Moreno},
  year = {2016},
  month = mar,
  abstract = {To guide behavior and learn from its consequences, the brain must represent  time over many scales. Yet, the neural signals used to encode time in the  seconds to minute range are not known. The striatum is the major input area of  the basal ganglia; it plays important roles in learning, motor function and  normal timing behavior in the range of seconds to minutes. We investigated  how striatal population activity might encode time. To do so, we recorded the  electrical activity from striatal neurons in rats performing the serial fixed interval  task, a dynamic version of the fixed Interval schedule of reinforcement. The  animals performed in conformity with proportional timing, but did not strictly  conform to scalar timing predictions, which might reflect a parallel strategy to  optimize the adaptation to changes in temporal contingencies and  consequently to improve reward rate over the session. Regarding the neural  activity, we found that neurons fired at delays spanning tens of seconds and  that this pattern of responding reflected the interaction between time and the  animals' ongoing sensorimotor state. Surprisingly, cells rescaled responses in  time when intervals changed, indicating that striatal populations encoded  relative time. Moreover, time estimates decoded from activity predicted trial-bytrial  timing behavior as animals adjusted to new intervals, and disrupting  striatal function with local infusion of muscimol led to a decrease in timing  performance. Because of practical limitations in testing for sufficiency a  biological system, we ran a simple simulation of the task; we have shown that  neural responses similar to those we observe are conceptually sufficient to  produce temporally adaptive behavior. Furthermore, we attempted to explain  temporal processes on the basis of ongoing behavior by decoding temporal  estimates from high-speed videos of the animals performing the task; we could  not explain the temporal report solely on basis of ongoing behavior. These  results suggest that striatal activity forms a scalable population firing rate code  for time, providing timing signals that animals use to guide their actions.},
  copyright = {openAccess},
  langid = {english},
  annotation = {Accepted: 2018-08-08T14:54:33Z},
  note = {\section{Annotations\\
(6/18/2022, 6:44:30 PM)}

\par
``identifying temporal regularities is extremely important for adaptive behavior. Indeed, to guide their behavior, animals operate with temporal information from different orders of magnitude, from microseconds to years.'' (Mello, 2016, p. 18)
\par
``interval timing (i.e. the ability to perceive, estimate and discriminate intervals between events in the range of seconds to minutes to hours) has been identified in organisms as diverse as insects [7], birds [8], fish [9], rat pups [10] and adult rodents [11], primates [12], human infants [13] and adults [14]. Interval timing is critical for important adaptive behaviors. In foraging, animals use temporal estimates to estimate how much reward per time (i.e., the rate of return) any given behavioral strategy or area can provide [15,16]'' (Mello, 2016, p. 18)
\par
``it has been suggested that interval timing requires rudimentary comparisons and estimations of quantities. These operations could be the basis for high cognitive faculties, such as arithmetics [20]. Moreover, interval timing is not an isolated faculty. It interacts directly with processes such as attention [21,22], memory [23], reward expectation [24] and arousal [25], so that variations in these processes cause temporal delusions. Therefore, Interval timing is not only a primitive ability that is useful to detect relevant patterns from the everchanging environment and generate anticipatory behaviors. But might underlie most, if not all, high cognitive functions of the human brain. By understanding the brain implements interval timing, we can gain insight into the processes supported by it, and perhaps identify unifying principles of how the nervous systems across species organize information about the environment and behavior.'' (Mello, 2016, p. 19)
\par
``In contrast to other sensorial modalities (e.g., visual, tactile, etc), timing has no sensorial organ'' (Mello, 2016, p. 19) Maybe we can call this "Building the Electric Time Organ"
\par
``Hence, the sense of time either emerges from an internal clock mechanism, which generates a trackable time varying signal, or alternatively, arises from learning the temporal statistics of change in sensory and/or motor signals, which vary naturally with time'' (Mello, 2016, p. 19)
\par
``The Basal Ganglia (BG), and especially the striatum, are necessary for time estimation in the supra-second range [26].'' (Mello, 2016, p. 19) BG
\par
``the means by which the BG might perform temporal computations remains elusive.'' (Mello, 2016, p. 20)
\par
``Psychophysical studies of interval timing Much of our knowledge about interval timing is derived from psychophysical studies. These studies are based on retrospective and prospective timing methodologies to collect time duration judgments from a subject [33].'' (Mello, 2016, p. 20)
\par
``studies of interval timing'' (Mello, 2016, p. 20) OK so we are getting a bunch of different types of interval timing tasks
\par
``the prospective study of time durations is based on the estimation, production, and reproduction of time intervals [34]. Estimation protocols require that a subject observes an interval and reports orally how much time has elapsed. Production protocols, on contrary, inform the subject about a temporal constraints (usually an interval between actions) he/she will have in order to perform an action. Usually a symbolic cue is associated with a particular interval, a spoken communication (e.g., ``five seconds''). Because estimation and production protocols require some verbal interaction, most of timing research that uses animal models employ the reproduction protocols. In this protocol the subject is presented to one interval with a given duration criterion, then the subject has to reproduce this interval. Animals are usually deprived of either food or water, so that they are motivated to perform an action (e.g., pressing a lever, pushing a button) in a programmed schedule of time in order to receive a reward (i.e., drop of water, food pellet).'' (Mello, 2016, p. 20)
\par
``The most often studied interval timing schedule is perhaps the fixed interval (FI) schedule of reinforcement and its variations. During a FI schedule, the behavior is reinforced for the first response (e.g., press of a lever) made after elapse of a pre-determined interval since the previous reinforcement. When the reinforcer is delivered, the cycle restarts'' (Mello, 2016, p. 20)
\par
``First, the subjects consume the reward (as explained, the reward !1'' (Mello, 2016, p. 20)
\par
``marks the end of the previous and the beginning of the subsequent FI). Secondly, the subjects generally engage in grooming or exploratory behaviors. Thirdly, the animals gradually orient their position and actions towards the response site (e.g., lever). Finally, despite the absence of any external time cues, the animals start to respond after a fixed proportion of the interval has elapsed. The responses under FI schedule generally are manifested in two characteristic patterns. The first one is the scallop performance. This pattern describes an increase in the frequency of responses as the end of the FI approaches. The second pattern, and more frequently observed in over-trained subjects, is the break-and-run. In this pattern, response frequency is kept at a fixed rate from the moment responding starts to the moment of the reinforcement. This fixed rate of response varies together with the reward rate (i.e., magnitude of reward per time in the FI; [36]), and the post reinforcement pause (PRP) of responding is proportional to the length of the interval.'' (Mello, 2016, p. 21)
\par
``The PRP is the duration of the interval between the acquisition of the reinforcer and the press start time (PST; i.e., the first response) to produce the subsequent reinforcer, and because it is sensitive to the length of the estimated interval, it is the standard metric of timing performance. It has two important features relevant to the interval timing studies: the sensitivity of mean accuracy to the FI and the scalar variance (scalar timing).'' (Mello, 2016, p. 21)
\par
``The mean accuracy sensitivity describes the observed phenomena that PSTs tend to occur in average around one half [35] to two thirds of the length of FI. The scalar variance feature characterizes that the dispersion of PST is a linear function of the average PST. Consequently, it also predicts that the coefficient of variation (standard deviation of PST divided by the average PST) is constant across all FIs. This latter feature is considered to be a manifestation of Weber-Fechner's Law in the time domain, also known as Scalar Timing [37]. The Weber-Fechner's law is obeyed by many sensory modalities [38, 39, 40, 41]. It states that the threshold to detect changes in magnitudes of stimuli (i.e., the just noticeable difference) is proportional to the magnitude. In other words, the just noticeable difference is a constant ratio between the measured magnitudes, for all magnitudes. Scalar timing affects behavior and neuronal activation by making them increasingly less precise as the timed interval lengthens [42].'' (Mello, 2016, p. 21)
\par
``Serial Fixed Interval Timing (SFI) task is a variation of FI task used in our lab to sample timing performance from a broad range of intervals. In the SFI task the reward became available at t seconds after the previous reward, provided that the animal has responded. Each reward marked the end of one trial and the start of the next one. t varied in a block-wise fashion over intervals from 12 seconds to 1 minute. The performance in this task was consistent with previous results in dynamic versions of FI task [34, 35, 43, 44, 45, 46] in three aspects. Firstly, average PST is a function of the FI. Secondly, the response rate is proportional to the reward rate. Finally, rodents seem to adopt strategies that take into account for the whole distribution of FIs in the session. So that, PST was relatively later in short FI trials than it was within trials with long FIs. Because they adapted quickly to the new interval, taking less than 5 trials, this violation to the scalar variance property could reflect a strategy to facilitate exploration of intervals. The SFI task offers the possibility to analyze steady and changing timing conditions, while providing statistical power to infer parametric relationships between temporal demands, behavior and neural activity'' (Mello, 2016, p. 22)
\par
``in timing tasks and explain the mechanisms of interval timing [Franois cited in 47, 48]. These models diverge in how well and how generally they can predict temporal performance, and what are the underlying mechanisms supporting interval timing. Broadly, these models can be grouped into at least three categories that vary in assumptions and explanatory power, !2'' (Mello, 2016, p. 22)
\par
``namely: information-processing models, beat-frequency models and sequential-state models'' (Mello, 2016, p. 23)
\par
``the scalar expectancy theory (SET) model (Figure 1.1). SET assumes: an internal poisson-variable pacemaker that generates pulses, an accumulator, a reference memory, a switch and a comparator. When a time marker (cue or reward) is received, the switch allows the pulses to be stored in the accumulator. These pulses were accumulated until a short time after the reinforcement. The rate by which these pulses were generated depended on many other psychological or behavioral variables, such: as arousal, reinforcement magnitude, attention and mood [49]. By the time of the reinforcement, the value stored in the accumulator is transferred to the reference memory and the accumulator is reset to zero.'' (Mello, 2016, p. 23) I have a feeling this may also be called a pacemaker accumulator, but this also clarifies why they think these episodes are encoded in the striatum
\par
``The perceived duration was a monotonic function of the total number of pulses transferred into the accumulator. The behavioral response is dependent on the ratio between the values stored in the accumulator and reference memory. For instance, when the difference falls below a threshold (which may also vary) in a FI task, responding at a steady rate begins. This feature explains that steady!22 Pacemaker generate pulses Switch start/stop accumulation process Accumulator accumulate pulses over time Memory store values passed by the accumulator Theshold sets a criteria for action to be taken Comparator (memory - accumulator) / memory Stimulus Response Figure 1.1 | Schematics of the scalar expectancy theory model. Components of the information processing model for time: pacemaker, accumulator, reference memory, switch and comparator. The pacemaker generates pulses through a Poisson-variable process. Environmental stimuli change the switch state, allowing the pulses to go into the accumulator. If the accumulator has a value stored during the moment of the stimulus, this value is passed to the memory and the accumulator is reset to zero. Finally the ratio of the difference between the values stored in the accumulator and the memory is compared with an established threshold to generate a behavioral response'' (Mello, 2016, p. 23)
\par
(Mello, 2016, p. 23) Fig 1.1
\par
``state measures of time discrimination, such as wait time (i.e., PST, break point) on fixed interval schedules or peak-rate time (on peak procedure), are proportional to the to-be-timed interval (i.e., proportional accuracy sensitivity to the FI; [35]). Because SET posits that the error generated during the accumulation of pulses is proportional to the duration criterion, it presents an explanation for scalar variance sensitivity to the interval. More importantly, SET incorporates two features that have been supported by experimental data. Firstly, the current time estimate (encoding) and the memory for times reinforced in the past (decoding) follow independent laws [27]; and secondly, the behavior is driven by some sort of comparison between current and remembered time of reinforcement [29]'' (Mello, 2016, p. 24)
\par
``, sequential-state models characterize orderly transitions between different states which can be used to encode time [51,52,53].'' (Mello, 2016, p. 24)
\par
``The behavior theory of timing (BeT) formulated by Killeen and Fetterman [54] and the learning-to-time (LeT) formulated by Machado inspired by BeT [30] are the most prominent of the sequential state models.'' (Mello, 2016, p. 24)
\par
``These models are based on the empirical observation that sequential chains of behaviors emerge in tasks where reward delivery is contingent on passage of time (e.g., FI, SFI; Figure 1.2A). For instance, in a FI task, behavior would transit from consummatory, to post-consummatory, to exploration, to reorientation to the source of reinforcer and finally to the reinforced behavior across the interval.'' (Mello, 2016, p. 25)
\par
``In BeT, each behavior is associated with a distinct underlying state. Transitions between states occur probabilistically driven by a poisson-variable pacemaker. The speed of this pacemaker depends on the rate of reinforcement, so that increases in reinforcement rate lead to an increase of the speed. The successive underlying states take on the role of a clock process.'' (Mello, 2016, p. 25)
\par
``Consequently, to perform at a temporal criteria, subjects would learn to use their temporally organized behavioral states as discriminative stimuli. Thus, instead of reading an internal clock, subjects are assumed to use their current sensorimotor states to tell time.'' (Mello, 2016, p. 25)
\par
``LeT extends BeT by positing that each underlying state is associated with an operant response, and that association strength varies through means of differential reinforcement in the context they were learned (Figure 1.2B).'' (Mello, 2016, p. 25)
\par
``Therefore, the strength of the operant response at a given moment is the result of the combination between the predominantly active state at that moment, and how strong is the association between this state and the response.'' (Mello, 2016, p. 25)
\par
``Two major distinctions between information-processing and sequentialstate models may argue for the broader explanatory model of the latter models. Firstly, in the former, the decision to respond is made only after the target time interval has elapsed, while in BeT it is done in anticipation to that time interval [53].'' (Mello, 2016, p. 25)
\par
``. In Information-processing models the memory stores are independent, and because of that this type models have no mechanism to accommodate'' (Mello, 2016, p. 25) I'm probably going to have to seek out more about BET and LET elsewhere
\par
``contextual effects. In contrast LeT is sensitive to the errors that occur during the learning of the two discriminations; these errors weaken the connection between the behavioral states and the associated operant responses. Therefore, these errors would bias green keys to be perceived as long and blue as short, regardless the fact that both cues relate to the exact same interval.'' (Mello, 2016, p. 26) +\\
I don't get what they are talking about here with color keys. It probably has something to do with figure 1.2-

Lol nope. no idea what they are referring to
\par
``The oscillation-based model uses a library of oscillatory pacemaker neurons, which could be independently entrained in different rhythms, to encode a temporal waveform by forming its Fourier series. Torras [cited in 57] said that this combination could be done either by choosing pacemakers with appropriate oscillation periods or through plastic changes to the period of oscillation of each cell. The beat-frequency model (BF; Figure 1.3A) and its updated and more biologically plausible version, the striatal beat-frequency model (SBF; Figure 1.3B), uses ``beats'' (i.e., frequency at which cells spike'' (Mello, 2016, p. 26)
\par
``simultaneously) between pairs or groups of oscillatory cells to store time intervals. After resetting the oscillations with a synchronizing event, a specific time can be encoded by selectively weighting the activity of oscillatory cells that are currently active at the time criterion. This process is equivalent to multiplication (e.g., 3 Hz and 5 Hz will first synchronize at 15 Hz), thus providing an efficient process to encode long intervals with neuronal mechanisms which operate in much shorter timescale.'' (Mello, 2016, p. 27) +
\par
``The SBF posits that loops involving the cortex (CTX), the basal ganglia and the thalamus implement these mechanisms. More specifically, the striatum would act as a coincidence detector; the DA signals would synchronize cortical and thalamic oscillations at every meaningful event (e.g., reward), hyper-polarizing the striatum membrane, and thereby resetting the integration mechanisms. DA signals would also serve as reinforcement/teaching cues, strengthening the cortico-striatal representation of a particular duration criterion. Once synchronized, neurons oscillate at their inherent periods, allowing the patterns of activity to become meaningful. Striatal spiny neurons fire when a previously reinforced pattern of input is detected, consequently informing that the time criterion was reached. The striatum can entrain itself in the current oscillatory inputs through the striato-thalamic-cortical loop, allowing for alterations of time perception.'' (Mello, 2016, p. 27)
\par
``In our current understanding, interval timing is a complex and primitive function of the brain which engages multiple areas of the brain depending on environmental and behavioral demands. Data from functional magnetic resonance imaging (fMRI) show that multiple areas have time dependent activity which is also affected by task and context, suggesting that interval timing is a distributed and complex process in the brain [62]. But, not all of these areas are required for, or modulate, timing performance equally.'' (Mello, 2016, p. 27)
\par
``As we saw, some models rely on the assumption that the cortex provides the temporal basis for time estimation. But timing experiments done in decorticated animals [64] call this hypothesis into question. These experiments showed that decorticated animals are still able to perform in interval timing tasks.'' (Mello, 2016, p. 28)
\par
``Many studies that attempted to affect interval timing through means of cerebellar lesions have failed [69]. Nonetheless, data from stroke patients [70] with lesion in the middle to superior lateral dentate nuclei, especially in the left hemisphere, suggest that the cerebellum is necessary for proper interval timing in durations lower than 12 seconds'' (Mello, 2016, p. 28)
\par
``Why and how cerebellum contributes to timing in this range is still to be shown. It might coordinate learned actions at a fine timescale [71], playing a mediating role between the sub-second timing [72] and the supra-second timing.'' (Mello, 2016, p. 28) todo\\
follow up on this. See if there's been any change in the past 6 years
\par
``On the other hand, a recent growing body of evidence [73-78] highlights the importance of the hippocampus, an area that is usually associated to spatial learning [79] and explicit memory [80], for interval timing. Gradual changes in hippocampal activity are strongly influenced by time and distance [76]. Additionally, lesions in the dorsal or ventral hippocampus produce leftward or rightward shifts in time estimation respectively [78]. Curiously, the effects that hippocampal inactivations have on time estimation seem to be stronger when the time scale estimated is over one minute [81], and the temporal discrimination is difficult (i.e., intervals with similar durations). Yet, the same group of studies could not provide the evidence that manipulations in the hippocampus produce disruptions on timing of intervals above a second and below one minute. Anatomically, the hippocampus is highly connected with other areas relevant to interval timing such as the nucleus accumbens (Nac; [82]) and the PFC [83]. This connectivity pattern strengthens the argument that the hippocampus has a relevant role in interval timing.'' (Mello, 2016, p. 29)
\par
``Arguably, the study of interval timing mechanisms in the BG has proven to be more prolific regarding unveiling the biological mechanisms of interval timing. Evidence from multiples sources implicates the BG, and more specifically the striatum, as a locus for the representation of supra-secondbelow-one-minute timing. Firstly, activity in the striatum is modulated by timing task as shown in studies using ensemble recording techniques in animals [84,85], and regional increase in blood flow captured by fMRI in humans during interval timing tasks [42,86]. Secondly, striatal lesions [26], diseases that affect the BG such as Huntington's [87] and Attention Deficit Disorder [88], all cause interval timing dysfunctions.'' (Mello, 2016, p. 29)
\par
``Furthermore, patients with disorders that involve meso-striatal dopaminergic pathways, such as schizophrenia [89,90,91] and Parkinson's disease (PD; [92,93]), display impaired performances during interval timing tasks. PD is characterized by a progressive degeneration of nigrostriatal dopaminergic projections, leading to low levels of dopamine (DA) in the striatum. These low levels of DA cause interval timing deficits which can be alleviated by L-dopa (L-3,4-dihydroxyphenylallanine; a precursor of dopamine) treatment [94]. Malapani et al. [92,93] leveraged this modulatory effect of DA !2'' (Mello, 2016, p. 29)
\par
``over interval timing and the therapeutic effect of L-dopa to segregate storage from retrieval dysfunction in the temporal memory in PD patients. Malapani's data suggest that DA has the power to increase discrimination between intervals on retrieval, to control the speed and the extension of internal representation of time during encoding'' (Mello, 2016, p. 30)
\par
``How directly DA affects time perception might be a difficult question to answer, because DA is involved in multiple processes other than timing. For instance, although genetic manipulations that affect the DA system in the BG [95] cause interval timing dysfunctions, a different source of evidence [96] suggests that DA dependent timing deficits might be a cofound of manipulations which affect directly animals' motivation'' (Mello, 2016, p. 30) I might want to focus on DA a bit
\par
``Altogether, the multiple areas involved in interval timing seem to constitute not one but multiple ``internal clocks'', which use diverse sources of information to implement aspects of interval timing. These areas appear to be mutually influenced by each other to generate congruent temporal estimations and subsequent adaptive behavior. A very clear example of such coordination of multiple clocks derives from the interaction among timing mechanisms of different time scales. For instance, the cerebellum exerts an influence in time estimation in the second to sub-second range. It is possible that the cerebellum exerts its influence to the BG either through modulation of thalamic input or through projections to VTA and PFC [97-100].'' (Mello, 2016, p. 30) Ok so at this point I feel like I am just getting a list of possible areas, but nothing giving me an actual layout or network of interaction and it's getting annoying.~

Like, I have no idea what else could be done but it's just one "could be this or these areas" after another.

I gotta rewrite this in some way there is a clear and total "this area, does all these things" the problem of course is that all the areas do multiple things, so you would have to repeat a lot for each part|
\par
``Conversely, circadian timing mechanisms can affect the interval timing indirectly by regulating DA [101]. Finally, the hippocampus might have a direct effect on the computations done in the striatum, especially in long intervals, in which animals are more likely to move (so distance can be an extra source of information about the rate of change of the environment), and when information about sequence is relevant [74,76].'' (Mello, 2016, p. 30)
\par
``Organization of the basal ganglia'' (Mello, 2016, p. 30) This whole section is probably something I will need to reread at some point but atm I'm getting nothing out of itc
\par
``. A variety of processes including reinforcement learning [104-106], motor control [107-111], limbic [112,115] and associative functions [116-119] depend on the BG'' (Mello, 2016, p. 31)
\par
``Functionally, it has been hypothesized that the BG receive inputs from other areas of the brain and act upon these inputs as a filter. According to this hypothesis, the BG would select information derived from cortical and thalamic activity and send the resulting information back to the cortical source of the information. In parallel, it would also send copy of the same information to other systems of the brain to implement behavior. Since DA modulates the gain of cortico-striatal synapses [138-141], and because reinforcement-based plasticity occurs in the BG, it is thought that this plasticity might influence the input selection process based on previous experience [142]. The striatum and DA are considered to be key components to this filtering process.'' (Mello, 2016, p. 34)
\par
``The way in which information travels through the multiple nuclei of the BG might offer clues about their functional role. In the canonical perspective of the BG, information from the striatum can pass through the nuclei of the BG by two different parallel circuits: the direct and the indirect pathways (Figure 1.4). Neurons within the striatum can project directly to the EP/SNr that constitute the output nuclei of the BG (direct pathway). Or instead, striatal neurons can project first to intermediate nuclei, namely GPe and STN, and then to the output nuclei EP/SNr (indirect pathway; [143]). These signaling pathways are regulated by DA in the striatum, and they have been the subject of intense study. It is known that driving activity in the direct pathway increases motor output (i.e., locomotion; [138,141,123]). On the other hand, stimulation of the indirect pathway seems to inhibit behavior [123]. For long it has been hypothesized that the direct pathway encodes the set of behavior plans available to the animals in a given context [19]. Thus, driving !3'' (Mello, 2016, p. 34)
\par
``the direct pathway would make these motor plans stronger, and consequently increase the motor output. Complementarily to this hypothesis, the indirect pathway would map the set of competing behaviors which must be suppressed so that the selected behavioral plan can occur with less interference. In this perspective, the BG would be responsible for filtering motor, associative and limbic information using a center-surround-like receptive field in the pertinent space (e.g., behavioral, cognitive) [19].'' (Mello, 2016, p. 35)
\par
``striatal neurons'' (Mello, 2016, p. 35) Striatal neurons appear to be important especially for SBF theory where they project the information to be stored in the STR
\par
``Decodings and decoders'' (Mello, 2016, p. 38) pick back up here.

Actually , while this section is more interesting to me, it is a good place to skip in relation to the current assignment
\par
``CHAPTER 2: A scalable population code for time in the striatum'' (Mello, 2016, p. 58) So I'm going to comment on this from the paper [[@melloScalablePopulationCode2015]]},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2YNSW467/Complete Thesis GM-FINAL 2016.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/L9NB8QF9/Mello - 2016 - Neural and Behavioral Mechanisms of Interval Timin.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EFP49CYR/43665.html}
}

@article{melloScalablePopulationCode2015,
  title = {A {{Scalable Population Code}} for {{Time}} in the {{Striatum}}},
  author = {Mello, Gustavo B. M. and Soares, Sofia and Paton, Joseph J.},
  year = {2015},
  month = may,
  journal = {Current Biology},
  volume = {25},
  number = {9},
  pages = {1113--1122},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2015.02.036},
  abstract = {To guide behavior and learn from its consequences, the brain must represent time over many scales. Yet, the neural signals used to encode time in the seconds-to-minute range are not known. The striatum is a major input area of the basal ganglia associated with learning and motor function. Previous studies have also shown that the striatum is necessary for normal timing behavior. To address how striatal signals might be involved in timing, we recorded from striatal neurons in rats performing an interval timing task. We found that neurons fired at delays spanning tens of seconds and that this pattern of responding reflected the interaction between time and the animals' ongoing sensorimotor state. Surprisingly, cells rescaled responses in time when intervals changed, indicating that striatal populations encoded relative time. Moreover, time estimates decoded from activity predicted timing behavior as animals adjusted to new intervals, and disrupting striatal function led to a decrease in timing performance. These results suggest that striatal activity forms a scalable population code for time, providing timing signals that animals use to guide their actions.},
  langid = {english},
  keywords = {time},
  note = {\section{Annotations\\
(6/18/2022, 6:44:58 PM)}

\par
``Highlights d Striatal neurons fire at different times over tens of seconds during timing behavior d Response times of striatal neurons rescaled with the interval being timed d Time coding by the population predicted timing behavior from trial to trial d Striatal neurons multiplexed information about action and time'' (Mello et al., 2015, p. 1112)
\par
``In Brief Time is fundamental for all behavior, yet how the brain encodes time is unknown. Mello, Soares, and Paton found that firing dynamics in populations of neurons in the rodent striatum robustly and flexibly encoded time over tens of seconds. These results supply new insight into how the basal ganglia might function during learning and action selection.'' (Mello et al., 2015, p. 1112)
\par
``Multiple lines of evidence implicate the basal ganglia (BG) as a locus for the representation of such temporal information. Lesions of the striatum in rats [2], disease states that affect the BG such as Parkinson's [3] and Huntington's disease [4], drugs that affect dopamine (DA) signaling [5], and genetic manipulations that affect the DA system in the BG [6] all result in interval timing dysfunction. Furthermore, human fMRI studies have found that the striatum, a main input area of the BG, is activated by tasks that involve the processing of interval information [7, 8].'' (Mello et al., 2015, p. 1113)
\par
``Strikingly, we found that temporal tuning stretched or contracted, rescaling with the interval being timed. Thus, striatal populations encoded relative time, flexibly adapting to the immediate demands of the environment. Finally, we ran a simple simulation of the task and show that neural responses resembling those we observe in the striatum are suitable as a basis for timing behavior.'' (Mello et al., 2015, p. 1113)
\par
(Mello et al., 2015, p. 1114) Fig. 1
\par
``mental Experimental Procedures for details) shown in Figure 2A. Some cells fired just after reward delivery, others fired in the middle of the delay, and others fired leading up to the next reward (Figures 2A, S2, and S3). This produced a slow-moving ``bump'' of activity that traversed the population during each FI.'' (Mello et al., 2015, p. 1115)
\par
``In theory, reading out the location of this bump in the population could provide an estimate of time within the FI. However, a core feature of interval timing behavior is that timing accuracy decreases with the magnitude of the interval being timed [9]'' (Mello et al., 2015, p. 1115)
\par
(Mello et al., 2015, p. 1115) Fig. 2
\par
``Based on previous studies [18\textendash 20], we expected that striatal neurons would display significant modulation by behaviors during the FI'' (Mello et al., 2015, p. 1116)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/64TQEZ6L/A Scalable Population Code for Time in the Striatum - 08.06.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6GBDPBNS/Mello et al_2015_A Scalable Population Code for Time in the Striatum.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7DSIURR3/S0960982215002055.html}
}

@article{mendoncaGraphBasedSkillAcquisition2019,
  title = {Graph-{{Based Skill Acquisition For Reinforcement Learning}}},
  author = {Mendon{\c C}a, Matheus R. F. and Ziviani, Artur and Barreto, Andr{\'E} M. S.},
  year = {2019},
  month = feb,
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {1},
  pages = {6:1--6:26},
  issn = {0360-0300},
  doi = {10.1145/3291045},
  abstract = {In machine learning, Reinforcement Learning (RL) is an important tool for creating intelligent agents that learn solely through experience. One particular subarea within the RL domain that has received great attention is how to define macro-actions, which are temporal abstractions composed of a sequence of primitive actions. This subarea, loosely called skill acquisition, has been under development for several years and has led to better results in a diversity of RL problems. Among the many skill acquisition approaches, graph-based methods have received considerable attention. This survey presents an overview of graph-based skill acquisition methods for RL. We cover a diversity of these approaches and discuss how they evolved throughout the years. Finally, we also discuss the current challenges and open issues in the area of graph-based skill acquisition for RL.},
  keywords = {centrality,clustering,graph analytics,reinforcement learning,Skill acquisition},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AIIUGV83/MendonÇa et al_2019_Graph-Based Skill Acquisition For Reinforcement Learning.pdf}
}

@article{menezesConstructingWattsStrogatzNetwork2017,
  title = {Constructing a {{Watts-Strogatz}} Network from a Small-World Network with Symmetric Degree Distribution},
  author = {Menezes, Mozart B. C. and Kim, Seokjin and Huang, Rongbing},
  year = {2017},
  month = jun,
  journal = {PLOS ONE},
  volume = {12},
  number = {6},
  pages = {e0179120},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0179120},
  abstract = {Though the small-world phenomenon is widespread in many real networks, it is still challenging to replicate a large network at the full scale for further study on its structure and dynamics when sufficient data are not readily available. We propose a method to construct a Watts-Strogatz network using a sample from a small-world network with symmetric degree distribution. Our method yields an estimated degree distribution which fits closely with that of a Watts-Strogatz network and leads into accurate estimates of network metrics such as clustering coefficient and degree of separation. We observe that the accuracy of our method increases as network size increases.},
  langid = {english},
  keywords = {Algorithms,Approximation methods,Built structures,Clustering coefficients,Network analysis,Probability distribution,Small world,Small world networks,Social networks},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EYYLJ4U7/Menezes et al. - 2017 - Constructing a Watts-Strogatz network from a small.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@menezesConstructingWattsStrogatzNetwork2017.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JIYQD8S6/article.html;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TGMPEF3U/article.html}
}

@article{menshTenSimpleRules2017,
  title = {Ten Simple Rules for Structuring Papers},
  author = {Mensh, Brett and Kording, Konrad},
  year = {2017},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {9},
  pages = {e1005619},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005619},
  langid = {english},
  keywords = {Careers,Cell differentiation,Communications,Crystals,Experimental design,Patient advocacy,Scientists,Syntax},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X7KCCR25/Mensh_Kording_2017_Ten simple rules for structuring papers.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/P8UVIKCP/article.html}
}

@article{meshulamCoarseGrainingHints2019,
  title = {Coarse\textendash Graining and Hints of Scaling in a Population of 1000+ Neurons},
  author = {Meshulam, Leenoy and Gauthier, Jeffrey L and Brody, Carlos D and Tank, David W and Bialek, William},
  year = {2019},
  pages = {1--21},
  issn = {9780813120584},
  doi = {10.1137/S106482750238911X},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z9DWAJM7/Meshulam et al. - 2019 - Coarse–graining and hints of scaling in a population of 1000 neurons.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@meshulamCoarseGrainingHints2019.md}
}

@article{metropolisEquationStateCalculations1953,
  title = {Equation of {{State Calculations}} by {{Fast Computing Machines}}},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  year = {1953},
  month = jun,
  journal = {The Journal of Chemical Physics},
  volume = {21},
  number = {6},
  pages = {1087--1092},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.1699114},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5CXW2HIM/Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IVTVQBI4/Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LCRRRSNG/Metropolis et al. - Equation of State Calculations by Fast Computing M.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@metropolisEquationStateCalculations1953.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FKRM4N69/1.html;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UUEYMGI2/1.html}
}

@book{metzingerNeuralCorrelatesConsciousness2000,
  title = {Neural Correlates of Consciousness : Empirical and Conceptual Questions},
  author = {Metzinger, Thomas},
  year = {2000},
  pages = {350},
  publisher = {{MIT Press}},
  abstract = {"A Bradford book." This book brings together an international group of neuroscientists and philosophers who are investigating how the content of subjective experience is correlated with events in the brain. The fundamental methodological problem in consciousness research is the subjectivity of the target phenomenon--the fact that conscious experience, under standard conditions, is always tied to an individual, first-person perspective. The core empirical question is whether and how physical states of the human nervous system can be mapped onto the content of conscious experience. The search for the neural correlates of consciousness (NCC) has become a highly active field of investigation in recent years. Methods such as single-cell recording in monkeys and brain imaging and electrophysiology in humans, applied to such phenomena as blindsight, implicit/explicit cognition, and binocular rivalry, have generated a wealth of data. The same period has seen the development of a number of theories about NCC location. This volume brings together the leading experimentalists and theoreticians in the field. Topics include foundational and evolutionary issues, global integration, vision, consciousness and the NMDA receptor complex, neuroimaging, implicit processes, intentionality and phenomenal volition, schizophrenia, social cognition, and the phenomenal self. Contributors Jackie Andrade, Ansgar Beckermann, David J. Chalmers, Francis Crick, Antonio R. Damasio, Gerald M. Edelman, Dominic ffytche, Hans Flohr, N.P. Franks, Vittorio Gallese, Melvyn A. Goodale, Valerie Gray Hardcastle, Beena Khurana, Christof Koch, W.R. Lieb, Erik D. Lumer, Thomas Metzinger, Kelly J. Murphy, Romi Nijhawan, Jo\"elle Proust, Antti Revonsuo, Gerhard Roth, Thomas Schmidt, Wolf Singer, Giulio Tononi. Introduction : consciousness research at the end of the twentieth century -- What is a neural correlate of consciousness? / David J. Chalmers -- The perennial problem of the reductive explainability of phenomenal consciousness : C.D. Broad on the explanatory gap / Ansgar Beckermann.},
  isbn = {978-0-262-27973-4},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@metzingerNeuralCorrelatesConsciousness2000.md}
}

@article{meyerAdaptiveRejectionMetropolis2008,
  title = {Adaptive Rejection {{Metropolis}} Sampling Using {{Lagrange}} Interpolation Polynomials of Degree 2},
  author = {Meyer, Renate and Cai, Bo and Perron, Fran{\c c}ois},
  year = {2008},
  journal = {Computational Statistics and Data Analysis},
  doi = {10.1016/j.csda.2008.01.005},
  abstract = {A crucial problem in Bayesian posterior computation is efficient sampling from a univariate distribution, e.g.~a full conditional distribution in applications of the Gibbs sampler. This full conditional distribution is usually non-conjugate, algebraically complex and computationally expensive to evaluate. We propose an alternative algorithm, called ARMS2, to the widely used adaptive rejection sampling technique ARS [Gilks, W.R., Wild, P., 1992. Adaptive rejection sampling for Gibbs sampling. Applied Statistics 41 (2), 337-348; Gilks, W.R., 1992. Derivative-free adaptive rejection sampling for Gibbs sampling. In: Bernardo, J.M., Berger, J.O., Dawid, A.P., Smith, A.F.M. (Eds.), Bayesian Statistics, Vol. 4. Clarendon, Oxford, pp. 641-649] for generating a sample from univariate log-concave densities. Whereas ARS is based on sampling from piecewise exponentials, the new algorithm uses truncated normal distributions and makes use of a clever auxiliary variable technique [Damien, P., Walker, S.G., 2001. Sampling truncated normal, beta, and gamma densities. Journal of Computational and Graphical Statistics 10 (2) 206-215]. Furthermore, we extend this algorithm to deal with non-log-concave densities to provide an enhanced alternative to adaptive rejection Metropolis sampling, ARMS [Gilks, W.R., Best, N.G., Tan, K.K.C., 1995. Adaptive rejection Metropolis sampling within Gibbs sampling. Applied Statistics 44, 455-472]. The performance of ARMS and ARMS2 is compared in simulations of standard univariate distributions as well as in Gibbs sampling of a Bayesian hierarchical state-space model used for fisheries stock assessment. \textcopyright{} 2008 Elsevier Ltd. All rights reserved.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LDYC8M7X/Meyer et al. - 2008 - Adaptive rejection Metropolis sampling using Lagra.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N9NGXWYP/Meyer, Cai, Perron - 2008 - Adaptive rejection Metropolis sampling using Lagrange interpolation polynomials of degree 2.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@meyerAdaptiveRejectionMetropolis2008.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZBDVEQKC/S016794730800008X.html}
}

@article{mezardExactMeanfieldInference2011,
  title = {Exact Mean-Field Inference in Asymmetric Kinetic {{Ising}} Systems},
  author = {M{\'e}zard, M and Sakellariou, J},
  year = {2011},
  month = jul,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2011},
  number = {07},
  pages = {L07001-L07001},
  publisher = {{IOP Publishing}},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BQ536LIH/Mézard, Sakellariou - 2011 - Exact mean-field inference in asymmetric kinetic Ising systems.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mezardExactMeanfieldInference2011.md}
}

@article{mezardMeanfieldMessagepassingEquations2017,
  title = {Mean-Field Message-Passing Equations in the {{Hopfield}} Model and Its Generalizations},
  author = {M{\'e}zard, Marc},
  year = {2017},
  journal = {Physical Review E},
  volume = {95},
  number = {2},
  doi = {10.1103/PhysRevE.95.022117},
  abstract = {Motivated by recent progress in using restricted Boltzmann machines as preprocessing algorithms for deep neural network, we revisit the mean-field equations [belief-propagation and Thouless-Anderson Palmer (TAP) equations] in the best understood of such machines, namely the Hopfield model of neural networks, and we explicit how they can be used as iterative message-passing algorithms, providing a fast method to compute the local polarizations of neurons. In the "retrieval phase", where neurons polarize in the direction of one memorized pattern, we point out a major difference between the belief propagation and TAP equations: The set of belief propagation equations depends on the pattern which is retrieved, while one can use a unique set of TAP equations. This makes the latter method much better suited for applications in the learning process of restricted Boltzmann machines. In the case where the patterns memorized in the Hopfield model are not independent, but are correlated through a combinatorial structure, we show that the TAP equations have to be modified. This modification can be seen either as an alteration of the reaction term in TAP equations or, more interestingly, as the consequence of message passing on a graphical model with several hidden layers, where the number of hidden layers depends on the depth of the correlations in the memorized patterns. This layered structure is actually necessary when one deals with more general restricted Boltzmann machines.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5SK4ZBVC/Mezard - 2016 - Mean-field message-passing equations in the Hopfield model and its generalizations.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mezardMeanfieldMessagepassingEquations2017.md}
}

@article{mhatreGridCellHexagonal2012,
  title = {Grid {{Cell Hexagonal Patterns Formed}} by {{Fast Self-Organized Learning Within Entorhinal Cortex}}},
  author = {Mhatre, Himanshu and Gorchetchnikov, Anatoli and Grossberg, Stephen},
  year = {2012},
  month = feb,
  journal = {Hippocampus},
  number = {2},
  pages = {320--334},
  doi = {10.1002/hipo.20901},
  abstract = {Grid cells in the dorsal segment of the medial entorhinal cortex (dMEC) show remarkable hexagonal activity patterns, at multiple spatial scales, during spatial navigation. It has previously been shown how a self-organizing map can convert firing patterns across entorhinal grid cells into hippocampal place cells that are capable of representing much larger spatial scales. Can grid cell firing fields also arise during navigation through learning within a self-organizing map? This article describes a simple and general mathematical property of the trigonometry of spatial navigation which favors hexagonal patterns. The article also develops a neural model that can learn to exploit this trigonometric relationship. This GRIDSmap self-organizing map model converts path integration signals into hexagonal grid cell patterns of multiple scales. GRIDSmap creates only grid cell firing patterns with the observed hexagonal structure, predicts how these hexagonal patterns can be learned from experience, and can process biologically plausible neural input and output signals during navigation. These results support an emerging unified computational framework based on a hierarchy of self-organizing maps for explaining how entorhinal-hippocampal interactions support spatial navigation.},
  keywords = {entorhinal cortex,grid cells,path integration,self-organized learning,spatial navigation},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/T3FBAF9X/Mhatre, Gorchetchnikov, Grossberg - 2012 - Grid Cell Hexagonal Patterns Formed by Fast Self-Organized Learning Within Entorhinal Cortex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mhatreGridCellHexagonal2012.md}
}

@article{michelMinorityReportsConsciousness2019,
  title = {Minority Reports: {{Consciousness}} and the Prefrontal Cortex},
  author = {Michel, Matthias and Morales, Jorge},
  year = {2019},
  month = sep,
  journal = {Mind \& Language},
  pages = {mila.12264-mila.12264},
  doi = {10.1111/mila.12264},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@michelMinorityReportsConsciousness2019.md}
}

@article{mikhaelAdaptingFlowTime2019,
  title = {Adapting the Flow of Time with Dopamine.},
  author = {Mikhael, John G. and Gershman, S.},
  year = {2019},
  journal = {Journal of neurophysiology},
  doi = {10.1152/jn.00817.2018},
  abstract = {DA's role in timekeeping is related to its most established role, as a critical component of reinforcement learning, to derive a neurobiologically plausible framework that reconciles a large body of DA's temporal effects, including pharmacological, behavioral, electrophysiological, and optogenetic. The modulation of interval timing by dopamine (DA) has been well established over decades of research. The nature of this modulation, however, has remained controversial: Although the pharmacological evidence has largely suggested that time intervals are overestimated with higher DA levels, more recent optogenetic work has shown the opposite effect. In addition, a large body of work has asserted DA's role as a "reward prediction error" (RPE), or a teaching signal that allows the basal ganglia to learn to predict future rewards in reinforcement learning tasks. Whether these two seemingly disparate accounts of DA may be related has remained an open question. By taking a reinforcement learning-based approach to interval timing, we show here that the RPE interpretation of DA naturally extends to its role as a modulator of timekeeping and furthermore that this view reconciles the seemingly conflicting observations. We derive a biologically plausible, DA-dependent plasticity rule that can modulate the rate of timekeeping in either direction and whose effect depends on the timing of the DA signal itself. This bidirectional update rule can account for the results from pharmacology and optogenetics as well as the behavioral effects of reward rate on interval timing and the temporal selectivity of striatal neurons. Hence, by adopting a single RPE interpretation of DA, our results take a step toward unifying computational theories of reinforcement learning and interval timing. NEW \& NOTEWORTHY How does dopamine (DA) influence interval timing? A large body of pharmacological evidence has suggested that DA accelerates timekeeping mechanisms. However, recent optogenetic work has shown exactly the opposite effect. In this article, we relate DA's role in timekeeping to its most established role, as a critical component of reinforcement learning. This allows us to derive a neurobiologically plausible framework that reconciles a large body of DA's temporal effects, including pharmacological, behavioral, electrophysiological, and optogenetic.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RPDCZGLD/Mikhael_Gershman_2019_Adapting the flow of time with dopamine.pdf}
}

@misc{Milgram1967Small,
  title = {Milgram (1967): {{The Small World Problem}} | {{SpringerLink}}},
  howpublished = {https://link.springer.com/chapter/10.1007\%2F978-3-658-21742-6\_94},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@Milgram1967Small.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N5PRS3HR/10.html}
}

@article{milgramSmallWorldProblem1967,
  title = {The Small World Problem},
  author = {Milgram, Stanley},
  year = {1967},
  journal = {Psychology today},
  volume = {2},
  number = {1},
  pages = {60--67},
  publisher = {{New York}},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WNBQ9FNQ/Milgram - 1967 - The small world problem.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@milgramSmallWorldProblem1967.md}
}

@techreport{millerPREFRONTALCORTEXCOGNITIVE2000,
  title = {{{THE PREFRONTAL CORTEX AND COGNITIVE CONTROL}}},
  author = {Miller, Earl K},
  year = {2000},
  abstract = {One of the enduring mysteries of brain function concerns the process of cognitive control. How does complex and seemingly wilful behaviour emerge from interactions between millions of neurons? This has long been suspected to depend on the prefrontal cortex-the neocortex at the anterior end of the brain-but now we are beginning to uncover its neural basis. Nearly all intended behaviour is learned and so depends on a cognitive system that can acquire and implement the 'rules of the game' needed to achieve a given goal in a given situation. Studies indicate that the prefrontal cortex is central in this process. It provides an infrastructure for synthesizing a diverse range of information that lays the foundation for the complex forms of behaviour observed in primates. TOP-DOWN Brain signals that convey knowledge derived from prior experience rather than sensory stimulation. NATURE REVIEWS | NEUROSCIENCE VOLUME 1 | OCTOBER 2000 | Humans and other animals can do more than reflexively react to sensory information that is immediate and salient. We engage in complex and extended behaviours geared towards often far-removed goals. To do so, we have evolved mechanisms that can override or augment reflexive and habitual reactions in order to orchestrate behaviour in accord with our intentions. These mechanisms are commonly referred to as 'cogni-tive' in nature and their function is to control lower-level sensory, memory and/or motor operations for a common purpose. So cognitive control is essential for what we recognize as intelligent behaviour. Insight into the neural mechanisms for cognitive control may come from what is arguably their most important feature: they are sculpted by experience. Virtually all intended behaviours are learned and so depend on a cognitive system that can acquire the rules of the game-what goals are available and what means can be used to achieve these goals 1-4. Take, for example, dining in a restaurant. We are not born knowing that this can be a rewarding experience or how to act in this situation. Instead, our experiences arm us with expectations about the important sensory information deserving our attention (for example, the wine list), typical events, appropriate actions and expected consequences (for example, paying the bill). This knowledge allows diverse brain processes to be orchestrated along a common internal theme. So a key function of the neural circuitry mediating cognitive control is to extract the goal-relevant features of our experiences for use in future circumstances. It has been proposed that the prefrontal cortex-a neocortical region that finds its greatest elaboration in humans-is centrally involved in this process 4-8. The prefrontal cortex (PFC) (FIG. 1) is an interconnected set of neocortical areas that have a unique, but overlapping, pattern of connectivity with virtually all sensory neocortical and motor systems and a wide range of subcortical structures 9-12. This provides an ideal infrastructure for synthesizing the diverse range of information needed for complex behaviour. The PFC also has widespread projections back to these systems that may allow it to exert a 'TOP-DOWN' influence on a wide range of brain processes 9-12. Indeed, the effects of PFC damage are most apparent when cognitive control is most needed-when the knowledge about a given situation must be used to select the appropriate goal-directed actions (BOX 1). Here I review recent neurophisiological studies in monkeys that have explored the neural basis of cogni-tive control. They indicate that a major function of the PFC is to extract information about the regularities across experiences and so impart rules that can be used to guide thought and action 8,13-15 .},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RAJJAFVC/Miller - 2000 - THE PREFRONTAL CORTEX AND COGNITIVE CONTROL.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@millerPREFRONTALCORTEXCOGNITIVE2000.md}
}

@misc{millidgeTheoreticalFrameworkInference2022,
  title = {A {{Theoretical Framework}} for {{Inference}} and {{Learning}} in {{Predictive Coding Networks}}},
  author = {Millidge, Beren and Song, Yuhang and Salvatori, Tommaso and Lukasiewicz, Thomas and Bogacz, Rafal},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12316},
  eprint = {2207.12316},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.12316},
  abstract = {Predictive coding (PC) is an influential theory in computational neuroscience, which argues that the cortex forms unsupervised world models by implementing a hierarchical process of prediction error minimization. PC networks (PCNs) are trained in two phases. First, neural activities are updated to optimize the network's response to external stimuli. Second, synaptic weights are updated to consolidate this change in activity -- an algorithm called \textbackslash emph\{prospective configuration\}. While previous work has shown how in various limits, PCNs can be found to approximate backpropagation (BP), recent work has demonstrated that PCNs operating in this standard regime, which does not approximate BP, nevertheless obtain competitive training and generalization performance to BP-trained networks while outperforming them on tasks such as online, few-shot, and continual learning, where brains are known to excel. Despite this promising empirical performance, little is understood theoretically about the properties and dynamics of PCNs in this regime. In this paper, we provide a comprehensive theoretical analysis of the properties of PCNs trained with prospective configuration. We first derive analytical results concerning the inference equilibrium for PCNs and a previously unknown close connection relationship to target propagation (TP). Secondly, we provide a theoretical analysis of learning in PCNs as a variant of generalized expectation-maximization and use that to prove the convergence of PCNs to critical points of the BP loss function, thus showing that deep PCNs can, in theory, achieve the same generalization performance as BP, while maintaining their unique advantages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,predictive coding},
  note = {Comment: 21/07/22 initial upload (finally)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7EP522CX/Millidge et al_2022_A Theoretical Framework for Inference and Learning in Predictive Coding Networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CC45RFB7/2207.html}
}

@inproceedings{millidgeUniversalHopfieldNetworks2022,
  title = {Universal {{Hopfield Networks}}: {{A General Framework}} for {{Single-Shot Associative Memory Models}}},
  shorttitle = {Universal {{Hopfield Networks}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Millidge, Beren and Salvatori, Tommaso and Song, Yuhang and Lukasiewicz, Thomas and Bogacz, Rafal},
  year = {2022},
  month = jun,
  pages = {15561--15583},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {A large number of neural network models of associative memory have been proposed in the literature. These include the classical Hopfield networks (HNs), sparse distributed memories (SDMs), and more recently the modern continuous Hopfield networks (MCHNs), which possess close links with self-attention in machine learning. In this paper, we propose a general framework for understanding the operation of such memory networks as a sequence of three operations: similarity, separation, and projection. We derive all these memory models as instances of our general framework with differing similarity and separation functions. We extend the mathematical framework of Krotov et al (2020) to express general associative memory models using neural network dynamics with local computation, and derive a general energy function that is a Lyapunov function of the dynamics. Finally, using our framework, we empirically investigate the capacity of using different similarity functions for these associative memory models, beyond the dot product similarity measure, and demonstrate empirically that Euclidean or Manhattan distance similarity metrics perform substantially better in practice on many tasks, enabling a more robust retrieval and higher memory capacity than existing~models.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4QVJZMJ8/Millidge et al_2022_Universal Hopfield Networks.pdf}
}

@misc{mitrovicRepresentationLearningInvariant2020,
  title = {Representation {{Learning}} via {{Invariant Causal Mechanisms}}},
  author = {Mitrovic, Jovana and McWilliams, Brian and Walker, Jacob and Buesing, Lars and Blundell, Charles},
  year = {2020},
  month = oct,
  number = {arXiv:2010.07922},
  eprint = {2010.07922},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.07922},
  abstract = {Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on \$51\$ out of \$57\$ games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,RL,self supervised,Statistics - Machine Learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N49Q3G3D/Mitrovic et al_2020_Representation Learning via Invariant Causal Mechanisms.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/R6F3HLY8/2010.html}
}

@misc{mizusakiPrePostsynapticallyExpressed2021,
  title = {Pre- and Postsynaptically Expressed Spike-Timing-Dependent Plasticity Contribute Differentially to Neuronal Learning},
  author = {Mizusaki, Beatriz E. P. and Li, Sally S. Y. and Costa, Rui Ponte and Sj{\"o}str{\"o}m, P. Jesper},
  year = {2021},
  month = sep,
  pages = {2021.09.01.458493},
  institution = {{bioRxiv}},
  abstract = {A plethora of experimental studies have shown that long-term synaptic plasticity can be expressed pre- or postsynaptically depending on a range of factors such as developmental stage, synapse type, and activity patterns. The functional consequences of this diversity are not clear, although it is understood that whereas postsynaptic expression of plasticity predominantly affects synaptic response amplitude, presynaptic expression alters both synaptic response amplitude and short-term dynamics. In most models of neuronal learning, long-term synaptic plasticity is implemented as changes in connective weights. The consideration of long-term plasticity as a fixed change in amplitude corresponds more closely to post-than to presynaptic expression, which means theoretical outcomes based on this choice of implementation may have a postsynaptic bias. To explore the functional implications of the diversity of expression of long-term synaptic plasticity, we adapted a model of long-term plasticity, more specifically spike-timing-dependent plasticity (STDP), such that it was expressed either independently pre- or postsynaptically, or in a mixture of both ways. We compared pair-based standard STDP models and a biologically tuned triplet STDP model, and investigated the outcomes in a minimal setting, using two different learning schemes: in the first, inputs were triggered at different latencies, and in the second a subset of inputs were temporally correlated. We found that presynaptic changes adjusted the speed of learning, while postsynaptic expression was more efficient at regulating spike timing and frequency. When combining both expression loci, postsynaptic changes amplified the response range, while presynaptic plasticity allowed control over postsynaptic firing rates, potentially providing a form of activity homeostasis. Our findings highlight how the seemingly innocuous choice of implementing synaptic plasticity by single weight modification may unwittingly introduce a postsynaptic bias in modelling outcomes. We conclude that pre- and postsynaptically expressed plasticity are not interchangeable, but enable complimentary functions. Author summary Differences between functional properties of pre- or postsynaptically expressed long-term plasticity have not yet been explored in much detail. In this paper, we used minimalist models of STDP with different expression loci, in search of fundamental functional consequences. Biologically, presynaptic expression acts mostly on neurotransmitter release, thereby altering short-term synaptic dynamics, whereas postsynaptic expression affects mainly synaptic gain. We compared models where plasticity was expressed only presynaptically or postsynaptically, or in both ways. We found that postsynaptic plasticity had a bigger impact over response times, while both pre- and postsynaptic plasticity were similarly capable of detecting correlated inputs. A model with biologically tuned expression of plasticity also completed these tasks over a range of frequencies. Also, postsynaptic spiking frequency was not directly affected by presynaptic plasticity of short-term plasticity alone, however in combination with a postsynaptic component, it helped restrain positive feedback, contributing to activity homeostasis. In conclusion, expression locus may determine affinity for distinct coding schemes while also contributing to keep activity within bounds. Our findings highlight the importance of carefully implementing expression of plasticity in biological modelling, since the locus of expression may affect functional outcomes in simulations.},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}

@techreport{mizusakiPrePostsynapticallyExpressed2021a,
  type = {Preprint},
  title = {Pre- and Postsynaptically Expressed Spike-Timing-Dependent Plasticity Contribute Differentially to Neuronal Learning},
  author = {Mizusaki, Beatriz E. P. and Li, Sally S. Y. and Costa, Rui Ponte and Sj{\"o}str{\"o}m, P. Jesper},
  year = {2021},
  month = sep,
  institution = {{Neuroscience}},
  doi = {10.1101/2021.09.01.458493},
  abstract = {A plethora of experimental studies have shown that long-term synaptic plasticity can be expressed pre- or postsynaptically depending on a range of factors such as developmental stage, synapse type, and activity patterns. The functional consequences of this diversity are not clear, although it is understood that whereas postsynaptic expression of plasticity predominantly affects synaptic response amplitude, presynaptic expression alters both synaptic response amplitude and short-term dynamics. In most models of neuronal learning, long-term synaptic plasticity is implemented as changes in connective weights. The consideration of long-term plasticity as a fixed change in amplitude corresponds more closely to post- than to presynaptic expression, which means theoretical outcomes based on this choice of implementation may have a postsynaptic bias. To explore the functional implications of the diversity of expression of long-term synaptic plasticity, we adapted a model of long-term plasticity, more specifically spike-timing-dependent plasticity (STDP), such that it was expressed either independently pre- or postsynaptically, or in a mixture of both ways. We compared pair-based standard STDP models and a biologically tuned triplet STDP model, and investigated the outcomes in a minimal setting, using two different learning schemes: in the first, inputs were triggered at different latencies, and in the second a subset of inputs were temporally correlated. We found that presynaptic changes adjusted the speed of learning, while postsynaptic expression was more efficient at regulating spike timing and frequency. When combining both expression loci, postsynaptic changes amplified the response range, while presynaptic plasticity allowed control over postsynaptic firing rates, potentially providing a form of activity homeostasis. Our findings highlight how the seemingly innocuous choice of implementing synaptic plasticity by single weight modification may unwittingly introduce a postsynaptic bias in modelling outcomes. We conclude that pre- and postsynaptically expressed plasticity are not interchangeable, but enable complimentary functions.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/M5GWXL5X/Mizusaki et al. - 2021 - Pre- and postsynaptically expressed spike-timing-d.pdf}
}

@inproceedings{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  pages = {1928--1937},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VGSMTZHX/Mnih et al_2016_Asynchronous Methods for Deep Reinforcement Learning.pdf}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computer science},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4TUELFJJ/Human-level control through deep reinforcement learning - 13.05.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MMLN3FB9/Mnih et al_2015_Human-level control through deep reinforcement learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9QEKXZTG/nature14236.html}
}

@misc{ModelingBrainFunction,
  title = {Modeling {{Brain Function}}: {{The World}} of {{Attractor Neural Networks}} - {{D}}. {{J}}. {{Amit}}, {{Daniel J}}. {{Amit}} - {{Google Books}}},
  howpublished = {https://books.google.no/books?hl=en\&lr=\&id=fvLYch1yQncC\&oi=fnd\&pg=PR13\&dq=Amit+modeling+brain+function\&ots=4ysT2D4JpL\&sig=p1H-5t\_xMaF0dj9qITp0XPIqPfI\&redir\_esc=y\#v=onepage\&q=Amit\%20modeling\%20brain\%20function\&f=false},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ModelingBrainFunction.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/T2PJUYLY/books.html}
}

@techreport{molano-mazonSYNTHESIZINGREALISTICNEURAL,
  title = {{{SYNTHESIZING REALISTIC NEURAL POPULATION ACTIVITY PATTERNS USING GENERATIVE ADVERSARIAL NETWORKS}}},
  author = {{Molano-Mazon}, Manuel and Onken, Arno and Piasini, Eugenio and Panzeri, Stefano},
  abstract = {The ability to synthesize realistic patterns of neural activity is crucial for studying neural information processing. Here we used the Generative Adversarial Networks (GANs) framework to simulate the concerted activity of a population of neurons. We adapted the Wasserstein-GAN variant to facilitate the generation of unconstrained neural population activity patterns while still benefiting from parameter sharing in the temporal domain. We demonstrate that our proposed GAN, which we termed Spike-GAN, generates spike trains that match accurately the first-and second-order statistics of datasets of tens of neurons and also approximates well their higher-order statistics. We applied Spike-GAN to a real dataset recorded from salamander retina and showed that it performs as well as state-of-the-art approaches based on the maximum entropy and the dichotomized Gaus-sian frameworks. Importantly, Spike-GAN does not require to specify a priori the statistics to be matched by the model, and so constitutes a more flexible method than these alternative approaches. Finally, we show how to exploit a trained Spike-GAN to construct 'importance maps' to detect the most relevant statistical structures present in a spike train. Spike-GAN provides a powerful, easy-to-use technique for generating realistic spiking neural activity and for describing the most relevant features of the large-scale neural population recordings studied in modern systems neuroscience.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SERWIEXW/Molano-Mazon et al. - Unknown - SYNTHESIZING REALISTIC NEURAL POPULATION ACTIVITY PATTERNS USING GENERATIVE ADVERSARIAL NETWORKS.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@molano-mazonSYNTHESIZINGREALISTICNEURAL.md}
}

@article{monahanYeastModelsPrionLike2018,
  title = {Yeast {{Models}} of {{Prion-Like Proteins That Cause Amyotrophic Lateral Sclerosis Reveal Pathogenic Mechanisms}}},
  author = {Monahan, Zachary T. and Rhoads, Shannon N. and Yee, Debra S. and Shewmaker, Frank P.},
  year = {2018},
  journal = {Frontiers in Molecular Neuroscience},
  volume = {11},
  number = {December},
  pages = {1--11},
  doi = {10.3389/fnmol.2018.00453},
  keywords = {als,amyloid,ewsr1,fus,hnrnpa2b1,prion,prion; FUS; TAF15; HNRNPA2B1; EWSR1; TDP-43; amylo,taf15,tdp-43},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LCB8XCR8/Monahan et al. - 2018 - Yeast Models of Prion-Like Proteins That Cause Amyotrophic Lateral Sclerosis Reveal Pathogenic Mechanisms.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@monahanYeastModelsPrionLike2018.md}
}

@article{monsalve-mercadoHippocampalSpikeTimingCorrelations2017,
  title = {Hippocampal {{Spike-Timing Correlations Lead}} to {{Hexagonal Grid Fields}}},
  author = {{Monsalve-Mercado}, Mauro M. and Leibold, Christian},
  year = {2017},
  journal = {Physical Review Letters},
  volume = {119},
  number = {3},
  pages = {1--5},
  doi = {10.1103/PhysRevLett.119.038101},
  abstract = {Space is represented in the mammalian brain by the activity of hippocampal place cells as well as in their spike-timing correlations. Here we propose a theory how this temporal code is transformed to spatial firing rate patterns via spike-timing-dependent synaptic plasticity. The resulting dynamics of synaptic weights resembles well-known pattern formation models in which a lateral inhibition mechanism gives rise to a Turing instability. We identify parameter regimes in which hexagonal firing patterns develop as they have been found in medial entorhinal cortex.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YEZQF2H5/Monsalve-Mercado, Leibold - 2017 - Hippocampal Spike-Timing Correlations Lead to Hexagonal Grid Fields.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@monsalve-mercadoHippocampalSpikeTimingCorrelations2017.md}
}

@article{montanari2009graphical,
  title = {Which Graphical Models Are Difficult to Learn?},
  author = {Montanari, Andrea and Pereira, Jose},
  year = {2009},
  journal = {Advances in Neural Information Processing Systems},
  volume = {22},
  pages = {1303--1311},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@montanari2009graphical.md}
}

@article{montemurroPhaseofFiringCodingNatural2008,
  title = {Phase-of-{{Firing Coding}} of {{Natural Visual Stimuli}} in {{Primary Visual Cortex}}},
  author = {Montemurro, Marcelo A. and Rasch, Malte J. and Murayama, Yusuke and Logothetis, Nikos K. and Panzeri, Stefano},
  year = {2008},
  month = mar,
  journal = {Current Biology},
  volume = {18},
  number = {5},
  pages = {375--380},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2008.02.023},
  abstract = {We investigated the hypothesis that neurons encode rich naturalistic stimuli in terms of their spike times relative to the phase of ongoing network fluctuations rather than only in terms of their spike count. We recorded local field potentials (LFPs) and multiunit spikes from the primary visual cortex of anaesthetized macaques while binocularly presenting a color movie. We found that both the spike counts and the low-frequency LFP phase were reliably modulated by the movie and thus conveyed information about it. Moreover, movie periods eliciting higher firing rates also elicited a higher reliability of LFP phase across trials. To establish whether the LFP phase at which spikes were emitted conveyed visual information that could not be extracted by spike rates alone, we compared the Shannon information about the movie carried by spike counts to that carried by the phase of firing. We found that at low LFP frequencies, the phase of firing conveyed 54\% additional information beyond that conveyed by spike counts. The extra information available in the phase of firing was crucial for the disambiguation between stimuli eliciting high spike rates of similar magnitude. Thus, phase coding may allow primary cortical neurons to represent several effective stimuli in an easily decodable format.},
  langid = {english},
  keywords = {SYSNEURO},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SGTSCK72/Montemurro et al_2008_Phase-of-Firing Coding of Natural Visual Stimuli in Primary Visual Cortex.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/R24LT7YG/S0960982208001681.html}
}

@article{monzSuInteraccionCon2001,
  title = {Y Su Interaccion Con Las Plantas},
  author = {Monz, Azucena and Area, Asconegui and Aires, Buenos and Mart, San},
  year = {2001},
  volume = {79},
  number = {1000 X},
  pages = {1--12},
  issn = {0027-8424},
  doi = {10.1097/PHM.0000000000000442},
  abstract = {Computational properties of use to biological or- ganisms or to the construction of computers can emerge as col- lective properties of systems -having a large number of simple equivalent components (or neurons). The physical meaning ofcon- tent-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to in- tegrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties in- clude some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details ofthe modeling},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ANT43AQX/Monz et al. - 2001 - Y su interaccion con las plantas.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@monzSuInteraccionCon2001.md}
}

@article{moraAreBiologicalSystems2011,
  title = {Are {{Biological Systems Poised}} at {{Criticality}}?},
  author = {Mora, Thierry and Bialek, William},
  year = {2011},
  journal = {J Stat Phys},
  volume = {144},
  pages = {268--302},
  doi = {10.1007/s10955-011-0229-4},
  abstract = {Many of life's most fascinating phenomena emerge from interactions among many elements-many amino acids determine the structure of a single protein, many genes determine the fate of a cell, many neurons are involved in shaping our thoughts and memories. Physicists have long hoped that these collective behaviors could be described using the ideas and methods of statistical mechanics. In the past few years, new, larger scale experiments have made it possible to construct statistical mechanics models of biological systems directly from real data. We review the surprising successes of this "inverse" approach, using examples from families of proteins, networks of neurons, and flocks of birds. Remarkably, in all these cases the models that emerge from the data are poised near a very special point in their parameter space-a critical point. This suggests there may be some deeper theoretical principle behind the behavior of these diverse systems.},
  keywords = {Biological networks,Biological networks ·,Collective behavior,Critical point,Critical point ·,Maximum entropy model,Maximum entropy model ·,Proteins,Proteins ·},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FWEX5B6R/Mora, Bialek - 2011 - Are Biological Systems Poised at Criticality.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N9KUJHS9/Mora, Bialek - 2011 - Are Biological Systems Poised at Criticality(2).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@moraAreBiologicalSystems2011.md}
}

@article{moraDynamicalCriticalityCollective2015,
  title = {Dynamical {{Criticality}} in the {{Collective Activity}} of a {{Population}} of {{Retinal Neurons}}},
  author = {Mora, Thierry and Deny, St{\'e}phane and Marre, Olivier},
  year = {2015},
  month = feb,
  journal = {Physical Review Letters},
  volume = {114},
  number = {7},
  pages = {078105},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.114.078105},
  abstract = {Recent experimental results based on multielectrode and imaging techniques have reinvigorated the idea that large neural networks operate near a critical point, between order and disorder. However, evidence for criticality has relied on the definition of arbitrary order parameters, or on models that do not address the dynamical nature of network activity. Here we introduce a novel approach to assess criticality that overcomes these limitations, while encompassing and generalizing previous criteria. We find a simple model to describe the global activity of large populations of ganglion cells in the rat retina, and show that their statistics are poised near a critical point. Taking into account the temporal dynamics of the activity greatly enhances the evidence for criticality, revealing it where previous methods would not. The approach is general and could be used in other biological networks.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IBNMKXAM/Mora et al. - 2015 - Dynamical Criticality in the Collective Activity o.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@moraDynamicalCriticalityCollective2015.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YFHX869K/PhysRevLett.114.html}
}

@inproceedings{morenoemelloMethodObtainNeuromorphic2019,
  title = {Method to {{Obtain Neuromorphic Reservoir Networks}} from {{Images}} of in {{Vitro Cortical Networks}}},
  booktitle = {2019 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {{Moreno e Mello}, Gustavo Borges and {Pontes-Filho}, Sidney and Sandvig, Ioanna and Valderhaug, Vibeke Devold and Zouganeli, Evi and Ramstad, Ola Huse and Sandvig, Axel and Nichele, Stefano},
  year = {2019},
  month = dec,
  pages = {2360--2366},
  doi = {10.1109/SSCI44817.2019.9002741},
  abstract = {In the brain, the structure of a network of neurons defines how these neurons implement the computations that underlie the mind and the behavior of animals and humans. Provided that we can describe the network of neurons as a graph. We can employ methods from graph theory to investigate its structure or use cellular automata to mathematically assess its function. Additionally, these graphs can provide biologically plausible designs for networks, which can be integrated as reservoirs to support computing. Although, software for the analysis of graphs and cellular automata are widely available. Graph extraction from the image of networks of brain cells remains difficult. Nervous tissue is heterogeneous, and differences in anatomy may reflect relevant differences in function. Here we introduce a deep learning based toolbox to extracts graphs from images of brain tissue. This toolbox provides an easy- to-use framework allowing system neuroscientists to generate graphs based on images of brain tissue by combining methods from image processing, deep learning, and graph theory. The goals are to simplify the training and usage of deep learning methods for computer vision and facilitate its integration into graph extraction pipelines. In this way, the toolbox provides an alternative to the required laborious manual process of tracing, sorting and classifying. We expect to democratize the machine learning methods to a wider community of users beyond the computer vision experts and improve the time-efficiency of graph extraction from large brain image datasets, which may lead to further understanding of the human mind.},
  keywords = {cellular automata,Computer vision,deep learning,graph,Image segmentation,inpainting,Machine learning,neural network,Neurons,Pipelines,reservoir computing,segmentation,Training},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@morenoemelloMethodObtainNeuromorphic2019.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/morenoemelloMethodObtainNeuromorphic2019-mdnotes.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Moreno e Mello et al_2019_Method to Obtain Neuromorphic Reservoir Networks from Images of in Vitro.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SQL4ASI5/9002741.html}
}

@article{moserGridCellsCortical2014,
  title = {Grid Cells and Cortical Representation},
  author = {Moser, Edvard I. and Roudi, Yasser and Witter, Menno P. and Kentros, Clifford and Bonhoeffer, Tobias and Moser, May-Britt},
  year = {2014},
  month = jul,
  journal = {Nature Reviews Neuroscience},
  volume = {15},
  number = {7},
  pages = {466--481},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nrn3766},
  abstract = {Nervous systems recreate properties of the environment in activity patterns referred to as neural representations. In this Review, Moser and colleagues examine how grid cells in the medial entorhinal cortex contribute to the neural representation of external space.},
  keywords = {Hippocampus,Network models},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WHGX63BJ/Moser et al. - 2014 - Grid cells and cortical representation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@moserGridCellsCortical2014.md}
}

@article{moserPlaceCellsGrid2008,
  title = {Place Cells, Grid Cells, and the Brain's Spatial Representation System},
  author = {Moser, Edvard I. and Kropff, Emilio and Moser, May-Britt},
  year = {2008},
  journal = {Annual Review of Neuroscience},
  volume = {31},
  pages = {69--89},
  issn = {0147-006X},
  doi = {10.1146/annurev.neuro.31.061307.090723},
  abstract = {More than three decades of research have demonstrated a role for hippocampal place cells in representation of the spatial environment in the brain. New studies have shown that place cells are part of a broader circuit for dynamic representation of self-location. A key component of this network is the entorhinal grid cells, which, by virtue of their tessellating firing fields, may provide the elements of a path integration-based neural map. Here we review how place cells and grid cells may form the basis for quantitative spatiotemporal representation of places, routes, and associated experiences during behavior and in memory. Because these cell types have some of the most conspicuous behavioral correlates among neurons in nonsensory cortical systems, and because their spatial firing structure reflects computations internally in the system, studies of entorhinal-hippocampal representations may offer considerable insight into general principles of cortical network dynamics.},
  langid = {english},
  pmid = {18284371},
  keywords = {Action Potentials,Animals,Entorhinal Cortex,Hippocampus,Humans,Memory,Nerve Net,Neural Pathways,Neurons,Orientation,Space Perception}
}

@article{moserSpatialRepresentationHippocampal2017,
  title = {Spatial Representation in the Hippocampal Formation: {{A}} History},
  author = {Moser, Edvard I. and Moser, May Britt and McNaughton, Bruce L.},
  year = {2017},
  journal = {Nature Neuroscience},
  volume = {20},
  number = {11},
  pages = {1448--1464},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nn.4653},
  abstract = {Since the first place cell was recorded and the cognitive-map theory was subsequently formulated, investigation of spatial representation in the hippocampal formation has evolved in stages. Early studies sought to verify the spatial nature of place cell activity and determine its sensory origin. A new epoch started with the discovery of head direction cells and the realization of the importance of angular and linear movement-integration in generating spatial maps. A third epoch began when investigators turned their attention to the entorhinal cortex, which led to the discovery of grid cells and border cells. This review will show how ideas about integration of self-motion cues have shaped our understanding of spatial representation in hippocampal\textendash entorhinal systems from the 1970s until today. It is now possible to investigate how specialized cell types of these systems work together, and spatial mapping may become one of the first cognitive functions to be understood in mechanistic detail.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DSJ243CC/Moser, Moser, McNaughton - 2017 - Spatial representation in the hippocampal formation A history.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@moserSpatialRepresentationHippocampal2017.md}
}

@article{munozColloquiumCriticalityDynamical2018,
  title = {Colloquium: {{Criticality}} and Dynamical Scaling in Living Systems},
  shorttitle = {Colloquium},
  author = {Mu{\~n}oz, Miguel A.},
  year = {2018},
  month = jul,
  journal = {Reviews of Modern Physics},
  volume = {90},
  number = {3},
  pages = {031001},
  publisher = {{American Physical Society}},
  doi = {10.1103/RevModPhys.90.031001},
  abstract = {A celebrated and controversial hypothesis suggests that some biological systems\textemdash parts, aspects, or groups of them\textemdash may extract important functional benefits from operating at the edge of instability, halfway between order and disorder, i.e., in the vicinity of the critical point of a phase transition. Criticality has been argued to provide biological systems with an optimal balance between robustness against perturbations and flexibility to adapt to changing conditions as well as to confer on them optimal computational capabilities, large dynamical repertoires, unparalleled sensitivity to stimuli, etc. Criticality, with its concomitant scale invariance, can be conjectured to emerge in living systems as the result of adaptive and evolutionary processes that, for reasons to be fully elucidated, select for it as a template upon which further layers of complexity can rest. This hypothesis is suggestive as it proposes that criticality could constitute a general and common organizing strategy in biology stemming from the physics of phase transitions. However, despite its implications, this is still in its infancy state as a well-founded theory and, as such, it has elicited some skepticism. From the experimental side, the advent of high-throughput technologies has created new prospects in the exploration of biological systems, and empirical evidence in favor of criticality has proliferated, with examples ranging from endogenous brain activity and gene-expression patterns to flocks of birds and insect-colony foraging, to name but a few. Some pieces of evidence are quite remarkable, while in some other cases empirical data are limited, incomplete, or not fully convincing. More stringent experimental setups and theoretical analyses are certainly needed to fully clarify the picture. In any case, the time seems ripe for bridging the gap between this theoretical conjecture and its empirical validation. Given the profound implications of shedding light on this issue, it is both pertinent and timely to review the state of the art and to discuss future strategies and perspectives.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KDLN52QP/Muñoz - 2018 - Colloquium Criticality and dynamical scaling in l.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@munozColloquiumCriticalityDynamical2018.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RIZIY3BN/RevModPhys.90.html}
}

@book{murphyMachineLearningProbabilistic2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  isbn = {978-0-262-01802-9},
  langid = {english},
  lccn = {Q325.5 .M87 2012},
  keywords = {Machine learning,Probabilities},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DX82JP99/Murphy - 2012 - Machine learning a probabilistic perspective.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VFMQ3KG6/Murphy - 2012 - Machine Learning A Probabilistic Perspective.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@murphyMachineLearningProbabilistic2012.md}
}

@article{murrayLearningMultipleVariablespeed2017,
  title = {Learning Multiple Variable-Speed Sequences in Striatum via Cortical Tutoring},
  author = {Murray, James M and Escola, G Sean},
  editor = {Frank, Michael J},
  year = {2017},
  month = may,
  journal = {eLife},
  volume = {6},
  pages = {e26084},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.26084},
  abstract = {Sparse, sequential patterns of neural activity have been observed in numerous brain areas during timekeeping and motor sequence tasks. Inspired by such observations, we construct a model of the striatum, an all-inhibitory circuit where sequential activity patterns are prominent, addressing the following key challenges: (i) obtaining control over temporal rescaling of the sequence speed, with the ability to generalize to new speeds; (ii) facilitating flexible expression of distinct sequences via selective activation, concatenation, and recycling of specific subsequences; and (iii) enabling the biologically plausible learning of sequences, consistent with the decoupling of learning and execution suggested by lesion studies showing that cortical circuits are necessary for learning, but that subcortical circuits are sufficient to drive learned behaviors. The same mechanisms that we describe can also be applied to circuits with both excitatory and inhibitory populations, and hence may underlie general features of sequential neural activity pattern generation in the brain.},
  keywords = {basal ganglia,circuit models,motor sequences},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5WAC9R2R/Murray_Escola_2017_Learning multiple variable-speed sequences in striatum via cortical tutoring.pdf}
}

@article{mvPopulationDynamicsTheta1996,
  title = {Population Dynamics and Theta Rhythm Phase Precession of Hippocampal Place Cell Firing: A Spiking Neuron Model},
  author = {MV, Tsodyks and WE, Skaggs and TJ, Sejnowski and BL, McNaughton},
  year = {1996},
  journal = {Hippocampus},
  volume = {6},
  pages = {271--271},
  keywords = {attractor,cell assembly,neural computation,oscillation},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ANY7N4DP/MV et al. - 1996 - Population dynamics and theta rhythm phase precession of hippocampal place cell firing a spiking neuron model.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mvPopulationDynamicsTheta1996.md}
}

@article{mydoshSpinGlassesRedux2015,
  title = {Spin Glasses: {{Redux}}: {{An}} Updated Experimental/Materials Survey},
  author = {Mydosh, J. A.},
  year = {2015},
  month = may,
  journal = {Reports on Progress in Physics},
  volume = {78},
  number = {5},
  pages = {052501--052501},
  publisher = {{Institute of Physics Publishing}},
  doi = {10.1088/0034-4885/78/5/052501},
  abstract = {This article reviews the 40+ year old spin-glass field and one of its earliest model interpretations as a spin density wave. Our description is from an experimental phenomenological point of view with emphasis on new spin glass materials and their relation to topical problems and strongly correlated materials in condensed matter physics. We first simply define a spin glass (SG), give its basic ingredients and explain how the spin glasses enter into the statistical mechanics of classical phase transitions. We then consider the four basic experimental properties to solidly characterize canonical spin glass behavior and introduce the early theories and models. Here the spin density wave (SDW) concept is used to explain the difference between a short-range SDW, i.e. a SG and, in contrast, a long-range SDW, i.e. a conventional magnetic phase transition. We continue with the present state of SG, its massive computer simulations and recent proposals of chiral glasses and quantum SG. We then collect and mention the various SG 'spin-off's'. A major section uncovers the fashionable unconventional materials that display SG-like freezing and glassy ground states, such as (high temperature) superconductors, heavy fermions, intermetallics and Heuslers, pyrochlor and spinels, oxides and chalogenides and exotics, e.g. quasicrystals. Some conclusions and future directions complete the review.},
  keywords = {new materials,phase transition,spin glass},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9ZPQ2JTB/Mydosh - 2015 - Spin glasses Redux An updated experimentalmaterials survey.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@mydoshSpinGlassesRedux2015.md}
}

@phdthesis{myhreBayesianOptimalExperimental2021,
  title = {Bayesian Optimal Experimental Design for Studying Synaptic Plasticity},
  author = {Myhre, Emil Alvar},
  year = {2021},
  month = jan,
  abstract = {The brain is the command center for the nervous system for humans, as well as for other species. Neurons are the fundamental cells of the brain, giving rise to the complex and pow- erful functioning brain that we possess by communicating through electrical and chemical signals. Learning and memory are often understood to be induced by changes in neural con- nections. This evolution of neural connectivity is referred to as synaptic plasticity. Insight and understanding of how these mechanisms are driven could be crucial within medical re- search for recognising and understanding neurological disorders. In this work, we approach synaptic plasticity from a mathematical perspective, aiming to devise a statistical inference framework to understand these dynamics. This work extends on the framework presented by Linderman and coauthors [1] for studying synaptic plasticity, where the dynamics are believed to follow some underlying pat- terns, called learning rules. Specifically, employing spike-timing-dependent plasticity STDP learning rules, as suggested in [2]. This work presents a recently developed statistical infer- ence method to infer the learning rule parameters. When applied to real data, the methods performance seems sensitive to external stimulation of the neurons and requires a lot of data to yield confident estimates, which might limit its applicability. Therefore, this work aims to develop a Bayesian optimal experimental design algorithm, which optimises the stimulation in order to minimise the amount of data required for obtaining adequate results. This is a novel approach for studying synaptic plasticity, which to our knowledge has not yet been explored. Experiments on synthetic data show that our algorithm improves significantly on tradi- tional stimulation protocols. The findings were formalised in an article, which could be of great interest to the plasticity community.},
  collaborator = {Dunn, Benjamin Adric and Battistin, Claudia},
  school = {NTNU},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@myhreBayesianOptimalExperimental2021.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/myhreBayesianOptimalExperimental2021-mdnotes.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Myhre_Bayesian optimal experimental design for studying synaptic plasticity.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@myhreBayesianOptimalExperimental.md}
}

@article{myungCountingProbabilityDistributions2000,
  title = {Counting Probability Distributions: {{Differential}} Geometry and Model Selection},
  author = {Myung, In Jae and Balasubramanian, Vijay and Pitt, Mark A.},
  year = {2000},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {97},
  number = {21},
  pages = {11170--11175},
  publisher = {{National Academy of Sciences}},
  doi = {10.1073/pnas.170283897},
  abstract = {A central problem in science is deciding among competing explanations of data containing random errors. We argue that assessing the 'complexity' of explanations is essential to a theoretically well-founded model selection procedure. We formulate model complexity in terms of the geometry of the space of probability distributions. Geometric complexity provides a clear intuitive understanding of several extant notions of model complexity. This approach allows us to reconceptualize the model selection problem as one of counting explanations that lie close to the 'truth.' We demonstrate the usefulness of the approach by applying it to the recovery of models in psychophysics.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7B9CQG7I/Myung, Balasubramanian, Pitt - 2000 - Counting probability distributions Differential geometry and model selection(3).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JJ7NC72T/Myung, Balasubramanian, Pitt - 2000 - Counting probability distributions Differential geometry and model selection(2).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TS8R3J89/Myung, Balasubramanian, Pitt - 2000 - Counting probability distributions Differential geometry and model selection.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@myungCountingProbabilityDistributions2000.md}
}

@article{nachumDataEfficientHierarchicalReinforcement2018,
  title = {Data-{{Efficient Hierarchical Reinforcement Learning}}},
  author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  year = {2018},
  month = oct,
  journal = {arXiv:1805.08296 [cs, stat]},
  eprint = {1805.08296},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: NIPS 2018},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SDJ5PWM7/Nachum et al_2018_Data-Efficient Hierarchical Reinforcement Learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YXU4S6BW/1805.html}
}

@article{naessBiophysicallyDetailedForward2021,
  title = {Biophysically Detailed Forward Modeling of the Neural Origin of {{EEG}} and {{MEG}} Signals},
  author = {N{\ae}ss, Solveig and Halnes, Geir and Hagen, Espen and Hagler, Donald J. and Dale, Anders M. and Einevoll, Gaute T. and Ness, Torbj{\o}rn V.},
  year = {2021},
  month = jan,
  journal = {NeuroImage},
  volume = {225},
  pages = {117467},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2020.117467},
  abstract = {Electroencephalography (EEG) and magnetoencephalography (MEG) are among the most important techniques for non-invasively studying cognition and disease in the human brain. These signals are known to originate from cortical neural activity, typically described in terms of current dipoles. While the link between cortical current dipoles and EEG/MEG signals is relatively well understood, surprisingly little is known about the link between different kinds of neural activity and the current dipoles themselves. Detailed biophysical modeling has played an important role in exploring the neural origin of intracranial electric signals, like extracellular spikes and local field potentials. However, this approach has not yet been taken full advantage of in the context of exploring the neural origin of the cortical current dipoles that are causing EEG/MEG signals. Here, we present a method for reducing arbitrary simulated neural activity to single current dipoles. We find that the method is applicable for calculating extracranial signals, but less suited for calculating intracranial electrocorticography (ECoG) signals. We demonstrate that this approach can serve as a powerful tool for investigating the neural origin of EEG/MEG signals. This is done through example studies of the single-neuron EEG contribution, the putative EEG contribution from calcium spikes, and from calculating EEG signals from large-scale neural network simulations. We also demonstrate how the simulated current dipoles can be used directly in combination with detailed head models, allowing for simulated EEG signals with an unprecedented level of biophysical details. In conclusion, this paper presents a framework for biophysically detailed modeling of EEG and MEG signals, which can be used to better our understanding of non-inasively measured neural activity in humans.},
  langid = {english},
  file = {C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@naessBiophysicallyDetailedForward2021.md;C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Zotero\\storage\\Næss et al_2021_Biophysically detailed forward modeling of the neural origin of EEG and MEG.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AF53Z5ML/S1053811920309526.html}
}

@article{nagabandiNeuralNetworkDynamics2017,
  title = {Neural {{Network Dynamics}} for {{Model-Based Deep Reinforcement Learning}} with {{Model-Free Fine-Tuning}}},
  author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S. and Levine, Sergey},
  year = {2017},
  month = dec,
  journal = {arXiv:1708.02596 [cs]},
  eprint = {1708.02596},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/S883G9CP/Nagabandi et al_2017_Neural Network Dynamics for Model-Based Deep Reinforcement Learning with.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Y4ZJQ6RE/1708.html}
}

@misc{najarroMetaLearningHebbianPlasticity2022,
  title = {Meta-{{Learning}} through {{Hebbian Plasticity}} in {{Random Networks}}},
  author = {Najarro, Elias and Risi, Sebastian},
  year = {2022},
  month = apr,
  number = {arXiv:2007.02686},
  eprint = {2007.02686},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2007.02686},
  abstract = {Lifelong learning and adaptability are two defining aspects of biological agents. Modern reinforcement learning (RL) approaches have shown significant progress in solving complex tasks, however once training is concluded, the found solutions are typically static and incapable of adapting to new information or perturbations. While it is still not completely understood how biological brains learn and adapt so efficiently from experience, it is believed that synaptic plasticity plays a prominent role in this process. Inspired by this biological mechanism, we propose a search method that, instead of optimizing the weight parameters of neural networks directly, only searches for synapse-specific Hebbian learning rules that allow the network to continuously self-organize its weights during the lifetime of the agent. We demonstrate our approach on several reinforcement learning tasks with different sensory modalities and more than 450K trainable plasticity parameters. We find that starting from completely random weights, the discovered Hebbian rules enable an agent to navigate a dynamical 2D-pixel environment; likewise they allow a simulated 3D quadrupedal robot to learn how to walk while adapting to morphological damage not seen during training and in the absence of any explicit reward or error signal in less than 100 timesteps. Code is available at https://github.com/enajx/HebbianMetaLearning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,DRL,hebbian},
  note = {Comment: v5: Typo in initialization values corrected. v4: Typo in equation in 3.1 corrected. v3: Bug that made diagonal patterns appear has been fixed. Simulations have been re-run and plots updated. v2: Figures 1, 7 and Table 1 updated, new results on 4.1 added, typos corrected, references added},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MH8UIMPL/Najarro_Risi_2022_Meta-Learning through Hebbian Plasticity in Random Networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VF7XPYX3/2007.html}
}

@book{nakajimaReservoirComputingTheory2021,
  title = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  shorttitle = {Reservoir {{Computing}}},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  series = {Natural {{Computing Series}}},
  publisher = {{Springer Singapore}},
  address = {{Singapore}},
  doi = {10.1007/978-981-13-1687-6},
  isbn = {9789811316869 9789811316876},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7FXBBHCD/Nakajima and Fischer - 2021 - Reservoir Computing Theory, Physical Implementati.pdf}
}

@article{nakaoTuringPatternsNetworkorganized2010,
  title = {Turing Patterns in Network-Organized Activator--Inhibitor Systems},
  author = {Nakao, Hiroya and Mikhailov, Alexander S},
  year = {2010},
  doi = {10.1038/NPHYS1651},
  abstract = {Turing instability in activator-inhibitor systems provides a paradigm of non-equilibrium self-organization; it has been extensively investigated for biological and chemical processes. Turing instability should also be possible in networks, and general mathematical methods for its treatment have been formulated previously. However, only examples of regular lattices and small networks were explicitly considered. Here we study Turing patterns in large random networks, which reveal striking differences from the classical behaviour. The initial linear instability leads to spontaneous differentiation of the network nodes into activator-rich and activator-poor groups. The emerging Turing patterns become furthermore strongly reshaped at the subsequent nonlinear stage. Multiple coexisting stationary states and hysteresis effects are observed. This peculiar behaviour can be understood in the framework of a mean-field theory. Our results offer a new perspective on self-organization phenomena in systems organized as complex networks. Potential applications include ecological metapopulations, synthetic ecosystems, cellular networks of early biological morphogenesis, and networks of coupled chemical nanoreactors. R eaction-diffusion systems support a wealth of complex self-organized patterns, such as stationary dissipative structures, travelling fronts and pulses, rotating spiral waves or chemical turbulence 1-6. Within the last decade, attention has been brought to a class of models representing their network analogues, where the interacting species occupy network nodes and are diffusively transported across the links 7-11. Such models typically arise when ecological metapopulations with dispersal connections between habitats are considered 12-16 or spreading of infections through transportation networks is investigated 17-21. They can also correspond to networks of diffusively coupled chemical reactors or biological cells 8-11. Network architecture makes the analysis of self-organization difficult, and therefore it has so far been largely restricted to such kinds of non-equilibrium pattern formation as epidemic spreading 19-21 or synchronization 22-24. More complex forms of network self-organization are, however, also possible. In 1952, Turing showed 1 that differences in the diffusion constants of activator and inhibitor species can bring about destabilization of the uniform state and lead to spontaneous emergence of periodic spatial patterns. The Turing patterns can emerge in autocatalytic chemical reactions with inhibition 2-4 , in processes of biological morphogenesis 25-29 , and in ecosystems 30-33. They provide a classical example of complex non-equilibrium self-organization. As early as 1971, Othmer and Scriven 8 pointed out that Turing instability can occur in network-organized systems and may play an important role in the early stages of biological morphogenesis, as morphogens diffuse over a network of intercellular connections. They have proposed a general mathematical framework for the analysis of such network instability, which has been subsequently explored 9-11. The examples of specific applications of the theory were, however, limited to regular lattices 8,9 or small networks 10,11. In studies of network phenomena, characteristic statistical features of collective dynamics were first revealed when large unstructured random networks, such as the Erd\"os-R\'enyi or scale-free networks 34-36 , were considered, for which powerful analytical methods, for example, the mean-field approximation 19-24 , can be applied. Detailed statistical investigations of the emerging stationary Turing patterns in such large random networks thus need to be performed and this is the aim of our present work. Activator-inhibitor systems on networks Activator-inhibitor systems in classical continuous media are described by {$\partial$} {$\partial$}t u(x,t) = f (u,v) + D act 2 u(x,t) {$\partial$} {$\partial$}t v(x,t) = g (u,v) + D inh 2 v(x,t) (1) where u(x, t) and v(x, t) are local densities of the activator and inhibitor species. Functions f (u,v) and g (u,v) specify local dynamics of the activator, which autocatalytically enhances its own production, and of the inhibitor, which suppresses the activator growth. D act and D inh are the diffusion constants of activator and inhibitor species. The Turing instability 1 sets in as the ratio D inh /D act of the two diffusion constants is increased and exceeds a threshold. It leads to spontaneous development of alternating activator-rich and activator-poor domains from the uniform background. The activator and the inhibitor may represent two different chemical species 2-4. In ecological models, the activator typically corresponds to the prey and the inhibitor to the predator 5,16,30,32. In our study, we consider the network analogue of model (1) where activator and inhibitor species occupy discrete nodes of a network and are diffusively transported over links connecting them. The links may represent diffusive connections between chemical reactors or dispersal of ecospecies from one habitat to another. The topology of a network with N nodes is defined by a symmetric adjacency matrix whose elements A ij take A ij = 1 if the nodes i and j (i,j = 1,...,N) are connected (i = j) and A ij = 0 otherwise. The degree (number of connections) of node i is given by k i = N j=1 A ij. For convenience, we always sort network nodes \{i\} in decreasing order of their degrees \{k i \} so that the condition k 1 {$\geq$} k 2 {$\geq$} {$\cdot\cdot\cdot$}k N holds. Diffusive transport of species into a certain node i is given NATURE PHYSICS | ADVANCE ONLINE PUBLICATION | www.nature.com/naturephysics 1},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4B4ULSAD/Nakao, Mikhailov - 2010 - Turing patterns in network-organized activator--inhibitor systems.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@nakaoTuringPatternsNetworkorganized2010.md}
}

@techreport{nameMATH505bFinal,
  title = {{{MATH}} 505b {{Final Project}}: {{MCMC}} and {{Ising Model}}},
  author = {Name, Student and Lu, Zhiyun},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WYR53HMQ/Name, Lu - Unknown - MATH 505b Final Project MCMC and Ising Model.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@nameMATH505bFinal.md}
}

@article{nasserParameterEstimationSpatioTemporal2014,
  title = {Parameter {{Estimation}} for {{Spatio-Temporal Maximum Entropy Distributions}}: {{Application}} to {{Neural Spike Trains}}},
  author = {Nasser, Hassan and Cessac, Bruno},
  year = {2014},
  month = apr,
  journal = {Entropy},
  volume = {16},
  number = {4},
  pages = {2244--2277},
  publisher = {{MDPI AG}},
  doi = {10.3390/e16042244},
  abstract = {We propose a numerical method to learn maximum entropy (MaxEnt) distributions with spatio-temporal constraints from experimental spike trains. This is an extension of two papers, [10] and [4], which proposed the estimation of parameters where only spatial constraints were taken into account. The extension we propose allows one to properly handle memory effects in spike statistics, for large-sized neural networks. \textcopyright{} 2014 by the authors.},
  keywords = {Convex duality,Gibbs distribution,Large-scale analysis,Maximum entropy,MEA recordings,Neural coding,Spatio-temporal constraints,Spike train},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/52LJAEPP/Nasser, Cessac - 2014 - Parameter Estimation for Spatio-Temporal Maximum Entropy Distributions Application to Neural Spike Trains(2).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7VS95HFJ/Nasser, Cessac - 2014 - Parameter estimation for spatio-temporal maximum entropy distributions application to neural spike trains.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@nasserParameterEstimationSpatioTemporal2014.md}
}

@article{nattermannTheoryRandomField1997,
  title = {Theory of the {{Random Field Ising Model}}},
  author = {Nattermann, T.},
  year = {1997},
  month = may,
  journal = {arXiv:cond-mat/9705295},
  eprint = {cond-mat/9705295},
  eprinttype = {arxiv},
  abstract = {A review is given on some recent developments in the theory of the Ising model in a random field. This model is a good representation of a large number of impure materials. After a short repetition of earlier arguments, which prove the absence of ferromagnetic order in \$d\textbackslash le 2\$ space dimensions for uncorrelated random fields, we consider different random field correlations and in particular the generation of uncorrelated from anti-correlated random fields by thermal fluctuations. In discussing the phase transition, we consider the transition to be characterized by a divergent correlation length and compare the critical exponents obtained from various methods (real space RNG, Monte Carlo calculations, weighted mean field theory etc.). The ferromagnetic transition is believed to be preceded by a spin glass transition which manifests itself by replica symmetry breaking. In the discussion of dynamical properties, we concentrate mainly on the zero temperature depinning transition of a domain wall, which represents a critical point far from equilibrium with new scaling relations and critical exponents.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  note = {Comment: contribution to the volume "Spin Glasses and Random Fields", ed. P. Young, World Scientific. Latex file and lprocl.sty (style-file). 23 pages, 5 figures},
  file = {C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Zotero\\storage\\Nattermann_1997_Theory of the Random Field Ising Model.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LYGB7CBS/9705295.html}
}

@article{naudBurstdependentSynapticPlasticity,
  title = {Burst-Dependent Synaptic Plasticity Can Coordinate Learning in Hierarchical Circuits},
  author = {Naud, Richard},
  pages = {30},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YWDG7ALV/Naud - Burst-dependent synaptic plasticity can coordinate.pdf}
}

@article{neckarBraindropMixedSignalNeuromorphic2019,
  title = {Braindrop: {{A Mixed-Signal Neuromorphic Architecture With}} a {{Dynamical Systems-Based Programming Model}}},
  shorttitle = {Braindrop},
  author = {Neckar, Alexander and Fok, Sam and Benjamin, Ben V. and Stewart, Terrence C. and Oza, Nick N. and Voelker, Aaron R. and Eliasmith, Chris and Manohar, Rajit and Boahen, Kwabena},
  year = {2019},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {107},
  number = {1},
  pages = {144--164},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2018.2881432},
  abstract = {Braindrop is the first neuromorphic system designed to be programmed at a high level of abstraction. Previous neuromorphic systems were programmed at the neurosynaptic level and required expert knowledge of the hardware to use. In stark contrast, Braindrop's computations are specified as coupled nonlinear dynamical systems and synthesized to the hardware by an automated procedure. This procedure not only leverages Braindrop's fabric of subthreshold analog circuits as dynamic computational primitives but also compensates for their mismatched and temperature-sensitive responses at the network level. Thus, a clean abstraction is presented to the user. Fabricated in a 28-nm FDSOI process, Braindrop integrates 4096 neurons in 0.65 mm2. Two innovations-sparse encoding through analog spatial convolution and weighted spike-rate summation though digital accumulative thinning-cut digital traffic drastically, reducing the energy Braindrop consumes per equivalent synaptic operation to 381 fJ for typical network configurations.},
  keywords = {Analog circuits,artificial neural networks,Artificial neural networks,asynchronous circuits,Brain models,Computer architecture,Decoding,Encoding,neuromorphics,Neuromorphics,Neurons},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@neckarBraindropMixedSignalNeuromorphic2019.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/neckarBraindropMixedSignalNeuromorphic2019-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/J23Q4KSK/Neckar et al_2019_Braindrop.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AJHWIM6I/8591981.html}
}

@misc{NeuralCorrelatesConsciousness,
  title = {Neural {{Correlates}} of {{Consciousness}}: {{Empirical}} and {{Conceptual Questions}} - {{Google Books}}},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@NeuralCorrelatesConsciousness.md}
}

@misc{NewNeurobiologyLanguage,
  title = {Towards a New Neurobiology of Language - {{PubMed}}},
  howpublished = {https://pubmed.ncbi.nlm.nih.gov/23055482/},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FGUSKN8F/23055482.html}
}

@techreport{nguyenBethePeierlsApproximationInverse2012,
  title = {Bethe-{{Peierls}} Approximation and the Inverse {{Ising}} Model},
  author = {Nguyen, H Chau and Berg, Johannes},
  year = {2012},
  abstract = {We apply the Bethe-Peierls approximation to the problem of the inverse Ising model and show how the linear response relation leads to a simple method to reconstruct couplings and fields of the Ising model. This reconstruction is exact on tree graphs, yet its computational expense is comparable to other mean-field methods. We compare the performance of this method to the independent-pair, naive mean-field, Thouless-Anderson-Palmer approximations, the Sessak-Monasson expansion, and susceptibility propagation in the Cayley tree, SK-model and random graph with fixed connectivity. At low temperatures, Bethe reconstruction outperforms all these methods, while at high temperatures it is comparable to the best method available so far (Sessak-Monasson). The relationship between Bethe reconstruction and other mean-field methods is discussed.},
  keywords = {()},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2VH2JMIT/Nguyen, Berg - 2012 - Bethe-Peierls approximation and the inverse Ising model.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@nguyenBethePeierlsApproximationInverse2012.md}
}

@article{nguyenInverseStatisticalProblems2017,
  title = {Inverse Statistical Problems: From the Inverse {{Ising}} Problem to Data Science},
  author = {Nguyen, H. Chau and Zecchina, Riccardo and Berg, Johannes},
  year = {2017},
  month = jul,
  journal = {Advances in Physics},
  volume = {66},
  number = {3},
  pages = {197--261},
  doi = {10.1080/00018732.2017.1341604},
  abstract = {Inverse problems in statistical physics are motivated by the challenges of `big data' in different fields, in particular high-throughput experiments in biology. In inverse problems, the usual procedure of statistical physics needs to be reversed: Instead of calculating observables on the basis of model parameters, we seek to infer parameters of a model based on observations. In this review, we focus on the inverse Ising problem and closely related problems, namely how to infer the coupling strengths between spins given observed spin correlations, magnetisations, or other data. We review applications of the inverse Ising problem, including the reconstruction of neural connections, protein structure determination, and the inference of gene regulatory networks. For the inverse Ising problem in equilibrium, a number of controlled and uncontrolled approximate solutions have been developed in the statistical mechanics community. A particularly strong method, pseudolikelihood, stems from statistics. We also review the inverse Ising problem in the non-equilibrium case, where the model parameters must be reconstructed based on non-equilibrium statistics.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BVWKJLWT/Nguyen, Zecchina, Berg - 2017 - Inverse statistical problems from the inverse Ising problem to data science(2).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MPG437Z2/Nguyen, Zecchina, Berg - 2017 - Inverse statistical problems from the inverse Ising problem to data science(2).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N9BZFA9J/Nguyen, Zecchina, Berg - 2017 - Inverse statistical problems from the inverse Ising problem to data science.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@nguyenInverseStatisticalProblems2017.md}
}

@article{nicolabulsoProjectProposalInverse2019,
  title = {Project Proposal: Inverse Ising Problem},
  author = {Nicola Bulso, Yasser Roudi},
  year = {2019},
  pages = {1--2},
  abstract = {Advances in multi-electrode tetrode recordings and calcium imaging have opened the way to simul- taneously observe many neurons and study their statistical properties and dependencies. Among the methods employed to study the network of interactions, Maximum Entropy models and Gen- eralized Linear models stem for their popularity in applications [1]. In particular, the Ising model has been largely applied to investigate functional connectivity in real neural datasets (see e.g. [2, 3, 4]). Besides its long history in statistical physics, this model is interesting because it corresponds to a Pairwise Maximum Entropy model: it makes the minimal number of assumptions while requiring that the model distribution should be able to reproduce firing rates and correlations among neurons [5]. However, in the Ising model, the parameter estimation step is computationally demanding and, in order to solve this issue, several approaches based on different approximations have been developed during the last decade. In particular many recently proposed and successful algorithms are based on the so called pseudo-likelihood approximation [6, 7]. This approximation basically consists of substituting the real likelihood function, which is hard to optimize with the sum of many logistic regression functions. These algorithms have been proven to be very accurate and efficient for estimating model parameters [6], but also in the more difficult task of graphical model selection [7, 8, 9]. Model Selection is the ability to reduce the number of model parameters in order to keep only the essential ones (Occam's razor rule). Applied to an Ising model on neural data, it corresponds to identify the structure of the network of functional interactions among neurons.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RYJAA4LL/projects.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@nicolabulsoProjectProposalInverse2019.md}
}

@article{nicolaSupervisedLearningSpiking2017,
  title = {Supervised Learning in Spiking Neural Networks with {{FORCE}} Training},
  author = {Nicola, Wilten and Clopath, Claudia},
  year = {2017},
  month = dec,
  journal = {Nature Communications},
  volume = {8},
  number = {1},
  pages = {2208},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-01827-3},
  abstract = {Populations of neurons display an extraordinary diversity in the behaviors they affect and display. Machine learning techniques have recently emerged that allow us to create networks of model neurons that display behaviors of similar complexity. Here we demonstrate the direct applicability of one such technique, the FORCE method, to spiking neural networks. We train these networks to mimic dynamical systems, classify inputs, and store discrete sequences that correspond to the notes of a song. Finally, we use FORCE training to create two biologically motivated model circuits. One is inspired by the zebra finch and successfully reproduces songbird singing. The second network is motivated by the hippocampus and is trained to store and replay a movie scene. FORCE trained networks reproduce behaviors comparable in complexity to their inspired circuits and yield information not easily obtainable with other techniques, such as behavioral responses to pharmacological manipulations and spike timing statistics.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Dynamical systems,Learning algorithms,Network models,Neural encoding},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3W6N9KGK/Nicola_Clopath_2017_Supervised learning in spiking neural networks with FORCE training.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UEVPXKLU/s41467-017-01827-3.html}
}

@article{nivReinforcementLearningMarr2016,
  title = {Reinforcement Learning with {{Marr}}},
  author = {Niv, Yael and Langdon, Angela},
  year = {2016},
  month = oct,
  journal = {Current Opinion in Behavioral Sciences},
  series = {Computational Modeling},
  volume = {11},
  pages = {67--73},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2016.04.005},
  abstract = {To many, the poster child for David Marr's famous three levels of scientific inquiry is reinforcement learning \textemdash{} a computational theory of reward optimization, which readily prescribes algorithmic solutions that evidence striking resemblance to signals found in the brain, suggesting a straightforward neural implementation. Here we review questions that remain open at each level of analysis, concluding that the path forward to their resolution calls for inspiration across levels, rather than a focus on mutual constraints.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z4F7DVD4/Niv_Langdon_2016_Reinforcement learning with Marr.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/98LNGP7G/S2352154616300821.html}
}

@article{nolanNeuralMechanismsSpatial2016,
  title = {Neural Mechanisms for Spatial Computation},
  author = {Nolan, Matthew F.},
  year = {2016},
  journal = {Journal of Physiology},
  volume = {594},
  number = {22},
  pages = {6487--6488},
  doi = {10.1113/JP273087},
  abstract = {Click on the article title to read more.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NVEFYZZZ/Nolan - 2016 - Neural mechanisms for spatial computation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@nolanNeuralMechanismsSpatial2016.md}
}

@techreport{NoTitle,
  title = {({{No Title}})},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UXM9P623/Unknown - Unknown - (No Title).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@NoTitle.md}
}

@misc{NoTitlea,
  title = {({{No Title}})},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@NoTitlea.md}
}

@misc{NovelModelSelection,
  title = {Novel {{Model Selection Criterion}} for {{Inference}} of {{Ising Models Draft}} 4 - 260321},
  journal = {Google Docs},
  abstract = {Novel Model Selection Criterion for Inference of Ising Models with Mike Follow along with the thesis at Tarlton.info},
  howpublished = {https://docs.google.com/presentation/u/0/d/1kJQDBgkYyYBqRPr1UqgQGlbQK-moUYdVmAusSorRypo/edit?fromCopy=true\&usp=embed\_facebook},
  langid = {english},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@NovelModelSelection.md}
}

@article{nussbaumIsingModelsLatent2019,
  title = {Ising {{Models}} with {{Latent Conditional Gaussian Variables}}},
  author = {Nussbaum, Frank and Giesen, Joachim},
  year = {2019},
  month = jan,
  abstract = {Ising models describe the joint probability distribution of a vector of binary feature variables. Typically, not all the variables interact with each other and one is interested in learning the presumably sparse network structure of the interacting variables. However, in the presence of latent variables, the conventional method of learning a sparse model might fail. This is because the latent variables induce indirect interactions of the observed variables. In the case of only a few latent conditional Gaussian variables these spurious interactions contribute an additional low-rank component to the interaction parameters of the observed Ising model. Therefore, we propose to learn a sparse + low-rank decomposition of the parameters of an Ising model using a convex regularized likelihood problem. We show that the same problem can be obtained as the dual of a maximum-entropy problem with a new type of relaxation, where the sample means collectively need to match the expected values only up to a given tolerance. The solution to the convex optimization problem has consistency properties in the high-dimensional setting, where the number of observed binary variables and the number of latent conditional Gaussian variables are allowed to grow with the number of training samples.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/F2JUMF4Z/Nussbaum, Giesen - 2019 - Ising Models with Latent Conditional Gaussian Variables.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@nussbaumIsingModelsLatent2019.md}
}

@misc{obenhausFunctionalNetworkTopography2021,
  title = {Functional Network Topography of the Medial Entorhinal Cortex},
  author = {Obenhaus, Horst A. and Zong, Weijian and Jacobsen, R. Irene and Rose, Tobias and Donato, Flavio and Chen, Liangyi and Cheng, Heping and Bonhoeffer, Tobias and Moser, May-Britt and Moser, Edvard I.},
  year = {2021},
  month = sep,
  pages = {2021.09.20.461016},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.09.20.461016},
  abstract = {The medial entorhinal cortex (MEC) creates a map of local space, based on the firing patterns of grid, head direction (HD), border, and object-vector (OV) cells. How these cell types are organized anatomically is debated. In-depth analysis of this question requires collection of precise anatomical and activity data across large populations of neurons during unrestrained behavior, which neither electrophysiological nor previous imaging methods fully afford. Here we examined the topographic arrangement of spatially modulated neurons in MEC and adjacent parasubiculum using miniaturized, portable two-photon microscopes, which allow mice to roam freely in open fields. Grid cells exhibited low levels of co-occurrence with OV cells and clustered anatomically, while border, HD and OV cells tended to intermingle. These data suggest that grid-cell networks might be largely distinct from those of border, HD and OV cells and that grid cells exhibit strong coupling among themselves but weaker links to other cell types.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Obenhaus et al_2021_Functional network topography of the medial entorhinal cortex.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/R8GVDZWU/2021.09.20.html}
}

@article{obregonQuantumSuperstatisticsCritical,
  title = {On {{Quantum Superstatistics}} and the {{Critical Behavior}} of {{Nonextensive Ideal Bose Gases}}},
  author = {Obreg{\'o}n, Octavio and Luis L{\'o}pez, Jos{\'e} and {Ortega-Cruz}, Marco},
  doi = {10.3390/e20100773},
  abstract = {We explore some important consequences of the quantum ideal Bose gas, the properties of which are described by a non-extensive entropy. We consider in particular two entropies that depend only on the probability. These entropies are defined in the framework of superstatistics, and in this context, such entropies arise when a system is exposed to non-equilibrium conditions, whose general effects can be described by a generalized Boltzmann factor and correspondingly by a generalized probability distribution defining a different statistics. We generalize the usual statistics to their quantum counterparts, and we will focus on the properties of the corresponding generalized quantum ideal Bose gas. The most important consequence of the generalized Bose gas is that the critical temperature predicted for the condensation changes in comparison with the usual quantum Bose gas. Conceptual differences arise when comparing our results with the ones previously reported regarding the q-generalized Bose-Einstein condensation. As the entropies analyzed here only depend on the probability, our results cannot be adjusted by any parameter. Even though these results are close to those of non-extensive statistical mechanics for q {$\sim$} 1, they differ and cannot be matched for any q.},
  keywords = {Bose-Einstein condensation,Non-Additive Entropies,superstatistics},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z62KV4WZ/Obregón, Luis López, Ortega-Cruz - Unknown - On Quantum Superstatistics and the Critical Behavior of Nonextensive Ideal Bose Gases.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@obregonQuantumSuperstatisticsCritical.md}
}

@article{obuchiLearningProbabilitiesRandom2015,
  title = {Learning Probabilities from Random Observables in High Dimensions: The Maximum Entropy Distribution and Others},
  author = {Obuchi, Tomoyuki and Cocco, Simona and Monasson, R{\'e}mi},
  year = {2015},
  month = mar,
  journal = {Journal of Statistical Physics},
  volume = {161},
  number = {3},
  pages = {598--632},
  publisher = {{Springer New York LLC}},
  doi = {10.1007/s10955-015-1341-7},
  abstract = {We consider the problem of learning a target probability distribution over a set of \$N\$ binary variables from the knowledge of the expectation values (with this target distribution) of \$M\$ observables, drawn uniformly at random. The space of all probability distributions compatible with these \$M\$ expectation values within some fixed accuracy, called version space, is studied. We introduce a biased measure over the version space, which gives a boost increasing exponentially with the entropy of the distributions and with an arbitrary inverse `temperature' \$\textbackslash Gamma\$. The choice of \$\textbackslash Gamma\$ allows us to interpolate smoothly between the unbiased measure over all distributions in the version space (\$\textbackslash Gamma=0\$) and the pointwise measure concentrated at the maximum entropy distribution (\$\textbackslash Gamma \textbackslash to \textbackslash infty\$). Using the replica method we compute the volume of the version space and other quantities of interest, such as the distance \$R\$ between the target distribution and the center-of-mass distribution over the version space, as functions of \$\textbackslash alpha=(\textbackslash log M)/N\$ and \$\textbackslash Gamma\$ for large \$N\$. Phase transitions at critical values of \$\textbackslash alpha\$ are found, corresponding to qualitative improvements in the learning of the target distribution and to the decrease of the distance \$R\$. However, for fixed \$\textbackslash alpha\$, the distance \$R\$ does not vary with \$\textbackslash Gamma\$, which means that the maximum entropy distribution is not closer to the target distribution than any other distribution compatible with the observable values. Our results are confirmed by Monte Carlo sampling of the version space for small system sizes (\$N\textbackslash le 10\$).},
  keywords = {Maximum entropy principle,Probabilistic inference,Replica method},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5ILGUHBW/Obuchi, Cocco, Monasson - 2015 - Learning probabilities from random observables in high dimensions the maximum entropy distribution and.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@obuchiLearningProbabilitiesRandom2015.md}
}

@inproceedings{obuchiLearningProbabilityDistributions2015,
  title = {Learning Probability Distributions from Smooth Observables and the Maximum Entropy Principle: {{Some}} Remarks},
  booktitle = {Journal of {{Physics}}: {{Conference Series}}},
  author = {Obuchi, Tomoyuki and Monasson, R{\'e}mi},
  year = {2015},
  month = sep,
  volume = {638},
  pages = {12018--12018},
  publisher = {{Institute of Physics Publishing}},
  doi = {10.1088/1742-6596/638/1/012018},
  abstract = {The maximum entropy principle (MEP) is a very useful working hypothesis in a wide variety of inference problems, ranging from biological to engineering tasks. To better understand the reasons of the success of MEP, we propose a statistical-mechanical formulation to treat the space of probability distributions constrained by the measures of (experimental) observables. In this paper we first review the results of a detailed analysis of the simplest case of randomly chosen observables. In addition, we investigate by numerical and analytical means the case of smooth observables, which is of practical relevance. Our preliminary results are presented and discussed with respect to the efficiency of the MEP.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7U3F638T/Obuchi, Monasson - 2015 - Learning probability distributions from smooth observables and the maximum entropy principle Some remarks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@obuchiLearningProbabilityDistributions2015.md}
}

@article{ochsnerFunctionalImagingStudies2012,
  title = {Functional Imaging Studies of Emotion Regulation: A Synthetic Review and Evolving Model of the Cognitive Control of Emotion.},
  author = {Ochsner, Kevin N and Silvers, Jennifer A and Buhle, Jason T},
  year = {2012},
  month = mar,
  journal = {Annals of the New York Academy of Sciences},
  volume = {1251},
  pages = {E1-24},
  publisher = {{NIH Public Access}},
  doi = {10.1111/j.1749-6632.2012.06751.x},
  abstract = {This paper reviews and synthesizes functional imaging research that over the past decade has begun to offer new insights into the brain mechanisms underlying emotion regulation. Toward that end, the first section of the paper outlines a model of the processes and neural systems involved in emotion generation and regulation. The second section surveys recent research supporting and elaborating the model, focusing primarily on studies of the most commonly investigated strategy, which is known as reappraisal. At its core, the model specifies how prefrontal and cingulate control systems modulate activity in perceptual, semantic, and affect systems as a function of one's regulatory goals, tactics, and the nature of the stimuli and emotions being regulated. This section also shows how the model can be generalized to understand the brain mechanisms underlying other emotion regulation strategies as well as a range of other allied phenomena. The third and last section considers directions for future research, including how basic models of emotion regulation can be translated to understand changes in emotion across the life span and in clinical disorders.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WNDG585N/Ochsner, Silvers, Buhle - 2012 - Functional imaging studies of emotion regulation a synthetic review and evolving model of the cognitive.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ochsnerFunctionalImagingStudies2012.md}
}

@article{odonnellPopulationTrackingModel2017,
  title = {The {{Population Tracking Model}}: {{A Simple}}, {{Scalable Statistical Model}} for {{Neural Population Data}}},
  shorttitle = {The {{Population Tracking Model}}},
  author = {O'Donnell, Cian and Gon{\c c}alves, J. Tiago and Whiteley, Nick and {Portera-Cailliau}, Carlos and Sejnowski, Terrence J.},
  year = {2017},
  month = jan,
  journal = {Neural Computation},
  volume = {29},
  number = {1},
  pages = {50--93},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00910},
  abstract = {Our understanding of neural population coding has been limited by a lack of analysis methods to characterize spiking data from large populations. The biggest challenge comes from the fact that the number of possible network activity patterns scales exponentially with the number of neurons recorded (). Here we introduce a new statistical method for characterizing neural population activity that requires semi-independent fitting of only as many parameters as the square of the number of neurons, requiring drastically smaller data sets and minimal computation time. The model works by matching the population rate (the number of neurons synchronously active) and the probability that each individual neuron fires given the population rate. We found that this model can accurately fit synthetic data from up to 1000 neurons. We also found that the model could rapidly decode visual stimuli from neural population data from macaque primary visual cortex about 65~ms after stimulus onset. Finally, we used the model to estimate the entropy of neural population activity in developing mouse somatosensory cortex and, surprisingly, found that it first increases, and then decreases during development. This statistical model opens new options for interrogating neural population data and can bolster the use of modern large-scale in vivo Ca and voltage imaging tools.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YHH9S2DV/O’Donnell et al. - 2017 - The Population Tracking Model A Simple, Scalable .pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@odonnellPopulationTrackingModel2017.md;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\odonnellPopulationTrackingModel2017-zotero.md;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\odonnellPopulationTrackingModel2017-zotero.md;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\odonnellPopulationTrackingModel2017.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IDLFPS2M/The-Population-Tracking-Model-A-Simple-Scalable.html}
}

@article{ohiorhenuanSparseCodingHighorder2010,
  title = {Sparse Coding and High-Order Correlations in Fine-Scale Cortical Networks},
  author = {Ohiorhenuan, Ifije E. and Mechler, Ferenc and Purpura, Keith P. and Schmid, Anita M. and Hu, Qin and Victor, Jonathan D.},
  year = {2010},
  month = jul,
  journal = {Nature},
  volume = {466},
  number = {7306},
  pages = {617--621},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nature09178},
  abstract = {Connectivity in the cortex is organized at multiple scales, suggesting that scale-dependent correlated activity is particularly important for understanding the behaviour of sensory cortices and their function in stimulus encoding. We analysed the scale-dependent structure of cortical interactions by using maximum entropy models to characterize multiple-tetrode recordings from primary visual cortex of anaesthetized macaque monkeys (Macaca mulatta). We compared the properties of firing patterns among local clusters of neurons ({$<$}300\^g\texteuro\textperthousand{} 1/4m apart) with those of neurons separated by larger distances (600\^g\texteuro "2,500\^g\texteuro\textperthousand{} 1/4m). Here we report that local firing patterns are distinctive: whereas multi-neuronal firing patterns at larger distances can be predicted by pairwise interactions, patterns within local clusters often show evidence of high-order correlations. Surprisingly, these local correlations are flexible and rapidly reorganized by visual input. Although they modestly reduce the amount of information that a cluster conveys, they also modify the format of this information, creating sparser codes by increasing the periods of total quiescence, and concentrating information into briefer periods of common activity. These results imply a hierarchical organization of neuronal correlations: simple pairwise correlations link neurons over scales of tens to hundreds of minicolumns, but on the scale of a few minicolumns, ensembles of neurons form complex subnetworks whose moment-to-moment effective connectivity is dynamically reorganized by the stimulus. \textcopyright{} 2010 Macmillan Publishers Limited. All rights reserved.},
  keywords = {Neurophysiology,Visual system},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JFING92E/Ohiorhenuan et al. - 2010 - Sparse coding and high-order correlations in fine-scale cortical networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ohiorhenuanSparseCodingHighorder2010.md}
}

@article{ohtaIntrinsicBurstsFacilitate2022,
  title = {Intrinsic Bursts Facilitate Learning of {{L\'evy}} Flight Movements in Recurrent Neural Network Models},
  author = {Ohta, Morihiro and Asabuki, Toshitake and Fukai, Tomoki},
  year = {2022},
  month = mar,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {4951},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-08953-z},
  abstract = {Isolated spikes and bursts of spikes are thought to provide the two major modes of information coding by neurons. Bursts are known to be crucial for fundamental processes between neuron pairs, such as neuronal communications and synaptic plasticity. Neuronal bursting also has implications in neurodegenerative diseases and mental disorders. Despite these findings on the roles of bursts, whether and how bursts have an advantage over isolated spikes in the network-level computation remains elusive. Here, we demonstrate in a computational model that not isolated spikes, but intrinsic bursts can greatly facilitate learning of L\'evy flight random walk trajectories by synchronizing burst onsets across a neural population. L\'evy flight is a hallmark of optimal search strategies and appears in cognitive behaviors such as saccadic eye movements and memory retrieval. Our results suggest that bursting is crucial for sequence learning by recurrent neural networks when sequences comprise long-tailed distributed discrete jumps.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Dynamical systems,Learning algorithms,Network models},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Ohta et al_2022_Intrinsic bursts facilitate learning of Lévy flight movements in recurrent.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6347G2JX/s41598-022-08953-z.html}
}

@article{oizumiPhenomenologyMechanismsConsciousness2014,
  title = {From the {{Phenomenology}} to the {{Mechanisms}} of {{Consciousness}}: {{Integrated Information Theory}} 3.0},
  author = {Oizumi, Masafumi and Albantakis, Larissa and Tononi, Giulio},
  year = {2014},
  journal = {PLoS Computational Biology},
  volume = {10},
  number = {5},
  publisher = {{Public Library of Science}},
  doi = {10.1371/journal.pcbi.1003588},
  abstract = {This paper presents Integrated Information Theory (IIT) of consciousness 3.0, which incorporates several advances over previous formulations. IIT starts from phenomenological axioms: information says that each experience is specific \textendash{} it is what it is by how it differs from alternative experiences; integration says that it is unified \textendash{} irreducible to non-interdependent components; exclusion says that it has unique borders and a particular spatio-temporal grain. These axioms are formalized into postulates that prescribe how physical mechanisms, such as neurons or logic gates, must be configured to generate experience (phenomenology). The postulates are used to define intrinsic information as ``differences that make a difference'' within a system, and integrated information as information specified by a whole that cannot be reduced to that specified by its parts. By applying the postulates both at the level of individual mechanisms and at the level of systems of mechanisms, IIT arrives at an identity: an experience is a maximally irreducible conceptual structure (MICS, a constellation of concepts in qualia space), and the set of elements that generates it constitutes a complex. According to IIT, a MICS specifies the quality of an experience and integrated information {$\Phi$}Max its quantity. From the theory follow several results, including: a system of mechanisms may condense into a major complex and non-overlapping minor complexes; the concepts that specify the quality of an experience are always about the complex itself and relate only indirectly to the external environment; anatomical connectivity influences complexes and associated MICS; a complex can generate a MICS even if its elements are inactive; simple systems can be minimally conscious; complicated systems can be unconscious; there can be true ``zombies'' \textendash{} unconscious feed-forward systems that are functionally equivalent to conscious complexes.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6SCDUGUN/Oizumi, Albantakis, Tononi - 2014 - From the Phenomenology to the Mechanisms of Consciousness Integrated Information Theory 3.0.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@oizumiPhenomenologyMechanismsConsciousness2014.md}
}

@article{okunDiverseCouplingNeurons2015,
  title = {Diverse Coupling of Neurons to Populations in Sensory Cortex},
  author = {Okun, Michael and Steinmetz, Nicholas A. and Cossell, Lee and Iacaruso, M. Florencia and Ko, Ho and Barth{\'o}, P{\'e}ter and Moore, Tirin and Hofer, Sonja B. and {Mrsic-Flogel}, Thomas D. and Carandini, Matteo and Harris, Kenneth D.},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {511--515},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14273},
  abstract = {Exploring the relationship between population coupling and neuronal activity reveals that neighbouring neurons can differ in their coupling to the overall firing rate of the population, the circuitry of which may potentially help to explain the complex activity patterns in cortical populations.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LREC9IVJ/Okun et al. - 2015 - Diverse coupling of neurons to populations in sens.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@okunDiverseCouplingNeurons2015.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YAZ3TPCP/nature14273.html}
}

@article{oladipupoResearchConceptLiquid2019,
  title = {Research on the {{Concept}} of {{Liquid State Machine}}},
  author = {Oladipupo, Gideon Gbenga},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.03354 [cs]},
  eprint = {1910.03354},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Liquid State Machine (LSM) is a neural model with real time computations which transforms the time varying inputs stream to a higher dimensional space. The concept of LSM is a novel field of research in biological inspired computation with most research effort on training the model as well as finding the optimum learning method. In this review, the performance of LSM model was investigated using two learning method, online learning and offline (batch) learning methods. The review revealed that optimal performance of LSM was recorded through online method as computational space and other complexities associated with batch learning is eliminated.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  note = {Comment: 12 pages, 7 figures and 1 table},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MKJHGAG9/Oladipupo_2019_Research on the Concept of Liquid State Machine.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DIYMU4KI/1910.html}
}

@techreport{opperAdvancedMeanField2002,
  title = {Advanced {{Mean Field Methods}}: {{Theory}} and {{Practice}}},
  author = {Opper, M and Saad, D},
  year = {2002},
  journal = {IEEE Transactions on Neural Networks},
  volume = {13},
  number = {0262150549},
  pages = {785--786},
  abstract = {Several problems in statistical physics, information sciences or neural computing require probabilistic modeling of multivariable systems composed of a large number of variables. Typically, the exact calculation of such models is computationally infeasible, hence there is a strong need for efficient, approximate methods in this area. One type of such methods well known to the statistical physics community are the Mean Field Methods (MFMs) in which the value of each random variable is approximated by the so-called effective field. Consequently, in the simple MFM approach the true (yet generally intractable) probability distribution of random variables is approximated by a factorized distribution, which then can be approached by variational optimization methods. More advanced MFMs are based on the TAP (Thouless, Anderson, Palmer) method which incorporates several non-trivial dependencies between variables neglected in simple MFM approximations. MFMs were originally introduced and developed for calculations of the spin-glass models in quantum mechanics or, equivalently, for the Hopfield-type models in neural networks. Recently, they have been applied extensively in the rapidly growing field of probabilistic graphical models. These models are "a marriage between probability theory and graph theory. They provide a natural tool for dealing with two problems that occur throughout applied mathematics and engineering-uncertainty and complexity-and in particular they are playing an increasingly important role in the design and analysis of machine learning algorithms\ldots "-Michael Jordan. One of the recent events related to Mean Field Methods was the NIPS 1999 Workshop on Advanced Mean Field Methods organized by Manfred Opper and David Saad. The book discussed here is a collection of articles presented at the Workshop, with a few other field-related papers. The book has 17 chapters, each of which presenting a separate piece of research and allowing for reading in isolation. Chapter 1, written by the Editors, outlines the book's scope and summarizes its content chapter by chapter. Chapters 2 and 3 provide a concise and comprehensive introduction to the main MFMs from the statistical physics point of view. The next five chapters are generally devoted to TAP-type approaches. Chapters 4, 5 and 7 offer generalizations of the classical TAP method by deriving TAP equations under less restricted assumptions. In Chapter 4, the approach based on a Taylor series expansion of the marginal probabilities is proposed and applied to several arbitrary probability distributions, for example those arising in stochastic neural networks with asymmetric couplings or in sigmoid belief networks. In Chapter 5, the approach developed for highly coupled systems with intensive connectivity is experimentally tested against the classical TAP formulation in case of extensively connected systems, such as the Hopfield associative memories. Chapter 7 presents a novel TAP-type approach to models defined by quadratic interactions, which does not require specific assumptions on the randomness of couplings (typical for the TAP formulation). Chapters 6 and 8 are devoted to applications of TAP-like methods: respectively, to the problem of decoding corrupted codewords that were encoded by the sparse parity-check error-correcting codes and to the average case performance analysis of the stochastic batch mode learning algorithms for one layer perceptron.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5CBKSJ9T/Opper, Saad - 2002 - Advanced Mean Field Methods Theory and Practice.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@opperAdvancedMeanField2002.md}
}

@techreport{opperTheorySolvingTAP2016,
  title = {A {{Theory}} of {{Solving TAP Equations}} for {{Ising Models}} with {{General Invariant Random Matrices}}},
  author = {Opper, Manfred and Winther, Ole},
  year = {2016},
  abstract = {We consider the problem of solving TAP mean field equations by iteration for Ising models with coupling matrices that are drawn at random from general invariant ensembles. We develop an analysis of iterative algorithms using a dynamical functional approach that in the thermodynamic limit yields an effective dynamics of a single variable trajectory. Our main novel contribution is the expression for the implicit memory term of the dynamics for general invariant ensembles. By subtracting these terms, that depend on magnetizations at previous time steps, the implicit memory terms cancel making the iteration dependent on a Gaussian distributed field only. The TAP magnetizations are stable fixed points if an AT stability criterion is fulfilled. We illustrate our method explicitly for coupling matrices drawn from the random orthogonal ensemble.},
  keywords = {0510-a,7510Nr Keywords: Ising Models,Dynamical Functional Theory,Free Probability,Iterative Convergent Algorithms,numbers: 0250r,Random Matrices,TAP Equations},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SZWLW6LW/Opper, Winther - 2016 - A Theory of Solving TAP Equations for Ising Models with General Invariant Random Matrices.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@opperTheorySolvingTAP2016.md}
}

@article{orlandiTransferEntropyReconstruction2014,
  title = {Transfer Entropy Reconstruction and Labeling of Neuronal Connections from Simulated Calcium Imaging},
  author = {Orlandi, Javier G. and Stetter, Olav and Soriano, Jordi and Geisel, Theo and Battaglia, Demian},
  year = {2014},
  month = jun,
  journal = {PLoS ONE},
  volume = {9},
  number = {6},
  pages = {98842--98842},
  publisher = {{Public Library of Science}},
  doi = {10.1371/journal.pone.0098842},
  abstract = {Neuronal dynamics are fundamentally constrained by the underlying structural network architecture, yet much of the details of this synaptic connectivity are still unknown even in neuronal cultures in vitro. Here we extend a previous approach based on information theory, the Generalized Transfer Entropy, to the reconstruction of connectivity of simulated neuronal networks of both excitatory and inhibitory neurons. We show that, due to the model-free nature of the developed measure, both kinds of connections can be reliably inferred if the average firing rate between synchronous burst events exceeds a small minimum frequency. Furthermore, we suggest, based on systematic simulations, that even lower spontaneous interburst rates could be raised to meet the requirements of our reconstruction algorithm by applying a weak spatially homogeneous stimulation to the entire network. By combining multiple recordings of the same in silico network before and after pharmacologically blocking inhibitory synaptic transmission, we show then how it becomes possible to infer with high confidence the excitatory or inhibitory nature of each individual neuron. \textcopyright{} 2014 Orlandi et al.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6WYJ3YAS/Orlandi et al. - 2014 - Transfer entropy reconstruction and labeling of neuronal connections from simulated calcium imaging.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@orlandiTransferEntropyReconstruction2014.md}
}

@techreport{ostilliCayleyTreesBethe2012,
  title = {Cayley {{Trees}} and {{Bethe Lattices}}, a Concise Analysis for Mathematicians and Physicists},
  author = {Ostilli, M},
  year = {2012},
  abstract = {We review critically the concepts and the applications of Cayley Trees and Bethe Lattices in statistical mechanics in a tentative effort to remove widespread misuse of these simple, but yet important-and different-ideal graphs. We illustrate, in particular, two rigorous techniques to deal with Bethe Lattices, based respectively on self-similarity and on the Kolmogorov consistency theorem, linking the latter with the Cavity and Belief Propagation methods, more known to the physics community.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RF3GUUC8/Ostilli - 2012 - Cayley Trees and Bethe Lattices, a concise analysis for mathematicians and physicists.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ostilliCayleyTreesBethe2012.md}
}

@article{ouyangTransitionUniformState1991,
  title = {Transition from a Uniform State to Hexagonal and Striped {{Turing}} Patterns},
  author = {Ouyang, Q. and Swinney, Harry L.},
  year = {1991},
  month = aug,
  journal = {Nature},
  volume = {352},
  number = {6336},
  pages = {610--612},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/352610a0},
  abstract = {CHEMICAL travelling waves have been studied experimentally for more than two decades1\textendash 5, but the stationary patterns predicted by Turing6 in 1952 were observed only recently7\textendash 9, as patterns localized along a band in a gel reactor containing a concentration gradient in reagents. The observations are consistent with a mathematical model for their geometry of reactor10 (see also ref. 11). Here we report the observation of extended (quasi-two-dimensional) Turing patterns and of a Turing bifurcation\textemdash a transition, as a control parameter is varied, from a spatially uniform state to a patterned state. These patterns form spontaneously in a thin disc-shaped gel in contact with a reservoir of reagents of the chlorite\textendash iodide\textendash malonic acid reaction12. Figure 1 shows examples of the hexagonal, striped and mixed patterns that can occur. Turing patterns have similarities to hydrodynamic patterns (see, for example, ref. 13), but are of particular interest because they possess an intrinsic wavelength and have a possible relationship to biological patterns14\textendash 17.},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6XS2KMWU/Ouyang, Swinney - 1991 - Transition from a uniform state to hexagonal and striped Turing patterns.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ouyangTransitionUniformState1991.md}
}

@article{panchenkoIntroductionSKModel2014,
  title = {Introduction to the {{SK}} Model},
  author = {Panchenko, Dmitry},
  year = {2014},
  month = nov,
  journal = {Current Developments in Mathematics},
  volume = {2014},
  number = {1},
  pages = {231--291},
  publisher = {{International Press of Boston}},
  abstract = {This is a review paper for the "Current Developments in Mathematics 2014" conference.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EH5QMBV2/Panchenko - 2014 - Introduction to the SK model.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@panchenkoIntroductionSKModel2014.md}
}

@article{pandarinathInferringSingletrialNeural2018,
  title = {Inferring Single-Trial Neural Population Dynamics Using Sequential Auto-Encoders},
  author = {Pandarinath, Chethan and O'Shea, Daniel J. and Collins, Jasmine and Jozefowicz, Rafal and Stavisky, Sergey D. and Kao, Jonathan C. and Trautmann, Eric M. and Kaufman, Matthew T. and Ryu, Stephen I. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V. and Abbott, L. F. and Sussillo, David},
  year = {2018},
  journal = {Nature Methods},
  volume = {15},
  number = {10},
  pages = {805--815},
  publisher = {{Springer US}},
  issn = {1087-0792},
  doi = {10.1038/s41592-018-0109-9},
  abstract = {Neuroscience is experiencing a data revolution in which simultaneous recording of many hundreds or thousands of neurons is revealing structure in population activity that is not apparent from single-neuron responses. This structure is typically extracted from trial-averaged data. Single-trial analyses are challenging due to incomplete sampling of the neural population, trial-to-trial variability, and fluctuations in action potential timing. Here we introduce Latent Factor Analysis via Dynamical Systems (LFADS), a deep learning method to infer latent dynamics from single-trial neural spiking data. LFADS uses a nonlinear dynamical system (a recurrent neural network) to infer the dynamics underlying observed population activity and to extract `de-noised' single-trial firing rates from neural spiking data. We apply LFADS to a variety of monkey and human motor cortical datasets, demonstrating its ability to predict observed behavioral variables with unprecedented accuracy, extract precise estimates of neural dynamics on single trials, infer perturbations to those dynamics that correlate with behavioral choices, and combine data from non-overlapping recording sessions (spanning months) to improve inference of underlying dynamics. In summary, LFADS leverages all observations of a neural population's activity to accurately model its dynamics on single trials, opening the door to a detailed understanding of the role of dynamics in performing computation and ultimately driving behavior.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AMA9JI3N/Pandarinath et al. - 2018 - Inferring single-trial neural population dynamics using sequential auto-encoders.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@pandarinathInferringSingletrialNeural2018.md}
}

@inproceedings{parisottoStabilizingTransformersReinforcement2020,
  title = {Stabilizing {{Transformers}} for {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Parisotto, Emilio and Song, Francis and Rae, Jack and Pascanu, Razvan and Gulcehre, Caglar and Jayakumar, Siddhant and Jaderberg, Max and Kaufman, Rapha{\"e}l Lopez and Clark, Aidan and Noury, Seb and Botvinick, Matthew and Heess, Nicolas and Hadsell, Raia},
  year = {2020},
  month = nov,
  pages = {7487--7498},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP). Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6ZX8LLE6/Parisotto et al. - 2020 - Stabilizing Transformers for Reinforcement Learnin.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/88MT2SFL/Parisotto et al_2020_Stabilizing Transformers for Reinforcement Learning.pdf}
}

@article{Parr2020,
  title = {Modules or Mean-Fields?},
  author = {Parr, Thomas and Sajid, Noor and Friston, Karl J.},
  year = {2020},
  journal = {Entropy},
  volume = {22},
  number = {5},
  pages = {1--25},
  issn = {10994300},
  doi = {10.3390/E22050552},
  abstract = {The segregation of neural processing into distinct streams has been interpreted by some as evidence in favour of a modular view of brain function. This implies a set of specialised 'modules', each of which performs a specific kind of computation in isolation of other brain systems, before sharing the result of this operation with other modules. In light of a modern understanding of stochastic non-equilibrium systems, like the brain, a simpler and more parsimonious explanation presents itself. Formulating the evolution of a non-equilibrium steady state system in terms of its density dynamics reveals that such systems appear on average to perform a gradient ascent on their steady state density. If this steady state implies a sufficiently sparse conditional independency structure, this endorses a mean-field dynamical formulation. This decomposes the density over all states in a system into the product of marginal probabilities for those states. This factorisation lends the system a modular appearance, in the sense that we can interpret the dynamics of each factor independently. However, the argument here is that it is factorisation, as opposed to modularisation, that gives rise to the functional anatomy of the brain or, indeed, any sentient system. In the following, we briefly overview mean-field theory and its applications to stochastic dynamical systems. We then unpack the consequences of this factorisation through simple numerical simulations and highlight the implications for neuronal message passing and the computational architecture of sentience.},
  keywords = {Bayesian mechanics,Density dynamics,Message passing,Modularity,Stochastic dynamics},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3S9BWUVH/Parr, Sajid, Friston - 2020 - Modules or mean-fields.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5VLNYESZ/Parr, Sajid, Friston - 2020 - Modules or mean-fields.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@Parr2020.md}
}

@article{pastoreIdentificationExcitatoryinhibitoryLinks2018,
  title = {Identification of Excitatory-Inhibitory Links and Network Topology in Large-Scale Neuronal Assemblies from Multi-Electrode Recordings},
  author = {Pastore, Vito Paolo and Massobrio, Paolo and Godjoski, Aleksandar and Martinoia, Sergio},
  year = {2018},
  journal = {PLoS Computational Biology},
  volume = {14},
  number = {8},
  pages = {1--25},
  issn = {1111111111},
  doi = {10.1371/journal.pcbi.1006381},
  abstract = {Functional-effective connectivity and network topology are nowadays key issues for studying brain physiological functions and pathologies. Inferring neuronal connectivity from electrophysiological recordings presents open challenges and unsolved problems. In this work, we present a cross-correlation based method for reliably estimating not only excitatory but also inhibitory links, by analyzing multi-unit spike activity from large-scale neuronal networks. The method is validated by means of realistic simulations of large-scale neuronal populations. New results related to functional connectivity estimation and network topology identification obtained by experimental electrophysiological recordings from high-density and large-scale (i.e., 4096 electrodes) microtransducer arrays coupled to in vitro neural populations are presented. Specifically, we show that: (i) functional inhibitory connections are accurately identified in in vitro cortical networks, providing that a reasonable firing rate and recording length are achieved; (ii) small-world topology, with scale-free and rich-club features are reliably obtained, on condition that a minimum number of active recording sites are available. The method and procedure can be directly extended and applied to in vivo multi-units brain activity recordings.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ICEGLGV5/Pastore et al. - 2018 - Identification of excitatory-inhibitory links and network topology in large-scale neuronal assemblies from multi.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@pastoreIdentificationExcitatoryinhibitoryLinks2018.md}
}

@article{pateriaHierarchicalReinforcementLearning2021,
  title = {Hierarchical {{Reinforcement Learning}}: {{A Comprehensive Survey}}},
  shorttitle = {Hierarchical {{Reinforcement Learning}}},
  author = {Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai},
  year = {2021},
  month = jun,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {5},
  pages = {109:1--109:35},
  issn = {0360-0300},
  doi = {10.1145/3453160},
  abstract = {Hierarchical Reinforcement Learning (HRL) enables autonomous decomposition of challenging long-horizon decision-making tasks into simpler subtasks. During the past years, the landscape of HRL research has grown profoundly, resulting in copious approaches. A comprehensive overview of this vast landscape is necessary to study HRL in an organized manner. We provide a survey of the diverse HRL approaches concerning the challenges of learning hierarchical policies, subtask discovery, transfer learning, and multi-agent learning using HRL. The survey is presented according to a novel taxonomy of the approaches. Based on the survey, a set of important open problems is proposed to motivate the future research in HRL. Furthermore, we outline a few suitable task domains for evaluating the HRL approaches and a few interesting examples of the practical applications of HRL in the Supplementary Material.},
  keywords = {Hierarchical reinforcement learning,hierarchical reinforcement learning survey,hierarchical reinforcement learning taxonomy,HRL,skill discovery,subtask discovery},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@pateriaHierarchicalReinforcementLearning2021.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/pateriaHierarchicalReinforcementLearning2021-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z5277BBT/Pateria et al_2021_Hierarchical Reinforcement Learning.pdf}
}

@article{patino-saucedoEventdrivenImplementationDeep2020,
  title = {Event-Driven Implementation of Deep Spiking Convolutional Neural Networks for Supervised Classification Using the {{SpiNNaker}} Neuromorphic Platform},
  author = {{Pati{\~n}o-Saucedo}, Alberto and {Rostro-Gonzalez}, Horacio and {Serrano-Gotarredona}, Teresa and {Linares-Barranco}, Bernab{\'e}},
  year = {2020},
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  pages = {319--328},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.09.008},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RCB68GY8/Patiño-Saucedo et al_2020_Event-driven implementation of deep spiking convolutional neural networks for.pdf}
}

@article{patonNeuralBasisTiming2018,
  title = {The {{Neural Basis}} of {{Timing}}: {{Distributed Mechanisms}} for {{Diverse Functions}}},
  shorttitle = {The {{Neural Basis}} of {{Timing}}},
  author = {Paton, Joseph J. and Buonomano, Dean V.},
  year = {2018},
  month = may,
  journal = {Neuron},
  volume = {98},
  number = {4},
  pages = {687--705},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.03.045},
  abstract = {Timing is critical to most forms of learning, behavior, and sensory-motor processing. Converging evidence supports the notion that, precisely because of its importance across a wide range of brain functions, timing relies on intrinsic and general properties of neurons and neural circuits; that is, the brain uses its natural cellular and network dynamics to solve a diversity of temporal computations. Many circuits have been shown to encode elapsed time in dynamically changing patterns of neural activity\textemdash so-called population clocks. But temporal processing encompasses a wide range of different computations, and just as there are different circuits and mechanisms underlying computations about space, there are a multitude of circuits and mechanisms underlying the ability to tell time and generate temporal patterns.},
  langid = {english},
  keywords = {Animals,Anticipation; Psychological,Behavior,Biological Clocks,Brain,Cognition,Humans,Learning,Models; Neurological,Neurons,Time,Time Perception},
  note = {\section{Annotations\\
(6/21/2022, 8:45:10 AM)}

\par
``studies in rodents have confirmed the lack of any direct relationship between circadian timing and interval timing on the scale of seconds (Lewis et al., 2003; Cordes and Gallistel, 2008; Papachristos et al., 2011)\textemdash of course, because the circadian rhythms modulate a wide variety of cognitive and physiological functions, it can affect performance on a wide range of tasks, including timing tasks (Golombek et al., 2014)'' (Paton and Buonomano, 2018, p. 687)
\par
``the mechanisms underlying timing on the intermediate scale of tens of milliseconds to tens of seconds remains a mystery'' (Paton and Buonomano, 2018, p. 687) The open question
\par
``these models loosely fit into two broad classes: dedicated and intrinsic models (Ivry and Schlerf, 2008). Dedicated models propose that the brain has a more or less centralized set of circuits for timing that account for timing across modalities, tasks, and scales within the range of hundreds of milliseconds to many seconds. In these models, timing relies on dedicated or specialized neural mechanisms. Intrinsic models propose that timing is an intrinsic computation of most neural circuits, and timing per se emerges from general properties of neurons and the inherent dynamics of neural circuits.'' (Paton and Buonomano, 2018, p. 688) So what I want to say is that timing is an emergent property of the timing menchanisms
\par
``Here, we argue that converging data strongly support intrinsic models. Indeed, we suggest that given the importance and universality of temporal computations, dedicated models would not make computational sense. This does not imply that there are not some brain areas involved in a range of temporal tasks that share similar temporal processing requirements, but rather that distinct temporal computations, such as processing a Morse code message and anticipating when a traffic light will change, rely on distinct circuits and mechanisms. Under this view, areas that are consistently implicated in timing tasks should not be thought of as a central clock, but as areas that are involved in tasks that are inherently temporal in naturee.g., since preparing and producing motor responses are inherently temporal in nature, motor areas should be consistently implicated in timing.'' (Paton and Buonomano, 2018, p. 688) And they argue the same
\par
``How the brain processes information about space provides a useful analogy for the intrinsic timing perspective. Like the temporal dimension, the spatial dimension permeates much of what the brain must accomplish, from localizing the position of objects in space, to guiding movements to grasp objects, and creating large-scale maps for spatial navigation. Mammals have many different maps of external space, including those in the colliculi, auditory cortex, visual cortex, hippocampus, and parietal cortex (Knudsen et al., 1987; Kandel et al., 2013). The multitude of spatial representations within the brain can map onto each other and form more general polymodal maps in the parietal cortex. Furthermore, consistent with the intrinsic perspective of timing, different maps of external space are computed in different ways and make distinct contributions to sensori-motor processing and cognition.'' (Paton and Buonomano, 2018, p. 688) overlap in representations of space and time
\par
(Paton and Buonomano, 2018, p. 688) Fig 1
\par
``the timing field is to establish the correct taxonomy of time (Meck and Ivry, 2016).'' (Paton and Buonomano, 2018, p. 688) todo\\
\#reading
\par
``That is, to determine which of the many different forms of timing rely on the same circuits and mechanisms.'' (Paton and Buonomano, 2018, p. 688)
\par
``As a first step toward a taxonomy of time, it is critical to distinguish between true timing tasks and time-dependent tasks. Timing tasks refer to those that are directly based on interval or duration and that require some sort of timing device to solve. In contrast, some tasks are defined by their temporal properties but are not considered timing tasks, such as judging whether two sensory events occur simultaneously or not (asynchrony tasks) or which of two events came first (temporal-order tasks). These tasks do not require a clock or timing device to solve. Standard examples of timing tasks include (Grondin, 2010): Interval/duration discrimination. discriminating which of two presented durations (or intervals) is the longest, or making a judgment as to whether an event is short or long relative to a standard (e.g., bisection task). Reproduction. reproducing the duration or temporal structure of a presented sensory stimulus\textemdash e.g., tapping an interval demarcated by two tones or reproducing the complex temporal structure of a presented Morse code pattern. Production. production of a simple or complex temporal pattern in the absence of any recent sensory presentation of the relevant interval or pattern\textemdash e.g., human subjects asked to press a key for ``1 second,'' or a rodent that produces a timed anticipatory motor response (e.g., an eyeblink that precedes the US, or licking in anticipation of a predicted reward).'' (Paton and Buonomano, 2018, p. 688) TIming Task
\par
``Figure 1. Taxonomy of Timing Tasks The continuum along at least two task dimensions are likely to be important for understanding the neural basis of timing: sensory versus motor and interval versus pattern timing. Some tasks (Interval Timing) require the discrimination (Sensory Timing) or production (Motor Timing) of simple durations or intervals (or anticipation of an external event). Other tasks (Pattern Timing) require the discrimination or production of complex temporal or spatiotemporal patterns\textemdash such as deciphering Morse code signals (Sensory timing) or tapping a complex temporal pattern (Motor Timing). Upper left: adapted from Gouve\textasciicircum{} a et al. (2015). Lower left: adapted from Kawai et al. (2015).'' (Paton and Buonomano, 2018, p. 688) +
\par
``Subsecond versus Suprasecond Timing. There is ample evidence that timing of very short and very long intervals relies on different mechanisms and areas; however, there is no clear boundary between what constitutes a short or long interval. Nevertheless, a loose distinction between sub- and supra-second timing is often made. Pharmacological (Rammsayer and Vogel, 1992; Rammsayer, 1999), psychophysical (Karmarkar and Buonomano, 2007; Spencer et al., 2009; Rammsayer et al., 2015), and imaging (Lewis and Miall, 2003) studies suggest that discriminating a short interval (e.g., 50\textendash 100 ms) recruits different circuits than the discrimination of longer ({$>$}1 s) intervals.'' (Paton and Buonomano, 2018, p. 689)
\par
``Interval versus Pattern Timing. Imaging studies suggest that tasks that require the production of simple intervals or specific patterns recruit different neural circuits (Grube et al., 2010; Teki et al., 2011). Indeed, the distinction between simple and complex timing seems critical because these timing tasks can have fundamentally different computational requirements (Hardy and Buonomano, 2016). Discriminating the duration of a single musical note or anticipating the arrival of a reward relies on the timing of isolated durations or intervals and can easily be solved with timing mechanisms analogous to a stopwatch. In contrast, recognizing the tempo of a song, the prosody of speech, or producing Morse code are tasks that are defined by the duration and interval of components, as well as by the overall global temporal structure of a sequence of these components. Critically, when such patterns are scaled in time, they can be identified as the same pattern (a song played at different tempos is still the same song).'' (Paton and Buonomano, 2018, p. 689) Interval Timing
\par
``David Marr distinguished between three levels of analyses: (1) a computational level that essentially defined the problem being addressed from a computational or information processing perspective; (2) an algorithmic level that sought to solve a problem algorithmically\textemdash that is, without regard to how the brain may actually implement such an algorithm; and (3) an implementational level, which, in the case of neuroscience, seeks to develop models implemented at the level of synapses, neurons, and neural circuits.'' (Paton and Buonomano, 2018, p. 689)
\par
``The first models of timing on the scale of hundreds of milliseconds and seconds were pacemaker-accumulator models (Creelman, 1962; Treisman, 1963)\textemdash and, by far, the most influential of these is referred to as scalar-expectancy theory (Gibbon, 1977).'' (Paton and Buonomano, 2018, p. 689)
\par
``pacemaker-accumulator models (Creelman, 1962; Treisman, 1963)'' (Paton and Buonomano, 2018, p. 689) These seem important
\par
``scalar-expectancy theory (Gibbon, 1977).'' (Paton and Buonomano, 2018, p. 689)
\par
``Like man-made clocks, pacemaker-accumulator models postulated a time-base or oscillator, and an accumulator or integrator that essentially provides a linear readout of elapsed time. Most pacemaker-accumulator models, however, concerned themselves with accounting for the behavioral data, such was whether Weber's law was satisfied, and not with a biological implementation.'' (Paton and Buonomano, 2018, p. 689)
\par
``Weber's law (or the scalar property) is a general feature of timing and represents an important benchmark for models of timing (Gibbon, 1977).'' (Paton and Buonomano, 2018, p. 689)
\par
``It refers to the observation that, for example, in motor timing tasks the SD of the response time across trials increases linearly with the mean time of the responses. While Weber's law is robust, it is not universal, and it generally applies to restricted temporal ranges, e.g., the Weber fraction (s/t) can differ significantly for intervals of a few hundred milliseconds, seconds, and tens of seconds (Lewis and Miall, 2009; Grondin, 2014).'' (Paton and Buonomano, 2018, p. 689)
\par
``Ramping models (e.g., Durstewitz, 2003; Simen et al., 2011; Balci and Simen, 2016) propose that time is encoded in monotonic changes in firing rate and that actions are produced when the firing rates reaches a threshold value. Such ramping neurons have been observed in a wide range of brain areas during timing tasks.'' (Paton and Buonomano, 2018, p. 689) Ramping Models
\par
``An alternative to encoding time in the monotonic changes in firing rate is that the nervous system encodes time in the dynamically changing population of neurons (population'' (Paton and Buonomano, 2018, p. 689) Population Models
\par
``clocks)\textemdash ranging from sequential chains of activity (Abeles, 1982) to complex patterns. This hypothesis, referred to as population clock, was first proposed in the context of the cerebellum (Buonomano and Mauk, 1994; Mauk and Donegan, 1997), and there is now a large amount of cumulative data supporting this hypothesis.'' (Paton and Buonomano, 2018, p. 690) +
\par
``a prototypical sensory timing task is interval (or duration) discrimination, whereas in animal studies the bisection task is often used. In a bisection task, subjects are trained to make one choice when presented with a stimulus of a long duration, and another choice when presented with a stimulus of a short duration. After training, subjects undergo a procedure wherein the majority of trials are equivalent to the training phase, but on probe trials they are presented with stimuli of intermediate duration between the long- and short-duration standards. By fitting psychometric curves to the probability of choice data across the presented stimuli, it is possible to estimate the point of indifference, i.e., the interval that subjects are equally likely to judge as long and short. On probe trials, subjects are not rewarded; thus, their categorical choices have historically been thought to reflect the subjective similarity to the intervals that were reinforced during training. However, recent work suggests that, instead of reflecting perceptual similarity between short and long standards, the point of indifference may reflect the point where short and long choices are of equal value to the animal and is thus subject to factors such as the degree to which the value of future rewards are discounted relative to immediate ones (Kopec and Brody, 2018).'' (Paton and Buonomano, 2018, p. 690) \#interval timing

\#timing task
\par
``Although some studies demonstrated that musicians are superior at interval discrimination (Keele et al., 1985), other studies suggested interval timing does not improve with practice (Rammsayer, 1994). Subsequent studies, however, revealed that interval learning undergoes robust learning\textemdash however, unlike some forms of perceptual learning, temporal perceptual learning is relatively slow and requires training across days (for a review, see Bueti and Buonomano, 2014).'' (Paton and Buonomano, 2018, p. 690) \#Interval Timing
\par
``One of the first studies to demonstrate temporal perceptual learning revealed that, after training subjects for 1 hr a day for 10 days, interval discrimination thresholds for a 100-ms interval improved from 24\% to 12\% (Wright et al., 1997). Importantly, despite the significant learning on the trained 100-ms interval, there was no detectable improvement on untrained 50-, 200-, and 500-ms intervals. This temporal specificity of temporal perceptual learning has been replicated in many studies and is now seen as a general characteristic of temporal perceptual learning (Nagarajan et al., 1998; Karmarkar and Buonomano, 2003; Buonomano et al., 2009; Wright et al., 2010; Bueti et al., 2012). Temporal specificity during interval-discrimination tasks constrain the neural mechanisms and models underlying sensory timing and argue against the notion of a single master clock.'' (Paton and Buonomano, 2018, p. 690)
\par
``Specifically, if the overall precision of a clock improved with practice, it would be expected to enhance performance across a range of intervals, not just the trained interval. Another critical question relates to ``spatial'' generalization of temporal learning\textemdash e.g., after training on a 100-ms interval demarcated by brief 1-kHz tones, do humans improve on their ability to discriminate that same interval now bounded by 4-kHz tone? Interestingly, most studies have reported robust spatial generalization, but the interpretation of this finding is complicated by the fact that spatial generalization lags temporal perceptual learning\textemdash suggesting that generalization to different tones may result from top-down mechanisms independent of the timing mechanisms per se (Wright et al., 2010).'' (Paton and Buonomano, 2018, p. 690)
\par
``Interval- and rate-tuned neurons have also been identified in the brainstem of weakly electric fish that use the temporal features of discharge from their electric organs to communicate (Figure 2A) (Carlson, 2009). The mechanism underlying temporal tuning in these cases is not fully understood, but it has been established that selectivity relies in part on dynamic changes in the balance of excitation and inhibition imposed by temporal summation and short-term synaptic plasticity (see below).'' (Paton and Buonomano, 2018, p. 690)
\par
``Temporally selective neurons have also been identified in the cortical circuits of birds and mammals'' (Paton and Buonomano, 2018, p. 691) Except that these are probably circuits.\\
Not the total result of one neutron,
\par
``Basal Ganglia'' (Paton and Buonomano, 2018, p. 691)
\par
``The basal ganglia (BG), a collection of subcortical nuclei that receive input from almost the entire cortical mantle as well as multiple thalamic areas, are often implicated in sensory and motor timing on the scale of hundreds of milliseconds to seconds. This is perhaps not surprising given that the BG contribute to reinforcement learning\textemdash forming predictions about future reward and selecting actions that lead to rewarding outcomes.'' (Paton and Buonomano, 2018, p. 691) \#RL
\par
``A fundamental aspect of learning to predict something is the ability to detect temporal contingencies (Balsam and Gallistel, 2009), the degree to which some event or action reduces uncertainty about another, and there is behavioral evidence that animals represent the temporal statistics of events required for performing probabilistic inference thought to underlie this manner of associative learning (Kheifets and Gallistel, 2012; Li and Dudman, 2013). In addition, execution of behavior often involves proper timing and sequencing of action.'' (Paton and Buonomano, 2018, p. 691) Perhaps part of the discussion on the\\
importance of time in DRL
\par
``Thus, the BG should at the very least have access to representations of timing information for both learning predictions and producing proper behavior.'' (Paton and Buonomano, 2018, p. 691) was the Ba part of the loop Gustavo mentioned?
\par
(Paton and Buonomano, 2018, p. 691) Fig 2
\par
``Figure 2. Example of Interval-Tuned Neurons (A) Voltage traces from a neuron in the midbrain of an electric fish to trains of electrical pulses presented at intervals of 100 (left), 50 (center), and 10 ms (right). The rows represent three separate repetitions of each train. This neuron was tuned to pulses delivered at intervals of 50 ms (right). Adapted from Carlson (2009). (B) Rastergram of a neuron from rat auditory cortex in response to five different stimuli, each composed of a 200-ms 3-kHz tone followed by a 50-ms 7-kHz (characteristic frequency [CF]) tone with different stimulus-onset asynchronies. Numbers represent the facilitation index. Rats were trained to detect an interval of 100 ms between both tones (red arrow), and this was the spatiotemporal pattern that elicited the maximal response across the population (right). Error bars represent SEMs. Adapted from Zhou et al. (2010). (C) Model of how STP can generate an interval selective neuron in a disynaptic circuit composed of an excitatory (blue) and inhibitory (red) neuron (traces from three intervals are overlaid). Left, the input to both neurons exhibits pairedpulse facilitation. Right, by adjusting the weights onto both the Ex and Inh neurons, it is possible to create an Ex neuron that functions as a 50-, 100-, or 200-ms detector. Adapted from Buonomano (2000).'' (Paton and Buonomano, 2018, p. 691) +
\par
``One piece of evidence that the BG play a causal role in sensory timing is data showing that inactivation via infusion of muscimol into the rat dorsal striatum impairs performance of a interval categorization task (Gouve\textasciicircum{} a et al., 2015). Recordings from single units around the site of muscimol infusions revealed rich and variable dynamics that, when viewed at the population level, encoded information about elapsed time during interval presentation. Furthermore, the timing information derived from simultaneously recorded ensembles of striatal neurons predicted the trial-to-trial variation in duration judgments produced by the animals (Figure 3). When population dynamics proceeded more quickly, rats were more likely to judge a given interval as being in the ``long'' category, and vice versa when population dynamics proceeded more slowly, indicating that striatal dynamics reflected the timing information that rats were using to guide their judgments (Gouve\textasciicircum{} a et al., 2015). These data demonstrate that the striatum was required, and striatal populations encoded information, for guiding what we would define as a sensory timing task.'' (Paton and Buonomano, 2018, p. 692)
\par
``comparing simultaneously recorded high-speed video and neural population activity revealed a clear asymmetry between when timing information appeared in neural activity and behavior, with neural activity leading behavior by 300 ms (Gouve\textasciicircum{} a et al., 2015). Thus, while time encoding by striatal neurons likely carries information about a plan for future action, it is unlikely to represent motor commands on their way out of the CNS, nor could it solely reflect the sensory consequences of action'' (Paton and Buonomano, 2018, p. 692)
\par
``Computational models of timing have not generally explicitly distinguished between sensory and motor timing. We argue that such a distinction is important, because the temporally selective neurons in the brainstem and sensory cortex seem to behave as temporal filters as opposed to timers, and are unlikely to be directly responsible for the production of timed motor patterns. Mechanistically, we can think of the sensory and motor timing distinction as relying on passive versus active neural mechanisms, respectively.'' (Paton and Buonomano, 2018, p. 692)
\par
``Passive neural mechanisms refer to those that react to the temporal structure of stimuli, but that are incapable of actively generating a timed response. A prototypical example of a passive mechanism is a band-pass temporal filter, which gates the information arriving at certain frequencies, but cannot actively produce a timed response. In contrast, motor and implicit timing require a circuit to actively generate a timed signal. We stress, however, that, while the distinction between sensory and motor timing is important, they can be overlapping, and indeed many models of motor timing can account for simple sensory timing (such as interval and duration discrimination).'' (Paton and Buonomano, 2018, p. 692)
\par
``Figure 3. Midbrain Dopamine Neurons and Striatal Dynamics May Interact to Regulate Timing (A) The speed with which striatal ensembles traverse neural space (top panel) predicts duration judgments (lower panel) in an interval-discrimination task. Colored schematic trajectories in top panel depict a quickly (red) or slowly (blue) evolving ensemble activity pattern during interval presentation in a space defined by the firing of simultaneously recorded striatal neurons. Psychometric curves for trials segregated on the basis of whether activity proceeded quickly or slowly during interval presentation. Adapted from Gouve\textasciicircum{} a et al. (2015). (B) Calcium signals collected from dopamine neurons in the SNc exhibited trial-to-trial variability during interval presentations (top panel) that predicted the timing judgments of mice during the same interval-discrimination task used during the data collected in (A) (adapted from Soares et al., 2016). Given the dense innervation of striatal networks (in black, center) by nigro-striatal dopamine neurons (in purple, center) and the fact that SNc dopamine neurons receive significant input from striatum, these data support a hypothesis where the two brain areas reciprocally influence each other's timing functions.'' (Paton and Buonomano, 2018, p. 692)
\par
``Hardy, N.F., Goudar, V., Romero-Sosa, J.L., and Buonomano, D. (2017). A model of temporal scaling correctly predicts that Weber's law is speed-dependent. bioRxiv. https://doi.org/10.1101/159590.'' (Paton and Buonomano, 2018, p. 702)
\par
``Meck, W.H., and Ivry, R.B. (2016). Editorial overview: Time in perception and action. Curr. Opin. Behav. Sci. 8, vi\textendash x.'' (Paton and Buonomano, 2018, p. 703)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/patonNeuralBasisTiming2018-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Paton_Buonomano_2018_The Neural Basis of Timing.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JJ34INJH/S0896627318302514.html}
}

@article{payeurBurstdependentSynapticPlasticity2021,
  title = {Burst-Dependent Synaptic Plasticity Can Coordinate Learning in Hierarchical Circuits},
  author = {Payeur, Alexandre and Guerguiev, Jordan and Zenke, Friedemann and Richards, Blake A. and Naud, Richard},
  year = {2021},
  month = jul,
  journal = {Nature Neuroscience},
  volume = {24},
  number = {7},
  pages = {1010--1019},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-021-00857-x},
  abstract = {Synaptic plasticity is believed to be a key physiological mechanism for learning. It is well established that it depends on pre- and postsynaptic activity. However, models that rely solely on pre- and postsynaptic activity for synaptic changes have, so far, not been able to account for learning complex tasks that demand credit assignment in hierarchical networks. Here we show that if synaptic plasticity is regulated by high-frequency bursts of spikes, then pyramidal neurons higher in a hierarchical circuit can coordinate the plasticity of lower-level connections. Using simulations and mathematical analyses, we demonstrate that, when paired with short-term synaptic dynamics, regenerative activity in the apical dendrites and synaptic plasticity in feedback pathways, a burst-dependent learning rule can solve challenging tasks that require deep network architectures. Our results demonstrate that well-known properties of dendrites, synapses and synaptic plasticity are sufficient to enable sophisticated learning in hierarchical circuits.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Learning algorithms,Sensory processing,Spike-timing-dependent plasticity},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Payeur et al_2021_Burst-dependent synaptic plasticity can coordinate learning in hierarchical.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Payeur et al_2021_Burst-dependent synaptic plasticity can coordinate learning in hierarchical2.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8TM5VWH7/s41593-021-00857-x.html}
}

@article{peerInferringSubnetworksPerturbed2001,
  title = {Inferring Subnetworks from Perturbed Expression Profiles},
  author = {Pe'er, Dana and Regev, Aviv and Elidan, Gal and Friedman, Nir},
  year = {2001},
  month = jun,
  journal = {Bioinformatics},
  volume = {17},
  number = {SUPPL. 1},
  pages = {S215-S224},
  publisher = {{Oxford University Press}},
  doi = {10.1093/bioinformatics/17.suppl_1.S215},
  abstract = {Genome-wide expression profiles of genetic mutants provide a wide variety of measurements of cellular responses to perturbations. Typical analysis of such data identifies genes affected by perturbation and uses clustering to group genes of similar function. In this paper we discover a finer structure of interactions between genes, such as causality, mediation, activation, and inhibition by using a Bayesian network framework. We extend this framework to correctly handle perturbations, and to identify significant subnetworks of interacting genes. We apply this method to expression data of S. cerevisiae mutants and uncover a variety of structured metabolic, signaling and regulatory pathways. \textcopyright{} Oxford University Press 2001.},
  keywords = {Contact,danab@cshujiacil},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/B7L9PFIA/Pe'er et al. - 2001 - Inferring subnetworks from perturbed expression profiles.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@peerInferringSubnetworksPerturbed2001.md}
}

@article{pensar2020high,
  title = {High-Dimensional Structure Learning of Binary Pairwise {{Markov}} Networks: A Comparative Numerical Study},
  author = {Pensar, Johan and Xu, Yingying and Puranen, Santeri and Pesonen, Maiju and Kabashima, Yoshiyuki and Corander, Jukka},
  year = {2020},
  journal = {Computational Statistics \& Data Analysis},
  volume = {141},
  pages = {62--76},
  publisher = {{Elsevier}},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@pensar2020high.md}
}

@techreport{pensarHighdimensionalStructureLearning,
  title = {High-Dimensional Structure Learning of Binary Pairwise {{Markov}} Networks: {{A}} Comparative Numerical Study},
  author = {Pensar, Johan and Xu, Yingying and Puranen, Santeri and Pesonen, Maiju and Kabashima, Yoshiyuki and Corander, Jukka},
  abstract = {Learning the undirected graph structure of a Markov network from data is a problem that has received a lot of attention during the last few decades. As a result of the general applicability of the model class, a myriad of methods have been developed in parallel in several research fields. Recently, as the size of the considered systems has increased, the focus of new methods has been shifted towards the high-dimensional domain. In particular, the introduction of the pseudo-likelihood function has pushed the limits of score-based methods originally based on the likelihood. At the same time, an array of methods based on simple pairwise tests have been developed to meet the challenges set by the increasingly large data sets in computational biology. Apart from being applicable on high-dimensional problems, methods based on the pseudo-likelihood and pairwise tests are fundamentally very different. In this work, we perform an extensive numerical study comparing the different types of methods on data generated by binary pairwise Markov networks. For sampling large networks, we use a parallelizable Gibbs sampler based on sparse restricted Boltzmann machines. Our results show that pairwise methods can be more accurate than pseudo-likelihood methods in settings often encountered in high-dimensional structure learning.},
  keywords = {comparative study,Ising model,Markov network,mutual information,pseudo-likelihood,structure learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/23VYIKXH/Pensar et al. - Unknown - High-dimensional structure learning of binary pairwise Markov networks A comparative numerical study.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@pensarHighdimensionalStructureLearning.md}
}

@article{pensarHighdimensionalStructureLearning2019,
  title = {High-Dimensional Structure Learning of Binary Pairwise {{Markov}} Networks: {{A}} Comparative Numerical Study},
  shorttitle = {High-Dimensional Structure Learning of Binary Pairwise {{Markov}} Networks},
  author = {Pensar, Johan and Xu, Yingying and Puranen, Santeri and Pesonen, Maiju and Kabashima, Yoshiyuki and Corander, Jukka},
  year = {2019},
  month = jul,
  journal = {arXiv:1901.04345 [cs, stat]},
  eprint = {1901.04345},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.1016/j.csda.2019.06.012},
  abstract = {Learning the undirected graph structure of a Markov network from data is a problem that has received a lot of attention during the last few decades. As a result of the general applicability of the model class, a myriad of methods have been developed in parallel in several research fields. Recently, as the size of the considered systems has increased, the focus of new methods has been shifted towards the high-dimensional domain. In particular, introduction of the pseudo-likelihood function has pushed the limits of score-based methods which were originally based on the likelihood function. At the same time, methods based on simple pairwise tests have been developed to meet the challenges arising from increasingly large data sets in computational biology. Apart from being applicable to high-dimensional problems, methods based on the pseudo-likelihood and pairwise tests are fundamentally very different. To compare the accuracy of the different types of methods, an extensive numerical study is performed on data generated by binary pairwise Markov networks. A parallelizable Gibbs sampler, based on restricted Boltzmann machines, is proposed as a tool to efficiently sample from sparse high-dimensional networks. The results of the study show that pairwise methods can be more accurate than pseudo-likelihood methods in settings often encountered in high-dimensional structure learning applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/W29XY8LX/Pensar et al. - 2019 - High-dimensional structure learning of binary pair.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@pensarHighdimensionalStructureLearning2019.md}
}

@article{pensarMarginalPseudoLikelihoodLearning2017,
  title = {Marginal {{Pseudo-Likelihood Learning}} of {{Discrete Markov Network Structures}}},
  author = {Pensar, Johan and Nyman, Henrik and Niiranen, Juha and Corander, Jukka},
  year = {2017},
  journal = {Bayesian Analysis},
  volume = {12},
  number = {4},
  pages = {1195--1215},
  doi = {10.1214/16-BA1032},
  abstract = {Markov networks are a popular tool for modeling multivariate distributions over a set of discrete variables. The core of the Markov network representation is an undirected graph which elegantly captures the dependence structure over the variables. Traditionally, the Bayesian approach of learning the graph structure from data has been done under the assumption of chordality since non-chordal graphs are difficult to evaluate for likelihood-based scores. Recently, there has been a surge of interest towards the use of regularized pseudo-likelihood methods as such approaches can avoid the assumption of chordality. Many of the currently available methods necessitate the use of a tuning parameter to adapt the level of regularization for a particular dataset. Here we introduce the marginal pseudo-likelihood which has a built-in regularization through marginalization over the graph-specific nuisance parameters. We prove consistency of the resulting graph estimator via comparison with the pseudo-Bayesian information criterion. To identify high-scoring graph structures in a high-dimensional setting we design a two-step algorithm that exploits the decomposable structure of the score. Using synthetic and existing benchmark networks, the marginal pseudo-likelihood method is shown to perform favorably against recent popular structure learning methods.},
  keywords = {Bayesian inference,Markov networks,non-chordal graph,pseudo-likelihood,regularization,structure learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DLYAE5C9/Pensar et al. - 2017 - Marginal Pseudo-Likelihood Learning of Discrete Markov Network Structures.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@pensarMarginalPseudoLikelihoodLearning2017.md}
}

@article{perettoCollectivePropertiesNeural1984,
  title = {Collective Properties of Neural Networks: {{A}} Statistical Physics Approach},
  author = {Peretto, P.},
  year = {1984},
  month = feb,
  journal = {Biological Cybernetics},
  volume = {50},
  number = {1},
  pages = {51--62},
  publisher = {{Springer-Verlag}},
  doi = {10.1007/BF00317939},
  abstract = {Among the various models proposed so far to account for the properties of neural networks, the one devised by Little and the one derived by Hopfield prove to be the most interesting because they allow the use of statistical mechanics techniques. The link between the Hopfield model and the statistical mechanics is provided by the existence of an extensive quantity. When the synaptic plasticity behaves according to a Hebbian procedure, the analogy with the classical spin glass models studied by Van Hemmen is complete. In particular exact solutions describing the steady states of noisy systems are found. On the other hand, the Little model introduces a Markovian dynamics. One shows that the evolution equation obeys the microreversibility principle if the synaptic efficiencies are symmetrical. Therefore, assuming that such a symmetry materializes, the Little model has to obey a Gibbs statistics. The corresponding Hamiltonian is derived accordingly. At last, using these results, both models are shown to display associative memory properties. In particular the storage capacity of neural networks working along with the Little dynamics is similar to the capacity of Hopfield neural networks. The conclusion drawn from the study of the Hopfield model can be extended to the Little model, which is certainly a more realistic description of the biological situation.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZDZKDJ6W/Peretto - 1984 - Collective properties of neural networks A statistical physics approach.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@perettoCollectivePropertiesNeural1984.md}
}

@article{perezSynapticPropertiesCells2018,
  title = {The {{Synaptic Properties}} of {{Cells Define}} the {{Hallmarks}} of {{Interval Timing}} in a {{Recurrent Neural Network}}},
  author = {P{\'e}rez, Oswaldo and Merchant, Hugo},
  year = {2018},
  journal = {The Journal of Neuroscience},
  doi = {10.1523/JNEUROSCI.2651-17.2018},
  abstract = {It is suggested that physiological values of the time constants for paired-pulse facilitation and GABAb, as well as the internal state of the network, determine the bias and scalar properties of interval timing. Extensive research has described two key features of interval timing. The bias property is associated with accuracy and implies that time is overestimated for short intervals and underestimated for long intervals. The scalar property is linked to precision and states that the variability of interval estimates increases as a function of interval duration. The neural mechanisms behind these properties are not well understood. Here we implemented a recurrent neural network that mimics a cortical ensemble and includes cells that show paired-pulse facilitation and slow inhibitory synaptic currents. The network produces interval selective responses and reproduces both bias and scalar properties when a Bayesian decoder reads its activity. Notably, the interval-selectivity, timing accuracy, and precision of the network showed complex changes as a function of the decay time constants of the modeled synaptic properties and the level of background activity of the cells. These findings suggest that physiological values of the time constants for paired-pulse facilitation and GABAb, as well as the internal state of the network, determine the bias and scalar properties of interval timing. SIGNIFICANCE STATEMENT Timing is a fundamental element of complex behavior, including music and language. Temporal processing in a wide variety of contexts shows two primary features: time estimates exhibit a shift toward the mean (the bias property) and are more variable for longer intervals (the scalar property). We implemented a recurrent neural network that includes long-lasting synaptic currents, which cannot only produce interval-selective responses but also follow the bias and scalar properties. Interestingly, only physiological values of the time constants for paired-pulse facilitation and GABAb, as well as intermediate background activity within the network can reproduce the two key features of interval timing.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BGHPYFCD/Pérez_Merchant_2018_The Synaptic Properties of Cells Define the Hallmarks of Interval Timing in a.pdf}
}

@article{perezSynapticPropertiesCells2018a,
  title = {The {{Synaptic Properties}} of {{Cells Define}} the {{Hallmarks}} of {{Interval Timing}} in a {{Recurrent Neural Network}}},
  author = {P{\'e}rez, Oswaldo and Merchant, Hugo},
  year = {2018},
  month = apr,
  journal = {Journal of Neuroscience},
  volume = {38},
  number = {17},
  pages = {4186--4199},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2651-17.2018},
  abstract = {Extensive research has described two key features of interval timing. The bias property is associated with accuracy and implies that time is overestimated for short intervals and underestimated for long intervals. The scalar property is linked to precision and states that the variability of interval estimates increases as a function of interval duration. The neural mechanisms behind these properties are not well understood. Here we implemented a recurrent neural network that mimics a cortical ensemble and includes cells that show paired-pulse facilitation and slow inhibitory synaptic currents. The network produces interval selective responses and reproduces both bias and scalar properties when a Bayesian decoder reads its activity. Notably, the interval-selectivity, timing accuracy, and precision of the network showed complex changes as a function of the decay time constants of the modeled synaptic properties and the level of background activity of the cells. These findings suggest that physiological values of the time constants for paired-pulse facilitation and GABAb, as well as the internal state of the network, determine the bias and scalar properties of interval timing. SIGNIFICANCE STATEMENT Timing is a fundamental element of complex behavior, including music and language. Temporal processing in a wide variety of contexts shows two primary features: time estimates exhibit a shift toward the mean (the bias property) and are more variable for longer intervals (the scalar property). We implemented a recurrent neural network that includes long-lasting synaptic currents, which cannot only produce interval-selective responses but also follow the bias and scalar properties. Interestingly, only physiological values of the time constants for paired-pulse facilitation and GABAb, as well as intermediate background activity within the network can reproduce the two key features of interval timing.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2018 the authors 0270-6474/18/384186-14\$15.00/0},
  langid = {english},
  pmid = {29615484},
  keywords = {interval timing,long-term synaptic properties,recurrent neural network,spontaneous activity},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IEVKNGVF/Pérez_Merchant_2018_The Synaptic Properties of Cells Define the Hallmarks of Interval Timing in a.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AW37U4W4/4186.html}
}

@article{perezvicenteOptimisedNetworkSparsely1989,
  title = {Optimised Network for Sparsely Coded Patterns},
  author = {Perez Vicente, C. J. and Amit, D. J.},
  year = {1989},
  journal = {Journal of Physics A: General Physics},
  volume = {22},
  number = {5},
  pages = {559--569},
  doi = {10.1088/0305-4470/22/5/018},
  abstract = {The performance of attractor neural networks storing sparsely coded\textbackslash npatterns has been shown to be greatly improved on shifting from the\textbackslash n-1, +1 representation of neural states to the 0, 1 representation.\textbackslash nHere the authors show that when this shift is considered as a special\textbackslash ncase of the transformation of the dynamical variables which depends\textbackslash non a continuous parameter, the value of the parameter can be chosen\textbackslash nto improve the performance of the network even further for every\textbackslash nvalue of the bias in the patterns.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RE4XDM4P/Perez Vicente, Amit - 1989 - Optimised network for sparsely coded patterns.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@perezvicenteOptimisedNetworkSparsely1989.md}
}

@article{petterIntegratingModelsInterval2018,
  title = {Integrating {{Models}} of {{Interval Timing}} and {{Reinforcement Learning}}},
  author = {Petter, Elijah A. and Gershman, Samuel J. and Meck, Warren H.},
  year = {2018},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  series = {Special {{Issue}}: {{Time}} in the {{Brain}}},
  volume = {22},
  number = {10},
  pages = {911--922},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2018.08.004},
  abstract = {We present an integrated view of interval timing and reinforcement learning (RL) in the brain. The computational goal of RL is to maximize future rewards, and this depends crucially on a representation of time. Different RL systems in the brain process time in distinct ways. A model-based system learns `what happens when', employing this internal model to generate action plans, while a model-free system learns to predict reward directly from a set of temporal basis functions. We describe how these systems are subserved by a computational division of labor between several brain regions, with a focus on the basal ganglia and the hippocampus, as well as how these regions are influenced by the neuromodulator dopamine.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/36FD6U9Y/Integrating Models of Interval Timing and Reinforcement Learning - 11.07.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CMFVF84G/Petter et al_2018_Integrating Models of Interval Timing and Reinforcement Learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/F5N8UY7D/S1364661318301931.html}
}

@article{petterTemporalProcessingIntrinsic2016,
  title = {Temporal {{Processing}} by {{Intrinsic Neural Network Dynamics}}},
  author = {Petter, Elijah A. and Merchant, Hugo},
  year = {2016},
  month = oct,
  journal = {Timing \& Time Perception},
  volume = {4},
  number = {4},
  pages = {399--410},
  publisher = {{Brill Academic Publishers}},
  issn = {2213445X},
  doi = {10.1163/22134468-00002074},
  abstract = {It is becoming more apparent that there are rich contributions to temporal processing across the brain. Temporal dynamics have been found from lower brain structures all the way to cortical regions. Specifically, in vitro cortical preparations have been extremely useful in understanding how local circuits can time. While many of these results depict vastly different processing than a traditional central clock metaphor they still leave questions as to how this information is integrated. We therefore review evidence to place the results pertaining to local circuit timers into the larger context of temporal perception and generalization.},
  keywords = {basal ganglia,BIOLOGICAL neural networks,cortex,generalization,hippocampus,HIPPOCAMPUS (Brain),interval timing,Intrinsic timers,time perception,TIME perception},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SVYDEBB7/Petter_Merchant_2016_Temporal Processing by Intrinsic Neural Network Dynamics.pdf}
}

@article{peyracheInternallyOrganizedMechanisms2015,
  title = {Internally Organized Mechanisms of the Head Direction Sense},
  author = {Peyrache, Adrien and Lacroix, Marie M and Petersen, Peter C and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2015},
  month = apr,
  journal = {Nature Neuroscience},
  volume = {18},
  number = {4},
  pages = {569--575},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nn.3968},
  abstract = {Recording from population of head-direction cells across brain states, the authors provide experimental demonstration of the existence of internally organized attractor: the sequential activity of head direction neurons observed in the waking mouse persists during sleep, and this 'neuronal compass' always points toward well-defined directions.},
  keywords = {Navigation,Replay,Thalamus},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IHJCWKE7/Peyrache et al. - 2015 - Internally organized mechanisms of the head direction sense.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@peyracheInternallyOrganizedMechanisms2015.md}
}

@article{pfeifferDeepLearningSpiking2018,
  title = {Deep {{Learning With Spiking Neurons}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Deep {{Learning With Spiking Neurons}}},
  author = {Pfeiffer, Michael and Pfeil, Thomas},
  year = {2018},
  journal = {Frontiers in Neuroscience},
  volume = {12},
  issn = {1662-453X},
  abstract = {Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years, and investigate potential use cases. Neuromorphic approaches and conventional machine learning should not be considered simply two solutions to the same classes of problems, instead it is possible to identify and exploit their task-specific advantages. Deep SNNs offer great opportunities to work with new types of event-based sensors, exploit temporal codes and local on-chip learning, and we have so far just scratched the surface of realizing these advantages in practical applications.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7E5HHFFU/Pfeiffer_Pfeil_2018_Deep Learning With Spiking Neurons.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WSNMDLTK/Deep Learning With Spiking Neurons Opportunities and Challenges - 11.05.22.md}
}

@article{pfeifferHippocampalPlaceCell2013,
  title = {Hippocampal Place Cell Sequences Depict Future Paths to Remembered Goals {{HHS Public Access}}},
  author = {Pfeiffer, Brad E and Foster, David J},
  year = {2013},
  journal = {Nature},
  volume = {497},
  number = {7447},
  pages = {74--79},
  doi = {10.1038/nature12112},
  abstract = {Effective navigation requires planning extended routes to remembered goal locations. Hippocampal place cells have been proposed to play a role in navigational planning but direct evidence has been lacking. Here, we show that prior to goal-directed navigation in an open arena, the hippocampus generates brief sequences encoding spatial trajectories strongly biased to progress from the subject's current location to a known goal location. These sequences predict immediate future behavior, even in cases when the specific combination of start and goal locations is novel. These results suggest that hippocampal sequence events previously characterized in linearly constrained environments as 'replay' are also capable of supporting a goal-directed, trajectory-finding mechanism, which identifies important places and relevant behavioral paths, at specific times when memory retrieval is required, and in a manner which could be used to control subsequent navigational behavior. A fundamental purpose of memory lies in utilizing previous experience to inform current choices, directing behavior toward reward and away from negative consequences based upon knowledge of prior outcomes in similar situations. Goal-directed spatial navigation-planning extended routes to remembered locations-requires both memory of the goal location and knowledge of the intervening terrain in order to determine an efficient and safe path. The hippocampus has long been known to play a critical role in spatial memory 1,2 and memory for events 3,4 , and it has been proposed that the hippocampus might play a fundamental role in calculating routes to goals, especially under conditions demanding behavioral flexibility 1,5-8. This proposal stems largely from the discovery that excitatory neurons of the hippocampus exhibit spatially localized place responses during exploration 1. However, it has been a challenge to understand how individual place responses tied to the current location might be informative about other locations that the animal cares about, such as the remembered goal 9 , or the set of locations defining a route 10,11 .},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MR9T8BGP/Pfeiffer, Foster - 2013 - Hippocampal place cell sequences depict future paths to remembered goals HHS Public Access.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@pfeifferHippocampalPlaceCell2013.md}
}

@misc{phuongFormalAlgorithmsTransformers2022,
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  year = {2022},
  month = jul,
  number = {arXiv:2207.09238},
  eprint = {2207.09238},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.09238},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: 16 pages, 15 algorithms},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KS43A3BM/Phuong_Hutter_2022_Formal Algorithms for Transformers.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VFB7MPNN/2207.html}
}

@article{pittiGatedSpikingNeural2020,
  title = {Gated Spiking Neural Network Using {{Iterative Free-Energy Optimization}} and Rank-Order Coding for Structure Learning in Memory Sequences ({{INFERNO GATE}})},
  author = {Pitti, Alexandre and Quoy, Mathias and Lavandier, Catherine and Boucenna, Sofiane},
  year = {2020},
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  pages = {242--258},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.09.023},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YBB9PFVH/Pitti et al_2020_Gated spiking neural network using Iterative Free-Energy Optimization and.pdf}
}

@article{plefkaConvergenceConditionTAP1982,
  title = {Convergence Condition of the {{TAP}} Equation for the Infinite-Ranged {{Ising}} Spin Glass Model},
  author = {Plefka, T},
  year = {1982},
  month = jun,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {15},
  number = {6},
  pages = {1971--1978},
  issn = {0305-4470, 1361-6447},
  doi = {10.1088/0305-4470/15/6/035},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@plefkaConvergenceConditionTAP1982.md}
}

@book{poeppelCognitiveNeurosciences2020,
  title = {The {{Cognitive Neurosciences}}},
  editor = {Poeppel, David and Mangun, George R. and Gazzaniga, Michael S.},
  year = {2020},
  month = may,
  edition = {Sixth},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {The sixth edition of the foundational reference on cognitive neuroscience, with entirely new material that covers the latest research, experimental approaches, and measurement methodologies.},
  isbn = {978-0-262-04325-0},
  langid = {english},
  note = {Specifically Chapter 7 on Synaptic plasticity by Whitlock and Moser}
}

@article{poeppelNewNeurobiologyLanguage2012,
  title = {Towards a New Neurobiology of Language},
  author = {Poeppel, David and Emmorey, Karen and Hickok, Gregory and Pylkk{\"a}nen, Liina},
  year = {2012},
  month = oct,
  journal = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  volume = {32},
  number = {41},
  pages = {14125--14131},
  issn = {1529-2401},
  doi = {10.1523/JNEUROSCI.3244-12.2012},
  abstract = {Theoretical advances in language research and the availability of increasingly high-resolution experimental techniques in the cognitive neurosciences are profoundly changing how we investigate and conceive of the neural basis of speech and language processing. Recent work closely aligns language research with issues at the core of systems neuroscience, ranging from neurophysiological and neuroanatomic characterizations to questions about neural coding. Here we highlight, across different aspects of language processing (perception, production, sign language, meaning construction), new insights and approaches to the neurobiology of language, aiming to describe promising new areas of investigation in which the neurosciences intersect with linguistic research more closely than before. This paper summarizes in brief some of the issues that constitute the background for talks presented in a symposium at the Annual Meeting of the Society for Neuroscience. It is not a comprehensive review of any of the issues that are discussed in the symposium.},
  langid = {english},
  pmcid = {PMC3495005},
  pmid = {23055482},
  keywords = {Animals,Brain,Humans,Language,Neural Pathways,Sign Language,Speech,Speech Perception},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Poeppel et al_2012_Towards a new neurobiology of language.pdf}
}

@article{poliFunctionalConnectivityVitro2015,
  title = {Functional Connectivity in in Vitro Neuronal Assemblies},
  author = {Poli, Daniele and Pastore, Vito P. and Massobrio, Paolo},
  year = {2015},
  journal = {Frontiers in Neural Circuits},
  volume = {9},
  number = {October},
  pages = {1--14},
  issn = {1662-5110},
  doi = {10.3389/fncir.2015.00057},
  abstract = {Complex network topologies represent the necessary substrate to support complex brain functions. In this work, we reviewed in vitro neuronal networks coupled to Micro-Electrode Arrays (MEAs) as biological substrate. Networks of dissociated neurons developing in vitro and coupled to MEAs, represent a valid experimental model for studying the mechanisms governing the formation, organization and conservation of neuronal cell assemblies. In this review, we present some examples of the use of statistical Cluster Coefficients and Small World indices to infer topological rules underlying the dynamics exhibited by homogeneous and engineered neuronal networks.},
  keywords = {correlation,functional connectivity,graph theory,in vitro,micro-electrode,neuronal net,neuronal network dynamics},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3A6E42WS/Poli, Pastore, Massobrio - 2015 - Functional connectivity in in vitro neuronal assemblies(2).pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YPNEPXA6/Poli, Pastore, Massobrio - 2015 - Functional connectivity in in vitro neuronal assemblies.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@poliFunctionalConnectivityVitro2015.md}
}

@misc{pontes-filhoAssessingRobustnessCritical2022,
  title = {Assessing the Robustness of Critical Behavior in Stochastic Cellular Automata},
  author = {{Pontes-Filho}, Sidney and Lind, Pedro and Nichele, Stefano},
  year = {2022},
  month = aug,
  number = {arXiv:2208.00746},
  eprint = {2208.00746},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.00746},
  abstract = {There is evidence that biological systems, such as the brain, work at a critical regime robust to noise, and are therefore able to remain in it under perturbations. In this work, we address the question of robustness of critical systems to noise. In particular, we investigate the robustness of stochastic cellular automata (CAs) at criticality. A stochastic CA is one of the simplest stochastic models showing criticality. The transition state of stochastic CA is defined through a set of probabilities. We systematically perturb the probabilities of an optimal stochastic CA known to produce critical behavior, and we report that such a CA is able to remain in a critical regime up to a certain degree of noise. We present the results using error metrics of the resulting power-law fitting, such as Kolmogorov-Smirnov statistic and Kullback-Leibler divergence. We discuss the implication of our results in regards to future realization of brain-inspired artificial intelligence systems.},
  archiveprefix = {arXiv},
  keywords = {68T01,Computer Science - Information Theory,Computer Science - Neural and Evolutionary Computing,I.6.6,Nonlinear Sciences - Cellular Automata and Lattice Gases},
  note = {Comment: 11 pages and 7 figures},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5CMBAKKR/Pontes-Filho et al_2022_Assessing the robustness of critical behavior in stochastic cellular automata.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SA2RNGGL/2208.html}
}

@inproceedings{pontes-filhoBidirectionalLearningRobust2019,
  title = {Bidirectional {{Learning}} for {{Robust Neural Networks}}},
  booktitle = {2019 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {{Pontes-Filho}, Sidney and Liwicki, Marcus},
  year = {2019},
  month = jul,
  eprint = {1805.08006},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1--8},
  doi = {10.1109/IJCNN.2019.8852120},
  abstract = {A multilayer perceptron can behave as a generative classifier by applying bidirectional learning (BL). It consists of training an undirected neural network to map input to output and vice-versa; therefore it can produce a classifier in one direction, and a generator in the opposite direction for the same data. The learning process of BL tries to reproduce the neuroplasticity stated in Hebbian theory using only backward propagation of errors. In this paper, two novel learning techniques are introduced which use BL for improving robustness to white noise static and adversarial examples. The first method is bidirectional propagation of errors, which the error propagation occurs in backward and forward directions. Motivated by the fact that its generative model receives as input a constant vector per class, we introduce as a second method the hybrid adversarial networks (HAN). Its generative model receives a random vector as input and its training is based on generative adversarial networks (GAN). To assess the performance of BL, we perform experiments using several architectures with fully and convolutional layers, with and without bias. Experimental results show that both methods improve robustness to white noise static and adversarial examples, and even increase accuracy, but have different behavior depending on the architecture and task, being more beneficial to use the one or the other. Nevertheless, HAN using a convolutional architecture with batch normalization presents outstanding robustness, reaching state-of-the-art accuracy on adversarial examples of hand-written digits.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 8 pages, 4 figures, submitted to 2019 International Joint Conference on Neural Networks},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VMHAAANY/Pontes-Filho_Liwicki_2019_Bidirectional Learning for Robust Neural Networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SPYERV6H/1805.html}
}

@misc{pontes-filhoConceptualBioInspiredFramework2020,
  title = {A {{Conceptual Bio-Inspired Framework}} for the {{Evolution}} of {{Artificial General Intelligence}}},
  author = {{Pontes-Filho}, Sidney and Nichele, Stefano},
  year = {2020},
  month = sep,
  number = {arXiv:1903.10410},
  eprint = {1903.10410},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1903.10410},
  abstract = {In this work, a conceptual bio-inspired parallel and distributed learning framework for the emergence of general intelligence is proposed, where agents evolve through environmental rewards and learn throughout their lifetime without supervision, i.e., self-learning through embodiment. The chosen control mechanism for agents is a biologically plausible neuron model based on spiking neural networks. Network topologies become more complex through evolution, i.e., the topology is not fixed, while the synaptic weights of the networks cannot be inherited, i.e., newborn brains are not trained and have no innate knowledge of the environment. What is subject to the evolutionary process is the network topology, the type of neurons, and the type of learning. This process ensures that controllers that are passed through the generations have the intrinsic ability to learn and adapt during their lifetime in mutable environments. We envision that the described approach may lead to the emergence of the simplest form of artificial general intelligence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: 7 pages, 2 figures, accepted to "The 3rd Special Session on Biologically Inspired Parallel and Distributed Computing, Algorithms and Solutions" (BICAS 2020)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UJ6K73YL/Pontes-Filho_Nichele_2020_A Conceptual Bio-Inspired Framework for the Evolution of Artificial General.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6BWF7VAA/1903.html}
}

@book{pontes-filhoEvoDynamicFrameworkEvolution2020,
  title = {{{EvoDynamic}}: {{A Framework}} for the {{Evolution}} of {{Generally Represented Dynamical Systems}} and {{Its Application}} to {{Criticality}}},
  shorttitle = {{{EvoDynamic}}},
  author = {{Pontes-Filho}, Sidney and Lind, Pedro and Yazidi, Anis and Zhang, Jianhua and Hammer, Hugo Lewi and Mello, Gustavo and Sandvig, Ioanna and Tufte, Gunnar and Nichele, Stefano},
  year = {2020},
  journal = {133-148},
  publisher = {{Springer}},
  doi = {10.1007/978-3-030-43722-0_9},
  abstract = {Dynamical systems possess a computational capacity that may be exploited in a reservoir computing paradigm. This paper presents a general representation of dynamical systems which is based on matrix multiplication. That is similar to how an artificial neural network (ANN) is represented in a deep learning library and its computation can be faster because of the optimized matrix operations that such type of libraries have. Initially, we implement the simplest dynamical system, a cellular automaton. The mathematical fundamentals behind an ANN are maintained, but the weights of the connections and the activation function are adjusted to work as an update rule in the context of cellular automata. The advantages of such implementation are its usage on specialized and optimized deep learning libraries, the capabilities to generalize it to other types of networks and the possibility to evolve cellular automata and other dynamical systems in terms of connectivity, update and learning rules. Our implementation of cellular automata constitutes an initial step towards a more general framework for dynamical systems. Our objective is to evolve such systems to optimize their usage in reservoir computing and to model physical computing substrates. Furthermore, we present promising preliminary results toward the evolution of complex behavior and criticality using genetic algorithm in stochastic elementary cellular automata.},
  isbn = {978-3-030-43722-0},
  langid = {english},
  annotation = {Accepted: 2021-01-26T08:24:17Z},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@pontes-filhoEvoDynamicFrameworkEvolution2020.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/pontes-filhoEvoDynamicFrameworkEvolution2020-mdnotes.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Pontes-Filho et al_2020_EvoDynamic.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WKGMV3JI/2724664.html}
}

@article{pontes-filhoNeuroinspiredGeneralFramework2020,
  title = {A Neuro-Inspired General Framework for the Evolution of Stochastic Dynamical Systems: {{Cellular}} Automata, Random {{Boolean}} Networks and Echo State Networks towards Criticality},
  author = {{Pontes-Filho}, Sidney and Lind, Pedro and Yazidi, Anis and Zhang, Jianhua and Hammer, Hugo and Mello, Gustavo B.M. and Sandvig, Ioanna and Tufte, Gunnar and Nichele, Stefano},
  year = {2020},
  month = oct,
  journal = {Cognitive Neurodynamics},
  volume = {14},
  number = {5},
  pages = {657--674},
  publisher = {{Springer Science+Business Media B.V.}},
  issn = {18714099},
  doi = {10.1007/s11571-020-09600-x},
  abstract = {Although deep learning has recently increased in popularity, it suffers from various problems including high computational complexity, energy greedy computation, and lack of scalability, to mention a few. In this paper, we investigate an alternative brain-inspired method for data analysis that circumvents the deep learning drawbacks by taking the actual dynamical behavior of biological neural networks into account. For this purpose, we develop a general framework for dynamical systems that can evolve and model a variety of substrates that possess computational capacity. Therefore, dynamical systems can be exploited in the reservoir computing paradigm, i.e., an untrained recurrent nonlinear network with a trained linear readout layer. Moreover, our general framework, called EvoDynamic, is based on an optimized deep neural network library. Hence, generalization and performance can be balanced. The EvoDynamic framework contains three kinds of dynamical systems already implemented, namely cellular automata, random Boolean networks, and echo state networks. The evolution of such systems towards a dynamical behavior, called criticality, is investigated because systems with such behavior may be better suited to do useful computation. The implemented dynamical systems are stochastic and their evolution with genetic algorithm mutates their update rules or network initialization. The obtained results are promising and demonstrate that criticality is achieved. In addition to the presented results, our framework can also be utilized to evolve the dynamical systems connectivity, update and learning rules to improve the quality of the reservoir used for solving computational tasks and physical substrate modeling.},
  keywords = {Criticality,Dynamical systems,Evolution,Implementation,Reservoir computing},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6C6WRWHQ/Pontes-Filho et al. - 2020 - A neuro-inspired general framework for the evolution of stochastic dynamical systems Cellular automata, ran.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TRPKNBD8/Pontes-Filho et al. - 2020 - A neuro-inspired general framework for the evolution of stochastic dynamical systems Cellular automata, ran.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@pontes-filhoNeuroinspiredGeneralFramework2020.md}
}

@article{pontes-filhoNeuroinspiredGeneralFramework2020a,
  title = {A Neuro-Inspired General Framework for the Evolution of Stochastic Dynamical Systems: {{Cellular}} Automata, Random {{Boolean}} Networks and Echo State Networks towards Criticality},
  shorttitle = {A Neuro-Inspired General Framework for the Evolution of Stochastic Dynamical Systems},
  author = {{Pontes-Filho}, Sidney and Lind, Pedro and Yazidi, Anis and Zhang, Jianhua and Hammer, Hugo and Mello, Gustavo B. M. and Sandvig, Ioanna and Tufte, Gunnar and Nichele, Stefano},
  year = {2020},
  month = oct,
  journal = {Cognitive Neurodynamics},
  volume = {14},
  number = {5},
  pages = {657--674},
  issn = {1871-4080, 1871-4099},
  doi = {10.1007/s11571-020-09600-x},
  abstract = {Although deep learning has recently increased in popularity, it suffers from various problems including high computational complexity, energy greedy computation, and lack of scalability, to mention a few. In this paper, we investigate an alternative brain-inspired method for data analysis that circumvents the deep learning drawbacks by taking the actual dynamical behavior of biological neural networks into account. For this purpose, we develop a general framework for dynamical systems that can evolve and model a variety of substrates that possess computational capacity. Therefore, dynamical systems can be exploited in the reservoir computing paradigm, i.e., an untrained recurrent nonlinear network with a trained linear readout layer. Moreover, our general framework, called EvoDynamic, is based on an optimized deep neural network library. Hence, generalization and performance can be balanced. The EvoDynamic framework contains three kinds of dynamical systems already implemented, namely cellular automata, random Boolean networks, and echo state networks. The evolution of such systems towards a dynamical behavior, called criticality, is investigated because systems with such behavior may be better suited to do useful computation. The implemented dynamical systems are stochastic and their evolution with genetic algorithm mutates their update rules or network initialization. The obtained results are promising and demonstrate that criticality is achieved. In addition to the presented results, our framework can also be utilized to evolve the dynamical systems connectivity, update and learning rules to improve the quality of the reservoir used for solving computational tasks and physical substrate modeling.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TLAFL76J/Pontes-Filho et al. - 2020 - A neuro-inspired general framework for the evoluti.pdf;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@pontes-filhoNeuroinspiredGeneralFramework2020a.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/pontes-filhoNeuroinspiredGeneralFramework2020a-mdnotes.md}
}

@article{ponulakIntroductionSpikingNeural2011,
  title = {Introduction to Spiking Neural Networks: {{Information}} Processing, Learning and Applications},
  shorttitle = {Introduction to Spiking Neural Networks},
  author = {Ponulak, Filip and Kasinski, Andrzej},
  year = {2011},
  month = jan,
  journal = {Acta neurobiologiae experimentalis},
  volume = {71},
  number = {4},
  pages = {409--433},
  issn = {1689-0035},
  abstract = {The concept that neural information is encoded in the firing rate of neurons has been the dominant paradigm in neurobiology for many years. This paradigm has also been adopted by the theory of artificial neural networks. Recent physiological experiments demonstrate, however, that in many parts of the nervous system, neural code is founded on the timing of individual action potentials. This finding has given rise to the emergence of a new class of neural models, called spiking neural networks. In this paper we summarize basic properties of spiking neurons and spiking networks. Our focus is, specifically, on models of spike-based information coding, synaptic plasticity and learning. We also survey real-life applications of spiking models. The paper is meant to be an introduction to spiking neural networks for scientists from various disciplines interested in spike-based neural processing.},
  langid = {english},
  pmid = {22237491},
  keywords = {SNN},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WE7CRQNB/Ponulak and Kasiński - Introduction to spiking neural networks Informati.pdf}
}

@article{ponulakSupervisedLearningSpiking2010,
  title = {Supervised {{Learning}} in {{Spiking Neural Networks}} with {{ReSuMe}}: {{Sequence Learning}}, {{Classification}}, and {{Spike Shifting}}},
  shorttitle = {Supervised {{Learning}} in {{Spiking Neural Networks}} with {{ReSuMe}}},
  author = {Ponulak, Filip and Kasi{\'n}ski, Andrzej},
  year = {2010},
  month = feb,
  journal = {Neural Computation},
  volume = {22},
  number = {2},
  pages = {467--510},
  issn = {0899-7667},
  doi = {10.1162/neco.2009.11-08-901},
  abstract = {Learning from instructions or demonstrations is a fundamental property of our brain necessary to acquire new knowledge and develop novel skills or behavioral patterns. This type of learning is thought to be involved in most of our daily routines. Although the concept of instruction-based learning has been studied for several decades, the exact neural mechanisms implementing this process remain unrevealed. One of the central questions in this regard is, How do neurons learn to reproduce template signals (instructions) encoded in precisely timed sequences of spikes?Here we present a model of supervised learning for biologically plausible neurons that addresses this question. In a set of experiments, we demonstrate that our approach enables us to train spiking neurons to reproduce arbitrary template spike patterns in response to given synaptic stimuli even in the presence of various sources of noise. We show that the learning rule can also be used for decision-making tasks. Neurons can be trained to classify categories of input signals based on only a temporal configuration of spikes. The decision is communicated by emitting precisely timed spike trains associated with given input categories. Trained neurons can perform the classification task correctly even if stimuli and corresponding decision times are temporally separated and the relevant information is consequently highly overlapped by the ongoing neural activity.Finally, we demonstrate that neurons can be trained to reproduce sequences of spikes with a controllable time shift with respect to target templates. A reproduced signal can follow or even precede the targets. This surprising result points out that spiking neurons can potentially be applied to forecast the behavior (firing times) of other reference neurons or networks.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HDEX5C33/Supervised-Learning-in-Spiking-Neural-Networks.html}
}

@article{prenticeErrorRobustModesRetinal2016,
  title = {Error-{{Robust Modes}} of the {{Retinal Population Code}}},
  author = {Prentice, Jason S. and Marre, Olivier and Ioffe, Mark L. and Loback, Adrianna R. and Tka{\v c}ik, Ga{\v s}per and Ii, Michael J. Berry},
  year = {2016},
  month = nov,
  journal = {PLOS Computational Biology},
  volume = {12},
  number = {11},
  pages = {e1005148},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005148},
  abstract = {Across the nervous system, certain population spiking patterns are observed far more frequently than others. A hypothesis about this structure is that these collective activity patterns function as population codewords\textendash collective modes\textendash carrying information distinct from that of any single cell. We investigate this phenomenon in recordings of {$\sim$}150 retinal ganglion cells, the retina's output. We develop a novel statistical model that decomposes the population response into modes; it predicts the distribution of spiking activity in the ganglion cell population with high accuracy. We found that the modes represent localized features of the visual stimulus that are distinct from the features represented by single neurons. Modes form clusters of activity states that are readily discriminated from one another. When we repeated the same visual stimulus, we found that the same mode was robustly elicited. These results suggest that retinal ganglion cells' collective signaling is endowed with a form of error-correcting code\textendash a principle that may hold in brain areas beyond retina.},
  langid = {english},
  keywords = {Action potentials,Entropy,Ganglion cells,Hidden Markov models,Neurons,Probability distribution,Retina,Vision},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QX8N6VN6/Prentice et al. - 2016 - Error-Robust Modes of the Retinal Population Code.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@prenticeErrorRobustModesRetinal2016.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5HUI6MMD/article.html}
}

@article{pressePrinciplesMaximumEntropy2013,
  title = {Principles of Maximum Entropy and Maximum Caliber in Statistical Physics},
  author = {Press{\'e}, Steve and Ghosh, Kingshuk and Lee, Julian and Dill, Ken A.},
  year = {2013},
  month = jul,
  journal = {Reviews of Modern Physics},
  volume = {85},
  number = {3},
  pages = {1115--1141},
  doi = {10.1103/RevModPhys.85.1115},
  abstract = {The variational principles called maximum entropy (MaxEnt) and maximum caliber (MaxCal) are reviewed. MaxEnt originated in the statistical physics of Boltzmann and Gibbs, as a theoretical tool for predicting the equilibrium states of thermal systems. Later, entropy maximization was also applied to matters of information, signal transmission, and image reconstruction. Recently, since the work of Shore and Johnson, MaxEnt has been regarded as a principle that is broader than either physics or information alone. MaxEnt is a procedure that ensures that inferences drawn from stochastic data satisfy basic self-consistency requirements. The different historical justifications for the entropy S=-{$\sum$}ipilog⁡pi and its corresponding variational principles are reviewed. As an illustration of the broadening purview of maximum entropy principles, maximum caliber, which is path entropy maximization applied to the trajectories of dynamical systems, is also reviewed. Examples are given in which maximum caliber is used to interpret dynamical fluctuations in biology and on the nanoscale, in single-molecule and few-particle systems such as molecular motors, chemical reactions, biological feedback circuits, and diffusion in microfluidics devices.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TSY9RKPP/Pressé et al. - 2013 - Principles of maximum entropy and maximum caliber in statistical physics.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@pressePrinciplesMaximumEntropy2013.md}
}

@article{princeCCNGACWorkshop2021,
  title = {{{CCN GAC Workshop}}: {{Issues}} with Learning in Biological Recurrent Neural Networks},
  shorttitle = {{{CCN GAC Workshop}}},
  author = {Prince, Luke Y. and Boven, Ellen and Eyono, Roy Henha and Ghosh, Arna and Pemberton, Joe and Scherr, Franz and Clopath, Claudia and Costa, Rui Ponte and Maass, Wolfgang and Richards, Blake A.},
  year = {2021},
  journal = {arXiv preprint arXiv:2105.05382},
  eprint = {2105.05382},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{princeCurrentStateFuture2022,
  title = {Current {{State}} and {{Future Directions}} for {{Learning}} in {{Biological Recurrent Neural Networks}}: {{A Perspective Piece}}},
  shorttitle = {Current {{State}} and {{Future Directions}} for {{Learning}} in {{Biological Recurrent Neural Networks}}},
  author = {Prince, Luke Y. and Eyono, Roy Henha and Boven, Ellen and Ghosh, Arna and Pemberton, Joe and Scherr, Franz and Clopath, Claudia and Costa, Rui Ponte and Maass, Wolfgang and Richards, Blake A. and Savin, Cristina and Wilmes, Katharina Anna},
  year = {2022},
  month = jan,
  journal = {arXiv:2105.05382 [cs, q-bio]},
  eprint = {2105.05382},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {We provide a brief review of the common assumptions about biological learning with findings from experimental neuroscience and contrast them with the efficiency of gradient-based learning in recurrent neural networks. The key issues discussed in this review include: synaptic plasticity, neural circuits, theory-experiment divide, and objective functions. We conclude with recommendations for both theoretical and experimental neuroscientists when designing new studies that could help bring clarity to these issues.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/G5BCFLWD/Prince et al_2022_Current State and Future Directions for Learning in Biological Recurrent Neural.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TT4WJ34W/2105.html}
}

@article{princeNeocorticalInhibitoryInterneuron2021,
  title = {Neocortical Inhibitory Interneuron Subtypes Are Differentially Attuned to Synchrony- and Rate-Coded Information},
  author = {Prince, Luke Y. and Tran, Matthew M. and Grey, Dorian and Saad, Lydia and Chasiotis, Helen and Kwag, Jeehyun and Kohl, Michael M. and Richards, Blake A.},
  year = {2021},
  month = aug,
  journal = {Communications Biology},
  volume = {4},
  number = {1},
  pages = {1--16},
  publisher = {{Nature Publishing Group}},
  issn = {2399-3642},
  doi = {10.1038/s42003-021-02437-y},
  abstract = {Neurons can carry information with both the synchrony and rate of their spikes. However, it is unknown whether distinct subtypes of neurons are more sensitive to information carried by synchrony versus rate, or vice versa. Here, we address this question using patterned optical stimulation in slices of somatosensory cortex from mouse lines labelling fast-spiking (FS) and regular-spiking (RS) interneurons. We used optical stimulation in layer 2/3 to encode a 1-bit signal using either the synchrony or rate of activity. We then examined the mutual information between this signal and the interneuron responses. We found that for a synchrony encoding, FS interneurons carried more information in the first five milliseconds, while both interneuron subtypes carried more information than excitatory neurons in later responses. For a rate encoding, we found that RS interneurons carried more information after several milliseconds. These data demonstrate that distinct interneuron subtypes in the neocortex have distinct sensitivities to synchrony versus rate codes.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Cellular neuroscience,Neural circuits,Whisker system},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Prince et al_2021_Neocortical inhibitory interneuron subtypes are differentially attuned to.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SVW5H59V/s42003-021-02437-y.html}
}

@article{proppExactSamplingCoupled1996,
  title = {Exact Sampling with Coupled {{Markov}} Chains and Applications to Statistical Mechanics},
  author = {Propp, James Gary and Wilson, David Bruce},
  year = {1996},
  journal = {Random Structures \& Algorithms},
  volume = {9},
  number = {1-2},
  pages = {223--252},
  issn = {1098-2418},
  doi = {10.1002/(SICI)1098-2418(199608/09)9:1/2<223::AID-RSA14>3.0.CO;2-O},
  abstract = {For many applications it is useful to sample from a finite set of objects in accordance with some particular distribution. One approach is to run an ergodic (i.e., irreducible aperiodic) Markov chain whose stationary distribution is the desired distribution on this set; after the Markov chain has run for M steps, with M sufficiently large, the distribution governing the state of the chain approximates the desired distribution. Unfortunately, it can be difficult to determine how large M needs to be. We describe a simple variant of this method that determines on its own when to stop and that outputs samples in exact accordance with the desired distribution. The method uses couplings which have also played a role in other sampling schemes; however, rather than running the coupled chains from the present into the future, one runs from a distant point in the past up until the present, where the distance into the past that one needs to go is determined during the running of the algorithm itself. If the state space has a partial order that is preserved under the moves of the Markov chain, then the coupling is often particularly efficient. Using our approach, one can sample from the Gibbs distributions associated with various statistical mechanics models (including Ising, random-cluster, ice, and dimer) or choose uniformly at random from the elements of a finite distributive lattice. \textcopyright{} 1996 John Wiley \& Sons, Inc.},
  langid = {english},
  keywords = {chain,markov,mc},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291098-2418\%28199608/09\%299\%3A1/2\%3C223\%3A\%3AAID-RSA14\%3E3.0.CO\%3B2-O},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VDBY2FMY/Propp and Wilson - 1996 - Exact sampling with coupled Markov chains and appl.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@proppExactSamplingCoupled1996.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QU4F7I7C/2223AID-RSA143.0.html}
}

@article{puchallaRedundancyPopulationCode2005,
  title = {Redundancy in the Population Code of the Retina},
  author = {Puchalla, Jason L. and Schneidman, Elad and Harris, Robert A. and Berry, Michael J.},
  year = {2005},
  month = may,
  journal = {Neuron},
  volume = {46},
  number = {3},
  pages = {493--504},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2005.03.026},
  abstract = {We have explored the manner in which the population of retinal ganglion cells collectively represent the visual world. Ganglion cells in the salamander were recorded simultaneously with a multielectrode array during stimulation with both artificial and natural visual stimuli, and the mutual information that single cells and pairs of cells conveyed about the stimulus was estimated. We found significant redundancy between cells spaced as far as 500 mum apart. When we used standard methods for defining functional types, only ON-type and OFF-type cells emerged as truly independent information channels. Although the average redundancy between nearby cell pairs was moderate, each ganglion cell shared information with many neighbors, so that visual information was represented approximately 10-fold within the ganglion cell population. This high degree of retinal redundancy suggests that design principles beyond coding efficiency may be important at the population level.},
  langid = {english},
  pmid = {15882648},
  keywords = {Animals,Models; Neurological,Photic Stimulation,Retinal Ganglion Cells,Urodela,Visual Pathways,Visual Perception},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@puchallaRedundancyPopulationCode2005.md}
}

@article{pulvermullerBiologicalConstraintsNeural2021,
  title = {Biological Constraints on Neural Network Models of Cognitive Function},
  author = {Pulverm{\"u}ller, Friedemann and Tomasello, Rosario and {Henningsen-Schomers}, Malte R. and Wennekers, Thomas},
  year = {2021},
  month = jun,
  journal = {Nature Reviews Neuroscience},
  pages = {1--15},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-021-00473-5},
  abstract = {Neural network models have potential for improving our understanding of brain functions. In this Perspective, Pulverm\"uller and colleagues examine various aspects of such models that may need to be constrained to make them more neurobiologically realistic and therefore better tools for understanding brain function.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Cognitive neuroscience;Human behaviour;Network models Subject\_term\_id: cognitive-neuroscience;human-behaviour;network-models},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BUBWDDB7/s41583-021-00473-5.html}
}

@article{pulvermullerBiologicalConstraintsNeural2021a,
  title = {Biological Constraints on Neural Network Models of Cognitive Function},
  author = {Pulverm{\"u}ller, Friedemann and Tomasello, Rosario and {Henningsen-Schomers}, Malte R. and Wennekers, Thomas},
  year = {2021},
  month = jun,
  journal = {Nature Reviews Neuroscience},
  pages = {1--15},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-021-00473-5},
  abstract = {Neural network models are potential tools for improving our understanding of complex brain functions. To address this goal, these models need to be neurobiologically realistic. However, although neural networks have advanced dramatically in recent years and even achieve human-like performance on complex perceptual and cognitive tasks, their similarity to aspects of brain anatomy and physiology is imperfect. Here, we discuss different types of neural models, including localist, auto-associative, hetero-associative, deep and whole-brain networks, and identify aspects under which their biological plausibility can be improved. These aspects range from the choice of model neurons and of mechanisms of synaptic plasticity and learning to implementation of inhibition and control, along with neuroanatomical properties including areal structure and local and long-range connectivity. We highlight recent advances in developing biologically grounded cognitive theories and in mechanistically explaining, on the basis of these brain-constrained neural models, hitherto unaddressed issues regarding the nature, localization and ontogenetic and phylogenetic development of higher brain functions. In closing, we point to possible future clinical applications of brain-constrained modelling.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Cognitive neuroscience;Human behaviour;Network models Subject\_term\_id: cognitive-neuroscience;human-behaviour;network-models},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FAFUB77T/Pulvermüller et al_2021_Biological constraints on neural network models of cognitive function.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PUXH3SDR/s41583-021-00473-5.html}
}

@article{rajakumarStimulusDrivenSpontaneousDynamics2021,
  title = {Stimulus-{{Driven}} and {{Spontaneous Dynamics}} in {{Excitatory-Inhibitory Recurrent Neural Networks}} for {{Sequence Representation}}},
  author = {Rajakumar, A. and Rinzel, J. and Chen, Z.},
  year = {2021},
  journal = {Neural Computation},
  doi = {10.1162/neco_a_01418},
  abstract = {A general framework for understanding neural sequence representation in the excitatory-inhibitory RNN is provided and the stability of dynamic attractors while training the RNN to learn two sequences is examined. Abstract Recurrent neural networks (RNNs) have been widely used to model sequential neural dynamics (``neural sequences'') of cortical circuits in cognitive and motor tasks. Efforts to incorporate biological constraints and Dale's principle will help elucidate the neural representations and mechanisms of underlying circuits. We trained an excitatory-inhibitory RNN to learn neural sequences in a supervised manner and studied the representations and dynamic attractors of the trained network. The trained RNN was robust to trigger the sequence in response to various input signals and interpolated a time-warped input for sequence representation. Interestingly, a learned sequence can repeat periodically when the RNN evolved beyond the duration of a single sequence. The eigenspectrum of the learned recurrent connectivity matrix with growing or damping modes, together with the RNN's nonlinearity, were adequate to generate a limit cycle attractor. We further examined the stability of dynamic attractors while training the RNN to learn two sequences. Together, our results provide a general framework for understanding neural sequence representation in the excitatory-inhibitory RNN.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X6TP8JCE/Rajakumar et al_2021_Stimulus-Driven and Spontaneous Dynamics in Excitatory-Inhibitory Recurrent.pdf}
}

@article{ramsauerHopfieldNetworksAll2020,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  year = {2020},
  month = dec,
  journal = {arXiv:2008.02217 [cs, stat]},
  eprint = {2008.02217},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  note = {Comment: 10 pages (+ appendix); 12 figures; Blog: https://ml-jku.github.io/hopfield-layers/; GitHub: https://github.com/ml-jku/hopfield-layers},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/T942KAZL/Ramsauer et al. - 2020 - Hopfield Networks is All You Need.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ramsauerHopfieldNetworksAll2020.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PP7KJ8VZ/2008.html}
}

@techreport{ranganathBlackBoxVariational,
  title = {Black {{Box Variational Inference}}},
  author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M},
  abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis. These efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochas-tic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TWQ7T3H8/Ranganath, Gerrish, Blei - Unknown - Black Box Variational Inference.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ranganathBlackBoxVariational.md}
}

@article{raudiesDifferencesVisualSpatialInput2015,
  title = {Differences in {{Visual-Spatial Input May Underlie Different Compression Properties}} of {{Firing Fields}} for {{Grid Cell Modules}} in {{Medial Entorhinal Cortex}}},
  author = {Raudies, Florian and Hasselmo, Michael E.},
  year = {2015},
  journal = {PLoS Computational Biology},
  volume = {11},
  number = {11},
  pages = {1--27},
  doi = {10.1371/journal.pcbi.1004596},
  abstract = {Firing fields of grid cells in medial entorhinal cortex show compression or expansion after manipulations of the location of environmental barriers. This compression or expansion could be selective for individual grid cell modules with particular properties of spatial scaling. We present a model for differences in the response of modules to barrier location that arise from different mechanisms for the influence of visual features on the computation of location that drives grid cell firing patterns. These differences could arise from differences in the position of visual features within the visual field. When location was computed from the movement of visual features on the ground plane (optic flow) in the ventral visual field, this resulted in grid cell spatial firing that was not sensitive to barrier location in modules modeled with small spacing between grid cell firing fields. In contrast, when location was computed from static visual features on walls of barriers, i.e. in the more dorsal visual field, this resulted in grid cell spatial firing that compressed or expanded based on the barrier locations in modules modeled with large spacing between grid cell firing fields. This indicates that different grid cell modules might have differential properties for computing location based on visual cues, or the spatial radius of sensitivity to visual cues might differ between modules.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/G574PYDN/Raudies, Hasselmo - 2015 - Differences in Visual-Spatial Input May Underlie Different Compression Properties of Firing Fields for Grid C.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@raudiesDifferencesVisualSpatialInput2015.md}
}

@article{ravichandran-schmidtTestingStatedependentModel2022,
  title = {Testing the State-Dependent Model of Time Perception against Experimental Evidence},
  author = {{Ravichandran-Schmidt}, Pirathitha and Hass, Joachim},
  year = {2022},
  journal = {bioRxiv},
  doi = {10.1101/2021.12.31.474629},
  abstract = {This work incorporates the state-dependent computational model, which encodes time in the dynamic evolution of network states without the need for a specific network structure into a biologically plausible prefrontal cortex (PFC) model based on in vivo and in vitro recordings of rodents and shows that the naturally occurring heterogeneity in cellular and synaptic parameters in the PFC is sufficient to encode time over several hundreds of milliseconds. Coordinated movements, speech and other actions are impossible without precise timing. Realistic computational models of interval timing in the mammalian brain are expected to provide key insights into the underlying mechanisms of timing. Existing computational models of time perception have only been partially replicating experimental observations, such as the linear increase of time, the dopaminergic modulation of this increase, and the scalar property, i.e., the linear increase of the standard deviation of temporal estimates. In this work, we incorporate the state-dependent computational model, which encodes time in the dynamic evolution of network states without the need for a specific network structure into a biologically plausible prefrontal cortex (PFC) model based on in vivo and in vitro recordings of rodents. Specifically, we stimulated 1000 neurons in the beginning and in the end of a range of different time intervals, extracted states of neurons and trained the readout layer based on these states using least squares to predict the respective inter stimulus interval. We show that the naturally occurring heterogeneity in cellular and synaptic parameters in the PFC is sufficient to encode time over several hundreds of milliseconds. The readout faithfully represents the duration between two stimuli applied to the superficial layers of the network, thus fulfilling the requirement of a linear encoding of time. A simulated activation of the D2 dopamine receptor leads to an overestimation and an inactivation to an underestimation of time, in line with experimental results. Furthermore, we show that the scalar property holds true for intervals of several hundred milliseconds, and provide a mechanistic explanation for the origin of the scalar property as well as its deviations. We conclude that this model can represent durations up to 750 ms in a biophysically plausible setting, compatible with experimental findings in this regime.},
  keywords = {reservoir},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2VI8L8YV/Ravichandran-Schmidt_Hass_2022_Testing the state-dependent model of time perception against experimental.pdf}
}

@article{ravikumarHighdimensionalIsingModel2010,
  title = {High-Dimensional {{Ising}} Model Selection Using L1-Regularized Logistic Regression},
  author = {Ravikumar, Pradeep and Wainwright, Martin J and Lafferty, John D},
  year = {2010},
  journal = {The Annals of Statistics},
  volume = {38},
  number = {3},
  pages = {1287--1319},
  doi = {10.1214/09-AOS691},
  abstract = {We consider the problem of estimating the graph associated with a binary Ising Markov random field. We describe a method based on 1-regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an 1-constraint. The method is analyzed under high-dimensional scaling in which both the number of nodes p and maximum neighborhood size d are allowed to grow as a function of the number of observations n. Our main results provide sufficient conditions on the triple (n, p, d) and the model parameters for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. With coherence conditions imposed on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes n = (d 3 log p) with exponentially decaying error. When these same conditions are imposed directly on the sample matrices, we show that a reduced sample size of n = (d 2 log p) suffices for the method to estimate neighborhoods consistently. Although this paper focuses on the binary graphical models, we indicate how a generalization of the method of the paper would apply to general discrete Markov random fields.},
  keywords = {62F12,68T99,convex risk minimization,Graphical models,high-dimensional asymptotics,l1-regularization,Markov random fields,model selection,structure learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/22C3BSKR/Ravikumar, Wainwright, Lafferty - 2010 - High-dimensional Ising model selection using l1-regularized logistic regression.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ravikumarHighdimensionalIsingModel2010.md}
}

@article{recanatesiMetastableAttractorsExplain2021,
  title = {Metastable {{Attractors Explain}} the {{Variable Timing}} of {{Stable Behavioral Action Sequences}}},
  author = {Recanatesi, Stefano and Pereira, Ulises and Murakami, M. and Mainen, Z. and Mazzucato, L.},
  year = {2021},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.3869115},
  abstract = {A robust network motif is suggested as a novel mechanism to support critical aspects of animal behavior and a framework for investigating its circuit origins via correlated variability is established. Natural animal behavior displays rich lexical and temporal dynamics, even in a stable environment. This implies that behavioral variability arises from sources within the brain, but the origin and mechanics of these processes remain largely unknown. Here, we focus on the observation that the timing of self-initiated actions shows large variability even when they are executed in stable, well-learned sequences. Could this mix of reliability and stochasticity arise within the same circuit? We trained rats to perform a stereotyped sequence of self-initiated actions and recorded neural ensemble activity in secondary motor cortex (M2), which is known to reflect trial-by-trial action timing fluctuations. Using hidden Markov models we established a robust and accurate dictionary between ensemble activity patterns and actions. We then showed that metastable attractors, representing activity patterns with the requisite combination of reliable sequential structure and high transition timing variability, could be produced by reciprocally coupling a high dimensional recurrent network and a low dimensional feedforward one. Transitions between attractors were generated by correlated variability arising from the feedback loop between the two networks. This mechanism predicted a specific structure of low-dimensional noise correlations that were empirically verified in M2 ensemble dynamics. This work suggests a robust network motif as a novel mechanism to support critical aspects of animal behavior and establishes a framework for investigating its circuit origins via correlated variability.    Funding Information: L.M. was supported by a National Institute of Deafness and Other Communication Disorders Grant K25-DC013557, by National Institute of Neurological Disorders and Stroke Grant R01-NS118461 (BRAIN Initiative), and by a Swartz Foundation Award 66438.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DHT38I9L/Recanatesi et al_2021_Metastable Attractors Explain the Variable Timing of Stable Behavioral Action.pdf}
}

@misc{RewardEnough,
  title = {Reward Is {{Enough}}},
  abstract = {In this paper we hypothesise that the objective of maximising reward is  enough to drive behaviour that exhibits most if not all attributes of  intelligence that are studied in natural and artificial intelligence,  including knowledge, learning, perception, social intelligence, language  and generalisation. This is in contrast to the view that specialised  problem formulations are needed for each attribute of intelligence, based  on other signals or objectives. The reward-is-enough hypothesis suggests  that agents with powerful reinforcement learning algorithms when placed in  rich environments with simple rewards could develop the kind of broad,  multi-attribute intelligence that constitutes an artificial general  intelligence.},
  howpublished = {https://www.deepmind.com/publications/reward-is-enough},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/22GG5DAY/reward-is-enough.html}
}

@misc{RewardEnoughElsevier,
  title = {Reward Is Enough | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.artint.2021.103535},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0004370221000862?token=6BBFAE4560D90D89DF1E69216BAEB49700C8D931B7A6AC474DA86E3837250D204751721AFDF27923E9F3C240C7AB0F4D\&originRegion=eu-west-1\&originCreation=20220522145915},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HDZYRNJG/S0004370221000862.html}
}

@techreport{rezendeTamingVAEs,
  title = {Taming {{VAEs}}},
  author = {Rezende, Danilo J and Viola, Fabio},
  abstract = {In spite of remarkable progress in deep latent variable generative modeling, training still remains a challenge due to a combination of optimization and generalization issues. In practice, a combination of heuristic algorithms (such as hand-crafted annealing of KL-terms) is often used in order to achieve the desired results, but such solutions are not robust to changes in model architecture or dataset. The best settings can often vary dramatically from one problem to another, which requires doing expensive parameter sweeps for each new case. Here we develop on the idea of training VAEs with additional constraints as a way to control their behaviour. We first present a detailed theoretical analysis of constrained VAEs, expanding our understanding of how these models work. We then introduce and analyze a practical algorithm termed Generalized ELBO with Constrained Optimization, GECO. The main advantage of GECO for the machine learning practitioner is a more intuitive, yet principled, process of tuning the loss. This involves defining of a set of constraints, which typically have an explicit relation to the desired model performance, in contrast to tweaking abstract hyper-parameters which implicitly affect the model behavior. Encouraging experimental results in several standard datasets indicate that GECO is a very robust and effective tool to balance reconstruction and compression constraints.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/V8NSB3FH/Rezende, Viola - Unknown - Taming VAEs.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@rezendeTamingVAEs.md}
}

@techreport{ricci-tersenghiBetheApproximationSolving2012,
  title = {The {{Bethe}} Approximation for Solving the Inverse {{Ising}} Problem: A Comparison with Other Inference Methods},
  author = {{Ricci-Tersenghi}, Federico},
  year = {2012},
  abstract = {The inverse Ising problem consists in inferring the coupling constants of an Ising model given the correlation matrix. The fastest methods for solving this problem are based on mean-field approximations, but which one performs better in the general case is still not completely clear. In the first part of this work, I summarize the formulas for several mean-field approximations and I derive new analytical expressions for the Bethe approximation, which allow to solve the inverse Ising problem without running the Susceptibility Propagation algorithm (thus avoiding the lack of convergence). In the second part, I compare the accuracy of different mean field approximations on several models (diluted ferromagnets and spin glasses) defined on random graphs and regular lattices, showing which one is in general more effective. A simple improvement over these approximations is proposed. Also a fundamental limitation is found in using methods based on TAP and Bethe approximations in presence of an external field. Mean-field approximations (MFA) are very important tools in statistical mechanics, since they provide an approximated description of a physical system in terms of few parameters (e.g. local magnetizations). Among MFA the one based on the Bethe approximation (BA) is very effective. In recent years the BA-originally derived for the ferromagnetic model on regular lattices [1]-has been extended, under the name of Cavity Method, to models having arbitrary couplings and topologies [2]. Although the BA is exact only for tree-like topologies, its application to models defined on random graphs has proved very successful: see e.g. the cases of low-density parity check codes, spin glasses and constraint satisfaction problems, all nicely reviewed in Ref. 3. The inverse Ising problem, originally known as Boltzmann machine learning, consists in inferring coupling constants of an Ising model (both pairwise interactions and external fields) given the vector of magnetizations and the matrix of pairwise correlations. In recent years the inverse Ising problem has received a lot of attention, specially in connection to inference in biological problems [4-7]. The inverse Ising problems can be viewed as the dual problem with respect to the 'direct' problem of estimating magnetizations and correlation given the Hamiltonian. So, under any MFA,},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6ASD8B85/Ricci-Tersenghi - 2012 - The Bethe approximation for solving the inverse Ising problem a comparison with other inference methods.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ricci-tersenghiBetheApproximationSolving2012.md}
}

@article{richardsDeepLearningFramework2019,
  title = {A Deep Learning Framework for Neuroscience},
  author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and {de Berker}, Archy and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Ken and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, Jo{\~a}o and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
  year = {2019},
  month = nov,
  journal = {Nature neuroscience},
  volume = {22},
  number = {11},
  pages = {1761--1770},
  issn = {1097-6256},
  doi = {10.1038/s41593-019-0520-2},
  abstract = {Systems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. Conversely, artificial intelligence attempts to design computational systems based on the tasks they will have to solve. In the case of artificial neural networks, the three components specified by design are the objective functions, the learning rules, and architectures. With the growing success of deep learning, which utilizes brain-inspired architectures, these three designed components have increasingly become central to how we model, engineer and optimize complex artificial learning systems. Here we argue that a greater focus on these components would also benefit systems neuroscience. We give examples of how this optimization-based framework can drive theoretical and experimental progress in neuroscience. We contend that this principled perspective on systems neuroscience will help to generate more rapid progress.},
  pmcid = {PMC7115933},
  pmid = {31659335},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/T6I8LB5F/Richards et al_2019_A deep learning framework for neuroscience.pdf}
}

@article{rissanenFisherInformationStochastic1996,
  title = {Fisher {{Information}} and {{Stochastic Complexity}}},
  author = {Rissanen, Jorma J},
  year = {1996},
  volume = {42},
  number = {1},
  pages = {40--47},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZSPKPK9E/Rissanen - 1996 - Fisher Information and Stochastic Complexity.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@rissanenFisherInformationStochastic1996.md}
}

@article{rissanenModellingShortestData1978,
  title = {Modelling by the Shortest Data Description},
  author = {Rissanen, J},
  year = {1978},
  journal = {Automatica},
  volume = {14},
  pages = {465--471},
  abstract = {A computable version of Kolmogorov Complexity in statistical modelling. Usable to automate model selection.},
  keywords = {COMBIB. COMPLEXITY. HAVE. KOLMOGOROV. STATISTICS},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MZBIYQH8/Modeling By Shortest Data Description.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@rissanenModellingShortestData1978.md}
}

@article{rissanenStochasticComplexityMDL1987,
  title = {Stochastic Complexity and the {{MDL}} Principle},
  author = {Rissanen, Jorma},
  year = {1987},
  journal = {Econometric Reviews},
  volume = {6},
  number = {1},
  pages = {85--102},
  doi = {10.1080/07474938708800126},
  abstract = {A search for the stochastic complexity of the observed data, as the greatest lower bound with which the data can be encoded, represents a global maximum likelihood principle, which permits comparison of models regardless of the number of parameters in them. For important special classes, such as the gaussian and the multinomial models, formulas for the stochastic complexity give new and powerful model selection criteria, while in the general case approximations can be computed with the MDL principle. Once a model is found with which the stochastic complexity is reached, there is nothing further to learn from the data with the proposed models. The basic notions are reviewed and numerical examples are given. \textcopyright{} 1987, Taylor \& Francis Group, LLC. All rights reserved.},
  keywords = {estimation of laws,global maximum likelihood,shortest code length,small sample hypothesis testing},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/C2WLFDY8/rissanen1987.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@rissanenStochasticComplexityMDL1987.md}
}

@techreport{robertMetropolisHastingsAlgorithm,
  title = {The {{Metropolis-Hastings}} Algorithm},
  author = {Robert, C P},
  abstract = {This article is a self-contained introduction to the Metropolis-Hastings algorithm, this ubiquitous tool for producing dependent simulations from an arbitrary distribution. The document illustrates the principles of the methodology on simple examples with R codes and provides entries to the recent extensions of the method.},
  keywords = {and phrases: Bayesian inference,Gibbs sampler,Hamiltonian Monte Carlo,intractable density,Langevin diffusion,Markov chains,MCMC meth-ods,Metropolis-Hastings algorithm},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VW96JA5H/Robert - Unknown - The Metropolis-Hastings algorithm.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@robertMetropolisHastingsAlgorithm.md}
}

@article{robertsonTrendsGeneralSystems1973,
  title = {Trends in {{General Systems Theory}}},
  author = {Robertson, Roland and Klir, George J.},
  year = {1973},
  journal = {The British Journal of Sociology},
  volume = {24},
  number = {1},
  pages = {119--119},
  doi = {10.2307/588807},
  keywords = {http://archive.org/details/TrendsInGeneralSystemsT},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SH4UBCSC/Robertson, Klir - 1973 - Trends in General Systems Theory.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@robertsonTrendsGeneralSystems1973.md}
}

@article{roelfsemaControlSynapticPlasticity2018,
  title = {Control of Synaptic Plasticity in Deep Cortical Networks},
  author = {Roelfsema, Pieter R. and Holtmaat, Anthony},
  year = {2018},
  month = mar,
  journal = {Nature Reviews Neuroscience},
  volume = {19},
  number = {3},
  pages = {166--180},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn.2018.6},
  abstract = {In addition to presynaptic and postsynaptic mechanisms, synaptic plasticity depends on neuromodulatory substances and feedback connections from higher-order cortical and thalamic brain regionsSynaptic plasticity in the brain depends on reward-prediction errors and on selective attention. Neuromodulatory systems code for the reward-prediction errors, and feedback connections from the response-selection stage mediate top-down attention effectsThe combined influence of feedback connections and neuromodulatory substances on plasticity enables powerful learning rules for the training of 'deep', multilayered neuronal networksFeedback connections project to cortical layers that are distinct from feedforward input, where they impinge on distal dendritic segments, separate excitatory neuronal populations or inhibitory interneuronsFeedback connections gate plasticity in cortical pyramidal neurons by promoting NMDA-receptor-driven calcium entry into dendrites and by disinhibiting the cortical column through activation of vasoactive-intestinal-peptide-positive interneurons (among others)Synaptic tags are biochemical processes that make synapses eligible for plasticity. Neuromodulators released later can interact with tagged synapses to increase or decrease synaptic strength},
  copyright = {2018 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Cortex,Neurotransmitters,Spike-timing-dependent plasticity,Synaptic plasticity,Thalamus},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Cortex;Neurotransmitters;Spike-timing-dependent plasticity;Synaptic plasticity;Thalamus Subject\_term\_id: cortex;neurotransmitters;spike-timing-dependent-plasticity;synaptic-plasticity;thalamus},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Roelfsema_Holtmaat_2018_Control of synaptic plasticity in deep cortical networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/X3QWXYQS/nrn.2018.html}
}

@article{rollsInformationSpatialView1998,
  title = {Information {{About Spatial View}} in an {{Ensemble}} of {{Primate Hippocampal Cells}}},
  author = {Rolls, Edmund T. and Treves, Alessandro and Robertson, Robert G. and {Georges-Fran{\c c}ois}, Pierre and Panzeri, Stefano},
  year = {1998},
  journal = {Journal of Neurophysiology},
  volume = {79},
  number = {4},
  pages = {1797--1813},
  issn = {0022-3077 (Print)\textbackslash r0022-3077 (Linking)},
  doi = {10.1152/jn.1998.79.4.1797},
  abstract = {Hippocampal function was analyzed by making recordings from hippocampal neurons in monkeys actively walking in the laboratory. "Spatial view" cells, which respond when the monkey looks at a part of the environment, were analyzed. To assess quantitatively the information about the spatial environment represented by these cells, we applied information theoretic techniques to their responses. The average information provided by these cells about which location the monkey was looking at was 0.32 bits, and the mean across cells of the maximum information conveyed about which location was being looked at was 1.19 bits, measured in a period of 0.5 s. There were 16 locations for this analysis, each being one-quarter of one of the walls of the room. It also was shown that the mean spontaneous rate of firing of the neurons was 0.1 spikes/s, that the mean firing rate in the center of the spatial field of the neurons was 13.2 spikes/s, and that the mean sparseness of the representation measured in a 25-ms period was 0.04 and in a 500-ms time period was 0.19. (The sparseness is approximately equivalent to the proportion of the 25- or 500-ms periods in which the neurons showed one or more spikes.) Next it was shown that the mean size of the view fields of the neurons was 0.9 of a wall. In an approach to the issue of how an ensemble of neurons might together provide more precise information about spatial location than a single neuron, it was shown that in general the neurons had different centers for their view fields. It then was shown that the information from an ensemble of these cells about where in space is being looked at increases approximately linearly with the number of cells in the ensemble. This indicates that the number of places that can be represented increases approximately exponentially with the number of cells in the population. It is concluded that there is an accurate representation of space "out there" in the primate hippocampus. This representation of space out there would be an appropriate part of a primate memory system involved in memories of where in an environment an object was seen, and more generally in the memory of particular events or episodes, for which a spatial component normally provides part of the context.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7AK85GTA/Rolls et al. - 1998 - Information About Spatial View in an Ensemble of Primate Hippocampal Cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@rollsInformationSpatialView1998.md}
}

@article{rollsNeuralNetworksBrain1997,
  title = {Neural {{Networks}} and {{Brain Function Abstract}} and {{Keywords}}},
  author = {Rolls, Edmund and Treves, Alessandro},
  year = {1997},
  number = {October 2013},
  pages = {1--29},
  issn = {9780198524328},
  doi = {10.1093/acprof},
  keywords = {autoassociators,brain systems,competitive networks,multilayer backpropagation,neural networks,neurocomputation,neuroscience,pattern associators},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5YW7WPH3/Rolls, Treves - 1997 - Neural Networks and Brain Function Abstract and Keywords.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@rollsNeuralNetworksBrain1997.md}
}

@inproceedings{rolnickExperienceReplayContinual2019a,
  title = {Experience {{Replay}} for {{Continual Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy and Wayne, Gregory},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Interacting with a complex world involves continual learning, in which tasks and data distributions change over time. A continual learning system should demonstrate both plasticity (acquisition of new knowledge) and stability (preservation of old knowledge). Catastrophic forgetting is the failure of stability, in which new experience overwrites previous experience. In the brain, replay of past experience is widely believed to reduce forgetting, yet it has been largely overlooked as a solution to forgetting in deep reinforcement learning. Here, we introduce CLEAR, a replay-based method that greatly reduces catastrophic forgetting in multi-task reinforcement learning. CLEAR leverages off-policy learning and behavioral cloning from replay to enhance stability, as well as on-policy learning to preserve plasticity. We show that CLEAR performs better than state-of-the-art deep learning techniques for mitigating forgetting, despite being significantly less complicated and not requiring any knowledge of the individual tasks being learned.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3IT9DXZK/Rolnick et al_2019_Experience Replay for Continual Learning.pdf}
}

@article{rongalaCuneateSpikingNeural2020,
  title = {Cuneate Spiking Neural Network Learning to Classify Naturalistic Texture Stimuli under Varying Sensing Conditions},
  author = {Rongala, Udaya B. and Mazzoni, Alberto and Spanne, Anton and J{\"o}rntell, Henrik and Oddo, Calogero M.},
  year = {2020},
  month = mar,
  journal = {Neural Networks},
  volume = {123},
  pages = {273--287},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.11.020},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KZ68ZMPT/Rongala et al_2020_Cuneate spiking neural network learning to classify naturalistic texture.pdf}
}

@inproceedings{rosasOptimizingCodeRate2014,
  title = {Optimizing the Code Rate for Achieving Energy-Efficient Wireless Communications},
  booktitle = {2014 {{IEEE Wireless Communications}} and {{Networking Conference}} ({{WCNC}})},
  author = {Rosas, Fernando and Brante, Glauber and Souza, Richard Demo and Oberli, Christian},
  year = {2014},
  month = apr,
  pages = {775--780},
  issn = {1558-2612},
  doi = {10.1109/WCNC.2014.6952166},
  abstract = {Error correcting coding is a well-known technique for reducing the required signal-to-noise ratio (SNR) needed to attain a given bit error-rate. Nevertheless, this reduction comes at the cost of extra energy consumption introduced by the baseband processing required for encoding and decoding the data. No complete analysis of the trade-off between coding gain and baseband consumption of communications over wireless channels has been reported so far. In this paper, we study the energy-consumption of BCH codes with various code rates over AWGN and Rayleigh fading channels. Our results show that codes with low code rate are optimal for performing long-range communications, while is better to use less coding redundancy when the transmission distance is short. Our results also show that the transmission range of a low-power communication device can be increased up to 25\% for AWGN channels and up to 300\% for Rayleigh fading channels by using an optimized BCH code.},
  keywords = {Decoding,Encoding,Energy consumption,Fading,Power demand,Receivers,Signal to noise ratio},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YQ97F3I4/6952166.html}
}

@inproceedings{rosasOptimizingCodeRate2014a,
  title = {Optimizing the Code Rate for Achieving Energy-Efficient Wireless Communications},
  booktitle = {2014 {{IEEE Wireless Communications}} and {{Networking Conference}} ({{WCNC}})},
  author = {Rosas, Fernando and Brante, Glauber and Souza, Richard Demo and Oberli, Christian},
  year = {2014},
  month = apr,
  pages = {775--780},
  issn = {1558-2612},
  doi = {10.1109/WCNC.2014.6952166},
  abstract = {Error correcting coding is a well-known technique for reducing the required signal-to-noise ratio (SNR) needed to attain a given bit error-rate. Nevertheless, this reduction comes at the cost of extra energy consumption introduced by the baseband processing required for encoding and decoding the data. No complete analysis of the trade-off between coding gain and baseband consumption of communications over wireless channels has been reported so far. In this paper, we study the energy-consumption of BCH codes with various code rates over AWGN and Rayleigh fading channels. Our results show that codes with low code rate are optimal for performing long-range communications, while is better to use less coding redundancy when the transmission distance is short. Our results also show that the transmission range of a low-power communication device can be increased up to 25\% for AWGN channels and up to 300\% for Rayleigh fading channels by using an optimized BCH code.},
  keywords = {Decoding,Encoding,Energy consumption,Fading,Power demand,Receivers,Signal to noise ratio},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PMETTYSU/6952166.html}
}

@inproceedings{rosasOptimizingCodeRate2014b,
  title = {Optimizing the Code Rate for Achieving Energy-Efficient Wireless Communications},
  booktitle = {2014 {{IEEE Wireless Communications}} and {{Networking Conference}} ({{WCNC}})},
  author = {Rosas, Fernando and Brante, Glauber and Souza, Richard Demo and Oberli, Christian},
  year = {2014},
  month = apr,
  pages = {775--780},
  issn = {1558-2612},
  doi = {10.1109/WCNC.2014.6952166},
  abstract = {Error correcting coding is a well-known technique for reducing the required signal-to-noise ratio (SNR) needed to attain a given bit error-rate. Nevertheless, this reduction comes at the cost of extra energy consumption introduced by the baseband processing required for encoding and decoding the data. No complete analysis of the trade-off between coding gain and baseband consumption of communications over wireless channels has been reported so far. In this paper, we study the energy-consumption of BCH codes with various code rates over AWGN and Rayleigh fading channels. Our results show that codes with low code rate are optimal for performing long-range communications, while is better to use less coding redundancy when the transmission distance is short. Our results also show that the transmission range of a low-power communication device can be increased up to 25\% for AWGN channels and up to 300\% for Rayleigh fading channels by using an optimized BCH code.},
  keywords = {Decoding,Encoding,Energy consumption,Fading,Power demand,Receivers,Signal to noise ratio},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QUPAL6ID/6952166.html}
}

@article{rosayModelingGridFields2018,
  title = {Modeling Grid Fields Instead of Modeling Grid Cells Underlying Microscopic Neural System},
  author = {Rosay, Sophie and Weber, Simon N and Mulas, Marcello},
  year = {2018},
  journal = {bioR},
  pages = {1--27},
  doi = {10.1101/481747},
  keywords = {effective model,grid cells,hippocampus,physics of 2d},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/A8T3BVZU/Rosay, Weber, Mulas - 2018 - Modeling grid fields instead of modeling grid cells underlying microscopic neural system.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@rosayModelingGridFields2018.md}
}

@article{roscowLearningOfflineMemory2021a,
  title = {Learning Offline: Memory Replay in Biological and Artificial Reinforcement Learning},
  shorttitle = {Learning Offline},
  author = {Roscow, Emma L. and Chua, Raymond and Costa, Rui Ponte and Jones, Matt W. and Lepora, Nathan},
  year = {2021},
  month = oct,
  journal = {Trends in Neurosciences},
  volume = {44},
  number = {10},
  pages = {808--821},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2021.07.007},
  abstract = {Learning to act in an environment to maximise rewards is among the brain's key functions. This process has often been conceptualised within the framework of reinforcement learning, which has also gained prominence in machine learning and artificial intelligence (AI) as a way to optimise decision making. A common aspect of both biological and machine reinforcement learning is the reactivation of previously experienced episodes, referred to as replay. Replay is important for memory consolidation in biological neural networks and is key to stabilising learning in deep neural networks. Here, we review recent developments concerning the functional roles of replay in the fields of neuroscience and AI. Complementary progress suggests how replay might support learning processes, including generalisation and continual learning, affording opportunities to transfer knowledge across the two fields to advance the understanding of biological and artificial learning and memory.},
  langid = {english},
  keywords = {computation,deep neural networks,hippocampus,Q-learning,reward},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/D8UF52I9/Roscow et al_2021_Learning offline.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QNVBF2P7/Learning offline memory replay in biological and artificial reinforcement learning - 10.05.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JKLAG2SF/S0166223621001442.html}
}

@misc{rossbroichFluctuationdrivenInitializationSpiking2022,
  title = {Fluctuation-Driven Initialization for Spiking Neural Network Training},
  author = {Rossbroich, Julian and Gygax, Julia and Zenke, Friedemann},
  year = {2022},
  month = jun,
  number = {arXiv:2206.10226},
  eprint = {2206.10226},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.10226},
  abstract = {Spiking neural networks (SNNs) underlie low-power, fault-tolerant information processing in the brain and could constitute a power-efficient alternative to conventional deep neural networks when implemented on suitable neuromorphic hardware accelerators. However, instantiating SNNs that solve complex computational tasks in-silico remains a significant challenge. Surrogate gradient (SG) techniques have emerged as a standard solution for training SNNs end-to-end. Still, their success depends on synaptic weight initialization, similar to conventional artificial neural networks (ANNs). Yet, unlike in the case of ANNs, it remains elusive what constitutes a good initial state for an SNN. Here, we develop a general initialization strategy for SNNs inspired by the fluctuation-driven regime commonly observed in the brain. Specifically, we derive practical solutions for data-dependent weight initialization that ensure fluctuation-driven firing in the widely used leaky integrate-and-fire (LIF) neurons. We empirically show that SNNs initialized following our strategy exhibit superior learning performance when trained with SGs. These findings generalize across several datasets and SNN architectures, including fully connected, deep convolutional, recurrent, and more biologically plausible SNNs obeying Dale's law. Thus fluctuation-driven initialization provides a practical, versatile, and easy-to-implement strategy for improving SNN training performance on diverse tasks in neuromorphic engineering and computational neuroscience.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,plasticity,Quantitative Biology - Neurons and Cognition,SNN},
  note = {Comment: 30 pages, 7 figures, plus supplementary material},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/24WN6Z9E/Rossbroich et al_2022_Fluctuation-driven initialization for spiking neural network training.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GCMFXI9C/2206.html}
}

@article{rossProteinAggregationNeurodegenerative2004,
  title = {Protein Aggregation and Neurodegenerative Disease},
  author = {Ross, Christopher A. and Poirier, Michelle A.},
  year = {2004},
  month = jul,
  journal = {Nature Medicine},
  volume = {10 Suppl},
  pages = {S10-17},
  issn = {1078-8956},
  doi = {10.1038/nm1066},
  abstract = {Neurodegenerative diseases such as Alzheimer's disease (AD), Parkinson's disease (PD), Huntington's disease (HD), amyotrophic lateral sclerosis (ALS) and prion diseases are increasingly being realized to have common cellular and molecular mechanisms including protein aggregation and inclusion body formation. The aggregates usually consist of fibers containing misfolded protein with a beta-sheet conformation, termed amyloid. There is partial but not perfect overlap among the cells in which abnormal proteins are deposited and the cells that degenerate. The most likely explanation is that inclusions and other visible protein aggregates represent an end stage of a molecular cascade of several steps, and that earlier steps in the cascade may be more directly tied to pathogenesis than the inclusions themselves. For several diseases, genetic variants assist in explaining the pathogenesis of the more common sporadic forms and developing mouse and other models. There is now increased understanding of the pathways involved in protein aggregation, and some recent clues have emerged as to the molecular mechanisms of cellular toxicity. These are leading to approaches toward rational therapeutics.},
  langid = {english},
  pmid = {15272267},
  keywords = {Amyloid beta-Peptides,Animals,Humans,Huntingtin Protein,Inclusion Bodies,Mice,Models; Biological,Nerve Tissue Proteins,Neurodegenerative Diseases,Nuclear Proteins,Peptides,Protein Folding,Protein Structure; Secondary,Ubiquitin}
}

@article{Roudi2011,
  title = {Dynamical {{TAP}} Equations for Non-Equilibrium {{Ising}} Spin Glasses},
  author = {Roudi, Yasser and Hertz, John},
  year = {2011},
  month = mar,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2011},
  number = {03},
  pages = {P03031},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2011/03/P03031},
  keywords = {disordered systems (theory),spin glasses (theory),stochastic processes (theory)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6VEEKFBT/Roudi, Hertz - 2011 - Dynamical TAP equations for non-equilibrium Ising spin glasses.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WCSXRFEM/Roudi, Hertz - 2011 - Dynamical TAP equations for non-equilibrium Ising spin glasses(2).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@Roudi2011.md}
}

@article{roudiIsingModelNeural,
  title = {Ising Model for Neural Data: {{Model}} Quality and Approximate Methods for Extracting Functional Connectivity},
  author = {Roudi, Yasser and Tyrcha, Joanna and Hertz, John},
  doi = {10.1103/PhysRevE.79.051915},
  abstract = {We study pairwise Ising models for describing the statistics of multineuron spike trains, using data from a simulated cortical network. We explore efficient ways of finding the optimal couplings in these models and examine their statistical properties. To do this, we extract the optimal couplings for subsets of size up to 200 neurons, essentially exactly, using Boltzmann learning. We then study the quality of several approximate methods for finding the couplings by comparing their results with those found from Boltzmann learning. Two of these methods-inversion of the Thouless-Anderson-Palmer equations and an approximation proposed by Sessak and Monasson-are remarkably accurate. Using these approximations for larger subsets of neurons, we find that extracting couplings using data from a subset smaller than the full network tends systematically to overestimate their magnitude. This effect is described qualitatively by infinite-range spin-glass theory for the normal phase. We also show that a globally correlated input to the neurons in the network leads to a small increase in the average coupling. However, the pair-to-pair variation in the couplings is much larger than this and reflects intrinsic properties of the network. Finally, we study the quality of these models by comparing their entropies with that of the data. We find that they perform well for small subsets of the neurons in the network, but the fit quality starts to deteriorate as the subset size grows, signaling the need to include higher-order correlations to describe the statistics of large networks.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YQJEIPKA/Roudi, Tyrcha, Hertz - Unknown - Ising model for neural data Model quality and approximate methods for extracting functional connectivit.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@roudiIsingModelNeural.md}
}

@techreport{roudiIsingModelNeural2009,
  title = {The {{Ising Model}} for {{Neural Data}}: {{Model Quality}} and {{Approximate Methods}} for {{Extracting Functional Connectivity}}},
  author = {Roudi, Yasser and Tyrcha, Joanna and Hertz, John},
  year = {2009},
  abstract = {We study pairwise Ising models for describing the statistics of multi-neuron spike trains, using data from a simulated cortical network. We explore efficient ways of finding the optimal couplings in these models and examine their statistical properties. To do this, we extract the optimal couplings for subsets of size up to 200 neurons, essentially exactly, using Boltzmann learning. We then study the quality of several approximate methods for finding the couplings by comparing their results with those found from Boltzmann learning. Two of these methods-inversion of the TAP equations and an approximation proposed by Sessak and Monasson-are remarkably accurate. Using these approximations for larger subsets of neurons, we find that extracting couplings using data from a subset smaller than the full network tends systematically to overestimate their magnitude. This effect is described qualitatively by infinite-range spin glass theory for the normal phase. We also show that a globally-correlated input to the neurons in the network lead to a small increase in the average coupling. However, the pair-to-pair variation of the couplings is much larger than this and reflects intrinsic properties of the network. Finally, we study the quality of these models by comparing their entropies with that of the data. We find that they perform well for small subsets of the neurons in the network, but the fit quality starts to deteriorate as the subset size grows, signalling the need to include higher order correlations to describe the statistics of large networks.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FKDQYE8N/Roudi, Tyrcha, Hertz - 2009 - The Ising Model for Neural Data Model Quality and Approximate Methods for Extracting Functional Connectivi.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@roudiIsingModelNeural2009.md}
}

@article{roudiMeanFieldTheory2011,
  title = {Mean Field Theory for Nonequilibrium Network Reconstruction},
  author = {Roudi, Yasser and Hertz, John A.},
  year = {2011},
  month = jan,
  journal = {Physical Review Letters},
  volume = {106},
  number = {4},
  doi = {10.1103/PhysRevLett.106.048702},
  abstract = {There has been recent progress on the problem of inferring the structure of interactions in complex networks when they are in stationary states satisfying detailed balance, but little has been done for non-equilibrium systems. Here we introduce an approach to this problem, considering, as an example, the question of recovering the interactions in an asymmetrically-coupled, synchronously-updated Sherrington-Kirkpatrick model. We derive an exact iterative inversion algorithm and develop efficient approximations based on dynamical mean-field and Thouless-Anderson-Palmer equations that express the interactions in terms of equal-time and one time step-delayed correlation functions.},
  keywords = {0250Tt,0510Àa,7510Nr,numbers: 8975Hc},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4BI9CADP/Roudi, Hertz - 2010 - Mean Field Theory For Non-Equilibrium Network Reconstruction.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ETNJ7YC5/Roudi, Hertz - 2011 - Mean field theory for nonequilibrium network reconstruction.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@roudiMeanFieldTheory2011.md}
}

@article{roudiMultineuronalActivityFunctional2015,
  title = {Multi-Neuronal Activity and Functional Connectivity in Cell Assemblies},
  author = {Roudi, Yasser and Dunn, Benjamin and Hertz, John},
  year = {2015},
  journal = {Current Opinion in Neurobiology},
  volume = {32},
  pages = {38--44},
  publisher = {{Elsevier Ltd}},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2014.10.011},
  abstract = {Our ability to collect large amounts of data from many cells has been paralleled by the development of powerful statistical models for extracting information from this data. Here we discuss how the activity of cell assemblies can be analyzed using these models, focusing on the generalized linear models and the maximum entropy models and describing a number of recent studies that employ these tools for analyzing multi-neuronal activity. We show results from simulations comparing inferred functional connectivity, pairwise correlations and the real synaptic connections in simulated networks demonstrating the power of statistical models in inferring functional connectivity. Further development of network reconstruction techniques based on statistical models should lead to more powerful methods of understanding functional anatomy of cell assemblies.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IU24PHLR/Roudi, Dunn, Hertz - 2015 - Multi-neuronal activity and functional connectivity in cell assemblies.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@roudiMultineuronalActivityFunctional2015.md}
}

@article{roudiPairwiseMaximumEntropy2009,
  title = {Pairwise Maximum Entropy Models for Studying Large Biological Systems: {{When}} They Can Work and When They Can't},
  author = {Roudi, Yasser and Nirenberg, Sheila and Latham, Peter E.},
  year = {2009},
  month = may,
  journal = {PLoS Computational Biology},
  volume = {5},
  number = {5},
  pages = {1000380--1000380},
  publisher = {{Public Library of Science}},
  doi = {10.1371/journal.pcbi.1000380},
  abstract = {One of the most critical problems we face in the study of biological systems is building accurate statistical descriptions of them. This problem has been particularly challenging because biological systems typically contain large numbers of interacting elements, which precludes the use of standard brute force approaches. Recently, though, several groups have reported that there may be an alternate strategy. The reports show that reliable statistical models can be built without knowledge of all the interactions in a system; instead, pairwise interactions can suffice. These findings, however, are based on the analysis of small subsystems. Here, we ask whether the observations will generalize to systems of realistic size, that is, whether pairwise models will provide reliable descriptions of true biological systems. Our results show that, in most cases, they will not. The reason is that there is a crossover in the predictive power of pairwise models: If the size of the subsystem is below the crossover point, then the results have no predictive power for large systems. If the size is above the crossover point, then the results may have predictive power. This work thus provides a general framework for determining the extent to which pairwise models can be used to predict the behavior of large biological systems. Applied to neural data, the size of most systems studied so far is below the crossover point. \textcopyright{} 2009 Roudi et al.},
  keywords = {Behavior,Distance measurement,Entropy,Extrapolation,Neurons,Probability distribution,Protein folding,Soil perturbation},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BMRKWB8L/Roudi, Nirenberg, Latham - 2009 - Pairwise maximum entropy models for studying large biological systems When they can work and when they.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@roudiPairwiseMaximumEntropy2009.md}
}

@article{roudiStatisticalPhysicsPairwise2009,
  title = {Statistical Physics of Pairwise Probability Models},
  author = {Roudi, Yasser and Aurell, Erik and Hertz, John A.},
  year = {2009},
  journal = {Frontiers in Computational Neuroscience},
  volume = {3},
  number = {NOV},
  eprint = {0905.1410},
  eprinttype = {arxiv},
  pages = {1--15},
  issn = {16625188},
  doi = {10.3389/neuro.10.022.2009},
  abstract = {Statistical models for describing the probability distribution over the states of biological systems are commonly used for dimensional reduction. Among these models, pairwise models are very attractive in part because they can be fit using a reasonable amount of data: knowledge of the mean values and correlations between pairs of elements in the system is sufficient. Not surprisingly, then, using pairwise models for studying neural data has been the focus of many studies in recent years. In this paper, we describe how tools from statistical physics can be employed for studying and using pairwise models. We build on our previous work on the subject and study the relation between different methods for fitting these models and evaluating their quality. In particular, using data from simulated cortical networks we study how the quality of various approximate methods for inferring the parameters in a pairwise model depends on the time bin chosen for binning the data. We also study the effect of the size of the time bin on the model quality itself, again using simulated data. We show that using finer time bins increases the quality of the pairwise model. We offer new ways of deriving the expressions reported in our previous work for assessing the quality of pairwise models. \textcopyright{} 2009 Roudi, Aurell and Hertz.},
  archiveprefix = {arXiv},
  arxivid = {0905.1410},
  mendeley-tags = {Canonical Reading},
  keywords = {Canonical Reading,Inference,Inverse ising problem,Maximum entropy distribution,Pairwise model},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KFUCM7V7/Roudi, Aurell, Hertz - 2009 - Statistical physics of pairwise probability models.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XLXJDXXG/Roudi, Aurell, Hertz - 2009 - Statistical physics of pairwise probability models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@roudiStatisticalPhysicsPairwise2009.md}
}

@article{roweisNonlinearDimensionalityReduction2000,
  title = {Nonlinear {{Dimensionality Reduction}} by {{Locally Linear Embedding}}},
  author = {Roweis, S. T.},
  year = {2000},
  month = dec,
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2323--2326},
  issn = {00368075, 10959203},
  doi = {10.1126/science.290.5500.2323},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CHILUCWS/Roweis - 2000 - Nonlinear Dimensionality Reduction by Locally Line.pdf}
}

@article{roweisNonlinearDimensionalityReduction2000a,
  title = {Nonlinear {{Dimensionality Reduction}} by {{Locally Linear Embedding}}},
  author = {Roweis, Sam T. and Saul, Lawrence K.},
  year = {2000},
  month = dec,
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2323--2326},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.290.5500.2323},
  abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
  chapter = {Report},
  langid = {english},
  pmid = {11125150},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/Obsidian/Obsidian/Charlie Vault/Citations/@roweisNonlinearDimensionalityReduction2000.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Roweis_Saul_2000_Nonlinear Dimensionality Reduction by Locally Linear Embedding.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/J6LRDU3P/2323.html}
}

@article{rowlandStatisticsSamplesDistributional2019,
  title = {Statistics and {{Samples}} in {{Distributional Reinforcement Learning}}},
  author = {Rowland, Mark and Dadashi, Robert and Kumar, Saurabh and Munos, R{\'e}mi and Bellemare, Marc G. and Dabney, Will},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.08102 [cs, stat]},
  eprint = {1902.08102},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present a unifying framework for designing and analysing distributional reinforcement learning (DRL) algorithms in terms of recursively estimating statistics of the return distribution. Our key insight is that DRL algorithms can be decomposed as the combination of some statistical estimator and a method for imputing a return distribution consistent with that set of statistics. With this new understanding, we are able to provide improved analyses of existing DRL algorithms as well as construct a new algorithm (EDRL) based upon estimation of the expectiles of the return distribution. We compare EDRL with existing methods on a variety of MDPs to illustrate concrete aspects of our analysis, and develop a deep RL variant of the algorithm, ER-DQN, which we evaluate on the Atari-57 suite of games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,RL,Statistics - Machine Learning,TD},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@rowlandStatisticsSamplesDistributional2019.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/rowlandStatisticsSamplesDistributional2019-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HSBH9SGA/Rowland et al_2019_Statistics and Samples in Distributional Reinforcement Learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9FW3XWZU/1902.html}
}

@article{rowlandTenYearsGrid2016,
  title = {Ten {{Years}} of {{Grid Cells}}},
  author = {Rowland, David C. and Roudi, Yasser and Moser, May-Britt and Moser, Edvard I.},
  year = {2016},
  journal = {Annual Review of Neuroscience},
  volume = {39},
  number = {1},
  pages = {19--40},
  doi = {10.1146/annurev-neuro-070815-013824},
  abstract = {Themedial entorhinal cortex(MEC)creates a neural representation of space through a set of functionally dedicated cell types: grid cells, border cells, head direction cells, and speed cells. Grid cells, the most abundant functional cell type in the MEC, have hexagonally arranged firing fields that tile the surface of the environment. These cells were discovered only in 2005, but after 10 years of investigation, we are beginning to understand how they are organized in the MEC network, how their periodic firing fields might be generated, how they are shaped by properties of the environment, and how they interact with the rest of theMECnetwork. The aim of this review is to summarize what we know about grid cells and point out where our knowledge is still incomplete.},
  keywords = {association cortex,attractor networks,entorhinal cortex,hippocampus,memory,spatial navigation},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/95PJTRL5/Rowland et al. - 2016 - Ten Years of Grid Cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@rowlandTenYearsGrid2016.md}
}

@article{rubinovComplexNetworkMeasures2010,
  title = {Complex Network Measures of Brain Connectivity: {{Uses}} and Interpretations},
  shorttitle = {Complex Network Measures of Brain Connectivity},
  author = {Rubinov, Mikail and Sporns, Olaf},
  year = {2010},
  month = sep,
  journal = {NeuroImage},
  series = {Computational {{Models}} of the {{Brain}}},
  volume = {52},
  number = {3},
  pages = {1059--1069},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2009.10.003},
  abstract = {Brain connectivity datasets comprise networks of brain regions connected by anatomical tracts or by functional associations. Complex network analysis\textemdash a new multidisciplinary approach to the study of complex systems\textemdash aims to characterize these brain networks with a small number of neurobiologically meaningful and easily computable measures. In this article, we discuss construction of brain networks from connectivity data and describe the most commonly used network measures of structural and functional connectivity. We describe measures that variously detect functional integration and segregation, quantify centrality of individual brain regions or pathways, characterize patterns of local anatomical circuitry, and test resilience of networks to insult. We discuss the issues surrounding comparison of structural and functional network connectivity, as well as comparison of networks across subjects. Finally, we describe a Matlab toolbox (http://www.brain-connectivity-toolbox.net) accompanying this article and containing a collection of complex network measures and large-scale neuroanatomical connectivity datasets.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8JFN6Z8K/Rubinov, Sporns - 2010 - Complex network measures of brain connectivity Uses and interpretations.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ED36ZZ7P/Rubinov and Sporns - 2010 - Complex network measures of brain connectivity Us.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@rubinovComplexNetworkMeasures2010.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LET7FHKR/S105381190901074X.html}
}

@article{rueApproximateBayesianInference2009,
  title = {Approximate {{Bayesian}} Inference for Latent {{Gaussian}} Models by Using Integrated Nested {{Laplace}} Approximations},
  author = {Rue, H{\aa}vard and Martino, Sara and Chopin, Nicolas},
  year = {2009},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {71},
  number = {2},
  pages = {319--392},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2008.00700.x},
  abstract = {Summary. Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
  langid = {english},
  keywords = {Approximate Bayesian inference,Gaussian Markov random fields,Generalized additive mixed models,Laplace approximation,Parallel computing,Sparse matrices,Structured additive regression models},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2008.00700.x},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@rueApproximateBayesianInference2009.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/rueApproximateBayesianInference2009-mdnotes.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Rue et al_2009_Approximate Bayesian inference for latent Gaussian models by using integrated.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IW545DAE/j.1467-9868.2008.00700.html}
}

@article{russNaturallikeFunctionArtificial2005,
  title = {Natural-like Function in Artificial {{WW}} Domains},
  author = {Russ, William P. and Lowery, Drew M. and Mishra, Prashant and Yaffe, Michael B. and Ranganathan, Rama},
  year = {2005},
  month = sep,
  journal = {Nature},
  volume = {437},
  number = {7058},
  pages = {579--583},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nature03990},
  abstract = {Protein sequences evolve through random mutagenesis with selection for optimal fitness. Cooperative folding into a stable tertiary structure is one aspect of fitness, but evolutionary selection ultimately operates on function, not on structure. In the accompanying paper, we proposed a model for the evolutionary constraint on a small protein interaction module (the WW domain) through application of the SCA, a statistical analysis of multiple sequence alignments. Construction of artificial protein sequences directed only by the SCA showed that the information extracted by this analysis is sufficient to engineer the WW fold at atomic resolution. Here, we demonstrate that these artificial WW sequences function like their natural counterparts, showing class-specific recognition of proline-containing target peptides. Consistent with SCA predictions, a distributed network of residues mediates functional specificity in WW domains. The ability to recapitulate natural-like function in designed sequences shows that a relatively small quantity of sequence information is sufficient to specify the global energetics of amino acid interactions. \textcopyright{} 2005 Nature Publishing Group.},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/J3K2NTVV/Russ et al. - 2005 - Natural-like function in artificial WW domains.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@russNaturallikeFunctionArtificial2005.md}
}

@article{ryyppoRegionsInterestNodes2018,
  title = {Regions of {{Interest}} as Nodes of Dynamic Functional Brain Networks},
  author = {Ryypp{\"o}, Elisa and Glerean, Enrico and Brattico, Elvira and Saram{\"a}ki, Jari and Korhonen, Onerva},
  year = {2018},
  month = oct,
  journal = {Network Neuroscience},
  volume = {2},
  number = {4},
  pages = {513--535},
  issn = {2472-1751},
  doi = {10.1162/netn_a_00047},
  abstract = {The properties of functional brain networks strongly depend on how their nodes are chosen. Commonly, nodes are defined by Regions of Interest (ROIs), predetermined groupings of fMRI measurement voxels. Earlier, we demonstrated that the functional homogeneity of ROIs, captured by their spatial consistency, varies widely across ROIs in commonly used brain atlases. Here, we ask how ROIs behave as nodes of dynamic brain networks. To this end, we use two measures: spatiotemporal consistency measures changes in spatial consistency across time and network turnover quantifies the changes in the local network structure around an ROI. We find that spatial consistency varies non-uniformly in space and time, which is reflected in the variation of spatiotemporal consistency across ROIs. Furthermore, we see time-dependent changes in the network neighborhoods of the ROIs, reflected in high network turnover. Network turnover is nonuniformly distributed across ROIs: ROIs with high spatiotemporal consistency have low network turnover. Finally, we reveal that there is rich voxel-level correlation structure inside ROIs. Because the internal structure and the connectivity of ROIs vary in time, the common approach of using static node definitions may be surprisingly inaccurate. Therefore, network neuroscience would greatly benefit from node definition strategies tailored for dynamical networks., Regions of Interest (ROIs) are often used as the nodes of functional brain networks. ROIs consist of several fMRI measurement voxels that are assumed to be functionally homogeneous, that is, behave similarly. Earlier, we showed that the assumption of similar voxel dynamics is not always true: functional homogeneity varies widely across ROIs. In this paper, we demonstrate that functional homogeneity changes in time. These changes are connected to changes in local network structure around ROIs, which suggests that an ROI's functional homogeneity may reflect its role in the network. Finally, we show that there is rich, time-dependent structure of voxel-level connectivity inside ROIs. This leads us to ask if the dynamic brain networks can be described by any set of static ROIs.},
  pmcid = {PMC6147715},
  pmid = {30294707},
  keywords = {anatomy,functional connectivity},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AF4PUAZK/Ryyppö et al. - 2018 - Regions of Interest as nodes of dynamic functional.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@ryyppoRegionsInterestNodes2018.md}
}

@article{s.a.SynchronizationHindmarshRose2020,
  title = {Synchronization of {{Hindmarsh Rose Neurons}}},
  author = {S.A., Malik and A.H., Mir},
  year = {2020},
  month = mar,
  journal = {Neural Networks},
  volume = {123},
  pages = {372--380},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.11.024},
  langid = {english}
}

@inproceedings{sacramentoDendriticCorticalMicrocircuits2018,
  title = {Dendritic Cortical Microcircuits Approximate the Backpropagation Algorithm},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sacramento, Jo{\~a}o and Ponte Costa, Rui and Bengio, Yoshua and Senn, Walter},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances \textendash{} error backpropagation \textendash{} appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/D9UMNAE9/Sacramento et al_2018_Dendritic cortical microcircuits approximate the backpropagation algorithm.pdf}
}

@article{saetraGhostMachineBeing2018,
  title = {The {{Ghost}} in the {{Machine Being Human}} in the {{Age}} of {{AI}} and {{Machine Learning}}},
  author = {S{\ae}tra, Henrik Skaug},
  year = {2018},
  journal = {Human Arenas},
  number = {1},
  pages = {1--19},
  publisher = {{Human Arenas}},
  doi = {10.1007/s42087-018-0039-1},
  abstract = {Human beings have used technology to improve their efficiency throughout history. We continue to do so today, but we are no longer only using technology to perform physical tasks. Today, we make computers that are smart enough to challenge, and even surpass, us in many areas. Artificial intelligence\textemdash embodied or not\textemdash now drive our cars, trade stocks, socialise with our children, keep the elderly company and the lonely warm. At the same time, we use technology to gather vast amounts of data on ourselves. This, in turn, we use to train intelligent computers that ease and customise ever more ofour lives. The change that occurs in our relations to other people, and computers, change both how we act and how we are.What sort ofchallenges does this development pose for human beings? I argue that we are seeing an emerging challenge to the concept ofwhat it means to be human, as (a) we struggle to define what makes us special and try to come to terms with being surpassed in various ways by computers, and (b) the way we use and interact with technology changes us in ways we do not yet fully understand.},
  keywords = {Artificial intelligence,Artificial intelligence;Machine learning;Social re,cognition,Cognition,human identity,Human identity,Machine learning,social relations,Social relations},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AMQBUQTZ/Saetra - 2087 - A R E N A O F C H A N G I N G The Ghost in the Machine Being Human in the Age of AI and Machine Learning.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IK6TIQEN/Sætra - 2018 - The Ghost in the Machine Being Human in the Age of AI and Machine Learning.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@saetraGhostMachineBeing2018.md}
}

@article{safaLearningEventbasedSpatioTemporal2021,
  title = {Learning {{Event-based Spatio-Temporal Feature Descriptors}} via {{Local Synaptic Plasticity}}: {{A Biologically-realistic Perspective}} of {{Computer Vision}}},
  shorttitle = {Learning {{Event-based Spatio-Temporal Feature Descriptors}} via {{Local Synaptic Plasticity}}},
  author = {Safa, A. and Sahli, H. and Bourdoux, A. and Ocket, I. and Catthoor, F. and Gielen, G.},
  year = {2021},
  journal = {undefined},
  abstract = {An optimization-based theory describing spiking cortical ensembles equipped with Spike-TimingDependent Plasticity (STDP) learning, as empirically observed in the visual cortex is presented. We present an optimization-based theory describing spiking cortical ensembles equipped with Spike-TimingDependent Plasticity (STDP) learning, as empirically observed in the visual cortex. Using our methods, we build a class of fully-connected, convolutional and action-based feature descriptors for event-based camera that we respectively assess on N-MNIST, challenging CIFAR10-DVS and on the IBM DVS128 gesture dataset. We report significant accuracy improvements compared to conventional state-ofthe-art event-based feature descriptors (+8\% on CIFAR10DVS). We report large improvements in accuracy compared to state-of-the-art STDP-based systems (+10\% on N-MNIST, +7.74\% on IBM DVS128 Gesture). In addition to ultra-low-power learning in neuromorphic edge devices, our work helps paving the way towards a biologicallyrealistic, optimization-based theory of cortical vision.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/68XZSBFD/45479b302ee84b97fbe320add654b973e1ab4801.html}
}

@inproceedings{safaNewLookSpikeTimingDependent2021,
  title = {A {{New Look}} at {{Spike-Timing-Dependent Plasticity Networks}} for {{Spatio-Temporal Feature Learning}}},
  author = {Safa, A. and Ocket, I. and Bourdoux, A. and Sahli, H. and Catthoor, F. and Gielen, G.},
  year = {2021},
  abstract = {This work provides novel theoretical grounds for SNN and STDP parameter tuning which considerably reduces design time, and contributes to both ultra-low-power learning in neuromorphic edge devices, and towards a biologically-plausible, optimization-based theory of cortical vision. We present new theoretical foundations for unsupervised Spike-Timing-Dependent Plasticity (STDP) learning in spiking neural networks (SNNs). In contrast to empirical parameter search used in most previous works, we provide novel theoretical grounds for SNN and STDP parameter tuning which considerably reduces design time. Using our generic framework, we propose a class of global, action-based and convolutional SNN-STDP architectures for learning spatio-temporal features from event-based cameras. We assess our methods on the N-MNIST, the CIFAR10-DVS and the IBM DVS128 Gesture datasets, all acquired with a real-world event camera. Using our framework, we report significant improvements in classification accuracy compared to both conventional state-of-the-art event-based feature descriptors (+8.2\% on CIFAR10-DVS), and compared to state-of-the-art STDP-based systems (+9.3\% on NMNIST, +7.74\% on IBM DVS128 Gesture). Our work contributes to both ultra-low-power learning in neuromorphic edge devices, and towards a biologically-plausible, optimization-based theory of cortical vision.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PTY7KFYL/Safa et al_2021_A New Look at Spike-Timing-Dependent Plasticity Networks for Spatio-Temporal.pdf}
}

@article{samsonovichPathIntegrationCognitive1997,
  title = {Path Integration and Cognitive Mapping in a Continuous Attractor Neural Network Model.},
  author = {Samsonovich, A and McNaughton, B L},
  year = {1997},
  journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
  volume = {17},
  number = {15},
  pages = {5900--20},
  issn = {0270-6474 (Print)\textbackslash n0270-6474 (Linking)},
  doi = {10.1523/JNEUROSCI.17-15-05900.1997},
  abstract = {A minimal synaptic architecture is proposed for how the brain might perform path integration by computing the next internal representation of self-location from the current representation and from the perceived velocity of motion. In the model, a place-cell assembly called a "chart" contains a two-dimensional attractor set called an "attractor map" that can be used to represent coordinates in any arbitrary environment, once associative binding has occurred between chart locations and sensory inputs. In hippocampus, there are different spatial relations among place fields in different environments and behavioral contexts. Thus, the same units may participate in many charts, and it is shown that the number of uncorrelated charts that can be encoded in the same recurrent network is potentially quite large. According to this theory, the firing of a given place cell is primarily a cooperative effect of the activity of its neighbors on the currently active chart. Therefore, it is not particularly useful to think of place cells as encoding any particular external object or event. Because of its recurrent connections, hippocampal field CA3 is proposed as a possible location for this "multichart" architecture; however, other implementations in anatomy would not invalidate the main concepts. The model is implemented numerically both as a network of integrate-and-fire units and as a "macroscopic" (with respect to the space of states) description of the system, based on a continuous approximation defined by a system of stochastic differential equations. It provides an explanation for a number of hitherto perplexing observations on hippocampal place fields, including doubling, vanishing, reshaping in distorted environments, acquiring directionality in a two-goal shuttling task, rapid formation in a novel environment, and slow rotation after disorientation. The model makes several new predictions about the expected properties of hippocampal place cells and other cells of the proposed network.},
  keywords = {allocentric,attractor,ca3,cognitive map,dead reckoning,head direction,hippocampus,idiothetic,individual and multiple parallel,integrate-and-fire,it is known from,learning,path integration,place cells,recordings of,spatial},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WYD3FHXM/Samsonovich, McNaughton - 1997 - Path integration and cognitive mapping in a continuous attractor neural network model.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@samsonovichPathIntegrationCognitive1997.md}
}

@article{santhanamInformationTheoreticLimitsSelecting2012,
  title = {Information-{{Theoretic Limits}} of {{Selecting Binary Graphical Models}} in {{High Dimensions}}},
  author = {Santhanam, Narayana P and Wainwright, Martin J and Member, Senior},
  year = {2012},
  journal = {IEEE TRANSACTIONS ON INFORMATION THEORY},
  volume = {58},
  number = {7},
  pages = {4117--4117},
  doi = {10.1109/TIT.2012.2191659},
  abstract = {The problem of graphical model selection is to estimate the graph structure of a Markov random field given samples from it. We analyze the information-theoretic limitations of the problem of graph selection for binary Markov random fields under high-dimensional scaling, in which the graph size and the number of edges , and/or the maximal node degree , are allowed to increase to infinity as a function of the sample size. For pair-wise binary Markov random fields, we derive both necessary and sufficient conditions for correct graph selection over the class of graphs on vertices with at most edges, and over the class of graphs on vertices with maximum degree at most. For the class , we establish the existence of constants and such that if , any method has error probability at least uniformly over the family, and we demonstrate a graph decoder that succeeds with high probability uniformly over the family for sample sizes. Similarly, for the class , we exhibit constants and such that for , any method fails with probability at least , and we demonstrate a graph de-coder that succeeds with high probability for. Index Terms-High dimensional inference, KL divergence between Ising models, Markov random fields, sample complexity, structure of Ising models.},
  note = {the focus of the paper\textemdash are the information-theoretic limitations of graphical model selec- tion},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HNRIUQ3V/Santhanam, Wainwright, Member - 2012 - Information-Theoretic Limits of Selecting Binary Graphical Models in High Dimensions.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@santhanamInformationTheoreticLimitsSelecting2012.md}
}

@article{santos-pataModulatingGridCell2019,
  title = {Modulating Grid Cell Scale and Intrinsic Frequencies via Slow High-Threshold Conductances: {{A}} Simplified Model},
  shorttitle = {Modulating Grid Cell Scale and Intrinsic Frequencies via Slow High-Threshold Conductances},
  author = {{Santos-Pata}, Diogo and Zucca, Riccardo and {L{\'o}pez-Carral}, H{\'e}ctor and Verschure, Paul F.M.J.},
  year = {2019},
  month = nov,
  journal = {Neural Networks},
  volume = {119},
  pages = {66--73},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.06.011},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZHZPWY7F/Santos-Pata et al_2019_Modulating grid cell scale and intrinsic frequencies via slow high-threshold.pdf}
}

@article{sardinaAutomaticSynthesisGlobal2007,
  title = {Automatic Synthesis of a Global Behavior from Multiple Distributed Behaviors},
  author = {Sardina, S. and Patrizi, F. and De Giacomo, G.},
  year = {2007},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {22},
  number = {2},
  pages = {1063--1063},
  issn = {1577353234},
  abstract = {We consider the problem of synthesizing a team of local behavior controllers to realize a fully controllable target behavior from a set of available partially controllable behaviors that execute distributively within a shared partially predictable, but fully observable, environment. Available behaviors stand for existing distributed components and are represented with (finite) nondeterministic transition systems. The target behavior is assumed to be fully deterministic and stands for the collective behavior that the system as a whole needs to guarantee. We formally define the problem within a general framework, characterize its computational complexity, and propose techniques to actually generate a solution. Also, we investigate the relationship between the distributed solutions and the centralized ones, in which a single global controller is conceivable. Copyright \textcopyright{} 2007, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.},
  keywords = {Reasoning about Plans and Processes and Actions,Technical Papers},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7QFHVAVT/Sardina, Patrizi, De Giacomo - 2007 - Automatic synthesis of a global behavior from multiple distributed behaviors.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@sardinaAutomaticSynthesisGlobal2007.md}
}

@article{sasakiSpatialMemoryCircuits2015,
  title = {Spatial and Memory Circuits in the Medial Entorhinal Cortex},
  author = {Sasaki, Takuya and Leutgeb, Stefan and Leutgeb, Jill K.},
  year = {2015},
  journal = {Current Opinion in Neurobiology},
  volume = {32},
  pages = {16--23},
  publisher = {{Elsevier Ltd}},
  doi = {10.1016/j.conb.2014.10.008},
  abstract = {The large capacity of episodic memory is thought to be supported by the emergence of distinct hippocampal cell assemblies for unrelated memories, such that interference is minimized. In large-scale population recordings, the orthogonal nature of hippocampal representations across environments is evident in the complete reorganization of the firing locations of hippocampal place cells. Entorhinal grid cells provide inputs to the hippocampus, and their firing patterns shift relative to each other across different environments. Although this suggests that altered grid cell firing could generate distinct hippocampal population codes, it has recently been shown that new and distinct hippocampal place fields emerge while grid cell firing is compromised. We therefore propose that separate circuits within the medial entorhinal cortex are specialized for performing either spatial or memory-related computations.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z9ES2IQS/Sasaki, Leutgeb, Leutgeb - 2015 - Spatial and memory circuits in the medial entorhinal cortex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@sasakiSpatialMemoryCircuits2015.md}
}

@article{saulIntroductionLocallyLinear,
  title = {An {{Introduction}} to {{Locally Linear Embedding}}},
  author = {Saul, Lawrence K and Labs, T and Ave, Park and Park, Florham and Roweis, Sam T},
  pages = {13},
  abstract = {Many problems in information processing involve some form of dimensionality reduction. Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data. LLE attempts to discover nonlinear structure in high dimensional data by exploiting the local symmetries of linear reconstructions. Notably, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations\textemdash though capable of generating highly nonlinear embeddings\textemdash do not involve local minima. We illustrate the method on images of lips used in audiovisual speech synthesis.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WPG5G89J/Saul et al. - An Introduction to Locally Linear Embedding.pdf;/home/Mike/Insync/m@tarlton.info/Google Drive/Obsidian/Obsidian/Charlie Vault/Citations/@saulIntroductionLocallyLinear.md}
}

@article{saundersLocallyConnectedSpiking2019,
  title = {Locally Connected Spiking Neural Networks for Unsupervised Feature Learning},
  author = {Saunders, Daniel J. and Patel, Devdhar and Hazan, Hananel and Siegelmann, Hava T. and Kozma, Robert},
  year = {2019},
  month = nov,
  journal = {Neural Networks},
  volume = {119},
  pages = {332--340},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.08.016},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/I8USXLZW/Saunders et al_2019_Locally connected spiking neural networks for unsupervised feature learning.pdf}
}

@misc{saundersNotesSupervisedLearning2018,
  title = {Notes on ``{{Supervised}} Learning in Spiking Neural Networks with {{FORCE}} Training''},
  author = {Saunders, Dan},
  year = {2018},
  month = sep,
  journal = {Medium},
  abstract = {Paper available here.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4E9T6I8K/notes-on-supervised-learning-in-spiking-neural-networks-with-force-training-f5d6c9fbb4d.html}
}

@article{savelliOriginRolePath2019,
  title = {Origin and Role of Path Integration in the Cognitive Representations of the Hippocampus: Computational Insights into Open Questions},
  author = {Savelli, Francesco and Knierim, James J.},
  year = {2019},
  journal = {The Journal of Experimental Biology},
  volume = {222},
  number = {Suppl 1},
  pages = {jeb188912-jeb188912},
  doi = {10.1242/jeb.188912},
  abstract = {Path integration is a straightforward concept with varied connotations that are important to different disciplines concerned with navigation, such as ethology, cognitive science, robotics and neuroscience. In studying the hippocampal formation, it is fruitful to think of path integration as a computation that transforms a sense of motion into a sense of location, continuously integrated with landmark perception. Here, we review experimental evidence that path integration is intimately involved in fundamental properties of place cells and other spatial cells that are thought to support a cognitive abstraction of space in this brain system. We discuss hypotheses about the anatomical and computational origin of path integration in the well-characterized circuits of the rodent limbic system. We highlight how computational frameworks for map-building in robotics and cognitive science alike suggest an essential role for path integration in the creation of a new map in unfamiliar territory, and how this very role can help us make sense of differences in neurophysiological data from novel versus familiar and small versus large environments. Similar computational principles could be at work when the hippocampus builds certain non-spatial representations, such as time intervals or trajectories defined in a sensory stimulus space.},
  keywords = {boundary cell,cognitive map,grid cell,limbic system,place cell,robot navigation},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5SW6PTAB/Savelli, Knierim - 2019 - Origin and role of path integration in the cognitive representations of the hippocampus computational insights.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YKXXS9N5/Savelli, Knierim - 2019 - Origin and role of path integration in the cognitive representations of the hippocampus computational insig(2).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@savelliOriginRolePath2019.md}
}

@article{savinMaximumEntropyModels2017,
  title = {Maximum Entropy Models as a Tool for Building Precise Neural Controls},
  author = {Savin, Cristina and Tka{\v c}ik, Ga{\v s}per},
  year = {2017},
  journal = {Current Opinion in Neurobiology},
  doi = {10.1016/j.conb.2017.08.001},
  abstract = {Neural responses are highly structured, with population activity restricted to a small subset of the astronomical range of possible activity patterns. Characterizing these statistical regularities is important for understanding circuit computation, but challenging in practice. Here we review recent approaches based on the maximum entropy principle used for quantifying collective behavior in neural activity. We highlight recent models that capture population-level statistics of neural data, yielding insights into the organization of the neural code and its biological substrate. Furthermore, the MaxEnt framework provides a general recipe for constructing surrogate ensembles that preserve aspects of the data, but are otherwise maximally unstructured. This idea can be used to generate a hierarchy of controls against which rigorous statistical tests are possible.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HF7FYYCJ/Savin, Tkačik - 2017 - Maximum entropy models as a tool for building precise neural controls.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@savinMaximumEntropyModels2017.md}
}

@article{schaeferMalleableBrainPlasticity2017,
  title = {The Malleable Brain: Plasticity of Neural Circuits and Behavior - a Review from Students to Students},
  shorttitle = {The Malleable Brain},
  author = {Schaefer, Natascha and Rotermund, Carola and Blumrich, Eva-Maria and Lourenco, Mychael V. and Joshi, Pooja and Hegemann, Regina U. and Jamwal, Sumit and Ali, Nilufar and Garc{\'i}a Romero, Ezra Michelet and Sharma, Sorabh and Ghosh, Shampa and Sinha, Jitendra K. and Loke, Hannah and Jain, Vishal and Lepeta, Katarzyna and Salamian, Ahmad and Sharma, Mahima and Golpich, Mojtaba and Nawrotek, Katarzyna and Paidi, Ramesh K. and Shahidzadeh, Sheila M. and Piermartiri, Tetsade and Amini, Elham and Pastor, Veronica and Wilson, Yvette and Adeniyi, Philip A. and Datusalia, Ashok K. and Vafadari, Benham and Saini, Vedangana and {Su{\'a}rez-Pozos}, Edna and Kushwah, Neetu and Fontanet, Paula and Turner, Anthony J.},
  year = {2017},
  month = sep,
  journal = {Journal of Neurochemistry},
  volume = {142},
  number = {6},
  pages = {790--811},
  issn = {1471-4159},
  doi = {10.1111/jnc.14107},
  abstract = {One of the most intriguing features of the brain is its ability to be malleable, allowing it to adapt continually to changes in the environment. Specific neuronal activity patterns drive long-lasting increases or decreases in the strength of synaptic connections, referred to as long-term potentiation and long-term depression, respectively. Such phenomena have been described in a variety of model organisms, which are used to study molecular, structural, and functional aspects of synaptic plasticity. This review originated from the first International Society for Neurochemistry (ISN) and Journal of Neurochemistry (JNC) Flagship School held in Alpbach, Austria (Sep 2016), and will use its curriculum and discussions as a framework to review some of the current knowledge in the field of synaptic plasticity. First, we describe the role of plasticity during development and the persistent changes of neural circuitry occurring when sensory input is altered during critical developmental stages. We then outline the signaling cascades resulting in the synthesis of new plasticity-related proteins, which ultimately enable sustained changes in synaptic strength. Going beyond the traditional understanding of synaptic plasticity conceptualized by long-term potentiation and long-term depression, we discuss system-wide modifications and recently unveiled homeostatic mechanisms, such as synaptic scaling. Finally, we describe the neural circuits and synaptic plasticity mechanisms driving associative memory and motor learning. Evidence summarized in this review provides a current view of synaptic plasticity in its various forms, offers new insights into the underlying mechanisms and behavioral relevance, and provides directions for future research in the field of synaptic plasticity. Read the Editorial Highlight for this article on page 788. Cover Image for this issue: doi: 10.1111/jnc.13815.},
  langid = {english},
  pmid = {28632905},
  keywords = {associative learning,critical period,Hebbian plasticity,homeostatic plasticity,motorskill learning,synaptic plasticity},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Schaefer et al_2017_The malleable brain.pdf}
}

@article{schmidgallSpikePropamineDifferentiablePlasticity2021,
  title = {{{SpikePropamine}}: {{Differentiable Plasticity}} in {{Spiking Neural Networks}}},
  shorttitle = {{{SpikePropamine}}},
  author = {Schmidgall, Samuel and Ashkanazy, Julia and Lawson, W. and Hays, Joe},
  year = {2021},
  journal = {Frontiers in Neurorobotics},
  doi = {10.3389/fnbot.2021.629210},
  abstract = {The experimental results display that SNNs augmented with differentiable plasticity are sufficient for solving a set of challenging temporal learning tasks that a traditional SNN fails to solve, even in the presence of significant noise. The adaptive changes in synaptic efficacy that occur between spiking neurons have been demonstrated to play a critical role in learning for biological neural networks. Despite this source of inspiration, many learning focused applications using Spiking Neural Networks (SNNs) retain static synaptic connections, preventing additional learning after the initial training period. Here, we introduce a framework for simultaneously learning the underlying fixed-weights and the rules governing the dynamics of synaptic plasticity and neuromodulated synaptic plasticity in SNNs through gradient descent. We further demonstrate the capabilities of this framework on a series of challenging benchmarks, learning the parameters of several plasticity rules including BCM, Oja's, and their respective set of neuromodulatory variants. The experimental results display that SNNs augmented with differentiable plasticity are sufficient for solving a set of challenging temporal learning tasks that a traditional SNN fails to solve, even in the presence of significant noise. These networks are also shown to be capable of producing locomotion on a high-dimensional robotic learning task, where near-minimal degradation in performance is observed in the presence of novel conditions not seen during the initial training period.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/M9T5XNUW/Schmidgall et al_2021_SpikePropamine.pdf}
}

@article{schmidgallStableLifelongLearning2022,
  title = {Stable {{Lifelong Learning}}: {{Spiking}} Neurons as a Solution to Instability in Plastic Neural Networks},
  shorttitle = {Stable {{Lifelong Learning}}},
  author = {Schmidgall, Samuel and Hays, Joe},
  year = {2022},
  journal = {NICE},
  doi = {10.1145/3517343.3517345},
  abstract = {This work demonstrates that utilizing plasticity together with ANNs leads to instability beyond the pre-specified lifespan used during training, and presents a solution to this instability through the use of spiking neurons. Synaptic plasticity poses itself as a powerful method of self-regulated unsupervised learning in neural networks. A recent resurgence of interest has developed in utilizing Artificial Neural Networks (ANNs) together with synaptic plasticity for intra-lifetime learning. Plasticity has been shown to improve the learning capabilities of these networks in generalizing to novel environmental circumstances. However, the long-term stability of these trained networks has yet to be examined. This work demonstrates that utilizing plasticity together with ANNs leads to instability beyond the pre-specified lifespan used during training. This instability can lead to the dramatic decline of reward seeking behavior, or quickly lead to reaching environment terminal states. This behavior is shown to hold consistent for several plasticity rules on two different environments across many training time-horizons: a cart-pole balancing problem and a quadrupedal locomotion problem. We present a solution to this instability through the use of spiking neurons.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YH8LS3VS/Schmidgall_Hays_2022_Stable Lifelong Learning.pdf}
}

@inproceedings{schmittNeuromorphicHardwareLoop2017,
  title = {Neuromorphic Hardware in the Loop: {{Training}} a Deep Spiking Network on the {{BrainScaleS}} Wafer-Scale System},
  shorttitle = {Neuromorphic Hardware in the Loop},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Schmitt, Sebastian and Kl{\"a}hn, Johann and Bellec, Guillaume and Gr{\"u}bl, Andreas and G{\"u}ttler, Maurice and Hartel, Andreas and Hartmann, Stephan and Husmann, Dan and Husmann, Kai and Jeltsch, Sebastian and Karasenko, Vitali and Kleider, Mitja and Koke, Christoph and Kononov, Alexander and Mauch, Christian and M{\"u}ller, Eric and M{\"u}ller, Paul and Partzsch, Johannes and Petrovici, Mihai A. and Schiefer, Stefan and Scholze, Stefan and Thanasoulis, Vasilis and Vogginger, Bernhard and Legenstein, Robert and Maass, Wolfgang and Mayr, Christian and Sch{\"u}ffny, Ren{\'e} and Schemmel, Johannes and Meier, Karlheinz},
  year = {2017},
  month = may,
  pages = {2227--2234},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2017.7966125},
  abstract = {Emulating spiking neural networks on analog neuromorphic hardware offers several advantages over simulating them on conventional computers, particularly in terms of speed and energy consumption. However, this usually comes at the cost of reduced control over the dynamics of the emulated networks. In this paper, we demonstrate how iterative training of a hardware-emulated network can compensate for anomalies induced by the analog substrate. We first convert a deep neural network trained in software to a spiking network on the BrainScaleS wafer-scale neuromorphic system, thereby enabling an acceleration factor of 10000 compared to the biological time domain. This mapping is followed by the in-the-loop training, where in each training step, the network activity is first recorded in hardware and then used to compute the parameter updates in software via backpropagation. An essential finding is that the parameter updates do not have to be precise, but only need to approximately follow the correct gradient, which simplifies the computation of updates. Using this approach, after only several tens of iterations, the spiking network shows an accuracy close to the ideal software-emulated prototype. The presented techniques show that deep spiking networks emulated on analog neuromorphic devices can attain good computational performance despite the inherent variations of the analog substrate.},
  keywords = {Calibration,Hardware,Neural networks,Neuromorphics,Neurons,Training},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XAV7T3HQ/Schmitt et al_2017_Neuromorphic hardware in the loop.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WWGSGS8E/7966125.html}
}

@article{schneidmanWeakPairwiseCorrelations2006,
  title = {Weak Pairwise Correlations Imply Strongly Correlated Network States in a Neural Population},
  author = {Schneidman, Elad and Berry II, Michael J. and Segev, Ronen and Bialek, William},
  year = {2006},
  month = apr,
  journal = {Nature},
  volume = {440},
  number = {7087},
  eprint = {q-bio/0512013},
  eprinttype = {arxiv},
  pages = {1007--1012},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature04701},
  abstract = {Biological networks have so many possible states that exhaustive sampling is impossible. Successful analysis thus depends on simplifying hypotheses, but experiments on many systems hint that complicated, higher order interactions among large groups of elements play an important role. In the vertebrate retina, we show that weak correlations between pairs of neurons coexist with strongly collective behavior in the responses of ten or more neurons. Surprisingly, we find that this collective behavior is described quantitatively by models that capture the observed pairwise correlations but assume no higher order interactions. These maximum entropy models are equivalent to Ising models, and predict that larger networks are completely dominated by correlation effects. This suggests that the neural code has associative or error-correcting properties, and we provide preliminary evidence for such behavior. As a first test for the generality of these ideas, we show that similar results are obtained from networks of cultured cortical neurons.},
  archiveprefix = {arXiv},
  keywords = {Humanities and Social Sciences,multidisciplinary,Quantitative Biology - Neurons and Cognition,Quantitative Biology - Quantitative Methods,Science},
  note = {Comment: Full account of work presented at the conference on Computational and Systems Neuroscience (COSYNE), 17-20 March 2005, in Salt Lake City, Utah (http://cosyne.org)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JKCVJ5BE/Schneidman et al. - 2006 - Weak pairwise correlations imply strongly correlat.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RFJZ2YEA/Schneidman et al. - 2006 - Weak pairwise correlations imply strongly correlated network states in a neural population.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UMZX8YFL/Schneidman et al. - 2006 - Weak pairwise correlations imply strongly correlated network states in a neural population(2).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@schneidmanWeakPairwiseCorrelations2006.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Z98ALJUS/0512013.html}
}

@inproceedings{schrauwen2007overview,
  title = {An Overview of Reservoir Computing: Theory, Applications and Implementations},
  booktitle = {Proceedings of the 15th European Symposium on Artificial Neural Networks. p. 471-482 2007},
  author = {Schrauwen, Benjamin and Verstraeten, David and Van Campenhout, Jan},
  year = {2007},
  pages = {471--482},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@schrauwen2007overview.md}
}

@article{schrauwenOverviewReservoirComputing,
  title = {An Overview of Reservoir Computing: Theory, Applications and Implementations},
  author = {Schrauwen, Benjamin},
  pages = {12},
  abstract = {Training recurrent neural networks is hard. Recently it has however been discovered that it is possible to just construct a random recurrent topology, and only train a single linear readout layer. State-ofthe-art performance can easily be achieved with this setup, called Reservoir Computing. The idea can even be broadened by stating that any high dimensional, driven dynamic system, operated in the correct dynamic regime can be used as a temporal `kernel' which makes it possible to solve complex tasks using just linear post-processing techniques.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PXKE2E5M/Schrauwen - An overview of reservoir computing theory, applic.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@schrauwenOverviewReservoirComputing.md}
}

@article{schultzMultipleRewardSignals2000,
  title = {Multiple Reward Signals in the Brain},
  author = {Schultz, Wolfram},
  year = {2000},
  month = dec,
  journal = {Nature Reviews Neuroscience},
  volume = {1},
  number = {3},
  pages = {199--207},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/35044563},
  abstract = {Multiple reward signals in the brain},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TWLD34IF/Schultz - 2000 - Multiple reward signals in the brain.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@schultzMultipleRewardSignals2000.md}
}

@article{schultzNeuralSubstratePrediction1997,
  title = {A {{Neural Substrate}} of {{Prediction}} and {{Reward}}},
  author = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
  year = {1997},
  month = mar,
  journal = {Science},
  volume = {275},
  number = {5306},
  pages = {1593--1599},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.275.5306.1593},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Schultz et al_1997_A Neural Substrate of Prediction and Reward.pdf}
}

@techreport{schwarzEstimatingDimensionModel1978,
  title = {Estimating the {{Dimension}} of a {{Model}}},
  author = {Schwarz, Gideon},
  year = {1978},
  journal = {Source: The Annals of Statistics},
  volume = {6},
  number = {2},
  pages = {461--464},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/U2S4U2KV/Schwarz - 1978 - Estimating the Dimension of a Model.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@schwarzEstimatingDimensionModel1978.md}
}

@book{sciencesNicolaBulsoLearning2020,
  title = {Nicola {{Bulso Learning}} Functional Connectivity in the Under-Sampled Regime},
  author = {Sciences, Health},
  year = {2020},
  number = {April},
  isbn = {978-82-326-4604-3},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5NLK86M6/Sciences - 2020 - Nicola Bulso Learning functional connectivity in the under-sampled regime.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@sciencesNicolaBulsoLearning2020.md}
}

@article{serrano-pozoNeuropathologicalAlterationsAlzheimer2011,
  title = {Neuropathological Alterations in {{Alzheimer}} Disease},
  author = {{Serrano-Pozo}, Alberto and Frosch, Matthew P. and Masliah, Eliezer and Hyman, Bradley T.},
  year = {2011},
  month = sep,
  journal = {Cold Spring Harbor Perspectives in Medicine},
  volume = {1},
  number = {1},
  pages = {a006189},
  issn = {2157-1422},
  doi = {10.1101/cshperspect.a006189},
  abstract = {The neuropathological hallmarks of Alzheimer disease (AD) include "positive" lesions such as amyloid plaques and cerebral amyloid angiopathy, neurofibrillary tangles, and glial responses, and "negative" lesions such as neuronal and synaptic loss. Despite their inherently cross-sectional nature, postmortem studies have enabled the staging of the progression of both amyloid and tangle pathologies, and, consequently, the development of diagnostic criteria that are now used worldwide. In addition, clinicopathological correlation studies have been crucial to generate hypotheses about the pathophysiology of the disease, by establishing that there is a continuum between "normal" aging and AD dementia, and that the amyloid plaque build-up occurs primarily before the onset of cognitive deficits, while neurofibrillary tangles, neuron loss, and particularly synaptic loss, parallel the progression of cognitive decline. Importantly, these cross-sectional neuropathological data have been largely validated by longitudinal in vivo studies using modern imaging biomarkers such as amyloid PET and volumetric MRI.},
  langid = {english},
  pmcid = {PMC3234452},
  pmid = {22229116},
  keywords = {Aging,Alzheimer Disease,Cerebral Amyloid Angiopathy,Cognitive Dysfunction,Humans,Lewy Body Disease,Magnetic Resonance Imaging,Neurofibrillary Tangles,Plaque; Amyloid,Positron-Emission Tomography},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Serrano-Pozo et al_2011_Neuropathological alterations in Alzheimer disease.pdf}
}

@article{shalliceOrganisationMind2012,
  title = {The {{Organisation}} of {{Mind}}},
  author = {Shallice, Tim and Cooper, Richard P.},
  year = {2012},
  journal = {Cortex},
  volume = {48},
  number = {10},
  pages = {1366--1370},
  publisher = {{Oxford University Press}},
  doi = {10.1016/j.cortex.2011.07.004},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6AWH9YDR/Tim Shallice, Richard Cooper - The Organisation of Mind-Oxford University Press (2011).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@shalliceOrganisationMind2012.md}
}

@article{shandilyaInferringNetworkTopology2011,
  title = {Inferring Network Topology from Complex Dynamics},
  author = {Shandilya, Srinivas Gorur and Timme, Marc},
  year = {2011},
  month = jan,
  journal = {New Journal of Physics},
  volume = {13},
  number = {1},
  pages = {013004--013004},
  publisher = {{IOP Publishing}},
  doi = {10.1088/1367-2630/13/1/013004},
  abstract = {Inferring the network topology from dynamical observations is a fundamental problem pervading research on complex systems. Here, we present a simple, direct method for inferring the structural connection topology of a network, given an observation of one collective dynamical trajectory. The general theoretical framework is applicable to arbitrary network dynamical systems described by ordinary differential equations. No interference (external driving) is required and the type of dynamics is hardly restricted in any way. In particular, the observed dynamics may be arbitrarily complex; stationary, invariant or transient; synchronous or asynchronous and chaotic or periodic. Presupposing a knowledge of the functional form of the dynamical units and of the coupling functions between them, we present an analytical solution to the inverse problem of finding the network topology from observing a time series of state variables only. Robust reconstruction is achieved in any sufficiently long generic observation of the system. We extend our method to simultaneously reconstructing both the entire network topology and all parameters appearing linear in the system's equations of motion. Reconstruction of network topology and system parameters is viable even in the presence of external noise that distorts the original dynamics substantially. The method provides a conceptually new step towards reconstructing a variety of real-world networks, including gene and protein interaction networks and neuronal circuits. \textcopyright{} IOP Publishing Ltd and Deutsche Physikalische Gesellschaft.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RLBLZQVX/Shandilya, Timme - 2011 - Inferring network topology from complex dynamics.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@shandilyaInferringNetworkTopology2011.md}
}

@article{shankarScaleInvariantInternalRepresentation2012,
  title = {A {{Scale-Invariant Internal Representation}} of {{Time}}},
  author = {Shankar, Karthik H. and Howard, Marc W.},
  year = {2012},
  month = jan,
  journal = {Neural Computation},
  volume = {24},
  number = {1},
  pages = {134--193},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00212},
  abstract = {We propose a principled way to construct an internal representation of the temporal stimulus history leading up to the present moment. A set of leaky integrators performs a Laplace transform on the stimulus function, and a linear operator approximates the inversion of the Laplace transform. The result is a representation of stimulus history that retains information about the temporal sequence of stimuli. This procedure naturally represents more recent stimuli more accurately than less recent stimuli; the decrement in accuracy is precisely scale invariant. This procedure also yields time cells that fire at specific latencies following the stimulus with a scale-invariant temporal spread. Combined with a simple associative memory, this representation gives rise to a moment-to-moment prediction that is also scale invariant in time. We propose that this scale-invariant representation of temporal stimulus history could serve as an underlying representation accessible to higher-level behavioral and cognitive mechanisms. In order to illustrate the potential utility of this scale-invariant representation in a variety of fields, we sketch applications using minimal performance functions to problems in classical conditioning, interval timing, scale-invariant learning in autoshaping, and the persistence of the recency effect in episodic memory across timescales.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IUNY5KE6/Shankar_Howard_2012_A Scale-Invariant Internal Representation of Time.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JKF7XHXI/A-Scale-Invariant-Internal-Representation-of-Time.html}
}

@techreport{shannonMathematicalTheoryCommunication1948,
  title = {A {{Mathematical Theory}} of {{Communication}}*},
  author = {Shannon, C E},
  year = {1948},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6CALDHL2/Shannon - 1948 - A Mathematical Theory of Communication.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@shannonMathematicalTheoryCommunication1948.md}
}

@article{sheikhRetrogradelyTransportableLentivirus2018,
  title = {Retrogradely {{Transportable Lentivirus Tracers}} for {{Mapping Spinal Cord Locomotor Circuits}}},
  author = {Sheikh, Imran S. and Keefe, Kathleen M. and Sterling, Noelle A. and Junker, Ian P. and Eneanya, Chidubem I. and Liu, Yingpeng and Tang, Xiao-Qing and Smith, George M.},
  year = {2018},
  journal = {Frontiers in Neural Circuits},
  volume = {12},
  number = {July},
  pages = {1--13},
  doi = {10.3389/fncir.2018.00060},
  keywords = {corticospinal,lentivirus,propriospinal,reticulospinal,retrograde tracing,retrograde tracing; lentivirus; propriospinal; cor,rubrospinal,spinal cord,spontaneous recovery},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/U2QEN3I2/Sheikh et al. - 2018 - Retrogradely Transportable Lentivirus Tracers for Mapping Spinal Cord Locomotor Circuits.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@sheikhRetrogradelyTransportableLentivirus2018.md}
}

@article{shilnikovArtGridFields2016,
  title = {The {{Art}} of {{Grid Fields}}: {{Geometry}} of {{Neuronal Time}}},
  author = {Shilnikov, Andrey L. and Maurer, Andrew Porter},
  year = {2016},
  journal = {Frontiers in Neural Circuits},
  volume = {10},
  number = {March},
  pages = {1--16},
  doi = {10.3389/fncir.2016.00012},
  abstract = {The discovery of grid cells in the entorhinal cortex has both elucidated our understanding of spatial representations in the brain, and germinated a large number of theoretical models regarding the mechanisms of these cells' striking spatial firing characteristics. These models cross multiple neurobiological levels that include intrinsic membrane resonance, dendritic integration, after hyperpolarization characteristics and attractor dynamics. Despite the breadth of the models, to our knowledge, parallels can be drawn between grid fields and other temporal dynamics observed in nature, much of which was described by Art Winfree and colleagues long before the initial description of grid fields. Using theoretical and mathematical investigations of oscillators, in a wide array of mediums far from the neurobiology of grid cells, Art Winfree has provided a substantial amount of research with significant and profound similarities. These theories provide specific inferences into the biological mechanisms and extraordinary resemblances across phenomenon. Therefore, this manuscript provides a novel interpretation on the phenomenon of grid fields, from the perspective of coupled oscillators, postulating that grid fields are the spatial representation of phase resetting curves in the brain. In contrast to prior models of gird cells, the current manuscript provides a sketch by which a small network of neurons, each with oscillatory components can operate to form grid cells, perhaps providing a unique hybrid between the competing attractor neural network and oscillatory interference models. The intention of this new interpretation of the data is to encourage novel testable hypotheses.},
  keywords = {grid cells,navigation,os,oscillators,phase resetting,theta},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HP2R53G5/Shilnikov, Maurer - 2016 - The Art of Grid Fields Geometry of Neuronal Time.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@shilnikovArtGridFields2016.md}
}

@article{shimonoFunctionalClustersHubs2015,
  title = {Functional Clusters, Hubs, and Communities in the Cortical Microconnectome},
  author = {Shimono, Masanori and Beggs, John M.},
  year = {2015},
  month = oct,
  journal = {Cerebral Cortex},
  volume = {25},
  number = {10},
  pages = {3743--3757},
  publisher = {{Oxford University Press}},
  issn = {14602199},
  doi = {10.1093/cercor/bhu252},
  abstract = {Although relationships between networks of different scales have been observed in macroscopic brain studies, relationships between structures of different scales in networks of neurons are unknown. To address this,we recorded fromup to 500 neurons simultaneously from slice cultures of rodent somatosensory cortex. We then measured directed effective networks with transfer entropy, previously validated in simulated cortical networks. These effective networks enabled us to evaluate distinctive nonrandom structures of connectivity at 2 different scales.We have 4 main findings. First, at the scale of 3-6 neurons (clusters), we found that high numbers of connections occurred significantly more often than expected by chance. Second, the distribution of the numberof connections per neuron (degree distribution) had a long tail, indicating that the network contained distinctively high-degree neurons, or hubs. Third, at the scale of tens to hundreds of neurons, we typically found 2-3 significantly large communities. Finally, we demonstrated that communities were relatively more robust than clusters against shuffling of connections. We conclude the microconnectome of the cortex has specific organization at different scales, as revealed by differences in robustness.We suggest that this information will help us to understand howthe microconnectome is robust against damage.},
  pmid = {25336598},
  keywords = {Cluster,Community,Hub,Microconnectome,Networks},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3XCWYTD8/Shimono, Beggs - 2015 - Functional clusters, hubs, and communities in the cortical microconnectome.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/739Z2DQQ/Shimono, Beggs - 2015 - Functional clusters, hubs, and communities in the cortical microconnectome.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@shimonoFunctionalClustersHubs2015.md}
}

@article{shipston-sharmanContinuousAttractorNetwork2016,
  title = {Continuous Attractor Network Models of Grid Cell Firing Based on Excitatory\textendash Inhibitory Interactions},
  author = {{Shipston-Sharman}, Oliver and Solanka, Lukas and Nolan, Matthew F.},
  year = {2016},
  journal = {Journal of Physiology},
  volume = {594},
  number = {22},
  pages = {6547--6557},
  publisher = {{Wiley-Blackwell}},
  doi = {10.1113/JP270630},
  abstract = {Neurons in the medial entorhinal cortex encode location through spatial firing fields that have a grid-like organisation. The challenge of identifying mechanisms for grid firing has been addressed through experimental and theoretical investigations of medial entorhinal circuits. Here, we discuss evidence for continuous attractor network models that account for grid firing by synaptic interactions between excitatory and inhibitory cells. These models assume that grid-like firing patterns are the result of computation of location from velocity inputs, with additional spatial input required to oppose drift in the attractor state. We focus on properties of continuous attractor networks that are revealed by explicitly considering excitatory and inhibitory neurons, their connectivity and their membrane potential dynamics. Models at this level of detail can account for theta-nested gamma oscillations as well as grid firing, predict spatial firing of inter-neurons as well as excitatory cells, show how gamma oscillations can be modulated independently from spatial computations, reveal critical roles for neuronal noise, and demonstrate that only a subset of excitatory cells in a network need have grid-like firing fields. Evaluating experimental data against predictions from detailed network models will be important for establishing the mechanisms mediating grid firing. Abstract figure legend Models based on excitatory-inhibitory interactions in layer 2 of the medial entorhinal cortex integrate velocity, oscillatory and spatial input to generate bump attractor states, grid firing patterns and theta-nested gamma oscillations. Abbreviations E\textendash I, excitatory\textendash inhibitory; L2PCs, layer 2 pyramidal cells; L2SCs, layer 2 stellate cells; MEC, medial entorhinal cortex.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7BDVK78L/Shipston-Sharman, Solanka, Nolan - 2016 - Continuous attractor network models of grid cell firing based on excitatory–inhibitory interac.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AVKF7WSM/Shipston-Sharman, Solanka, Nolan - 2016 - Continuous attractor network models of grid cell firing based on excitatory-inhibitory interac.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@shipston-sharmanContinuousAttractorNetwork2016.md}
}

@misc{shiSpatialTemporalCorrelations2022,
  title = {Spatial and Temporal Correlations in Neural Networks with Structured Connectivity},
  author = {Shi, Yan-Liang and Zeraati, Roxana and Levina, Anna and Engel, Tatiana A.},
  year = {2022},
  month = jul,
  number = {arXiv:2207.07930},
  eprint = {2207.07930},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.07930},
  abstract = {Correlated fluctuations in the activity of neural populations reflect the network's dynamics and connectivity. The temporal and spatial dimensions of neural correlations are interdependent. However, prior theoretical work mainly analyzed correlations in either spatial or temporal domains, oblivious to their interplay. We show that the network dynamics and connectivity jointly define the spatiotemporal profile of neural correlations. We derive analytical expressions for pairwise correlations in networks of binary units with spatially arranged connectivity in one and two dimensions. We find that spatial interactions among units generate multiple timescales in auto- and cross-correlations. Each timescale is associated with fluctuations at a particular spatial frequency, making a hierarchical contribution to the correlations. External inputs can modulate the correlation timescales when spatial interactions are nonlinear, and the modulation effect depends on the operating regime of network dynamics. These theoretical results open new ways to relate connectivity and dynamics in cortical networks via measurements of spatiotemporal neural correlations.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Quantitative Biology - Neurons and Cognition},
  note = {Comment: 25 pages, 20 figures},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SEPHKM3K/Shi et al_2022_Spatial and temporal correlations in neural networks with structured.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6NWMW46I/2207.html}
}

@techreport{shlensTutorialPrincipalComponent,
  title = {A {{Tutorial}} on {{Principal Component Analysis}}},
  author = {Shlens, Jonathon},
  abstract = {Principal component analysis (PCA) is a mainstay of modern data analysis-a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/C3R8AJ8F/Shlens - Unknown - A Tutorial on Principal Component Analysis.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@shlensTutorialPrincipalComponent.md}
}

@article{silverRewardEnough2021,
  title = {Reward Is Enough},
  author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  year = {2021},
  month = oct,
  journal = {Artificial Intelligence},
  volume = {299},
  pages = {103535},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103535},
  abstract = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.},
  langid = {english},
  keywords = {Artificial general intelligence,Artificial intelligence,Reinforcement learning,Reward},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6T2UGUGF/Silver et al_2021_Reward is enough.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4CZTCVJH/S0004370221000862.html}
}

@article{simmenPatternRetrievalThresholdlinear1996,
  title = {Pattern Retrieval in Threshold-Linear Associative Nets},
  author = {Simmen, Martin W and Treves, Alessandro and Rolls, Edmund T},
  year = {1996},
  month = jan,
  journal = {Network: Computation in Neural Systems},
  volume = {7},
  number = {1},
  pages = {109--122},
  issn = {0954-898X, 1361-6536},
  doi = {10.1080/0954898X.1996.11978657},
  abstract = {Networks of threshold-linear neurons have previously been introduced and analysed as distributed associative memory systems. Here, results from simulations of pattern retrieval in a large-scale, sparsely connected network are presented. The storage capacity lies near {$\alpha$} = 0.8 and 1.2 for binary and ternary patterns respectively, in reasonable accordance with theoretical estimates. The system is capable of retrieving states strongly correlated with one of the stored patterns even when the initial state is a highly degraded version of one of these patterns. This pattern completion ability holds for an extensive number of memory patterns, up to {$\alpha$} {$\approx$} {$\alpha$}c/2, thereby increasing the credibility of the model as an effective associative memory.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DXCBPYJX/Simmen et al. - 1996 - Pattern retrieval in threshold-linear associative .pdf}
}

@article{singerRewardedOutcomesEnhance/12//,
  title = {Rewarded {{Outcomes Enhance Reactivation}} of {{Experience}} in the {{Hippocampus}}},
  author = {Singer, Annabelle C. and Frank, Loren M.},
  year = {/12//},
  journal = {Neuron},
  volume = {64},
  number = {6},
  pages = {910--921},
  publisher = {{Kennedy and Shapiro}},
  doi = {10.1016/j.neuron.2009.11.016},
  abstract = {Remembering experiences that lead to reward is essential for survival. The hippocampus is required for forming and storing memories of events and places, but the mechanisms that associate specific experiences with rewarding outcomes are not understood. Event memory storage is thought to depend on the reactivation of previous experiences during hippocampal sharp wave-ripples (SWRs). We used a novel sequence switching task that allowed us to examine the interaction between SWRs and reward. We compared SWR activity after animals traversed spatial trajectories and either received or did not receive a reward. Here we show that rat hippocampal CA3 principal cells are significantly more active during SWRs following receipt of reward. This SWR activity was further enhanced during learning and reactivated coherent elements of the paths associated with the reward location. This enhanced reactivation in response to reward could be a mechanism to bind rewarding outcomes to the experiences that precede them. How do we remember experiences that lead to reward? Although the hippocampus is required for storing memories of the places and events that make up these experiences (Squire, 1982), little is known about the mechanisms that associate specific experiences with their outcomes. Studies in rodents examining hippocampal responses to different outcomes have generally focused on the presence or absence of a reward such as food or an escape platform in a watermaze. These reports analyzed place field activity, where hippocampal excitatory cells ("place cells") fire in particular locations in space during active exploration. These studies found that the presence of reward or differences in motivational state can alter the firing rate or location of hippocampal place fields (Breese et al.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BJAKYKSD/Singer, Frank - 1989 - Rewarded Outcomes Enhance Reactivation of Experience in the Hippocampus.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@singerRewardedOutcomesEnhance12.md}
}

@misc{singhNeuroprospectingDeepRLAgents2021,
  title = {Neuroprospecting with {{DeepRL}} Agents},
  author = {Singh, Satpreet H.},
  year = {2021},
  abstract = {Some of the current technological and algorithmic challenges in this emerging niche that AI researchers could help address are described, and some potential opportunities for cross-pollination with AI are highlighted. A virtuous cycle between neuroscience and deep reinforcement learning is emerging and the AI community can do much to enable and accelerate it. Introduction: is cross-pollination between the of form the basis of the leading models of neuronal systems (e.g. perceptual, cognitive, motor control) and have eclipsed traditional mathematical models in their ability to explain experimental data [5]. While these studies have predominantly used supervised learning to train their networks, a small but steadily growing set of recent studies investigating complex naturalistic behavior and high-level cognitive functions such as meta-learning, decision-making and control [6, 7, 8, 9, 10, 11] have instead used used deep reinforcement learning (DRL) [12, 13] to train their neural network models in the agent setting. Recent perspectives, written for an audience with a background in neurobiology, discuss the opportunities that this alternative framework offers to neuroscience [14, 15]. Here we describe some of the current technological and algorithmic challenges in this emerging niche that AI researchers could help address, and also highlight some potential opportunities for cross-pollination with AI.},
  howpublished = {https://www.semanticscholar.org/paper/Neuroprospecting-with-DeepRL-agents-Singh/6c49d768cff533cb9cafca0c3c3a4081a024ba77},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SNQP6LQZ/Singh - Neuroprospecting with DeepRL agents.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Y9768SCP/6c49d768cff533cb9cafca0c3c3a4081a024ba77.html}
}

@article{sinhaBehaviorrelatedGeneRegulatory2020,
  title = {Behavior-Related Gene Regulatory Networks: {{A}} New Level of Organization in the Brain},
  shorttitle = {Behavior-Related Gene Regulatory Networks},
  author = {Sinha, Saurabh and Jones, Beryl M. and Traniello, Ian M. and Bukhari, Syed A. and Halfon, Marc S. and Hofmann, Hans A. and Huang, Sui and Katz, Paul S. and Keagy, Jason and Lynch, Vincent J. and Sokolowski, Marla B. and Stubbs, Lisa J. and {Tabe-Bordbar}, Shayan and Wolfner, Mariana F. and Robinson, Gene E.},
  year = {2020},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {38},
  pages = {23270--23279},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1921625117},
  abstract = {Neuronal networks are the standard heuristic model today for describing brain activity associated with animal behavior. Recent studies have revealed an extensive role for a completely distinct layer of networked activities in the brain\textemdash the gene regulatory network (GRN)\textemdash that orchestrates expression levels of hundreds to thousands of genes in a behavior-related manner. We examine emerging insights into the relationships between these two types of networks and discuss their interplay in spatial as well as temporal dimensions, across multiple scales of organization. We discuss properties expected of behavior-related GRNs by drawing inspiration from the rich literature on GRNs related to animal development, comparing and contrasting these two broad classes of GRNs as they relate to their respective phenotypic manifestations. Developmental GRNs also represent a third layer of network biology, playing out over a third timescale, which is believed to play a crucial mediatory role between neuronal networks and behavioral GRNs. We end with a special emphasis on social behavior, discuss whether unique GRN organization and cis-regulatory architecture underlies this special class of behavior, and review literature that suggests an affirmative answer.},
  chapter = {Perspective},
  copyright = {\textcopyright{} 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {32661177},
  keywords = {behavior,development,network},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Sinha et al_2020_Behavior-related gene regulatory networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@sinhaBehaviorrelatedGeneRegulatory2020.md}
}

@article{sjalanderEPICEnergyEfficientHighPerformance2020,
  title = {{{EPIC}}: {{An Energy-Efficient}}, {{High-Performance GPGPU Computing Research Infrastructure}}},
  shorttitle = {{{EPIC}}},
  author = {Sj{\"a}lander, Magnus and Jahre, Magnus and Tufte, Gunnar and Reissmann, Nico},
  year = {2020},
  month = dec,
  journal = {arXiv:1912.05848 [cs]},
  eprint = {1912.05848},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The pursuit of many research questions requires massive computational resources. State-of-the-art research in physical processes using simulations, the training of neural networks for deep learning, or the analysis of big data are all dependent on the availability of sufficient and performant computational resources. For such research, access to a high-performance computing infrastructure is indispensable. Many scientific workloads from such research domains are inherently parallel and can benefit from the data-parallel architecture of general purpose graphics processing units (GPGPUs). However, GPGPU resources are scarce at Norway's national infrastructure. EPIC is a GPGPU enabled computing research infrastructure at NTNU. It enables NTNU's researchers to perform experiments that otherwise would be impossible, as time-to-solution would simply take too long.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/S332RXHQ/Själander et al. - 2020 - EPIC An Energy-Efficient, High-Performance GPGPU .pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@sjalanderEPICEnergyEfficientHighPerformance2020.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Y7UHC3LS/1912.html}
}

@techreport{skaggsInformationTheoreticApproachDeciphering,
  title = {An {{Information-Theoretic Approach}} to {{Deciphering}} the {{Hippocampal Code}}},
  author = {Skaggs, William E and Mcnaughton, Bruce L and Gothard, Katalin M and Markus, Etan J},
  abstract = {Information theory is used to derive a simple formula for the amount of information conveyed by the ering rate of a neuron about any experimentally measured variable or combination of variables (e.g. running speed, head direction, location of the animal, etc.). The derivation treats the cell as a communication channel whose input is the measured variable and whose output is the cell's spike train. Applying the formula, we end systematic diierences in the information content of hippocampal \textbackslash place cells" in diierent experimental conditions.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/APCXU7D4/Skaggs et al. - Unknown - An Information-Theoretic Approach to Deciphering the Hippocampal Code.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@skaggsInformationTheoreticApproachDeciphering.md}
}

@techreport{slatkinLinkageDisequilibriumUnderstanding2008,
  type = {Preprint},
  title = {Deep Neural Networks Learn General and Clinically Relevant Representations of the Ageing Brain},
  author = {Leonardsen, Esten H. and Peng, Han and Kaufmann, Tobias and Agartz, Ingrid and Andreassen, Ole A. and Celius, Elisabeth Gulowsen and Espeseth, Thomas and Harbo, Hanne F. and H{\o}gest{\o}l, Einar A. and {de Lange}, Ann-Marie and Marquand, Andre F. and {Vidal-Pi{\~n}eiro}, Didac and Roe, James M. and Selb{\ae}k, Geir and S{\o}rensen, {\O}ystein and Smith, Stephen M. and Westlye, Lars T. and Wolfers, Thomas and Wang, Yunpeng},
  year = {2021},
  month = oct,
  institution = {{Neurology}},
  doi = {10.1101/2021.10.29.21265645},
  abstract = {The discrepancy between chronological age and the apparent age of the brain based on neuroimaging data \textemdash{} the brain age delta \textemdash{} has emerged as a reliable marker of brain health. With an increasing wealth of data, approaches to tackle heterogeneity in data acquisition are vital. To this end, we compiled raw structural magnetic resonance images into one of the largest and most diverse datasets assembled (n=53542), and trained convolutional neural networks (CNNs) to predict age. We achieved state-of-the-art performance on unseen data from unknown scanners (n=2553), and showed that higher brain age delta is associated with diabetes, alcohol intake and smoking. Using transfer learning, the intermediate representations learned by our model complemented and partly outperformed brain age delta in predicting common brain disorders. Our work shows we can achieve generalizable and biologically plausible brain age predictions using CNNs trained on heterogeneous datasets, and transfer them to clinical use cases.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AQ7DP8JH/Leonardsen et al. - 2021 - Deep neural networks learn general and clinically .pdf}
}

@misc{SlowProcessesNeurons,
  title = {Slow {{Processes}} of {{Neurons Enable}} a {{Biologically Plausible Approximation}} to {{Policy Gradient}}},
  langid = {american}
}

@misc{SlowProcessesNeuronsa,
  title = {Slow {{Processes}} of {{Neurons Enable}} a {{Biologically Plausible Approximation}} to {{Policy Gradient}}},
  langid = {american},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YRPSRYZI/slow-processes-of-neurons-enable-a-biologically-plausible-approximation-to-policy-gradient.html}
}

@article{socolichEvolutionaryInformationSpecifying2005,
  title = {Evolutionary Information for Specifying a Protein Fold},
  author = {Socolich, Michael and Lockless, Steve W. and Russ, William P. and Lee, Heather and Gardner, Kevin H. and Ranganathan, Rama},
  year = {2005},
  month = sep,
  journal = {Nature},
  volume = {437},
  number = {7058},
  pages = {512--518},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nature03991},
  abstract = {Classical studies show that for many proteins, the information required for specifying the tertiary structure is contained in the amino acid sequence. Here, we attempt to define the sequence rules for specifying a protein fold by computationally creating artificial protein sequences using only statistical information encoded in a multiple sequence alignment and no tertiary structure information. Experimental testing of libraries of artificial WW domain sequences shows that a simple statistical energy function capturing coevolution between amino acid residues is necessary and sufficient to specify sequences that fold into native structures. The artificial proteins show thermodynamic stabilities similar to natural WW domains, and structure determination of one artificial protein shows excellent agreement with the WW fold at atomic resolution. The relative simplicity of the information used for creating sequences suggests a marked reduction to the potential complexity of the protein-folding problem. \textcopyright{} 2005 Nature Publishing Group.},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N5XA2VD7/Socolich et al. - 2005 - Evolutionary information for specifying a protein fold.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@socolichEvolutionaryInformationSpecifying2005.md}
}

@article{soltoggioBornLearnInspiration2017,
  title = {Born to {{Learn}}: The {{Inspiration}}, {{Progress}}, and {{Future}} of {{Evolved Plastic Artificial Neural Networks}}},
  author = {Soltoggio, Andrea and Stanley, Kenneth O. and Risi, Sebastian},
  year = {2017},
  month = mar,
  doi = {10.1016/j.neunet.2018.07.013},
  abstract = {Biological plastic neural networks are systems of extraordinary computational capabilities shaped by evolution, development, and lifetime learning. The interplay of these elements leads to the emergence of adaptive behavior and intelligence. Inspired by such intricate natural phenomena, Evolved Plastic Artificial Neural Networks (EPANNs) use simulated evolution in-silico to breed plastic neural networks with a large variety of dynamics, architectures, and plasticity rules: these artificial systems are composed of inputs, outputs, and plastic components that change in response to experiences in an environment. These systems may autonomously discover novel adaptive algorithms, and lead to hypotheses on the emergence of biological adaptation. EPANNs have seen considerable progress over the last two decades. Current scientific and technological advances in artificial neural networks are now setting the conditions for radically new approaches and results. In particular, the limitations of hand-designed networks could be overcome by more flexible and innovative solutions. This paper brings together a variety of inspiring ideas that define the field of EPANNs. The main methods and results are reviewed. Finally, new opportunities and developments are presented.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FPE7JELA/Soltoggio, Stanley, Risi - 2017 - Born to Learn the Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@soltoggioBornLearnInspiration2017.md}
}

@article{somanHierarchicalAntiHebbianNetwork2018,
  title = {A Hierarchical Anti-{{Hebbian}} Network Model for the Formation of Spatial Cells in Three-Dimensional Space},
  author = {Soman, Karthik and Chakravarthy, Srinivasa and Yartsev, Michael M.},
  year = {2018},
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  doi = {10.1038/s41467-018-06441-5},
  abstract = {Three dimensional (3D) spatial cells in the mammalian hippocampal formation are believed to support the existence of 3D cognitive maps. Modeling studies are crucial to comprehend the neural principles governing the formation of these maps, yet to date very few have addressed this topic in 3D space. Here, we present a hierarchical network model for the formation of 3D spatial cells using anti-hebbian network. Built on empirical data, the model accounts for the natural emergence of 3D place, border and grid-cells as well as a new type of previously undescribed spatial cell type which we call plane cells. It further explains the plausible reason behind the place and grid-cell anisotropic coding that has been observed in rodents and the potential discrepancy with the predicted periodic coding during 3D volumetric navigation. Lastly, it provides evidence for the importance of unsupervised learning rules in guiding the formation of higher dimensional cognitive maps.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AVAJIW7Z/Soman, Chakravarthy, Yartsev - 2018 - A hierarchical anti-Hebbian network model for the formation of spatial cells in three-dimensional.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@somanHierarchicalAntiHebbianNetwork2018.md}
}

@article{songCompetitiveHebbianLearning2000,
  title = {Competitive {{Hebbian}} Learning through Spike-Timing-Dependent Synaptic Plasticity},
  author = {Song, Sen and Miller, Kenneth D. and Abbott, L. F.},
  year = {2000},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {3},
  number = {9},
  pages = {919--926},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/78829},
  abstract = {Hebbian models of development and learning require both activity-dependent synaptic plasticity and a mechanism that induces competition between different synapses. One form of experimentally observed long-term synaptic plasticity, which we call spike-timing-dependent plasticity (STDP), depends on the relative timing of pre- and postsynaptic action potentials. In modeling studies, we find that this form of synaptic modification can automatically balance synaptic strengths to make postsynaptic firing irregular but more sensitive to presynaptic spike timing. It has been argued that neurons in vivo operate in such a balanced regime. Synapses modifiable by STDP compete for control of the timing of postsynaptic action potentials. Inputs that fire the postsynaptic neuron with short latency or that act in correlated groups are able to compete most successfully and develop strong synapses, while synapses of longer-latency or less-effective inputs are weakened.},
  copyright = {2000 Nature America Inc.},
  langid = {english},
  keywords = {plasticity,STDP},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@songCompetitiveHebbianLearning2000.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/songCompetitiveHebbianLearning2000-mdnotes.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Song et al_2000_Competitive Hebbian learning through spike-timing-dependent synaptic plasticity.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@songCompetitiveHebbianLearning2000.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VKF5Z2W8/nn0900_919.html}
}

@article{songRewardbasedTrainingRecurrent2017,
  title = {Reward-Based Training of Recurrent Neural Networks for Cognitive and Value-Based Tasks},
  author = {Song, H Francis and Yang, Guangyu R and Wang, Xiao-Jing},
  year = {2017},
  month = jan,
  journal = {eLife},
  volume = {6},
  pages = {e21492},
  issn = {2050-084X},
  doi = {10.7554/eLife.21492},
  abstract = {Trained neural network models, which exhibit features of neural activity recorded from behaving animals, may provide insights into the circuit mechanisms of cognitive functions through systematic analysis of network activity and connectivity. However, in contrast to the graded error signals commonly used to train networks through supervised learning, animals learn from reward feedback on definite actions through reinforcement learning. Reward maximization is particularly relevant when optimal behavior depends on an animal's internal judgment of confidence or subjective preferences. Here, we implement reward-based training of recurrent neural networks in which a value network guides learning by using the activity of the decision network to predict future reward. We show that such models capture behavioral and electrophysiological findings from well-known experimental paradigms. Our work provides a unified framework for investigating diverse cognitive and value-based computations, and predicts a role for value representation that is essential for learning, but not executing, a task.           ,              A major goal in neuroscience is to understand the relationship between an animal's behavior and how this is encoded in the brain. Therefore, a typical experiment involves training an animal to perform a task and recording the activity of its neurons \textendash{} brain cells \textendash{} while the animal carries out the task. To complement these experimental results, researchers ``train'' artificial neural networks \textendash{} simplified mathematical models of the brain that consist of simple neuron-like units \textendash{} to simulate the same tasks on a computer. Unlike real brains, artificial neural networks provide complete access to the ``neural circuits'' responsible for a behavior, offering a way to study and manipulate the behavior in the circuit.             One open issue about this approach has been the way in which the artificial networks are trained. In a process known as reinforcement learning, animals learn from rewards (such as juice) that they receive when they choose actions that lead to the successful completion of a task. By contrast, the artificial networks are explicitly told the correct action. In addition to differing from how animals learn, this limits the types of behavior that can be studied using artificial neural networks.             Recent advances in the field of machine learning that combine reinforcement learning with artificial neural networks have now allowed Song et al. to train artificial networks to perform tasks in a way that mimics the way that animals learn. The networks consisted of two parts: a ``decision network'' that uses sensory information to select actions that lead to the greatest reward, and a ``value network'' that predicts how rewarding an action will be. Song et al. found that the resulting artificial ``brain activity'' closely resembled the activity found in the brains of animals, confirming that this method of training artificial neural networks may be a useful tool for neuroscientists who study the relationship between brains and behavior.             The training method explored by Song et al. represents only one step forward in developing artificial neural networks that resemble the real brain. In particular, neural networks modify connections between units in a vastly different way to the methods used by biological brains to alter the connections between neurons. Future work will be needed to bridge this gap.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LDX4XI5M/Song et al. - 2017 - Reward-based training of recurrent neural networks.pdf}
}

@misc{SourcelevelEEGGraph,
  title = {Source-Level {{EEG}} and Graph Theory Reveal Widespread Functional Network Alterations in Focal Epilepsy - {{ScienceDirect}}},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S1388245721005459},
  keywords = {\#phdapp},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HCLPEPMF/Source-level EEG and graph theory reveal widesprea.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WCGY9XYR/S1388245721005459.html}
}

@article{spallaContinuousAttractorsDynamic2020,
  title = {Continuous Attractors for Dynamic Memories},
  author = {Spalla, Davide and Cornacchia, I. M. and Treves, A.},
  year = {2020},
  doi = {10.1101/2020.11.08.373084},
  abstract = {A continuous attractor network model with a memory-dependent asymmetric component in the synaptic connectivity that spontaneously breaks the equilibrium of the memory configurations and produces dynamic retrieval, and can robustly retrieve multiple dynamical memories. Episodic memory has a dynamic nature: when we recall past episodes, we retrieve not only their content, but also their temporal structure. The phenomenon of replay, in the hippocampus of mammals, offers a remarkable example of this temporal dynamics. However, most quantitative models of memory treat memories as static configurations, neglecting the temporal unfolding of the retrieval process. Here we introduce a continuous attractor network model with a memory-dependent asymmetric component in the synaptic connectivity, that spontaneously breaks the equilibrium of the memory configurations and produces dynamic retrieval. The detailed analysis of the model with analytical calculations and numerical simulations shows that it can robustly retrieve multiple dynamical memories, and that this feature is largely independent on the details of its implementation. By calculating the storage capacity we show that the dynamic component does not impair memory capacity, and can even enhance it in certain regimes.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VCBMY7QY/Spalla et al_2020_Continuous attractors for dynamic memories.pdf}
}

@article{spiegelhalterBayesianMeasuresModel2002,
  title = {Bayesian Measures of Model Complexity and Fit},
  author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and Linde, Angelika Van Der},
  year = {2002},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {64},
  number = {4},
  pages = {583--639},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00353},
  abstract = {Summary. We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the `hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.},
  langid = {english},
  keywords = {Bayesian model comparison,Decision theory,Deviance information criterion,Effective number of parameters,Hierarchical models,Information theory,Leverage,Markov chain Monte Carlo methods,Model dimension},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00353},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/V5UV39H2/Spiegelhalter et al. - 2002 - Bayesian measures of model complexity and fit.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@spiegelhalterBayesianMeasuresModel2002.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WQEMI46F/1467-9868.html}
}

@article{spinolaWhatHumansLearn2013,
  title = {What Do Humans Learn in a Double, Temporal Bisection Task: {{Absolute}} or Relative Stimulus Durations?},
  shorttitle = {What Do Humans Learn in a Double, Temporal Bisection Task},
  author = {Sp{\'i}nola, Igor and Machado, Armando and {de Carvalho}, Marilia Pinheiro and Tonneau, Fran{\c c}ois},
  year = {2013},
  month = may,
  journal = {Behavioural Processes},
  series = {{{SQAB}} 2012:{{Timing}}},
  volume = {95},
  pages = {40--49},
  issn = {0376-6357},
  doi = {10.1016/j.beproc.2013.01.003},
  abstract = {The relative-coding hypothesis of temporal discrimination asserts that humans learn to respond to the relative duration of stimuli (``short'' and ``long''). The most frequently used procedure to test the hypothesis is the double bisection task. In one task, participants learn that red and green are the correct comparisons following 2s (short) and 5s (long) samples respectively. In another task, participants learn that triangle and circle are the correct comparisons following 3.5s (short) and 6.5s (long) samples, respectively. Later the samples of one task are tested with the comparisons of the other task, and vice versa. According to the hypothesis, participants will choose red following a 3.5s sample because that sample is short and red is the comparison that goes with short. Similarly, they will choose circle following 5s samples because that sample is long and circle goes with long. We replicated this procedure and improved it by introducing several sample durations during testing to obtain the whole psychometric function of each task. Results from Experiment 1 only partially corroborated the relative-coding hypothesis. Results from Experiment 2 did not corroborate the hypothesis. The combined data from Experiments 1 and 2 partially corroborate the hypothesis. Alternatively, we present an explanation of relative-coding-like results that posits exclusively absolute coding of temporal stimuli. This article is part of a Special Issue entitled: SQAB 2012.},
  langid = {english},
  keywords = {Double bisection task,Humans,Psychometric function,Relative versus absolute,Timing},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8ATQKLNE/S0376635713000041.html}
}

@article{spirtesConstructingBayesianNetwork2000,
  title = {Constructing {{Bayesian Network Models}} of {{Gene Expression Networks}} from {{Microarray Data}}},
  author = {Spirtes, Pater and Glymour, Clark and Scheines, Richard and Kauffman, Stuart and Aimale, Valerio and Wimberly, Frank},
  year = {2000},
  month = jan,
  publisher = {{Carnegie Mellon University}},
  doi = {10.1184/R1/6491291.v1},
  abstract = {Through their transcript products genes regulate the rates at which an immense variety of transcripts and subsequent proteins occur. Understanding the mechanisms that determine which genes are expressed, and when they are expressed, is one of the keys to genetic manipulation for many purposes, including the development of new treatments for disease. Viewing each gene in a genome as a distinct variable that is either on (expresses) or off (does not express), or more realistically as a continuous variable (the rate of expression), the values of some of these variables influence the values of others through the regulatory proteins they express, including, of course, the possibility that the rate of expression of a gene at one time may, in various circumstances, influence the rate of expression of that same gene at a later time. If we imagine an arrow drawn from each gene expression variable at a given time to a gene variable whose expression it influences a short while after, the result is a network, technically a directed acyclic graph (DAG). For example, the DAG in Figure 1 is a representation of a system in which the expression level of gene G1 at time 1 (denoted as G1(1)) causes the expression level of G2(2), which in turn causes the expression level of G3(3). The arrows in Figure 1 which do not have a variable at their tails are ``error terms'' which represent all of the causes of a variable other than the ones explicitly represented in the DAG. The DAG describes more than associations\textemdash it describes causal connections among gene expression rates. A shock to a cell\textemdash by mutation, heating, chemical treatment, etc. may alter the DAG describing the relations among gene expressions, for example by activating a gene that was otherwise not expressed, producing a cascade of new expression effects. Although ``knockout'' experiments (which lower a gene's expression level) can reveal some of the underlying causal network of gene expression levels, unless guided by information from other sources, such experiments are limited in how much of the network structure they can reveal, due to the sheer number of possible combinations of experimental manipulations of genes necessary to reveal the complete causal network. Recent developments have made it possible to compare quantitatively the expression of tens of thousands of genes in cells from different sources in a single experiment, and to trace gene expression over time in thousands of genes simultaneously. cDNA microarrays are already producing extensive data, much of it available on the web. Thus there are calls for analytic software that can be applied to microarray and other data to help infer regulatory networks (Weinzierl, 1999). In this paper we will review current techniques that are available for searching for the causal relations between variables, describe algorithmic and data gathering obstacles to applying these techniques to gene expression levels, and describe the prospects for overcoming these obstacles.},
  langid = {english},
  keywords = {Constraint based,pensar2017},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/APGRHPY8/Spirtes et al. - 2000 - Constructing Bayesian Network Models of Gene Expre.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UP24Y4PY/Spirtes et al. - 2000 - Constructing Bayesian Network Models of Gene Expre.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@spirtesConstructingBayesianNetwork2000.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CP9LJNGA/1.html;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/J2964XY9/1.html}
}

@article{spornsClassesNetworkConnectivity2001,
  title = {Classes of Network Connectivity and Dynamics},
  author = {Sporns, Olaf and Tononi, Giulio},
  year = {2001},
  journal = {Complexity},
  volume = {7},
  number = {1},
  pages = {28--38},
  issn = {1099-0526},
  doi = {10.1002/cplx.10015},
  abstract = {Many kinds of complex systems exhibit characteristic patterns of temporal correlations that emerge as the result of functional interactions within a structured network. One such complex system is the brain, composed of numerous neuronal units linked by synaptic connections. The activity of these neuronal units gives rise to dynamic states that are characterized by specific patterns of neuronal activation and co-activation. These patterns, called functional connectivity, are possible neural correlates of perceptual and cognitive processes. Which functional connectivity patterns arise depends on the anatomical structure of the underlying network, which in turn is modified by a broad range of activity-dependent processes. Given this intricate relationship between structure and function, the question of how patterns of anatomical connectivity constrain or determine dynamical patterns is of considerable theoretical importance. The present study develops computational tools to analyze networks in terms of their structure and dynamics. We identify different classes of network, including networks that are characterized by high complexity. These highly complex networks have distinct structural characteristics such as clustered connectivity and short wiring length similar to those of large-scale networks of the cerebral cortex. \textcopyright{} 2002 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {complexity,Complexity,connectivity,Connectivity,covariance,Covariance,information theory,Information theory,networks,Networks},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cplx.10015},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GIP26GF5/cplx.10015.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Q4ZHD597/Sporns and Tononi - 2001 - Classes of network connectivity and dynamics.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@spornsClassesNetworkConnectivity2001.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YM3PRFAY/cplx.html}
}

@article{spornsHumanConnectomeComplex2011,
  title = {The Human Connectome: {{A}} Complex Network},
  author = {Sporns, Olaf},
  year = {2011},
  month = apr,
  journal = {Annals of the New York Academy of Sciences},
  volume = {1224},
  number = {1},
  pages = {109--125},
  publisher = {{Blackwell Publishing Inc.}},
  doi = {10.1111/j.1749-6632.2010.05888.x},
  abstract = {The human brain is a complex network. An important first step toward understanding the function of such a network is to map its elements and connections, to create a comprehensive structural description of the network architecture. This paper reviews current empirical efforts toward generating a network map of the human brain, the human connectome, and explores how the connectome can provide new insights into the organization of the brain's structural connections and their role in shaping functional dynamics. Network studies of structural connectivity obtained from noninvasive neuroimaging have revealed a number of highly nonrandom network attributes, including high clustering and modularity combined with high efficiency and short path length. The combination of these attributes simultaneously promotes high specialization and high integration within a modular small-world architecture. Structural and functional networks share some of the same characteristics, although their relationship is complex and nonlinear. Future studies of the human connectome will greatly expand our knowledge of network topology and dynamics in the healthy, developing, aging, and diseased brain. \textcopyright{} 2011 New York Academy of Sciences.},
  keywords = {Brain anatomy,Complex systems,Diffusion imaging,Networks,Neural dynamics,Resting state},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/H7CQ8GP3/Sporns - 2011 - The human connectome A complex network.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LASHJIT8/Sporns - 2011 - The human connectome A complex network.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@spornsHumanConnectomeComplex2011.md}
}

@article{spornsStructureFunctionComplex2013,
  title = {Structure and Function of Complex Brain Networks},
  author = {Sporns, Olaf},
  year = {2013},
  month = sep,
  journal = {Dialogues in Clinical Neuroscience},
  volume = {15},
  number = {3},
  pages = {247--262},
  issn = {1294-8322},
  abstract = {An increasing number of theoretical and empirical studies approach the function of the human brain from a network perspective. The analysis of brain networks is made feasible by the development of new imaging acquisition methods as well as new tools from graph theory and dynamical systems. This review surveys some of these methodological advances and summarizes recent findings on the architecture of structural and functional brain networks. Studies of the structural connectome reveal several modules or network communities that are interlinked by hub regions mediating communication processes between modules. Recent network analyses have shown that network hubs form a densely linked collective called a ``rich club,'' centrally positioned for attracting and dispersing signal traffic. In parallel, recordings of resting and task-evoked neural activity have revealed distinct resting-state networks that contribute to functions in distinct cognitive domains. Network methods are increasingly applied in a clinical context, and their promise for elucidating neural substrates of brain and mental disorders is discussed.},
  pmcid = {PMC3811098},
  pmid = {24174898},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Sporns_2013_Structure and function of complex brain networks.pdf}
}

@article{spreemannUsingPersistentHomology2018,
  title = {Using Persistent Homology to Reveal Hidden Covariates in Systems Governed by the Kinetic {{Ising}} Model},
  author = {Spreemann, Gard and Dunn, Benjamin and Botnan, Magnus Bakke and Baas, Nils A.},
  year = {2018},
  month = mar,
  journal = {Physical Review E},
  volume = {97},
  number = {3},
  pages = {032313},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.97.032313},
  abstract = {We propose a method, based on persistent homology, to uncover topological properties of a priori unknown covariates in a system governed by the kinetic Ising model with time-varying external fields. As its starting point the method takes observations of the system under study, a list of suspected or known covariates, and observations of those covariates. We infer away the contributions of the suspected or known covariates, after which persistent homology reveals topological information about unknown remaining covariates. Our motivating example system is the activity of neurons tuned to the covariates physical position and head direction, but the method is far more general.},
  file = {C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@spreemannUsingPersistentHomology2018.md;C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Zotero\\storage\\Spreemann et al_2018_Using persistent homology to reveal hidden covariates in systems governed by.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/K2NMNZCM/PhysRevE.97.html}
}

@article{sreenivasanGridCellsGenerate2011,
  title = {Grid Cells Generate an Analog Error-Correcting Code for Singularly Precise Neural Computation},
  author = {Sreenivasan, Sameet and Fiete, Ila},
  year = {2011},
  month = oct,
  journal = {Nature Neuroscience},
  volume = {14},
  number = {10},
  pages = {1330--1337},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.2901},
  abstract = {Mammalian grid cells have a spatially periodic pattern of responses to location, which is a puzzling feature. Here, the authors demonstrate that this pattern of activity is compatible with a coding scheme that allows for very accurate localization.},
  copyright = {2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational neuroscience;Learning and memory Subject\_term\_id: computational-neuroscience;learning-and-memory},
  file = {C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@sreenivasanGridCellsGenerate2011.md;C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Zotero\\storage\\Sreenivasan_Fiete_2011_Grid cells generate an analog error-correcting code for singularly precise.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N8AQ5FX5/nn.html}
}

@techreport{sridharanIsingModelMarkov,
  title = {The {{Ising}} Model and {{Markov}} Chain {{Monte Carlo}}},
  author = {Sridharan, Ramesh},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZZP4KG5K/Sridharan - Unknown - The Ising model and Markov chain Monte Carlo.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@sridharanIsingModelMarkov.md}
}

@techreport{srivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  pages = {1929--1958},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  keywords = {deep learning,model combination,neural networks,regularization},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZFACB8JV/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@srivastavaDropoutSimpleWay2014.md}
}

@article{stanekNuclearBodiesNews2017,
  title = {Nuclear Bodies: News Insights into Structure and Function},
  shorttitle = {Nuclear Bodies},
  author = {Stan{\v e}k, David and Fox, Archa H.},
  year = {2017},
  month = jun,
  journal = {Current Opinion in Cell Biology},
  volume = {46},
  pages = {94--101},
  issn = {1879-0410},
  doi = {10.1016/j.ceb.2017.05.001},
  abstract = {The cell nucleus contains a number of different dynamic bodies that are variously composed of proteins and generally, but not always, specific RNA molecules. Recent studies have revealed new understanding about nuclear body formation and function in different aspects of nuclear metabolism. Here, we focus on findings describing the role of nuclear bodies in the biogenesis of specific ribonucleoprotein complexes, processing of key mRNAs, and subnuclear sequestration of protein factors. We highlight how nuclear bodies are involved in stress responses, innate immunity and tumorigenesis. We further review organization of nuclear bodies and principles that govern their assembly, highlighting the pivotal role of scaffolding noncoding RNAs, and liquid-liquid phase separation, which are transforming our picture of nuclear body formation.},
  langid = {english},
  pmid = {28577509},
  keywords = {Animals,Cell Nucleus,Humans,Intranuclear Inclusion Bodies,Nuclear Proteins,RNA; Untranslated}
}

@article{stanleyDesigningNeuralNetworks2019,
  title = {Designing Neural Networks through Neuroevolution},
  author = {Stanley, Kenneth O. and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
  year = {2019},
  month = jan,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {1},
  pages = {24--35},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/s42256-018-0006-z},
  abstract = {Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field's contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence.},
  keywords = {Computer science,Software},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KL9KAIAY/Stanley et al. - 2019 - Designing neural networks through neuroevolution.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@stanleyDesigningNeuralNetworks2019.md}
}

@inproceedings{stanleyEvolvingAdaptiveNeural2003,
  title = {Evolving Adaptive Neural Networks with and without Adaptive Synapses},
  booktitle = {The 2003 {{Congress}} on {{Evolutionary Computation}}, 2003. {{CEC}} '03.},
  author = {Stanley, K.O. and Bryant, B.D. and Miikkulainen, R.},
  year = {2003},
  month = dec,
  volume = {4},
  pages = {2557-2564 Vol.4},
  doi = {10.1109/CEC.2003.1299410},
  abstract = {A potentially powerful application of evolutionary computation (EC) is to evolve neural networks for automated control tasks. However, in such tasks environments can be unpredictable and fixed control policies may fail when conditions suddenly change. Thus, there is a need to evolve neural networks that can adapt, i.e. change their control policy dynamically as conditions change. In this paper, we examine two methods for evolving neural networks with dynamic policies. The first method evolves recurrent neural networks with fixed connection weights, relying on internal state changes to lead to changes in behavior. The second method evolves local rules that govern connection weight changes. The surprising experimental result is that the former method can be more effective than evolving networks with dynamic weights, calling into question the intuitive notion that networks with dynamic synapses are necessary for evolving solutions to adaptive tasks.},
  keywords = {Adaptive systems,Application software,Automatic control,Backpropagation,Biological neural networks,Computer networks,hebbian without spiking,Neural networks,Organisms,Plastics,Recurrent neural networks},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4CTD85S5/Stanley et al_2003_Evolving adaptive neural networks with and without adaptive synapses.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NII5IJ63/1299410.html}
}

@article{steinmetzChallengesOpportunitiesLargescale2018,
  title = {Challenges and Opportunities for Large-Scale Electrophysiology with {{Neuropixels}} Probes},
  author = {Steinmetz, Nicholas A. and Koch, Christof and Harris, Kenneth D. and Carandini, Matteo},
  year = {2018},
  journal = {Current Opinion in Neurobiology},
  volume = {50},
  pages = {92--100},
  publisher = {{The Authors}},
  doi = {10.1016/j.conb.2018.01.009},
  abstract = {Electrophysiological methods are the gold standard in neuroscience because they reveal the activity of individual neurons at high temporal resolution and in arbitrary brain locations. Microelectrode arrays based on complementary metal-oxide semiconductor (CMOS) technology, such as Neuropixels probes, look set to transform these methods. Neuropixels probes provide {$\sim$}1000 recording sites on an extremely narrow shank, with on-board amplification, digitization, and multiplexing. They deliver low-noise recordings from hundreds of neurons, providing a step change in the type of data available to neuroscientists. Here we discuss the opportunities afforded by these probes for large-scale electrophysiology, the challenges associated with data processing and anatomical localization, and avenues for further improvements of the technology.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/C9DQE66S/Steinmetz et al. - 2018 - Challenges and opportunities for large-scale electrophysiology with Neuropixels probes.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@steinmetzChallengesOpportunitiesLargescale2018.md}
}

@article{stellaSelforganizationGridCells2015,
  title = {The Self-Organization of Grid Cells in {{3D}}},
  author = {Stella, Federico and Trevesy, Alessandro},
  year = {2015},
  journal = {eLife},
  volume = {2015},
  number = {4},
  pages = {1--21},
  doi = {10.7554/eLife.05913},
  abstract = {What sort of grid cells do we expect to see in bats exploring a three-dimensional environment? How long will it take for them to emerge? We address these questions within our self-organization model based on firing-rate adaptation. The model indicates that the answer to the first question may be simple, and to the second one rather complex. The mathematical analysis of the simplified version of the model points at asymptotic states resembling FCC and HCP crystal structures, which are calculated to be very close to each other in terms of cost function. The simulation of the full model, however, shows that the approach to such asymptotic states involves several sub-processes over distinct time scales. The smoothing of the initially irregular multiple fields of individual units and their arrangement into hexagonal grids over certain best planes are observed to occur relatively fast, even in large 3D volumes. The correct mutual orientation of the planes, however, and the coordinated arrangement of different units, take a longer time, with the network showing no sign of convergence towards either a pure FCC or HCP ordering.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DY56MB5T/Stella, Trevesy - 2015 - The self-organization of grid cells in 3D.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@stellaSelforganizationGridCells2015.md}
}

@article{stemmlerSpatialCognitionGrid2017,
  title = {Spatial {{Cognition}}: {{Grid Cells Harbour Three Complementary Positional Codes}}},
  author = {Stemmler, Martin and Herz, Andreas V.M.},
  year = {2017},
  journal = {Current Biology},
  volume = {27},
  number = {15},
  pages = {R755-R758},
  publisher = {{Elsevier Ltd}},
  doi = {10.1016/j.cub.2017.06.067},
  abstract = {The firing fields of mammalian grid cells, which map an animal's environment, lie on hexagonal lattices. Three new studies report significant field-to-field differences in the firing rates, a finding with far-reaching consequences for how grid fields form and encode spatial information.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8FJV9BYS/Stemmler, Herz - 2017 - Spatial Cognition Grid Cells Harbour Three Complementary Positional Codes.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@stemmlerSpatialCognitionGrid2017.md}
}

@article{stensolaEntorhinalGridMap2012,
  title = {The Entorhinal Grid Map Is Discretized},
  author = {Stensola, Hanne and Stensola, Tor and Solstad, Trygve and Fr{\O}land, Kristian and Moser, May Britt and Moser, Edvard I.},
  year = {2012},
  journal = {Nature},
  volume = {492},
  number = {7427},
  pages = {72--78},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687 (Electronic)\textbackslash r0028-0836 (Linking)},
  doi = {10.1038/nature11649},
  abstract = {The medial entorhinal cortex (MEC) is part of the brain's circuit for dynamic representation of self-location. The metric of this representation is provided by grid cells, cells with spatial firing fields that tile environments in a periodic hexagonal pattern. Limited anatomical sampling has obscured whether the grid system operates as a unified system or a conglomerate of independent modules. Here we show with recordings from up to 186 grid cells in individual rats that grid cells cluster into a small number of layer-spanning anatomically overlapping modules with distinct scale, orientation, asymmetry and theta-frequency modulation. These modules can respond independently to changes in the geometry of the environment. The discrete topography of the grid-map, and the apparent autonomy of the modules, differ from the graded topography of maps for continuous variables in several sensory systems, raising the possibility that the modularity of the grid map is a product of local self-organizing network dynamics.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QFC36DPD/Stensola et al. - 2012 - The entorhinal grid map is discretized.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@stensolaEntorhinalGridMap2012.md}
}

@article{sternsonHypothalamicSurvivalCircuits2013,
  title = {Hypothalamic Survival Circuits: {{Blueprints}} for Purposive Behaviors},
  author = {Sternson, Scott M.},
  year = {2013},
  journal = {Neuron},
  volume = {77},
  number = {5},
  pages = {810--824},
  publisher = {{Elsevier Inc.}},
  doi = {10.1016/j.neuron.2013.02.018},
  abstract = {Neural processes that direct an animal's actions toward environmental goals are critical elements for understanding behavior. The hypothalamus is closely associated with motivated behaviors required for survival and reproduction. Intense feeding, drinking, aggressive, and sexual behaviors can be produced by a simple neuronal stimulus applied to discrete hypothalamic regions. What can these "evoked behaviors"teach us about the neural processes that determine behavioral intent and intensity? Small populations of neurons sufficient to evoke a complex motivated behavior may be used as entry points to identify circuits that energize and direct behavior to specific goals. Here, I review recent applications of molecular genetic, optogenetic, and pharmacogenetic approaches that overcome previous limitations for analyzing anatomically complex hypothalamic circuits and their interactions with the rest of the brain. These new tools have the potential to bridge the gaps between neurobiological and psychological thinking about the mechanisms of complex motivated behavior. \textcopyright{} 2013 Elsevier Inc.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TNSHBZDN/Sternson - 2013 - Hypothalamic survival circuits Blueprints for purposive behaviors.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@sternsonHypothalamicSurvivalCircuits2013.md}
}

@article{stevensonInferringFunctionalConnections2008,
  title = {Inferring Functional Connections between Neurons},
  author = {Stevenson, Ian H. and Rebesco, James M. and Miller, Lee E. and K{\"o}rding, Konrad P.},
  year = {2008},
  month = dec,
  journal = {Current Opinion in Neurobiology},
  volume = {18},
  number = {6},
  pages = {582--588},
  publisher = {{Elsevier Current Trends}},
  doi = {10.1016/j.conb.2008.11.005},
  abstract = {A central question in neuroscience is how interactions between neurons give rise to behavior. In many electrophysiological experiments, the activity of a set of neurons is recorded while sensory stimuli or movement tasks are varied. Tools that aim to reveal underlying interactions between neurons from such data can be extremely useful. Traditionally, neuroscientists have studied these interactions using purely descriptive statistics (cross-correlograms or joint peri-stimulus time histograms). However, the interpretation of such data is often difficult, particularly as the number of recorded neurons grows. Recent research suggests that model-based, maximum likelihood methods can improve these analyses. In addition to estimating neural interactions, application of these techniques has improved decoding of external variables, created novel interpretations of existing electrophysiological data, and may provide new insight into how the brain represents information. \textcopyright{} 2008 Elsevier Ltd. All rights reserved.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BJ73BWXG/Stevenson et al. - 2008 - Inferring functional connections between neurons.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@stevensonInferringFunctionalConnections2008.md}
}

@article{stoicaReviewInformationCriterion2004,
  title = {A Review of Information Criterion Rules},
  author = {Stoica, Petre and Sel{\'e}n, Yngve},
  year = {2004},
  journal = {IEEE Signal Processing Magazine},
  volume = {21},
  number = {4},
  pages = {36--47},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/MSP.2004.1311138},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RAR29IAM/01311138(1).pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@stoicaReviewInformationCriterion2004.md}
}

@article{stringerHighdimensionalGeometryPopulation2019,
  title = {High-Dimensional Geometry of Population Responses in Visual Cortex},
  author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D.},
  year = {2019},
  month = jul,
  journal = {Nature},
  volume = {571},
  number = {7765},
  pages = {361--365},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1346-5},
  abstract = {A neuronal population encodes information most efficiently when its stimulus responses are high-dimensional and uncorrelated, and most robustly when they are lower-dimensional and correlated. Here we analysed the dimensionality of the encoding of natural images by large populations of neurons in the visual cortex of awake mice. The evoked population activity was high-dimensional, and correlations obeyed an unexpected power law: the nth principal component variance scaled as 1/n. This scaling was not inherited from the power law spectrum of natural images, because it persisted after stimulus whitening. We proved mathematically that if the variance spectrum was to decay more slowly then the population code could not be smooth, allowing small changes in input to dominate population activity. The theory also predicts larger power-law exponents for lower-dimensional stimulus ensembles, which we validated experimentally. These results suggest that coding smoothness may represent a fundamental constraint that determines correlations in neural population codes.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Neural encoding;Striate cortex Subject\_term\_id: neural-encoding;striate-cortex},
  file = {C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@stringerHighdimensionalGeometryPopulation2019.md;C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Zotero\\storage\\Stringer et al_2019_High-dimensional geometry of population responses in visual cortex.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QM2KFN2B/s41586-019-1346-5.html}
}

@article{stringerSpontaneousBehaviorsDrive2019,
  title = {Spontaneous Behaviors Drive Multidimensional, Brainwide Activity},
  author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Reddy, Charu Bai and Carandini, Matteo and Harris, Kenneth D.},
  year = {2019},
  month = apr,
  journal = {Science},
  volume = {364},
  number = {6437},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aav7893},
  abstract = {Neuron activity across the brain How is it that groups of neurons dispersed through the brain interact to generate complex behaviors? Three papers in this issue present brain-scale studies of neuronal activity and dynamics (see the Perspective by Huk and Hart). Allen et al. found that in thirsty mice, there is widespread neural activity related to stimuli that elicit licking and drinking. Individual neurons encoded task-specific responses, but every brain area contained neurons with different types of response. Optogenetic stimulation of thirst-sensing neurons in one area of the brain reinstated drinking and neuronal activity across the brain that previously signaled thirst. Gr\"undemann et al. investigated the activity of mouse basal amygdala neurons in relation to behavior during different tasks. Two ensembles of neurons showed orthogonal activity during exploratory and nonexploratory behaviors, possibly reflecting different levels of anxiety experienced in these areas. Stringer et al. analyzed spontaneous neuronal firing, finding that neurons in the primary visual cortex encoded both visual information and motor activity related to facial movements. The variability of neuronal responses to visual stimuli in the primary visual area is mainly related to arousal and reflects the encoding of latent behavioral states. Science, this issue p. eaav3932, p. eaav8736, p. eaav7893; see also p. 236 Structured Abstract INTRODUCTIONIn the absence of sensory inputs, the brain produces structured patterns of activity, which can be as large as or larger than sensory-driven activity. Ongoing activity exists even in primary sensory cortices and has been hypothesized to reflect recapitulation of previous sensory experiences, or expectations of possible sensory events. Alternatively, ongoing activity could be related to behavioral and cognitive states. RATIONALEMuch previous work has linked spontaneous neural activity to behavior through one-dimensional measures like running speed and pupil diameter. However, mice perform diverse behaviors consisting of whisking, licking, sniffing, and other facial movements. We hypothesized that there exists a multidimensional representation of behavior in visual cortex and that previously reported ``noise'' during stimulus presentations may in fact be behaviorally driven. To investigate this, we recorded the activity of \textasciitilde 10,000 neurons in visual cortex of awake mice using two-photon calcium imaging, while simultaneously monitoring the facial movements using an infrared camera. In a second set of experiments, we recorded the activity of thousands of neurons across the brain using eight simultaneous Neuropixels probes, again videographically monitoring facial behavior. RESULTSFirst, we found that ongoing activity in visual cortex is high dimensional: More than a hundred latent dimensions could be reliably extracted from the population activity. We found that a third of this activity could be predicted by a multidimensional model of the mouse's behavior, extracted from the face video. This behaviorally related activity was not limited to visual cortex. We observed multidimensional representations of behavior in electrophysiological recordings from frontal, sensorimotor, and retrosplenial cortex; hippocampus; striatum; thalamus; and midbrain. Even though both behavior and neural activity contained fast\textendash time scale fluctuations on the order of 200 ms, they were only related to each other at a time scale of about 1 s. We next investigated how this spontaneous, behavior-related signal interacts with stimulus responses. The representation of sensory stimuli and behavioral variables was mixed in the same neurons: The fractions of each neuron's variance explained by stimuli and by behavior were only slightly negatively correlated, and neurons with similar stimulus responses did not have more similar behavioral correlates. Nevertheless, at a population level, the neural dimensions encoding motor variables overlapped with those encoding visual stimuli along only one dimension, which coherently increased or decreased the activity of the entire population. Activity in all other behaviorally driven dimensions continued unperturbed regardless of sensory stimulation. CONCLUSIONThe brainwide representation of behavioral variables suggests that information encoded nearly anywhere in the forebrain is combined with behavioral state variables into a mixed representation. We found that these multidimensional signals are present both during ongoing activity and during passive viewing of a stimulus. This suggests that previously reported noise during stimulus presentations may consist primarily of behavioral-state information. What benefit could this ubiquitous mixing of sensory and motor information provide? The most appropriate behavior for an animal to perform at any moment depends on the combination of available sensory data, ongoing motor actions, and purely internal variables such as motivational drives. Integration of sensory inputs with motor actions must therefore occur somewhere in the nervous system. Our data indicate that it happens as early as primary sensory cortex. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/364/6437/eaav7893/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Large-scale neural population recordings can be predicted from behavior.We used new recording technologies to simultaneously monitor the activity of \textasciitilde 10,000 neurons in a single brain area and \textasciitilde 3000 neurons from across the brain (top left). These neurons showed slow\textendash time scale patterns of coactivation restricted to subsets of neurons which were distributed across the brain (top right). The patterns of neural activity appeared to be driven by specific spontaneous behaviors that the animals engaged in during the experiment. We tracked these spontaneous behaviors by projecting a video recording of the mouse face onto a set of canonical ``eigenfaces'' (bottom left) and used these projections to predict a large fraction of the neural activity (bottom right). t, time; PC, principal component. Neuronal populations in sensory cortex produce variable responses to sensory stimuli and exhibit intricate spontaneous activity even without external sensory input. Cortical variability and spontaneous activity have been variously proposed to represent random noise, recall of prior experience, or encoding of ongoing behavioral and cognitive variables. Recording more than 10,000 neurons in mouse visual cortex, we observed that spontaneous activity reliably encoded a high-dimensional latent state, which was partially related to the mouse's ongoing behavior and was represented not just in visual cortex but also across the forebrain. Sensory inputs did not interrupt this ongoing signal but added onto it a representation of external stimuli in orthogonal dimensions. Thus, visual cortical population activity, despite its apparently noisy structure, reliably encodes an orthogonal fusion of sensory and multidimensional behavioral information. Neurons in the primary visual cortex encode both visual information and motor activity. Neurons in the primary visual cortex encode both visual information and motor activity.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2019, American Association for the Advancement of Science. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  langid = {english},
  pmid = {31000656},
  file = {C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@stringerSpontaneousBehaviorsDrive2019.md;C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Zotero\\storage\\Stringer et al_2019_Spontaneous behaviors drive multidimensional, brainwide activity.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MWTGMUYF/eaav7893.html}
}

@misc{StructureinformedFunctionalConnectivity,
  title = {Structure-Informed Functional Connectivity Driven by Identifiable and State-Specific Control Regions},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8233121/}
}

@article{subramoneySlowProcessesNeurons,
  title = {Slow Processes of Neurons Enable a Biologically Plausible Approximation to Policy Gradient},
  author = {Subramoney, Anand and Scherr, Franz and Bellec, Guillaume and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  pages = {9},
  abstract = {Recurrent neural networks underlie the astounding information processing capabilities of the brain, and play a key role in many state-of-the-art algorithms in deep reinforcement learning. But it has remained an open question how such networks could learn from rewards in a biologically plausible manner, with synaptic plasticity that is both local and online. We describe such an algorithm that approximates actor-critic policy gradient in recurrent neural networks. Building on an approximation of backpropagation through time (BPTT): e-prop, and using the equivalence between forward and backward view in reinforcement learning (RL), we formulate a novel learning rule for RL that is both online and local, called reward-based e-prop. This learning rule uses neuroscience inspired slow processes and top-down signals, while still being rigorously derived as an approximation to actor-critic policy gradient. To empirically evaluate this algorithm, we consider a delayed reaching task, where an arm is controlled using a recurrent network of spiking neurons. In this task, we show that reward-based e-prop performs as well as an agent trained with actor-critic policy gradient with biologically implausible BPTT.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5I54V3WV/Subramoney et al. - Slow processes of neurons enable a biologically pl.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MM9F2WLQ/slow-processes-of-neurons-enable-a-biologically-plausible-approximation-to-policy-gradient.html}
}

@article{sunBurstingPotentiatesNeuro2021,
  title = {Bursting Potentiates the Neuro\textendash{{AI}} Connection},
  author = {Sun, Weinan and Zhao, Xinyu and Spruston, Nelson},
  year = {2021},
  month = jul,
  journal = {Nature Neuroscience},
  volume = {24},
  number = {7},
  pages = {905--906},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-021-00844-2},
  abstract = {For decades, researchers have wondered whether algorithms used by artificial neural networks might be implemented by biological networks. Payeur et al. have strengthened the connection between neuroscience and artificial intelligence by showing that biologically plausible mechanisms can approximate key features of an essential artificial intelligence learning algorithm.},
  copyright = {2021 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Learning algorithms,Network models,Neural circuits,Synaptic plasticity},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/T7DJU4LH/s41593-021-00844-2.html}
}

@article{sungSimultaneousEmulationSynaptic2022,
  title = {Simultaneous Emulation of Synaptic and Intrinsic Plasticity Using a Memristive Synapse},
  author = {Sung, Sang Hyun and Kim, Tae Jin and Shin, Hyera and Im, Tae Hong and Lee, Keon Jae},
  year = {2022},
  month = may,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {2811},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-30432-2},
  abstract = {Neuromorphic computing targets the hardware embodiment of neural network, and device implementation of individual neuron and synapse has attracted considerable attention. The emulation of synaptic plasticity has shown promising results after the advent of memristors. However, neuronal intrinsic plasticity, which involves in learning process through interactions with synaptic plasticity, has been rarely demonstrated. Synaptic and intrinsic plasticity occur concomitantly in learning process, suggesting the need of the simultaneous implementation. Here, we report a neurosynaptic device that mimics synaptic and intrinsic plasticity concomitantly in a single cell. Threshold switch and phase change memory are merged in threshold switch-phase change memory device. Neuronal intrinsic plasticity is demonstrated based on bottom threshold switch layer, which resembles the modulation of firing frequency in biological neuron. Synaptic plasticity is also introduced through the nonvolatile switching of top phase change layer. Intrinsic and synaptic plasticity are simultaneously emulated in a single cell to establish the positive feedback between them. A positive feedback learning loop which mimics the retraining process in biological system is implemented in threshold switch-phase change memory array for accelerated training.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Electrical and electronic engineering,Electronic devices},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/65HP8IL9/Sung et al_2022_Simultaneous emulation of synaptic and intrinsic plasticity using a memristive.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2AJVMCMM/s41467-022-30432-2.html}
}

@article{sunReviewDesignsApplications2020,
  title = {A {{Review}} of {{Designs}} and {{Applications}} of {{Echo State Networks}}},
  author = {Sun, Chenxi and Song, Moxian and Hong, Shenda and Li, Hongyan},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.02974 [cs]},
  eprint = {2012.02974},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recurrent Neural Networks (RNNs) have demonstrated their outstanding ability in sequence tasks and have achieved state-of-the-art in wide range of applications, such as industrial, medical, economic and linguistic. Echo State Network (ESN) is simple type of RNNs and has emerged in the last decade as an alternative to gradient descent training based RNNs. ESN, with a strong theoretical ground, is practical, conceptually simple, easy to implement. It avoids non-converging and computationally expensive in the gradient descent methods. Since ESN was put forward in 2002, abundant existing works have promoted the progress of ESN, and the recently introduced Deep ESN model opened the way to uniting the merits of deep learning and ESNs. Besides, the combinations of ESNs with other machine learning models have also overperformed baselines in some applications. However, the apparent simplicity of ESNs can sometimes be deceptive and successfully applying ESNs needs some experience. Thus, in this paper, we categorize the ESN-based methods to basic ESNs, DeepESNs and combinations, then analyze them from the perspective of theoretical studies, network designs and specific applications. Finally, we discuss the challenges and opportunities of ESNs by summarizing the open questions and proposing possible future works.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {\section{Annotations\\
(6/13/2022, 7:04:40 PM)}

\par
``Echo State Network (ESN) is simple type of RNNs and has emerged in the last decade as an alternative to gradient descent training based RNNs'' (Sun et al., 2020, p. 1)
\par
``avoids non-converging and computationally expensive in the gradient descent methods'' (Sun et al., 2020, p. 1)
\par
``n 2002, abundant existing works have promoted the progress of ESN, and the recently introduced Deep ESN model opened the way to uniting the merits of deep learning and ESNs. Besides, the combinations of ESNs with other machine learning models have also overperformed baselines in some applications. However, the apparent simplicity of ESNs can sometimes be deceptive and successfully applying ESNs needs some experienc'' (Sun et al., 2020, p. 1)
\par
``RNNs represent'' (Sun et al., 2020, p. 1)
\par
``most closely resembling biological brains, the substrate of natural intelligence'' (Sun et al., 2020, p. 1)
\par
``deep RNN are able to develop in their internal states a multiple time-scales representation of the temporal information'' (Sun et al., 2020, p. 1)
\par
``bifurcations'' (Sun et al., 2020, p. 1) What does this mean?
\par
``Long short-memory (LSTM) and gated recurrent unit (GRU) are advanced designs to mitigate shortcomings of RNN. But when the length of input sequence exceeds a certain limit, the gradient will still disappear. Meanwhile, each LSTM cell have four full connection layers, if the time span of LSTM is large and the network is very deep, the calculation will be very heavy and time-consuming. Further, too many parameters will lead to over fitting risk.'' (Sun et al., 2020, p. 1)
\par
(Sun et al., 2020, p. 2)
\par
``Echo State Network (ESN) [6] is one of the key RC. ESNs are practical, conceptually simple, and easy to implement. ESNs employ the multiple high-dimensional projection in the large number of states of the reservoir with strong nonlinear mapping capabilities, to capture the dynamics of the input. This basic idea was first clearly spelled out in a neuroscientific model of the corticostriatal processing loop [3]. ESNs enjoy, under mild conditions, the so-called echo state property [6], that ensures that the effect of the initial condition vanishes after a finite transient. The inputs with more similar short-term history will evoke closer echo states, which ensure the dynamical stability of the reservoir.'' (Sun et al., 2020, p. 2) \#ESN\\
Lookup more on the Coticostriatal processing loop
\par
``ensure the dynamical stability of the reservoir. Recently, the introduction of the Deep Echo State Network (DeepESN) model [11, 12] allowed to study the properties of layered RNN architectures separately from the learning aspects. Remarkably, such studies pointed out that the structured state space organization with multiple time-scales dynamics in deep RNNs is intrinsic to the nature of compositionality of recurrent neural modules. The interest in the study of the DeepESN model is hence twofold. On the one hand, it allows to shed light on the intrinsic properties of state dynamics of layered RNN architectures. On the other hand it enables the design of extremely efficiently trained deep neural networks for temporal data.'' (Sun et al., 2020, p. 2)
\par
``Meanwhile, with the development of deep learning, a variety of neural network structures have been proposed, like auto-encoder (AE) [13], generative adversarial networks (GAN) [14], convolutional neural networks (CNNs) [15] and restricted Boltzmann machine (RBM) [16]. ESNs have combined these different structures and achieved state-of-the-art performance in specific tasks and different applications, such as industrial [17], medical [18], financial [19] and robotics with reinforcement learning [20, 21].'' (Sun et al., 2020, p. 2) How have ESNs combined these exactly? I assume it's supposing through its complex and undefined architecture?
\par
(Sun et al., 2020, p. 2)
\par
``Only parameters of readout weights Wout are subject to training.'' (Sun et al., 2020, p. 3)
\par
``Which can obtain a closed-form solution by extremely fast algorithms such as ridge regression.'' (Sun et al., 2020, p. 3)
\par
``After training phase, ESNs can give predictions \textasciicircum{} Y of new data \textasciicircum '' (Sun et al., 2020, p. 3) Is the data in the same "genre"? Not sure what I meant by this.
\par
``A network with state transition equation F with has the echo state property if for each input sequence U = [u(1), u(2), ...u(N )] and all couples of initial states x, x{${'}$}, it could hold the condition in Equation 7. ||F (U, x) - F (U, x{${'}$})|| \textrightarrow{} 0 as N \textrightarrow{} {$\infty$}'' (Sun et al., 2020, p. 3) What exactly does this mean?
\par
``L2-norm'' (Sun et al., 2020, p. 3) As in the regularization?
\par
``||Wres||D = {$\sigma$}(DWresD-1)'' (Sun et al., 2020, p. 4) wtf is this
\par
``By the effect analysis and experiments in slow dynamic systems, noisy time series and time-warped dynamic patterns, leaky-ESN outperformed than ESN and could incorporate a time constant to act as a low-pass filter and the dynamics can be slowed down'' (Sun et al., 2020, p. 4) How would this be implemented in an SNN?

Also what does the "time constant low pass filter " mean?
\par
(Sun et al., 2020, p. 5) Fig 2
\par
``non-periodic dynamical systems'' (Sun et al., 2020, p. 5)
\par
``principle neuron reinforcement (PNR) algorithm'' (Sun et al., 2020, p. 5)
\par
``[2] Doya K. Bifurcations in the learning of recurrent neural networks. Proceedings of IEEE International Symposium on Circuits and Systems, 6:pp. 2777\textendash 2780, 1992.'' (Sun et al., 2020, p. 22)
\par
``[3] Sotirios P. Chatzis and Yiannis Demiris. The copula echo state network. Pattern Recognit., 45(1):570\textendash 577, 2012.'' (Sun et al., 2020, p. 22) corticostriatal
\par
``[6] Herbert Jaeger. Adaptive nonlinear system identification with echo state networks. In Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British Columbia, Canada], pages 593\textendash 600, 2002.'' (Sun et al., 2020, p. 22)
\par
``[12] Claudio Gallicchio, Alessio Micheli, and Luca Pedrelli. Deep reservoir computing: A critical experimental analysis. Neurocomputing, 268(dec.11):87\textendash 99, 2017. [13] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008, 2008.'' (Sun et al., 2020, p. 22) Deep ESN related
\par
``[20] Leandro Maciel, Fernando A. C. Gomide, David Santos, and Rosangela Ballini. Exchange rate forecasting using echo state networks for trading strategies. In IEEE Conference on Computational Intelligence for Financial Engineering \& Economics, CIFEr 2014, London, UK, March 27-28, 2014, pages 40\textendash 47, 2014. [21] Julian Whitman, Raunaq M. Bhirangi, Matthew J. Travers, and Howie Choset. Modular robot design synthesis with deep reinforcement learning. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 10418\textendash 10425, 2020.'' (Sun et al., 2020, p. 23) Reinforcement learning related
\par
``[29] Norbert Michael Mayer and Matthew Browne. Echo state networks and self-prediction. In Biologically Inspired Approaches to Advanced Information Technology, First International Workshop, BioADIT 2004, Lausanne, Switzerland, January 29-30, 2004. Revised Selected Papers, pages 40\textendash 48, 2004.'' (Sun et al., 2020, p. 23)
\par
Comment: 37 pages, 5 figures, 2 tables},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Q893MCFP/Sun et al_2020_A Review of Designs and Applications of Echo State Networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EN3GF9DZ/2012.html}
}

@article{sussmanSaturatedReconstructionVolume2015,
  title = {Saturated {{Reconstruction}} of a {{Volume}} of {{Neocortex}}},
  author = {Sussman, Daniel~Lewis and Kaynig, Verena and Lee, Dongil and Hayworth, Kenneth~Jeffrey and Roncal, William~Gray and Priebe, Carey~Eldin and Pfister, Hanspeter and Tapia, Juan~Carlos and Kasthuri, Narayanan and Jones, Thouis~Raymond and Conchello, Jos{\'e}~Angel and Seung, H.~Sebastian and Morgan, Josh~Lyskowski and {V{\'a}zquez-Reina}, Amelio and Burns, Randal and Vogelstein, Joshua~Tzvi and Roberts, Mike and Lichtman, Jeff~William and {Knowles-Barley}, Seymour and Schalek, Richard~Lee and Berger, Daniel~Raimund},
  year = {2015},
  journal = {Cell},
  volume = {162},
  number = {3},
  pages = {648--661},
  doi = {10.1016/j.cell.2015.06.054},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/79LCQFDQ/Sussman et al. - 2015 - Saturated Reconstruction of a Volume of Neocortex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@sussmanSaturatedReconstructionVolume2015.md}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BG5UMA5Z/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/W7I9K9XY/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@suttonReinforcementLearningIntroduction2018.md}
}

@article{swearingenPatternRespondingPeakInterval2010,
  title = {The {{Pattern}} of {{Responding}} in the {{Peak-Interval Procedure}} with {{Gaps}}: {{An Individual-Trials Analysis}}},
  shorttitle = {The {{Pattern}} of {{Responding}} in the {{Peak-Interval Procedure}} with {{Gaps}}},
  author = {Swearingen, Joshua E. and Buhusi, Catalin V.},
  year = {2010},
  month = oct,
  journal = {Journal of experimental psychology. Animal behavior processes},
  volume = {36},
  number = {4},
  pages = {443--455},
  issn = {0097-7403},
  doi = {10.1037/a0019485},
  abstract = {Humans and lower animals time as if using a stopwatch that can be ``stopped'' or ``reset'' on command. This view is challenged by data from the peak-interval procedure with gaps: Unexpected retention intervals (gaps) delay the response function in a seemingly continuous fashion, from stop to reset. We evaluated whether these results are an artifact of averaging over trials, or whether subjects use discrete alternatives or a continuum of alternatives in individual-trials: A Probability-of-Reset hypothesis proposes that in individual gap trials subjects stochastically use discrete alternatives (stop/reset), such that when averaged over trials, the response distribution in gap trials falls in between ``stop'' and ``reset''. Alternatively, a Resource Allocation hypothesis proposes that during individual gap trials working memory for the pregap duration decays, such that the response function in individual gap trials is shifted rightward in a continuous fashion. Both hypotheses provided very good fits with the observed individual-trial distributions, although the Resource Allocation hypothesis generated reliably better fits. Results provide support for the usefulness of individual-trial analyses in dissociating theoretical alternatives in interval timing tasks.},
  pmcid = {PMC2964407},
  pmid = {20718550},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TAB47H3H/Swearingen_Buhusi_2010_The Pattern of Responding in the Peak-Interval Procedure with Gaps.pdf}
}

@article{taherkhaniReviewLearningBiologically2020,
  title = {A Review of Learning in Biologically Plausible Spiking Neural Networks},
  author = {Taherkhani, Aboozar and Belatreche, Ammar and Li, Yuhua and Cosma, Georgina and Maguire, Liam P. and McGinnity, T. M.},
  year = {2020},
  month = feb,
  journal = {Neural Networks},
  volume = {122},
  pages = {253--272},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.09.036},
  abstract = {Artificial neural networks have been used as a powerful processing tool in various areas such as pattern recognition, control, robotics, and bioinformatics. Their wide applicability has encouraged researchers to improve artificial neural networks by investigating the biological brain. Neurological research has significantly progressed in recent years and continues to reveal new characteristics of biological neurons. New technologies can now capture temporal changes in the internal activity of the brain in more detail and help clarify the relationship between brain activity and the perception of a given stimulus. This new knowledge has led to a new type of artificial neural network, the Spiking Neural Network (SNN), that draws more faithfully on biological properties to provide higher processing abilities. A review of recent developments in learning of spiking neurons is presented in this paper. First the biological background of SNN learning algorithms is reviewed. The important elements of a learning algorithm such as the neuron model, synaptic plasticity, information encoding and SNN topologies are then presented. Then, a critical review of the state-of-the-art learning algorithms for SNNs using single and multiple spikes is presented. Additionally, deep spiking neural networks are reviewed, and challenges and opportunities in the SNN field are discussed.},
  langid = {english},
  keywords = {Learning,Spiking neural network (SNN),Synaptic plasticity},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HLYDU73Q/Taherkhani et al_2020_A review of learning in biologically plausible spiking neural networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QZ8NEV4T/A review of learning in biologically plausible spiking neural networks - 11.05.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3UU86BUC/S0893608019303181.html}
}

@article{taiEquivariantTransformerNetworks2019,
  title = {Equivariant {{Transformer Networks}}},
  author = {Tai, Kai Sheng and Bailis, Peter and Valiant, Gregory},
  year = {2019},
  month = jan,
  abstract = {How can prior knowledge on the transformation invariances of a domain be incorporated into the architecture of a neural network? We propose Equivariant Transformers (ETs), a family of differentiable image-to-image mappings that improve the robustness of models towards pre-defined continuous transformation groups. Through the use of specially-derived canonical coordinate systems, ETs incorporate functions that are equivariant by construction with respect to these transformations. We show empirically that ETs can be flexibly composed to improve model robustness towards more complicated transformation groups in several parameters. On a real-world image classification task, ETs improve the sample efficiency of ResNet classifiers, achieving relative improvements in error rate of up to 15\% in the limited data regime while increasing model parameter count by less than 1\%.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/E5H6JIJJ/Tai, Bailis, Valiant - 2019 - Equivariant Transformer Networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@taiEquivariantTransformerNetworks2019.md}
}

@article{takaharaGeneralSystemsTheory1976,
  title = {General Systems Theory: {{Mathematical}} Foundations},
  author = {Takahara, Yasuhiko},
  year = {1976},
  journal = {Advances in Mathematics},
  volume = {20},
  number = {2},
  pages = {285--285},
  doi = {10.1016/0001-8708(76)90196-1},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7WBVXJIB/Takahara - 1976 - General systems theory Mathematical foundations.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@takaharaGeneralSystemsTheory1976.md}
}

@article{takahashiInformationTheoreticInterpretation2010,
  title = {Information Theoretic Interpretation of Frequency Domain Connectivity Measures},
  author = {Takahashi, Daniel Y. and Baccal{\'a}, Luiz A. and Sameshima, Koichi},
  year = {2010},
  month = dec,
  journal = {Biological Cybernetics},
  volume = {103},
  number = {6},
  pages = {463--469},
  issn = {1432-0770},
  doi = {10.1007/s00422-010-0410-x},
  abstract = {In order to provide adequate multivariate measures of information flow between neural structures, modified expressions of partial directed coherence (PDC) and directed transfer function (DTF), two popular multivariate connectivity measures employed in neuroscience, are introduced and their formal relationship to mutual information rates are proved.},
  langid = {english},
  pmid = {21153835},
  keywords = {Information Theory,Multivariate Analysis},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/QRL7ZYF8/Takahashi et al. - 2010 - Information theoretic interpretation of frequency .pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@takahashiInformationTheoreticInterpretation2010.md}
}

@article{tallotNeuralEncodingTime2020,
  title = {Neural Encoding of Time in the Animal Brain},
  author = {Tallot, L. and Doy{\`e}re, V.},
  year = {2020},
  journal = {Neuroscience \& Biobehavioral Reviews},
  doi = {10.1016/j.neubiorev.2019.12.033},
  abstract = {The processing of temporal intervals is essential to create causal maps and to predict future events so as to best adapt one's behavior. In this review, we explore the different brain activity patterns associated with processing durations and expressing temporally-adapted behavior in animals. We begin by describing succinctly some of the current models of the internal clock that can orient us in what to look for in brain activity. We then outline how durations can be decoded by single cell activity and which activity patterns could be associated with interval timing. We further point to similar patterns that have been observed at a more global level within brain areas (e.g. local field potentials) or, even, between these areas, that could represent another way of encoding duration or could constitute a necessary part for more complex temporal processing. Finally, we discuss to what extent neural data fit with internal clock models, and highlight improvements for experiments to obtain a more indepth understanding of the brain's temporal encoding and processing.},
  keywords = {temporal intervals},
  note = {\section{Annotations\\
(6/13/2022, 2:02:53 PM)}

\par
``see Buhusi and Meck, 2005'' (Tallot and Doy\`ere, 2020, p. 146)
\par
``interval timing - in the seconds to minutes range - is flexible, learned,'' (Tallot and Doy\`ere, 2020, p. 146)
\par
``and covers a larger range of durations to allow for a rapid adaptation to changes in the environment. I'' (Tallot and Doy\`ere, 2020, p. 146)
\par
``In contrast to the well-described dependence of circadian rhythm on the suprachiasmatic nucleus, many brain structures have been linked to interval timing'' (Tallot and Doy\`ere, 2020, p. 146)
\par
``many brain structures have been linked to interval timin'' (Tallot and Doy\`ere, 2020, p. 146)
\par
``timescale (in the range of a few hundred milliseconds)'' (Tallot and Doy\`ere, 2020, p. 146) So we have 3 different timescales.MeaningI was Probably using the word wrong
\par
``interval timing, which enables organisms to create temporal maps and manage predictions about the outcome of situations, and has therefore a strong cognitive component (Buhusi and Meck, 2005).'' (Tallot and Doy\`ere, 2020, p. 146)
\par
``Weber's law'' (Tallot and Doy\`ere, 2020, p. 147) Gotta look this up.
\par
``scalar expectancy theory the most influential timing model up to now - was developed by Gibbon (1977), and further improved by Church (1984). It expands on the memory stage of Treisman's internal clock by incorporating: a working memory component, a multiplicative factor for storage in a reference memory, and a decision rule to determine if `yes' or `no' the duration being measured is similar to previously encoded durations'' (Tallot and Doy\`ere, 2020, p. 147)
\par
``the importance of several brain areas in interval timing, results that have been discussed many times before, and will not be part of this review. Briefly, a few structures have been detected as playing a role in timing across a lot of studies: the supplementary motor area (SMA), the pre-SMA, the prefrontal cortex (PFC), the striatum, the substantia nigra, the inferior parietal cortex and the cerebellum (e.g. Brannon et al., 2008; Buhusi and Meck, 2005; Coull et al., 2011; Harrington et al., 2010, 2004; Lewis and Miall, 2006; Wiener et al., 2010).'' (Tallot and Doy\`ere, 2020, p. 147)
\par
``One of the core debates on the neurological basis of timing is whether it is dependent on one central timing center, or whether timing is present all over the brain in separate clusters.'' (Tallot and Doy\`ere, 2020, p. 147)
\par
``timing'' (Tallot and Doy\`ere, 2020, p. 147)
\par
``However there are also studies showing that brain slices can encode durations (up to 2 s) which implies that a very restricted amount of connected neurons is sufficient for simple temporal encoding (Chubykin et al., 2013; Goel and Buonomano, 2016; Johnson et al., 2010).'' (Tallot and Doy\`ere, 2020, p. 147)
\par
``To address the issue of the origin of the internal clock, assuming there is one, we must look at neuronal activity from individual neurons to groups of neurons in a large range of brain areas and in different types of timing tasks.'' (Tallot and Doy\`ere, 2020, p. 147)
\par
``Patterns of single cell firing activity and synchronous spike activity of neural ensembles could reflect a local processing of time. This synchronous cell activity can generate depolarization/hyperpolarization oscillatory rhythms either locally (through recurrent networks) or in distant brain areas, which can be recorded with local field potentials (LFP).'' (Tallot and Doy\`ere, 2020, p. 147) there we go. Neural oscillations\\
\& timing
\par
``Neural oscillations also give access to subthreshold depolarization'' (Tallot and Doy\`ere, 2020, p. 147) What does this mean?
\par
``which may not be translated into spikes by the integrating neuron'' (Tallot and Doy\`ere, 2020, p. 147) Open question?
\par
``As such, recordings of spike activity and oscillations provide complementary, non-fully-overlapping, information'' (Tallot and Doy\`ere, 2020, p. 147)
\par
``Neural oscillations are an ubiquitous property of brain function and have important roles in learning, memory and cognitive processes such as those involved in timing and time perception (for reviews, Buzs\'aki et al., 2013; Buzs\'aki and Draguhn, 2004; Hanslmayr and Staudigl, 2014; Matell and Meck, 2004). Slow ({$<$} 50 Hz) oscillations are associated with large fluctuations of neurons' membrane potential and are considered to cover large brain areas, whereas fast oscillations result from smaller fluctuations in membrane potential and should be restricted to smaller neural volumes. Changes in oscillatory rhythms' frequency or power may represent timing function at a network level.'' (Tallot and Doy\`ere, 2020, p. 147) Neural Oscillations A time. Several examples given.
\par
``studies have been interested in determining how single neuron activity (i.e. spikes) can encode durations; and it is a growing field of research, as about 30\% of these studies have been published within the last five years. Indeed'' (Tallot and Doy\`ere, 2020, p. 150) fee me like an interesting factoid, at least something to be mentioned\\
for further .
\par
``how can an event of less than ten milliseconds encode durations of several seconds to minutes?'' (Tallot and Doy\`ere, 2020, p. 150)
\par
``We are compiling here more than 80 studies that have in some way looked at different patterns of single neuron firing that can represent time in explicit (Table 1) or implicit (Table 2) temporal tasks. We have organized these studies according to the type of task used, as well as the brain area where timingrelated activity was reported. The studied species, as well as the range of durations used, are also mentioned. We have categorized the modification of neurons' firing patterns, as compared to baseline activity, in four types (see Fig. 1 for a schematic representation of the different activity patterns): (1) sustained change, (2) phasic change in activity at the stimulus onset or offset, whose amplitude and/or duration is proportional to the duration of the event, (3) peak modulation at a specific time point (`event time' cells), and (4) ramping activity. The changes reported are in majority in the direction of an increased cell firing, but several studies have also reported a decrease in cell firing, in particular when baseline levels of activity are high (e.g. Fuster and Alexander, 1973; Oshio et al., 2006).'' (Tallot and Doy\`ere, 2020, p. 150) Main statement of the paper.\\
Be sure to clip the two tables\\
mentioned.
\par
``Sustained change in activity is often described in working memory tasks, and may represent the temporary maintenance in memory of a stimulus until a response has to be produced'' (Tallot and Doy\`ere, 2020, p. 150)
\par
``Sustained increase or decrease in the number of spikes for the whole duration of a stimulus has been observed in 21 studies, in both implicit and explicit tasks. This'' (Tallot and Doy\`ere, 2020, p. 150)
\par
``delay conditioning'' (Tallot and Doy\`ere, 2020, p. 150)
\par
``trace conditioning'' (Tallot and Doy\`ere, 2020, p. 150)
\par
``They used a task somewhat related to a DRL-LH (differential reinforcement of low rates with limited hold) task, in which a brief (100 ms) visual'' (Tallot and Doy\`ere, 2020, p. 150)
\par
``f a reward for a fixed amount of time (1.5 s), but with increasing rewa'' (Tallot and Doy\`ere, 2020, p. 151)
\par
``Neuronal activity at either the onset or the offset of a stimulus may encode its duration through changes in the firing rate at its onset for representing its expected duration or at its offset for representing its passed duration. Such type of neural encoding has been observed for a large range of durations (from 1 to tens of seconds), and often when several durations are presented within the task, suggesting that it may have a role in differentiating durations (Chiba et al., 2015, 2008; Fiorillo et al., 2008; Jaramillo and Zador, 2011; Ohmae et al., 2008; Roux et al., 2003; Sakurai et al., 2004; Yumoto et al., 2011).'' (Tallot and Doy\`ere, 2020, p. 151)
\par
``Event time' cells'' (Tallot and Doy\`ere, 2020, p. 151) Doesn't really pertain to what\\
I 'm looking for. Single cell thingy.
\par
``Event time' cells'' (Tallot and Doy\`ere, 2020, p. 151)
\par
``Event time' activity relates to a transient increase or decrease of the firing rate of a neuron at the end of a learned interval, usually reinforcement time or when the animal must respond.'' (Tallot and Doy\`ere, 2020, p. 151)
\par
``Striatal Beat Frequency (SBF) model (Matell and Meck, 2000)'' (Tallot and Doy\`ere, 2020, p. 151) /Sounds cool. Probably not\\
super important
\par
``Ramping activity'' (Tallot and Doy\`ere, 2020, p. 151)
\par
``Ramping activity, when a neuron's firing rate increases or decreases gradually with passing time, either from baseline level or after an initial abrupt change in activity'' (Tallot and Doy\`ere, 2020, p. 151)
\par
``may represent the gradual increase in expectation of the animal. A neuron discharges more and more (or less and less) as time passes until it reaches a threshold which induces a specific response that is time appropriate'' (Tallot and Doy\`ere, 2020, p. 152)
\par
``it is difficult to distinguish between ramping and `event time' cells, because we cannot see the post-expected reinforcement activity.'' (Tallot and Doy\`ere, 2020, p. 152)
\par
``Ramping activity has often been described in delayed matching-tosample or non-matching-to-sample (i.e., working memory) tasks, and in expectation tasks, where the animal waits for a stimulus. These tasks are not typical timing tasks, but have a temporal component that can be modulated, i.e. the wait between the first and the second stimulus. The increased activity during the delay could represent an encoding of the hazard rate, that is, the longer the duration, the more likely the stimulus is to appear (Heinen and Liu, 1997; Janssen and Shadlen, 2005; Leon and Shadlen, 2003; Lucchetti and Bon, 2001; Renoult et al., 2006; Riehle et al., 1997). It'' (Tallot and Doy\`ere, 2020, p. 152)
\par
``Like any unbounded accumulator, however, it seems biologically impossible to encode long durations of more than a minute with ramping activity. There is a limit to the number of spikes a single cell can produce in a definite amount of time. This is where population coding might come into play by having different populations activating each other to represent longer durations than a single cell can'' (Tallot and Doy\`ere, 2020, p. 152)
\par
``single cells can encode durations but they are limited biologically in how high their firing rate can be, and other mechanisms have to be at play to make them fire at a specific time (outside of the presence of external stimuli)'' (Tallot and Doy\`ere, 2020, p. 152) Yeah. Dnb
\par
``Sequential time cells Sequential time cells (also known as `time cells') fire one after the other, creating, as a population, a range of firing across time which forms a bridge of activity between events separated by a constant time interval, thus encoding as a whole the event's duration or the interval between events (Fig. 2A).'' (Tallot and Doy\`ere, 2020, p. 154)
\par
``response field'' (Tallot and Doy\`ere, 2020, p. 154) Response field? what is this?
\par
``et al., 2009; Mello et al., 2015). MacDonald et al. (2011) have differentiated cells depending on whether or not they modify their peak firing time when the duration of the interval is changed. They named `absolute time cells' cells that show a peak response at a specific time point during the interval with a pattern that does not rescale when the duration is modified. `Relative time cells' show a similar peak response at a specific time but their activity is rescaled depending on the duration of the timed interval. Other cells may either lose their activity or change their activity to a non-similar and non-rescaled time point when the interval is modified. Other studies have also highlighted the dichotomy between relative versus absolute time cells (Kojima and Goldman-Rakic, 1982; Merchant et al., 2011). To describe these sequential time cells, MacDonald and collaborators (2011) used a go/no-go paradigm with a delay. The rats were trained to pair objects and odors, such that they had to retain in memory for 10 s the object that was presented at the beginning of a trial to know whether or not they should dig into a scented pot to get a reward. During the 10 s delay, neurons in the hippocampus fired sequentially to cover the whole duration with a firing pattern that was rearranged when the duration was changed. Most neurons were modulated by both space and time, and in a manner independent of locomotion, speed, or head placement. In another study, it was shown that very few neurons depend only on time (MacDonald et al., 2013, 2011). Therefore, these sequential time cells seem similar, or even identical, to the place cells described in the hippocampus (O'Keefe and Dostrovsky, 1971) and may interact with those cells to form spatiotemporal maps of the environment. More recently, Mello et al. (2015) have reported that 68\% of cells recorded in the dorsal striatum of rats Sequential Time cells Duration Coherence average over many trials A B C D Evoked Related Potential (ERP) Local Field Potential (LFP) Fig. 2. Schematic representation of possible encoding of time though modulation of population neural activity either through organization of single cells activity (A), or through local field potentials modulations (B-D). L. Tallot and V. Doy\`ere Neuroscience and Biobehavioral Reviews 115 (2020) 146\textendash 163 154'' (Tallot and Doy\`ere, 2020, p. 154)
\par
``They named `absolute time cells' cells that show a peak response at a specific time point during the interval with a pattern that does not rescale when the duration is modified'' (Tallot and Doy\`ere, 2020, p. 154) Need more clarification
\par
```Relative time cells' show a similar peak response at a specific time but their activity is rescaled depending on the duration of the timed interval. Other cells may either lose their activity or change their activity to a non-similar and non-rescaled time point when the interval is modified.'' (Tallot and Doy\`ere, 2020, p. 154)
\par
``These sequential time cells seem to constitute a ``pure'' time encoding which can support the encoding of long durations and, even, parallel encoding of multiple durations simultaneously. However, the questions remain of what makes these cells fire at a specific time (e.g., does it result from a local process or do they receive a temporal input from an upstream brain area), and of how to separate the different subpopulations representing different durations in different contexts.'' (Tallot and Doy\`ere, 2020, p. 155) multiple time scale eroding,\\
Also, ask Gustavo about this
\par
``Local field potentials (LFPs) LFPs represent the sum of depolarization/hyperpolarization of a population of neurons, reflecting action potentials as well as subthreshold electrical modulation such as EPSPs (excitatory post-synaptic potentials) and IPSPs (inhibitory post-synaptic potentials) (Buzs\'aki et al., 2012). It is important to note that LFPs represent both input activity (i.e. coming from upstream brain areas) as well as local computations and output, in contrast to single unit or multi-unit recordings which only reflect spikes, and thus output data of the recorded area.'' (Tallot and Doy\`ere, 2020, p. 155)
\par
``Some LFP modulations are time-locked to the onset of a stimulus and are called event related potentials or ERP (Fig. 2B). They can be observed when averaging a large number of trials under constant conditions'' (Tallot and Doy\`ere, 2020, p. 155)
\par
``The raw LFP signal (Fig. 2C) recorded from a brain area can also be decomposed in different frequency components (i.e., oscillations) that are considered to have different roles in neural processing. Data on oscillations are often presented in the form of power spectrum density (PSD), which represents the strength of different frequency bands in a signal. When looked at in a time-frequency manner, one can ask how the power of different frequency bands varies across a trial. Most frequency bands from slow oscillations (like delta and theta) to mid-range (like alpha and beta) and even high frequency oscillations (like gamma and epsilon) - have been described in several mammalian species, and neural oscillations seem to be a conserved phenomenon across mammalian evolution (Buzs\'aki et al., 2013).'' (Tallot and Doy\`ere, 2020, p. 155)
\par
``Neuronal oscillations have long been hypothesized as the major constituent of the internal clock (Buhusi and Meck, 2005; Treisman, 1963). Oscillations also seem to be involved in structuring events in time (for example events can be associated with the specific phase of an oscillation) (K\"osem et al., 2014; Mizuseki et al., 2009).'' (Tallot and Doy\`ere, 2020, p. 155)
\par
``Matell and Meck (2004) have proposed, within the frame of SBF model, that the ERP signal could be representative of a reset of cortical neurons at the beginning of a stimulus, which seems coherent with the observation of a time-dependent ERP in the frontal cortex for all tested durations.'' (Tallot and Doy\`ere, 2020, p. 156)
\par
``Implicit tasks'' (Tallot and Doy\`ere, 2020, p. 156) Implicit v. Explicit tasks
\par
``While changes in oscillatory power points to the involvement of a given brain area in timing - although the function of specific frequencies remains to be resolved - it does not give access to the recruitment of potential networks encompassing several structures. Synchronicity of oscillations between different structures, also called coherence (Fig. 2D), gives information about the communication between structures. When two structures are highly coherent, it is thought to make information transfer easier because the other structure is already primed to receive the spiking activity from the first (cf. the communication through coherence hypothesis, Fries, 2005).'' (Tallot and Doy\`ere, 2020, p. 156)
\par
``4. Open questions 4.1. Neural syntax of timing from single cells to networks How the animal brain encodes time remains a challenging question. As we just reviewed above, a large range of activity patterns (ramping, sustained and phasic) could be considered to be associated with passing time and this suggests that it may be their combined activities that underlie timing.'' (Tallot and Doy\`ere, 2020, p. 157)
\par
``equential time cells may share the role of single unit's sustained activity but for longer durations that cannot be coded by a single cell. Sequential time cells need to be part of sets (i.e., functional populations) with each set involved in encoding a specific duration, allowing for multiple durations to be encoded and decoded at the same time.'' (Tallot and Doy\`ere, 2020, p. 157)
\par
``Oscillations may provide a good way to produce those sets either through the phase locking of spikes to different phases of a frequency band (i.e., whether spikes are repeatedly more present at specific phases of the oscillations), or through phase amplitude coupling (PAC) of high frequency oscillations' power with low frequency oscillations' phase.'' (Tallot and Doy\`ere, 2020, p. 157)
\par
``To our knowledge very few studies studies have looked at this aspect of neuronal encoding in a timing task'' (Tallot and Doy\`ere, 2020, p. 157)
\par
``Phase-locking of spikes to delta oscillations (centered at 1.6 Hz) in the ACC have been reported to correlate with the growing expectation of a reward in a sustained attention task (Totah et al., 2013a). In that study, the phase-locking was increased significantly more in correct than incorrect trials within the 2 s period before the presentation of the stimulus indicating which hole to choose to get a reward (2 s before). A similar pattern was observed in the PL, but with phase-locking of spikes to beta oscillations (centered at 17 Hz). However, Nakazono et al. (2015), looking at single units and theta oscillations in the hippocampus in a temporal discrimination task, showed that only few cells were time-locked to the oscillations compared to the study by MacDonald et al. (2013). The authors suggested that the two types of activity may have different roles, albeit both related to time: theta oscillations might help the hippocampus to interact with the PFC at the correct time, whereas the spikes may encode both temporal and spatial/sensory information.'' (Tallot and Doy\`ere, 2020, p. 157)
\par
``4.2. Neural correlates of time associated with models A number of models of interval timing have been proposed, but neural proofs can be contradictory and do not, for now, allow for choosing one specific timing model'' (Tallot and Doy\`ere, 2020, p. 157)
\par
``The main group of internal clock models are pacemaker accumulator models (PA). Taking into account the intrinsic functioning of neurons and known network connectivity, Matell and Meck (2000) proposed the Striatal Beat Frequency model (SBF). It involves numerous cortical oscillators (representing the pacemaker and accumulator), and the detection of their coincident activation by the medium spiny neurons of the striatum on which they project.'' (Tallot and Doy\`ere, 2020, p. 157)
\par
``pacemaker accumulator models (PA)'' (Tallot and Doy\`ere, 2020, p. 157)
\par
``At the start of a stimulus, the oscillators are synchronized (maybe by a dopamine burst, Kononowicz, 2015) while keeping their own distinct frequencies, meaning that they will not remain synchronized over time. When a significant event happens (e.g., at the end of the stimulus or at the appearance of a reinforcement), the state of these different oscillators is encoded and stored by the striatum.'' (Tallot and Doy\`ere, 2020, p. 157) \ding{164}kfuck,thisisgood.\\
Butwhatis"StoredbySTR''mean?
\par
``By comparing the oscillators' ongoing activity to the memorized patterns, it is possible for the striatum to determine how much time has passed, provided that the oscillatory activity remains very stable across time'' (Tallot and Doy\`ere, 2020, p. 157)
\par
``Recently, replacing the memory and decision stage of the SBF by the ones from a well characterized cognitive architecture (the adaptive control of thought-rational, ACT-R), which has defined default parameters, has improved modeling of more complex'' (Tallot and Doy\`ere, 2020, p. 157)
\par
``temporal behaviors (such as timing of multiple overlapping intervals) (van Rijn et al., 2014). These systems from ACT-R may involve other brain areas (e.g., the hippocampus) for memory and decision-making processes.'' (Tallot and Doy\`ere, 2020, p. 158) +
\par
``Oscillations in the frontal cortex seem to be important for timing (e.g., Parker et al., 2014), and duration-related increased neuronal oscillations have been reported in the dorsomedial striatum (Dall\'erac et al., 2017) that could reflect a read-out of convergent oscillations from cortical neurons. However, no study has yet shown increased coherence between striatum and prefrontal cortex during interval timing. Potentially adding on to the SBF model, data from our lab seems also to implicate the amygdala as a modulator of cortico-striatal synapses to maintain the memory of a previous duration when learning a new duration. So both memories are maintained even though behavioral expression shifts quickly to the new duration (see Dall\'erac et al., 2017, Figure 10). However, some of the expected signs of the SBF model, such as phase reset at the beginning of the interval, have not been described in neurophysiological studies (Kononowicz and van Wassenhove, 2016).'' (Tallot and Doy\`ere, 2020, p. 158)
\par
``The TopDDM (Time Adaptative Opponent Poisson drift-diffusion model) is also a PA model, but based on the drift-diffusion model of decision-making (Balci and Simen, 2016; Simen et al., 2011). It considers, in very general terms, that the accumulation of time should be represented by a ramping activity which rate is inversely proportional to the duration encoded. As reported above, this type of activity has been described in multiple papers (e.g., Komura et al., 2001; Lebedev et al., 2008; Murakami et al., 2014).'' (Tallot and Doy\`ere, 2020, p. 158)
\par
``There are also non-clock-based models; they require a population of units (can be neurons or group of neurons) that respond differently across time, which is consistent with the existence of sequential time cells. Included in those models is the TILT (Timing from Inverse Laplace transform) model from Shankar and Howard (2012), as well as the multi-timescale model (MTS) from Staddon and Higa (1999), and the Spectral Timing Model (Grossberg and Merrill, 1992). TILT model can also explain the scalar property of time as well as the recency effect of episodic memory (Howard et al., 2015). Its principle is that the presentation of a stimulus will activate a certain number of t nodes which will maintain a stable firing rate across the period of to-be-encoded time, and after the end of the interval decay exponentially to result in the activation of T nodes through an inverse Laplace transform. This will produce units that are active at a specific time point after the beginning of the stimulus. The different T nodes are activated sequentially and do not influence one another, they are instead influenced by the activity in the t nodes. Activities resembling to t nodes (stable increased firing) and to T nodes (sequential time cells) that when active during the early part of the interval have smaller spread of activity than cells activated later have both been reported in the literature. However, it remains to be determined to what extent they are found in all brain areas.'' (Tallot and Doy\`ere, 2020, p. 158)
\par
``TILT (Timing from Inverse Laplace transform) model from Shankar and Howard (2012)'' (Tallot and Doy\`ere, 2020, p. 158)
\par
``and the Spectral Timing Model (Grossberg and Merrill, 1992).'' (Tallot and Doy\`ere, 2020, p. 158)
\par
``duration and spatial location are learned and processed together as shown through space-time-context binding (Malet-Karas et al., 2019). It would seem important to store durations and contextual cues in the same anatomical regions, therefore going in the direction of multiple clocks'' (Tallot and Doy\`ere, 2020, p. 158)
\par
``4.4. Temporal learning vs. temporal behavior'' (Tallot and Doy\`ere, 2020, p. 158) kinda dropped off here.\\
this section seems possibly relevant\\
so perhaps revisit,},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HBVFC5PX/Tallot_Doyère_2020_Neural encoding of time in the animal brain.pdf}
}

@techreport{tanakaHippocampalEngramMaps,
  title = {The Hippocampal Engram Maps Experience but Not Place},
  author = {Tanaka, Kazumasa Z and He, Hongshen and Tomar, Anupratap and Niisato, Kazue and Y Huang, Arthur J and McHugh, Thomas J},
  abstract = {Episodic memories are encoded by a sparse population of hippocampal neurons. In mice, optogenetic manipulation of this memory engram established that these neurons are indispensable and inducing for memory recall. However, little is known about their in vivo activity or precise role in memory. We found that during memory encoding, only a fraction of CA1 place cells function as engram neurons, distinguished by firing repetitive bursts paced at the theta frequency. During memory recall, these neurons remained highly context specific, yet demonstrated preferential remapping of their place fields. These data demonstrate a dissociation of precise spatial coding and contextual indexing by distinct hippocampal ensembles and suggest that the hippocampal engram serves as an index of memory content.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DMNBSM74/Tanaka et al. - Unknown - The hippocampal engram maps experience but not place.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tanakaHippocampalEngramMaps.md}
}

@techreport{tanakaMeanfieldTheoryBoltzmann1998,
  title = {Mean-Field Theory of {{Boltzmann}} Machine Learning},
  author = {Tanaka, Toshiyuki},
  year = {1998},
  abstract = {I present a mean-field theory for Boltzmann machine learning, derived by employing Thouless-Anderson-Palmer free energy formalism to a full extent. Using the Plefka expansion an extended theory that takes higher-order correction to mean-field free energy formalism into consideration is presented, from which the mean-field approximation of general orders, along with the linear response correction, are derived by truncat-ing the Plefka expansion up to desired orders. A theoretical foundation for an effective trick of using ''diagonal weights,'' introduced by Kappen and Rodr\'iguez, is also given. Because of the finite system size and a lack of scaling assumptions on interaction coefficients, the truncated free energy formalism cannot provide an exact description in the case of Boltzmann machines. Accuracies of mean-field approximations of several orders are compared by computer simulations. S1063-651X9805308-2},
  keywords = {0520y,7510Nr,8710e,numbers: 8435i},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VM43JFXK/Tanaka - 1998 - Mean-field theory of Boltzmann machine learning.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tanakaMeanfieldTheoryBoltzmann1998.md}
}

@article{tanakaRecentAdvancesPhysical2018,
  title = {Recent {{Advances}} in {{Physical Reservoir Computing}}: {{A Review}}},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and H{\'e}roux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  year = {2018},
  pages = {1--62},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for extracting features of the inputs. Further, training is carried out only in the readout. Thus, the major advantage of reservoir computing is fast and simple learning compared to other recurrent neural networks. Another advantage is that the reservoir can be realized using physical systems, substrates, and devices, instead of recurrent neural networks. In fact, such physical reservoir computing has attracted increasing attention in various fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  keywords = {1 correspondence and requests,2010 msc,37n20,68txx,addressed to g,dynamical systems,for materials should be,gouhei,machine learning,neural networks,neuromorphic device,nonlinear,reservoir computing,sat,t,u-},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LAIJWUTA/Tanaka et al. - 2018 - Recent Advances in Physical Reservoir Computing A Review.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tanakaRecentAdvancesPhysical2018.md}
}

@techreport{tanakaTheoryMeanField,
  title = {A {{Theory}} of {{Mean Field Approximation}}},
  author = {Tanaka, T},
  abstract = {I present a theory of mean field approximation based on information geometry. This theory includes in a consistent way the naive mean field approximation, as well as the TAP approach and the linear response theorem in statistical physics, giving clear information-theoretic interpretations to them.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KTM7T2CJ/Tanaka - Unknown - A Theory of Mean Field Approximation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tanakaTheoryMeanField.md}
}

@article{tangDeepReinforcementLearning2020,
  title = {Deep {{Reinforcement Learning}} with {{Population-Coded Spiking Neural Network}} for {{Continuous Control}}},
  author = {Tang, Guangzhi and Kumar, Neelesh and Yoo, Raymond and Michmizos, Konstantinos P.},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.09635 [cs]},
  eprint = {2010.09635},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The energy-efficient control of mobile robots is crucial as the complexity of their real-world applications increasingly involves high-dimensional observation and action spaces, which cannot be offset by limited on-board resources. An emerging non-Von Neumann model of intelligence, where spiking neural networks (SNNs) are run on neuromorphic processors, is regarded as an energy-efficient and robust alternative to the state-of-the-art real-time robotic controllers for low dimensional control tasks. The challenge now for this new computing paradigm is to scale so that it can keep up with real-world tasks. To do so, SNNs need to overcome the inherent limitations of their training, namely the limited ability of their spiking neurons to represent information and the lack of effective learning algorithms. Here, we propose a population-coded spiking actor network (PopSAN) trained in conjunction with a deep critic network using deep reinforcement learning (DRL). The population coding scheme dramatically increased the representation capacity of the network and the hybrid learning combined the training advantages of deep networks with the energy-efficient inference of spiking networks. To show the general applicability of our approach, we integrated it with a spectrum of both on-policy and off-policy DRL algorithms. We deployed the trained PopSAN on Intel's Loihi neuromorphic chip and benchmarked our method against the mainstream DRL algorithms for continuous control. To allow for a fair comparison among all methods, we validated them on OpenAI gym tasks. Our Loihi-run PopSAN consumed 140 times less energy per inference when compared against the deep actor network on Jetson TX2, and had the same level of performance. Our results support the efficiency of neuromorphic controllers and suggest our hybrid RL as an alternative to deep learning, when both energy-efficiency and robustness are important.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  note = {Comment: Conference on Robot Learning (CoRL) 2020, 14 pages, 7 figures},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8YVQHMSK/Tang et al_2020_Deep Reinforcement Learning with Population-Coded Spiking Neural Network for.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SLJKKSWU/2010.html}
}

@article{tangMaximumEntropyModel2008,
  title = {A Maximum Entropy Model Applied to Spatial and Temporal Correlations from Cortical Networks in Vitro},
  author = {Tang, Aonan and Jackson, David and Hobbs, Jon and Chen, Wei and Smith, Jodi L. and Patel, Hema and Prieto, Anita and Petrusca, Dumitru and Grivich, Matthew I. and Sher, Alexander and Hottowy, Pawel and Dabrowski, Wladyslaw and Litke, Alan M. and Beggs, John M.},
  year = {2008},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {28},
  number = {2},
  pages = {505--518},
  publisher = {{Society for Neuroscience}},
  doi = {10.1523/JNEUROSCI.3359-07.2008},
  abstract = {Multineuron firing patterns are often observed, yet are predicted to be rare by models that assume independent firing. To explain these correlated network states, two groups recently applied a second-order maximum entropy model that used only observed firing rates and pairwise interactions as parameters (Schneidman et al., 2006; Shlens et al., 2006). Interestingly, with these minimal assumptions they predicted 90-99\% of network correlations. If generally applicable, this approach could vastly simplify analyses of complex networks. However, this initial work was done largely on retinal tissue, and its applicability to cortical circuits is mostly unknown. This work also did not address the temporal evolution of correlated states. To investigate these issues, we applied the model to multielectrode data containing spontaneous spikes or local field potentials from cortical slices and cultures. The model worked slightly less well in cortex than in retina, accounting for 88 {$\pm$} 7\% (mean {$\pm$} SD) of network correlations. In addition, in 8 of 13 preparations, the observed sequences of correlated states were significantly longer than predicted by concatenating states from the model. This suggested that temporal dependencies are acommonfeature of cortical network activity, and should be considered in future models. We found a significant relationship between strong pairwise temporal correlations and observed sequence length, suggesting that pairwise temporal correlations may allow the model to be extended into the temporal domain. We conclude that although a second-order maximum entropy model successfully predicts correlated states in cortical networks, it should be extended to account for temporal correlations observed between states. Copyright \textcopyright{} 2008 Society for Neuroscience.},
  keywords = {Culture,Human tissue,Local field potential,Microelectrode array,Neuronal avalanche,Slice},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/95GWJ48A/Tang et al. - 2008 - A maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tangMaximumEntropyModel2008.md}
}

@article{tankMaplikeMicroOrganizationGrid2018,
  title = {A {{Map-like Micro-Organization}} of {{Grid Cells}} in the {{Medial Entorhinal Cortex}}},
  author = {Tank, David W. and Kinkhabwala, Amina A. and Gauthier, Jeffrey L. and Domnisoru, Cristina and Gu, Yi and Fiete, Ila R. and Yoon, Kijung and Lewallen, Sam},
  year = {2018},
  journal = {Cell},
  volume = {175},
  number = {3},
  pages = {736-750.e30},
  doi = {10.1016/j.cell.2018.08.066},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EQSCRHJ3/Tank et al. - 2018 - A Map-like Micro-Organization of Grid Cells in the Medial Entorhinal Cortex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tankMaplikeMicroOrganizationGrid2018.md}
}

@misc{tanStrategyBenchmarkConverting2020,
  title = {Strategy and {{Benchmark}} for {{Converting Deep Q-Networks}} to {{Event-Driven Spiking Neural Networks}}},
  author = {Tan, Weihao and Patel, Devdhar and Kozma, Robert},
  year = {2020},
  month = dec,
  number = {arXiv:2009.14456},
  eprint = {2009.14456},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2009.14456},
  abstract = {Spiking neural networks (SNNs) have great potential for energy-efficient implementation of Deep Neural Networks (DNNs) on dedicated neuromorphic hardware. Recent studies demonstrated competitive performance of SNNs compared with DNNs on image classification tasks, including CIFAR-10 and ImageNet data. The present work focuses on using SNNs in combination with deep reinforcement learning in ATARI games, which involves additional complexity as compared to image classification. We review the theory of converting DNNs to SNNs and extending the conversion to Deep Q-Networks (DQNs). We propose a robust representation of the firing rate to reduce the error during the conversion process. In addition, we introduce a new metric to evaluate the conversion process by comparing the decisions made by the DQN and SNN, respectively. We also analyze how the simulation time and parameter normalization influence the performance of converted SNNs. We achieve competitive scores on 17 top-performing Atari games. To the best of our knowledge, our work is the first to achieve state-of-the-art performance on multiple Atari games with SNNs. Our work serves as a benchmark for the conversion of DQNs to SNNs and paves the way for further research on solving reinforcement learning tasks with SNNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: Accepted by AAAI2021},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DWXSVHH6/Tan et al_2020_Strategy and Benchmark for Converting Deep Q-Networks to Event-Driven Spiking.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KLPSY28B/2009.html}
}

@phdthesis{tarltonNovelModelSelection2021,
  title = {Novel {{Model Selection Criterion}} for {{Inference}} of {{Ising Models}}},
  author = {Tarlton, Michael},
  year = {2021},
  month = apr,
  abstract = {In this thesis we evaluate the performance of the novel Model Selection criteria proposed in Bulso et al. 2019, for inference of network topologies. To this purpose, we consider networks of binary nodes whose probability of activation is modelled by Ising models and generate data by simulating the network dynamics. After which, we infer the network topology by implementing the proposed criterion in a Bayesian model selection framework and compare the inferred topology with the ground truth model. The performance of the proposed method in recovering the network structure is contrasted with that of other popular model selection criteria in varied configurations of Ising parameters, network topologies, and sample size. We begin by introducing the Equilibrium Ising model and proceed by describing the approximate solutions for making inferences in Ising models. The novel criteria is one of a class of selection methods adapting concepts from information theory, namely the Minimum Description Length; We will also discuss the nonscientific applications and parallels suitable to our approach. Our results reinforce those found in Bulso et al. 2019. The novel criteria performs similarly to other selection criteria in the experiment regimes tested, with certain exceptions that will be addressed. Unique behaviors identified in the larger regimes may propose further avenues of investigation in networks of larger size and diversity.},
  collaborator = {Bulso, Nicola and Roudi, Yasser},
  school = {NTNU},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@tarltonNovelModelSelection2021.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/tarltonNovelModelSelection2021-mdnotes.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Tarlton_Novel Model Selection Criterion for Inference of Ising Models.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@tarltonNovelModelSelection2021.md}
}

@article{tavanaeiDeepLearningSpiking2019,
  title = {Deep Learning in Spiking Neural Networks},
  author = {Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timoth{\'e}e and Maida, Anthony},
  year = {2019},
  month = mar,
  journal = {Neural Networks},
  volume = {111},
  pages = {47--63},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.12.002},
  abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.},
  langid = {english},
  keywords = {Biological plausibility,Deep learning,Machine learning,Power-efficient architecture,Spiking neural network},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JA8J5RRY/Tavanaei et al_2019_Deep learning in spiking neural networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EG6AVETB/S0893608018303332.html}
}

@article{tavoniFunctionalCouplingNetworks2017,
  title = {Functional Coupling Networks Inferred from Prefrontal Cortex Activity Show Experience-Related Effective Plasticity},
  author = {Tavoni, Gaia and Ferrari, Ulisse and Battaglia, Francesco P and Cocco, Simona and Monasson, R{\'e}mi},
  year = {2017},
  doi = {10.1162/netn_a_00014},
  abstract = {Functional coupling networks are widely used to characterize collective patterns of activity in neural populations. Here, we ask whether functional couplings reflect the subtle changes, such as in physiological interactions, believed to take place during learning. We infer functional network models reproducing the spiking activity of simultaneously recorded neurons in prefrontal cortex (PFC) of rats, during the performance of a cross-modal rule shift task (task epoch), and during preceding and following sleep epochs. A large-scale study of the 96 recorded sessions allows us to detect, in about 20\% of sessions, effective plasticity between the sleep epochs. These coupling modifications are correlated with the coupling values in the task epoch, and are supported by a small subset of the recorded neurons, which we identify by means of an automatized procedure. These potentiated groups increase their coativation frequency in the spiking data between the two sleep epochs, and, hence, participate to putative experience-related cell assemblies. Study of the reactivation dynamics of the potentiated groups suggests a possible connection with behavioral learning. Reactivation is largely driven by hippocampal ripple events when the rule is not yet learned, and may be much more autonomous, and presumably sustained by the potentiated PFC network, when learning is consolidated. AUTHOR SUMMARY Cell assemblies coding for memories are widely believed to emerge through synaptic modification resulting from learning, yet their identification from activity is very arduous. We propose a functional-connectivity-based approach to identify experience-related cell assemblies from multielectrode recordings in vivo, and apply it to the prefrontal cortex activity of rats recorded during a task epoch and the preceding and following sleep epochs. We infer functional couplings between the recorded cells in each epoch. Comparisons of the functional coupling networks across the epochs allow us to identify effective potentiation between the two sleep epochs. The neurons supporting these potentiated interactions strongly coactivate during the task and subsequent sleep epochs, but not in the preceding sleep, and, hence, presumably belong to an experience-related cell assembly. Study of the reactivation of this assembly in response to hippocampal ripple inputs suggests possible relations between the stage of behavorial learning and memory consolidation mechanisms.},
  keywords = {Cell assemblies,Effective plasticity,Ising model,Memory consolidation,Statistical inference},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/T2AUIUMH/Tavoni et al. - 2017 - Functional coupling networks inferred from prefrontal cortex activity show experience-related effective plasticit.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tavoniFunctionalCouplingNetworks2017.md}
}

@article{tayHowWriteSuperb2020,
  title = {How to Write a Superb Literature Review},
  author = {Tay, Andy},
  year = {2020},
  month = dec,
  journal = {Nature},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-020-03422-x},
  abstract = {Nature speaks to old hands and first timers about the work they did to make their reviews sing.},
  copyright = {2021 Nature},
  langid = {english},
  keywords = {Careers,Publishing,Research management},
  annotation = {Bandiera\_abtest: a Cg\_type: Career Feature Subject\_term: Careers, Research management, Publishing},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5LJ8Q9HR/d41586-020-03422-x.html}
}

@article{tello-ramosTimePlaceLearning2015,
  title = {Time\textendash Place Learning in Wild, Free-Living Hummingbirds},
  author = {{Tello-Ramos}, Maria C. and Hurly, T. Andrew and Higgott, Caitlin and Healy, Susan D.},
  year = {2015},
  month = jun,
  journal = {Animal Behaviour},
  volume = {104},
  pages = {123--129},
  issn = {0003-3472},
  doi = {10.1016/j.anbehav.2015.03.015},
  abstract = {Animals can learn to revisit locations at which foraging resources will renew over time. This ability is known as `time\textendash place' learning. Although there is clear evidence for time\textendash place learning from animals trained and tested in the laboratory, not all species learn time\textendash place tasks with similar ease. Since hummingbirds feed from food sources that are constant in space and that renew with time, it seems plausible these birds might learn time\textendash place associations and do so readily. Here, then, we tested whether wild, free-living rufous hummingbirds, Selasphorus rufus, could learn the time and place at which four artificial flower patches were rewarded. Flowers in each of the four patches contained reward for only 1h each day but the time and sequence in which patches were rewarded were repeated across days. Most birds learned when to visit each patch. To determine whether the birds used ordinal or circadian timing to choose the correct patch at the correct time, we ended the experiment with a single test trial in which we presented the patches (all flowers empty) only at the fourth hour of the day. The birds visited neither the patch that was normally rewarded first (daily ordinal timing) nor the fourth patch (time of day). These results suggest that the time component of the time\textendash place learning in these birds requires both ordinal and circadian information.},
  langid = {english},
  keywords = {daily timing,ordinal timing,rufous hummingbirds,time–place learning,trap lining},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/454L6QDJ/Tello-Ramos et al_2015_Time–place learning in wild, free-living hummingbirds.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VP6E7G3K/S0003347215001153.html}
}

@article{tennoeUncertainpyPythonToolbox2018,
  title = {Uncertainpy: {{A Python Toolbox}} for {{Uncertainty Quantification}} and {{Sensitivity Analysis}} in {{Computational Neuroscience}}},
  author = {Tenn{\o}e, Simen and Halnes, Geir and Einevoll, Gaute T.},
  year = {2018},
  journal = {Frontiers in Neuroinformatics},
  volume = {12},
  number = {August},
  pages = {1--29},
  issn = {1662-5196},
  doi = {10.3389/fninf.2018.00049},
  abstract = {The increasing incidence of obesity and overweight among children and adolescents will be reflected by the imminent increase in the number of obese patients who require more definitive methods of treatment. There is great interest in new, safe, simple, nonsurgical procedures for weight loss},
  keywords = {sensitivity analysis,uncertainty quantification},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MY46CN99/Tennøe, Halnes, Einevoll - 2018 - Uncertainpy A Python Toolbox for Uncertainty Quantification and Sensitivity Analysis in Computational.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tennoeUncertainpyPythonToolbox2018.md}
}

@article{teradaObjectiveEfficientInference2018,
  title = {Objective and Efficient Inference for Couplings in Neuronal Networks},
  author = {Terada, Yu and Obuchi, Tomoyuki and Isomura, Takuya and Kabashima, Yoshiyuki},
  year = {2018},
  pages = {1--10},
  abstract = {Inferring directional couplings from the spike data of networks is desired in various scientific fields such as neuroscience. Here, we apply a recently proposed objective procedure to the spike data obtained from the Hodgkin--Huxley type models and in vitro neuronal networks cultured in a circular structure. As a result, we succeed in reconstructing synaptic connections accurately from the evoked activity as well as the spontaneous one. To obtain the results, we invent an analytic formula approximately implementing a method of screening relevant couplings. This significantly reduces the computational cost of the screening method employed in the proposed objective procedure, making it possible to treat large-size systems as in this study.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WLBNPDZR/Terada et al. - 2018 - Objective and efficient inference for couplings in neuronal networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@teradaObjectiveEfficientInference2018.md}
}

@article{teylerHippocampalIndexingTheory,
  title = {The {{Hippocampal Indexing Theory}} and {{Episodic Memory}}: {{Updating}} the {{Index}}},
  author = {Teyler, Timothy J and Rudy, Jerry W},
  doi = {10.1002/hipo.20350},
  abstract = {A little over 20 years ago, (Teyler and DiScenna, 1986; Behav Neurosci 100:147-152) proposed the hippocampal memory index theory. It offered an account of episodic memory based on the intrinsic organization of the hippocampus, its synaptic physiology and its anatomical relationship to other regions of the brain. The essence of their idea was that the hippocampus was functionally designed and anatomically situated to capture information about neocortical activity generated by the individual features of behavioral episode. Moreover, because the hippocampus projects back to these neocortical regions the information it stored could serve as an index to the pattern of neocorti-cal activity produced by the episode. Consequently, a partial cue that activated the index could activate the neocortical patterns and thus retrieve the memory of the episode. In this article we revisit and update indexing theory. Our conclusion is that it has aged very well. Its core ideas can be seen in many contemporary theories and there is a wealth of data that support this conceptual framework. V In the mid 1980s, a series of articles by Teyler and DiScenna (1984a,b, 1985) appeared that culminated in what is the hippocampal memory indexing theory (Teyler and DiScenna, 1986). This theory was intended to provide an in principle account of the contribution of the hippocam-pus to the storage and retrieval of experiences that make up our daily lives. In essence it was a theory of how the hippocampus contributes to episodic memory. As such it was a theory about (a) the role of the hip-pocampus in memory formation, (b) the nature of the memory trace, and (c) memory retrieval. In addition, because of how indexing theory viewed the nature of connections between the hippocampus and the neocortex, it also provided a rationale for speculating about how the initial memory representation might change with age and repetition. Since that time other theories encompassing many of the same ideas have appeared. Moreover, there now exists an extensive literature that is relevant to the basic assumptions of the indexing theory. Thus, in our view the time is right to consider how well the earlier ideas have survived the test of time. The purpose of this article is to provide such an assessment.},
  keywords = {episodic memory,pattern completion,pattern separation},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VDBMKMRD/Teyler, Rudy - Unknown - The Hippocampal Indexing Theory and Episodic Memory Updating the Index.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@teylerHippocampalIndexingTheory.md}
}

@article{thoulessSolutionSolvableModel1977,
  title = {Solution of '{{Solvable}} Model of a Spin Glass'},
  author = {Thouless, D. J. and Anderson, P. W. and Palmer, R. G.},
  year = {1977},
  month = mar,
  journal = {Philosophical Magazine},
  volume = {35},
  number = {3},
  pages = {593--601},
  issn = {0031-8086},
  doi = {10.1080/14786437708235992},
  langid = {english},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@thoulessSolutionSolvableModel1977.md}
}

@article{timmeRevealingNetworksDynamics2014,
  title = {Revealing Networks from Dynamics: {{An}} Introduction},
  author = {Timme, Marc and Casadiego, Jose},
  year = {2014},
  month = aug,
  journal = {Journal of Physics A: Mathematical and Theoretical},
  volume = {47},
  number = {34},
  pages = {343001--343001},
  publisher = {{Institute of Physics Publishing}},
  doi = {10.1088/1751-8113/47/34/343001},
  abstract = {What can we learn from the collective dynamics of a complex network about its interaction topology? Taking the perspective from nonlinear dynamics, we briefly review recent progress on how to infer structural connectivity (direct interactions) from accessing the dynamics of the units. Potential applications range from interaction networks in physics, to chemical and metabolic reactions, protein and gene regulatory networks as well as neural circuits in biology and electric power grids or wireless sensor networks in engineering. Moreover, we briefly mention some standard ways of inferring effective or functional connectivity.},
  keywords = {complex networks,effective connectivity,functional connectivity,network dynamics,network inference,network reconstruction,network topology,structural connectivity,synchrony},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/E3XYGCPN/Timme, Casadiego - 2014 - Revealing networks from dynamics An introduction.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@timmeRevealingNetworksDynamics2014.md}
}

@article{tkacikIsingModelsNetworks2006,
  title = {Ising Models for Networks of Real Neurons},
  author = {Tkacik, Gasper and Schneidman, Elad and Berry II, Michael J. and Bialek, William},
  year = {2006},
  month = nov,
  journal = {arXiv:q-bio/0611072},
  eprint = {q-bio/0611072},
  eprinttype = {arxiv},
  abstract = {Ising models with pairwise interactions are the least structured, or maximum-entropy, probability distributions that exactly reproduce measured pairwise correlations between spins. Here we use this equivalence to construct Ising models that describe the correlated spiking activity of populations of 40 neurons in the retina, and show that pairwise interactions account for observed higher-order correlations. By first finding a representative ensemble for observed networks we can create synthetic networks of 120 neurons, and find that with increasing size the networks operate closer to a critical point and start exhibiting collective behaviors reminiscent of spin glasses.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition},
  note = {Comment: 4 pages, 3 figures},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3HU5A97U/Tkacik et al. - 2006 - Ising models for networks of real neurons.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tkacikIsingModelsNetworks2006.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/484VZX5D/0611072.html}
}

@article{tkacikSearchingCollectiveBehavior2014,
  title = {Searching for {{Collective Behavior}} in a {{Large Network}} of {{Sensory Neurons}}},
  author = {Tka{\v c}ik, Ga{\v s}per and Marre, Olivier and Amodei, Dario and Schneidman, Elad and Bialek, William and Ii, Michael J. Berry},
  year = {2014},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {10},
  number = {1},
  pages = {e1003408},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003408},
  abstract = {Maximum entropy models are the least structured probability distributions that exactly reproduce a chosen set of statistics measured in an interacting network. Here we use this principle to construct probabilistic models which describe the correlated spiking activity of populations of up to 120 neurons in the salamander retina as it responds to natural movies. Already in groups as small as 10 neurons, interactions between spikes can no longer be regarded as small perturbations in an otherwise independent system; for 40 or more neurons pairwise interactions need to be supplemented by a global interaction that controls the distribution of synchrony in the population. Here we show that such ``K-pairwise'' models\textemdash being systematic extensions of the previously used pairwise Ising models\textemdash provide an excellent account of the data. We explore the properties of the neural vocabulary by: 1) estimating its entropy, which constrains the population's capacity to represent visual information; 2) classifying activity patterns into a small set of metastable collective modes; 3) showing that the neural codeword ensembles are extremely inhomogenous; 4) demonstrating that the state of individual neurons is highly predictable from the rest of the population, allowing the capacity for error correction.},
  langid = {english},
  keywords = {Action potentials,Entropy,Neural networks,Neurons,Probability distribution,Retina,Statistical mechanics,Vision},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8RD6LEJY/Tkačik et al. - 2014 - Searching for Collective Behavior in a Large Netwo.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tkacikSearchingCollectiveBehavior2014.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8H2VE4V2/article.html}
}

@article{tkacikSimplestMaximumEntropy2013,
  title = {The Simplest Maximum Entropy Model for Collective Behavior in a Neural Network},
  author = {Tka{\v c}ik, Ga{\v s}per and Marre, Olivier and Mora, Thierry and Amodei, Dario and Ii, Michael J. Berry and Bialek, William},
  year = {2013},
  month = mar,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2013},
  number = {03},
  pages = {P03011},
  publisher = {{IOP Publishing}},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2013/03/P03011},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2GTYNBYX/Tkačik et al. - 2013 - The simplest maximum entropy model for collective .pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VZQQPWXM/Tkačik et al. - 2013 - The simplest maximum entropy model for collective .pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tkacikSimplestMaximumEntropy2013.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/L75F2N7Z/meta.html;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VX4RUQVY/meta.html}
}

@article{tkacikSpinGlassModels2009,
  title = {Spin Glass Models for a Network of Real Neurons},
  author = {Tkacik, Gasper and Schneidman, Elad and Berry, Michael J. and Bialek, William},
  year = {2009},
  month = dec,
  abstract = {Ising models with pairwise interactions are the least structured, or maximum-entropy, probability distributions that exactly reproduce measured pairwise correlations between spins. Here we use this equivalence to construct Ising models that describe the correlated spiking activity of populations of 40 neurons in the salamander retina responding to natural movies. We show that pairwise interactions between neurons account for observed higher-order correlations, and that for groups of 10 or more neurons pairwise interactions can no longer be regarded as small perturbations in an independent system. We then construct network ensembles that generalize the network instances observed in the experiment, and study their thermodynamic behavior and coding capacity. Based on this construction, we can also create synthetic networks of 120 neurons, and find that with increasing size the networks operate closer to a critical point and start exhibiting collective behaviors reminiscent of spin glasses. We examine closely two such behaviors that could be relevant for neural code: tuning of the network to the critical point to maximize the ability to encode diverse stimuli, and using the metastable states of the Ising Hamiltonian as neural code words.},
  keywords = {correlation,entropy,information,Monte Carlo,multi-information,neural networks},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DUGSZVQ6/Tkacik et al. - 2009 - Spin glass models for a network of real neurons.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tkacikSpinGlassModels2009.md}
}

@article{tkacikThermodynamicsNetworkNeurons2014,
  title = {Thermodynamics for a Network of Neurons: {{Signatures}} of Criticality},
  author = {Tkacik, Gasper and Mora, Thierry and Marre, Olivier and Amodei, Dario and Berry, Michael J. and Bialek, William},
  year = {2014},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {37},
  pages = {11508--11513},
  abstract = {The activity of a neural network is defined by patterns of spiking and silence from the individual neurons. Because spikes are (relatively) sparse, patterns of activity with increasing numbers of spikes are less probable, but with more spikes the number of possible patterns increases. This tradeoff between probability and numerosity is mathematically equivalent to the relationship between entropy and energy in statistical physics. We construct this relationship for populations of up to N=160 neurons in a small patch of the vertebrate retina, using a combination of direct and model-based analyses of experiments on the response of this network to naturalistic movies. We see signs of a thermodynamic limit, where the entropy per neuron approaches a smooth function of the energy per neuron as N increases. The form of this function corresponds to the distribution of activity being poised near an unusual kind of critical point. Networks with more or less correlation among neurons would not reach this critical state. We suggest further tests of criticality, and give a brief discussion of its functional significance.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZST2MH26/Tkacik et al. - 2014 - Thermodynamics for a network of neurons Signatures of criticality.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tkacikThermodynamicsNetworkNeurons2014.md}
}

@article{tonegawaPerspectiveMemoryEngram2015,
  title = {Perspective {{Memory Engram Cells Have Come}} of {{Age}}},
  author = {Tonegawa, Susumu and Liu, Xu and Ramirez, Steve and Redondo, Roger},
  year = {2015},
  journal = {Neuron},
  volume = {87},
  pages = {918--931},
  doi = {10.1016/j.neuron.2015.08.002},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/9MZEHEP3/Tonegawa et al. - 2015 - Perspective Memory Engram Cells Have Come of Age.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tonegawaPerspectiveMemoryEngram2015.md}
}

@article{tononiConsciousnessComplexity1998,
  title = {Consciousness and Complexity},
  author = {Tononi, Giulio and Edelman, Gerald M.},
  year = {1998},
  month = dec,
  journal = {Science},
  volume = {282},
  number = {5395},
  pages = {1846--1851},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.282.5395.1846},
  abstract = {Conventional approaches to understanding consciousness are generally concerned with the contribution of specific brain areas or groups of neurons. By contrast, it is considered here what kinds of neural processes can account for key properties of conscious experience. Applying measures of neural integration and complexity, together with an analysis of extensive neurological data, leads to a testable proposal - the dynamic core hypothesis - about the properties of the neural substrate of consciousness.},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tononiConsciousnessComplexity1998.md}
}

@article{tononiMeasureBrainComplexity1994,
  title = {A Measure for Brain Complexity: Relating Functional Segregation and Integration in the Nervous System},
  shorttitle = {A Measure for Brain Complexity},
  author = {Tononi, G. and Sporns, O. and Edelman, G. M.},
  year = {1994},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {91},
  number = {11},
  pages = {5033--5037},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.91.11.5033},
  abstract = {In brains of higher vertebrates, the functional segregation of local areas that differ in their anatomy and physiology contrasts sharply with their global integration during perception and behavior. In this paper, we introduce a measure, called neural complexity (CN), that captures the interplay between these two fundamental aspects of brain organization. We express functional segregation within a neural system in terms of the relative statistical independence of small subsets of the system and functional integration in terms of significant deviations from independence of large subsets. CN is then obtained from estimates of the average deviation from statistical independence for subsets of increasing size. CN is shown to be high when functional segregation coexists with integration and to be low when the components of a system are either completely independent (segregated) or completely dependent (integrated). We apply this complexity measure in computer simulations of cortical areas to examine how some basic principles of neuroanatomical organization constrain brain dynamics. We show that the connectivity patterns of the cerebral cortex, such as a high density of connections, strong local connectivity organizing cells into neuronal groups, patchiness in the connectivity among neuronal groups, and prevalent reciprocal connections, are associated with high values of CN. The approach outlined here may prove useful in analyzing complexity in other biological domains such as gene regulation and embryogenesis.},
  chapter = {Research Article},
  langid = {english},
  pmid = {8197179},
  keywords = {Animals,Axons,Brain,Computer Simulation,Humans,Neurons},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N584XZEH/Tononi et al. - 1994 - A measure for brain complexity relating functiona.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SRCY24MD/Tononi et al. - 1994 - A measure for brain complexity relating functiona.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TE33888X/Tononi et al. - 1994 - A measure for brain complexity relating functiona.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tononiMeasureBrainComplexity1994.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/T7M2PFYZ/5033.html}
}

@misc{TractableMethodDescribing,
  title = {A {{Tractable Method}} for {{Describing Complex Couplings}} between {{Neurons}} and {{Population Rate}}},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4989052/},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@TractableMethodDescribing.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/T2D396S9/PMC4989052.html}
}

@book{traubHippocampalMicrocircuits2010,
  title = {Hippocampal Microcircuits},
  author = {Traub, Roger D.},
  year = {2010},
  journal = {Hippocampus},
  volume = {21},
  pages = {345},
  doi = {10.1002/hipo.20876},
  abstract = {Neurons are the basic computational units of the nervous system. Information processing in the brain is critically dependent on the electrophysiological properties of individual neurons, which are determined by the presence and distribution of many functionally and pharmacologically different ion channels. The parameters that define the functional roles of individual neurons can be grouped into two major groups: on the one side are cellular morphology and topology, which dictate the connectivity of each neuron; on the other side are the different electrophysiological properties of each cell type, which are defined by the combined effects of neuronal active and passive properties and shape the integrative function of each individual cell. The type and timing of neuronal responses to synaptic inputs depend on the firing pattern of each neuron, which in turn is set by the interplay of intrinsic and synaptic electrophysiological properties.},
  isbn = {978-3-319-99102-3},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/M8L387T9/Traub - 2010 - Hippocampal microcircuits.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@traubHippocampalMicrocircuits2010.md}
}

@article{trousdaleImpactNetworkStructure2012,
  title = {Impact of {{Network Structure}} and {{Cellular Response}} on {{Spike Time Correlations}}},
  author = {Trousdale, James and Hu, Yu and {Shea-Brown}, Eric and Josi{\'c}, Kre{\v s}imir},
  year = {2012},
  month = mar,
  journal = {PLOS Computational Biology},
  volume = {8},
  number = {3},
  pages = {e1002408},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002408},
  abstract = {Novel experimental techniques reveal the simultaneous activity of larger and larger numbers of neurons. As a result there is increasing interest in the structure of cooperative \textendash{} or correlated \textendash{} activity in neural populations, and in the possible impact of such correlations on the neural code. A fundamental theoretical challenge is to understand how the architecture of network connectivity along with the dynamical properties of single cells shape the magnitude and timescale of correlations. We provide a general approach to this problem by extending prior techniques based on linear response theory. We consider networks of general integrate-and-fire cells with arbitrary architecture, and provide explicit expressions for the approximate cross-correlation between constituent cells. These correlations depend strongly on the operating point (input mean and variance) of the neurons, even when connectivity is fixed. Moreover, the approximations admit an expansion in powers of the matrices that describe the network architecture. This expansion can be readily interpreted in terms of paths between different cells. We apply our results to large excitatory-inhibitory networks, and demonstrate first how precise balance \textendash{} or lack thereof \textendash{} between the strengths and timescales of excitatory and inhibitory synapses is reflected in the overall correlation structure of the network. We then derive explicit expressions for the average correlation structure in randomly connected networks. These expressions help to identify the important factors that shape coordinated neural activity in such networks.},
  langid = {english},
  keywords = {Action potentials,Autocorrelation,Convolution,Fourier analysis,Membrane potential,Neural networks,Neurons,White noise},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3TILM5TN/Trousdale et al. - 2012 - Impact of Network Structure and Cellular Response .pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@trousdaleImpactNetworkStructure2012.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KJZB8CJ7/article.html}
}

@techreport{trubianoMarkovChainMonte2018,
  title = {Markov {{Chain Monte Carlo The Metropolis-Hastings Algorithm}}},
  author = {Trubiano, Anthony},
  year = {2018},
  keywords = {Constraint based,Pensar2017},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CYD8FQ5Y/Trubiano - 2018 - Markov Chain Monte Carlo The Metropolis-Hastings Algorithm.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@trubianoMarkovChainMonte2018.md}
}

@article{tsamardinosAlgorithmsLargeScale,
  title = {Algorithms for {{Large Scale Markov Blanket Discovery}}},
  author = {Tsamardinos, Ioannis and Aliferis, Constantin F and Statnikov, Alexander},
  pages = {5},
  abstract = {This paper presents a number of new algorithms for discovering the Markov Blanket of a target variable T from training data. The Markov Blanket can be used for variable selection for classification, for causal discovery, and for Bayesian Network learning. We introduce a low-order polynomial algorithm and several variants that soundly induce the Markov Blanket under certain broad conditions in datasets with thousands of variables and compare them to other state-of-the-art local and global methods with excellent results.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4WUXMFGS/Tsamardinos et al. - Algorithms for Large Scale Markov Blanket Discover.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/REWJLIWC/Tsamardinos et al. - Algorithms for Large Scale Markov Blanket Discover.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tsamardinosAlgorithmsLargeScale.md}
}

@article{tshitoyanUnsupervisedWordEmbeddings2019,
  title = {Unsupervised Word Embeddings Capture Latent Knowledge from Materials Science Literature},
  author = {Tshitoyan, Vahe and Dagdelen, John and Weston, Leigh and Dunn, Alexander and Rong, Ziqin and Kononova, Olga and Persson, Kristin A. and Ceder, Gerbrand and Jain, Anubhav},
  year = {2019},
  journal = {Nature},
  doi = {10.1038/s41586-019-1335-8},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7RUU4TLY/Tshitoyan et al. - 2019 - Unsupervised word embeddings capture latent knowledge from materials science literature.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tshitoyanUnsupervisedWordEmbeddings2019.md}
}

@article{tsodyksAssociativeMemoryHippocampal1995,
  title = {Associative Memory and Hippocampal Place Cells},
  author = {Tsodyks, Misha},
  year = {1995},
  journal = {International journal of neural systems},
  volume = {6},
  pages = {81--86},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/J2QX42F8/Tsodyks - 1995 - Associative memory and hippocampal place cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tsodyksAssociativeMemoryHippocampal1995.md}
}

@article{tullySpikeBasedBayesianHebbianLearning2016,
  title = {Spike-{{Based Bayesian-Hebbian Learning}} of {{Temporal Sequences}}},
  author = {Tully, Philip and Lind{\'e}n, Henrik and Hennig, M. and Lansner, A.},
  year = {2016},
  journal = {PLoS Comput. Biol.},
  doi = {10.1371/journal.pcbi.1004954},
  abstract = {A modular attractor memory network is proposed in which meta-stable sequential attractor transitions are learned through changes to synaptic weights and intrinsic excitabilities via the spike-based Bayesian Confidence Propagation Neural Network (BCPNN) learning rule, finding that the formation of distributed memories can be acquired through plasticity. Many cognitive and motor functions are enabled by the temporal representation and processing of stimuli, but it remains an open issue how neocortical microcircuits can reliably encode and replay such sequences of information. To better understand this, a modular attractor memory network is proposed in which meta-stable sequential attractor transitions are learned through changes to synaptic weights and intrinsic excitabilities via the spike-based Bayesian Confidence Propagation Neural Network (BCPNN) learning rule. We find that the formation of distributed memories, embodied by increased periods of firing in pools of excitatory neurons, together with asymmetrical associations between these distinct network states, can be acquired through plasticity. The model's feasibility is demonstrated using simulations of adaptive exponential integrate-and-fire model neurons (AdEx). We show that the learning and speed of sequence replay depends on a confluence of biophysically relevant parameters including stimulus duration, level of background noise, ratio of synaptic currents, and strengths of short-term depression and adaptation. Moreover, sequence elements are shown to flexibly participate multiple times in the sequence, suggesting that spiking attractor networks of this type can support an efficient combinatorial code. The model provides a principled approach towards understanding how multiple interacting plasticity mechanisms can coordinate hetero-associative learning in unison.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KDDG7VJ9/Tully et al_2016_Spike-Based Bayesian-Hebbian Learning of Temporal Sequences.pdf}
}

@article{tunesTimeEncodingMigratesPrefrontal2020,
  title = {Time-{{Encoding Migrates}} from {{Prefrontal Cortex}} to {{Dorsal Striatum During Learning}} of a {{Self-Timed Response Duration Task}}},
  author = {Tunes, G. and de Oliveira, Eliezyer Fermino and Vieira, Estev{\~a}o Uyr{\'a} Pardillos and Caetano, M. S. and Cravo, A. and Reyes, M.},
  year = {2020},
  journal = {bioRxiv},
  doi = {10.1101/2020.11.19.390286},
  abstract = {The results show a double dissociation between the roles of the medial prefrontal cortex and the dorsal striatum during temporal learning, where the former commits to early learning stages while the latter become more engaged as animals become more proficient in the task. Although time is a fundamental dimension of life, we do not know how the brain encodes the temporal information. Several brain areas underlie the temporal information, such as the hippocampus, prefrontal cortex, and striatum, but evidence of how they cooperate to process temporal information is scarce. Notably, the analysis of neural activity during learning are rare, mainly because timing tasks usually take a long time to train. Here we investigated how the time encoding evolves when animals learn to time a 1.5 s interval. We designed a novel training protocol where rats go from naive- to proficient-level timing performance within a single session, allowing us to investigate neuronal activity from very early learning stages. We used pharmacological experiments and machine-learning algorithms to evaluate the level of time encoding in the medial prefrontal cortex and the dorsal striatum. Our results show a double dissociation between the roles of the medial prefrontal cortex and the dorsal striatum during temporal learning, where the former commits to early learning stages while the latter become more engaged as animals become more proficient in the task.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZPBDJSR3/Tunes et al_2020_Time-Encoding Migrates from Prefrontal Cortex to Dorsal Striatum During.pdf}
}

@article{tupikovAdditionNewNeurons2021,
  title = {Addition of New Neurons and the Emergence of a Local Neural Circuit for Precise Timing},
  author = {Tupikov, Y. and Jin, D.},
  year = {2021},
  journal = {PLoS computational biology},
  doi = {10.1371/journal.pcbi.1008824},
  abstract = {This model incorporates realistic HVC features such as interneurons, spatial distributions of neurons, and distributed axonal delays and predicts that the birth order of the projection neurons correlates with their burst timing during the song. During development, neurons arrive at local brain areas in an extended period of time, but how they form local neural circuits is unknown. Here we computationally model the emergence of a network for precise timing in the premotor nucleus HVC in songbird. We show that new projection neurons, added to HVC post hatch at early stages of song development, are recruited to the end of a growing feedforward network. High spontaneous activity of the new neurons makes them the prime targets for recruitment in a self-organized process via synaptic plasticity. Once recruited, the new neurons fire readily at precise times, and they become mature. Neurons that are not recruited become silent and replaced by new immature neurons. Our model incorporates realistic HVC features such as interneurons, spatial distributions of neurons, and distributed axonal delays. The model predicts that the birth order of the projection neurons correlates with their burst timing during the song.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/45C3FWM9/Tupikov_Jin_2021_Addition of new neurons and the emergence of a local neural circuit for precise.pdf}
}

@article{tyrchaEffectNonstationarityModels2013,
  title = {The Effect of Nonstationarity on Models Inferred from Neural Data},
  author = {Tyrcha, Joanna and Roudi, Yasser and Marsili, Matteo and Hertz, John},
  year = {2013},
  month = mar,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2013},
  number = {3},
  pages = {P03005-P03005},
  publisher = {{IOP Publishing}},
  doi = {10.1088/1742-5468/2013/03/P03005},
  abstract = {Neurons subject to a common nonstationary input may exhibit a correlated firing behavior. Correlations in the statistics of neural spike trains also arise as the effect of interaction between neurons. Here we show that these two situations can be distinguished with machine learning techniques, provided that the data are rich enough. In order to do this, we study the problem of inferring a kinetic Ising model, stationary or nonstationary, from the available data. We apply the inference procedure to two data sets: one from salamander retinal ganglion cells and the other from a realistic computational cortical network model. We show that many aspects of the concerted activity of the salamander retinal neurons can be traced simply to the external input. A model of non-interacting neurons subject to a nonstationary external field outperforms a model with stationary input with couplings between neurons, even accounting for the differences in the number of model parameters. When couplings are added to the nonstationary model, for the retinal data, little is gained: the inferred couplings are generally not significant. Likewise, the distribution of the sizes of sets of neurons that spike simultaneously and the frequency of spike patterns as a function of their rank (Zipf plots) are well explained by an independent-neuron model with time-dependent external input, and adding connections to such a model does not offer significant improvement. For the cortical model data, robust couplings, well correlated with the real connections, can be inferred using the nonstationary model. Adding connections to this model slightly improves the agreement with the data for the probability of synchronous spikes but hardly affects the Zipf plot. \textcopyright{} 2013 IOP Publishing Ltd and SISSA Medialab srl.},
  keywords = {computational neuroscience,statistical inference},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IH6V5PAA/Tyrcha et al. - 2013 - The effect of nonstationarity on models inferred from neural data.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@tyrchaEffectNonstationarityModels2013.md}
}

@phdthesis{unknownMScThesisUnknown,
  title = {{{MSc}} Thesis from Unknown {{Gunnar Tufte}} Student},
  author = {Unknown},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7D9NFWRE/main_with_abstract.pdf}
}

@article{urdapilletaSelforganizationModularActivity2017,
  title = {Selforganization of Modular Activity of Grid Cells},
  author = {Urdapilleta, Eugenio and Si, Bailu and Treves, Alessandro},
  year = {2017},
  journal = {Hippocampus},
  volume = {27},
  number = {11},
  pages = {1204--1213},
  doi = {10.1002/hipo.22765},
  abstract = {A unique topographical representation of space is found in the concerted activity of grid cells in the rodent medial entorhinal cortex. Many among the principal cells in this region exhibit a hexagonal firing pattern, in which each cell expresses its own set of place fields (spatial phases) at the vertices of a triangular grid, the spacing and orientation of which are typically shared with neighboring cells. Grid spacing, in particular, has been found to increase along the dorso-ventral axis of the entorhinal cortex but in discrete steps, that is, with a modular structure. In this study, we show that such a modular activity may result from the self-organization of interacting units, which individually would not show discrete but rather continuously varying grid spacing. Within our "adaptation" network model, the effect of a continuously varying time constant, which determines grid spacing in the isolated cell model, is modulated by recurrent collateral connections, which tend to produce a few subnetworks, akin to magnetic domains, each with its own grid spacing. In agreement with experimental evidence, the modular structure is tightly defined by grid spacing, but also involves grid orientation and distortion, due to interactions across modules. Thus, our study sheds light onto a possible mechanism, other than simply assuming separate networks a priori, underlying the formation of modular grid representations.},
  keywords = {grid cells,modules,self-organization},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4GYX9IAL/Urdapilleta, Si, Treves - 2017 - Selforganization of modular activity of grid cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@urdapilletaSelforganizationModularActivity2017.md}
}

@article{valadez-godinezAccuracyComputationalCost2020,
  title = {On the Accuracy and Computational Cost of Spiking Neuron Implementation},
  author = {{Valadez-God{\'i}nez}, Sergio and Sossa, Humberto and {Santiago-Montero}, Ra{\'u}l},
  year = {2020},
  month = feb,
  journal = {Neural Networks},
  volume = {122},
  pages = {196--217},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.09.026},
  langid = {english}
}

@article{valderhaugCriticalityMeasureDeveloping2020,
  title = {Criticality as a Measure of Developing Proteinopathy in Engineered Human Neural Networks},
  author = {Valderhaug, Vibeke Devold and Heiney, Kristine and Ramstad, Ola Huse and Braathen, Geir and Kuan, Wei-Li and Nichele, Stefano and Sandvig, Axel and Sandvig, Ioanna},
  year = {2020},
  month = may,
  journal = {bioRxiv},
  pages = {2020.05.03.074666-2020.05.03.074666},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.05.03.074666},
  abstract = {A patterned spread of proteinopathy represents a common characteristic of many neurodegenerative diseases. In Parkinson's disease (PD), misfolded forms of alpha-synuclein proteins aggregate and accumulate in hallmark pathological inclusions termed Lewy bodies and Lewy neurites, which seems to affect selectively vulnerable neuronal populations and propagate within interconnected neuronal networks. Research findings suggest that these proteinopathic inclusions are present at very early timepoints in disease development, even before strong behavioural symptoms of dysfunction arise, but that these underlying pathologies might be masked by homeostatic processes working to maintain the function of the degenerating neural circuits. This study investigates whether inducing the PD-related alpha-synuclein pathology in engineered human neural networks can be associated with changes in network function, and particularly with network criticality states. Self-organised criticality represents the critical point between resilience against perturbation and adaptational flexibility, which appears to be a functional trait in self-organising neural networks, both in vitro and in vivo. By monitoring the developing neural network activity through the use of multielectrode arrays (MEAs) for a period of three weeks following proteinopathy induction, we show that although this developing pathology is not clearly manifest in standard measurements of network function, it may be discerned by differences in network criticality states. \#\#\# Competing Interest Statement The authors have declared no competing interest. *   PD :   Parkinson's disease AD :   Alzheimer's disease ALS :   Amyotrophic lateral sclerosis SNpc :   Substantia Nigra pars compacta MEA :   multielectrode array SoC :   Self-organized criticality PFF :   alpha-synucelin pre-formed fibrils iPSC :   induced pluripotent stem cells PBS :   Phosphate buffered saline AFM :   Atomic force microscopy TEM :   Transmission electron microscopy MFR :   Mean firing rate ISI :   Inter-spike intervals PISI :   Population inter-spike interval XC :   Cross-correlation},
  keywords = {alpha-synuclein,electrophysiology,function,PFF,plasticity,pre-formed fibrils,SoC},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LMUN2U7G/Valderhaug et al. - 2020 - Criticality as a measure of developing proteinopathy in engineered human neural networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/YVTS35IM/Valderhaug et al. - 2020 - Criticality as a measure of developing proteinopathy in engineered human neural networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@valderhaugCriticalityMeasureDeveloping2020.md}
}

@article{valderhaugFormationNeuralNetworks2019,
  title = {Formation of Neural Networks with Structural and Functional Features Consistent with Small-World Network Topology on Surface-Grafted Polymer Particles},
  author = {Valderhaug, Vibeke Devold and Glomm, Wilhelm Robert and Sandru, Eugenia Mariana and Yasuda, Masahiro and Sandvig, Axel and Sandvig, Ioanna},
  year = {2019},
  month = oct,
  journal = {Royal Society Open Science},
  volume = {6},
  number = {10},
  pages = {191086},
  publisher = {{Royal Society Publishing}},
  issn = {2054-5703},
  doi = {10.1098/rsos.191086},
  abstract = {\textexclamdown p\textquestiondown{} \textexclamdown italic\textquestiondown In vitro\textexclamdown/italic\textquestiondown{} electrophysiological investigation of neural activity at a network level holds tremendous potential for elucidating underlying features of brain function (and dysfunction). In standard neural network modelling systems, however, the fundamental three-dimensional (3D) character of the brain is a largely disregarded feature. This widely applied neuroscientific strategy affects several aspects of the structure\textendash function relationships of the resulting networks, altering network connectivity and topology, ultimately reducing the translatability of the results obtained. As these model systems increase in popularity, it becomes imperative that they capture, as accurately as possible, fundamental features of neural networks in the brain, such as small-worldness. In this report, we combine \textexclamdown italic\textquestiondown in vitro\textexclamdown/italic\textquestiondown{} neural cell culture with a biologically compatible scaffolding substrate, surface-grafted polymer particles (PPs), to develop neural networks with 3D topology. Furthermore, we investigate their electrophysiological network activity through the use of 3D multielectrode arrays. The resulting neural network activity shows emergent behaviour consistent with maturing neural networks capable of performing computations, i.e. activity patterns suggestive of both information segregation (desynchronized single spikes and local bursts) and information integration (network spikes). Importantly, we demonstrate that the resulting PP-structured neural networks show both structural and functional features consistent with small-world network topology. \textexclamdown/p\textquestiondown},
  keywords = {Connectivity,Electrophysiology,Neural networks,Polymer particles,Small-world,Three-dimensional structuring},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/EFV2NW8M/Valderhaug et al. - 2019 - Formation of neural networks with structural and functional features consistent with small-world network topo.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/KT7EMDHE/Valderhaug et al. - 2019 - Formation of neural networks with structural and functional features consistent with small-world network topo.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@valderhaugFormationNeuralNetworks2019.md}
}

@article{valderhaugStructuralFunctionalAlterations2020,
  title = {Structural and Functional Alterations Associated with the {{LRRK2 G2019S}} Mutation Revealed in Structured Human Neural Networks},
  author = {Valderhaug, Vibeke Devold and Ramstad, Ola Huse and {van de Wijdeven}, Rosanne and Heiney, Kristine and Nichele, Stefano and Sandvig, Axel and Sandvig, Ioanna},
  year = {2020},
  month = may,
  journal = {bioRxiv},
  pages = {2020.05.02.073726-2020.05.02.073726},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.05.02.073726},
  abstract = {Running title Structure and function in engineered human LRRK2 networks Total number of pages: 40 Total number of words: (i) whole manuscript: 8062; (ii) abstract: 235 Abstract Mutations in the LRRK2 gene have been widely linked to Parkinso\'nsParkinso\'nParkinso\'ns disease. The G2019S variant has been shown to contribute uniquely to both familial and sporadic forms of the disease. LRRK2-related mutations have been extensively studied, yet the wide variety of cellular and network events directly or indirectly related to these mutations remain poorly understood. In this study, we structured multi-nodal human neural networks carrying the G2019S mutation using custom-designed microfluidic chips coupled to microelectrode-arrays. By applying live imaging approaches, immunocytochemistry and computational modelling, we have revealed alterations in both the structure and function of the resulting neural networks when compared to controls. We provide first evidence of increased neuritic density associated with the G2019S LRRK2 mutation, while previous studies have found either a strong decrease, or no change, compared to controls. Additionally, we corroborate previous findings regarding increased baseline network activity compared to control neural networks. Furthermore, we can reveal additional network alterations attributable to the specific mutation by selectively inducing transient overexcitation to confined parts of the structured multi-nodal networks. These alterations, which we were able to capture both at the micro-and mesoscale manifested as differences in relative network activity and correlation, as well as in mitochondria activation, neuritic remodelling, and synaptic alterations. Our study thus provides important new insights into early signs of neural network pathology significantly expanding upon the current knowledge relating to the G2019S Parkinson's disease mutation.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6ABEBM4X/Valderhaug et al. - 2020 - Structural and functional alterations associated with the LRRK2 G2019S mutation revealed in structured human.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/D7WC2SU8/Valderhaug et al. - 2020 - Structural and functional alterations associated with the LRRK2 G2019S mutation revealed in structured human.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@valderhaugStructuralFunctionalAlterations2020.md}
}

@article{vanderlindeBayesianViewModel2012,
  title = {A {{Bayesian}} View of Model Complexity: {{{\emph{A Bayesian}}}}{\emph{ View of Model Complexity}}},
  shorttitle = {A {{Bayesian}} View of Model Complexity},
  author = {{van der Linde}, Angelika},
  year = {2012},
  month = aug,
  journal = {Statistica Neerlandica},
  volume = {66},
  number = {3},
  pages = {253--271},
  issn = {00390402},
  doi = {10.1111/j.1467-9574.2011.00518.x},
  abstract = {The paper addresses the problem of formally defining the `effective number of parameters' in a Bayesian model which is assumed to be given by a sampling distribution and a prior distribution for the parameters. The problem occurs in the derivation of information criteria for model comparison which often trade off `goodness of fit' and `model complexity'. It also arises in (frequentist) attempts to estimate the error variance in regression models with informative priors on the regression coefficients, for example in smoothing. It is argued that model complexity can be conceptualized as a feature of the joint distribution of the observed variables and the random parameters and might be formally described by a measure of dependence. The universal and accurate estimation of terms of model complexity is a challenging problem in practice. Several well-known criteria for model comparison are interpreted and discussed along these lines.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GECZMYGS/van der Linde - 2012 - A Bayesian view of model complexity A Bayesian.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@vanderlindeBayesianViewModel2012.md}
}

@techreport{vandermaatenMatlabToolboxDimensionality,
  title = {Matlab {{Toolbox}} for {{Dimensionality Reduction}}},
  author = {Van Der Maaten, Laurens},
  abstract = {The demonstration presents the Matlab Toolbox for Dimensionality Reduction. The toolbox is publicly available and contains implementations of virtually all state-of-the-art techniques for dimensionality reduction and intrinsic dimensionality estimation. It provides implementations of 27 techniques for di-mensionality reduction, 6 techniques for intrinsic dimensionality estimation, and additional functions for out-of-sample extension, data generation, and data prewhitening. The demonstration illustrates the func-tionality of the toolbox and provides insight into the capabilities of state-of-the-art techniques for dimen-sionality reduction.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WHKSI2CF/Van Der Maaten - Unknown - Matlab Toolbox for Dimensionality Reduction.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@vandermaatenMatlabToolboxDimensionality.md}
}

@article{vandermeerUnderstandingGeneticDeterminants2020,
  title = {Understanding the Genetic Determinants of the Brain with {{MOSTest}}},
  author = {{van der Meer}, Dennis and Frei, Oleksandr and Kaufmann, Tobias and Shadrin, Alexey A. and Devor, Anna and Smeland, Olav B. and Thompson, Wesley K. and Fan, Chun Chieh and Holland, Dominic and Westlye, Lars T. and Andreassen, Ole A. and Dale, Anders M.},
  year = {2020},
  month = jul,
  journal = {Nature Communications},
  volume = {11},
  pages = {3512},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17368-1},
  abstract = {Regional brain morphology has a complex genetic architecture, consisting of many common polymorphisms with small individual effects. This has proven challenging for genome-wide association studies~(GWAS). Due to the distributed nature of genetic signal across brain regions, multivariate analysis of regional measures may enhance discovery of genetic variants. Current multivariate approaches to GWAS are ill-suited for complex, large-scale data of this kind. Here, we introduce the Multivariate Omnibus Statistical Test (MOSTest), with an efficient computational design enabling rapid and reliable inference, and apply it to 171 regional brain morphology measures from 26,502 UK Biobank participants. At the conventional genome-wide significance threshold of {$\alpha$}\,=\,5\,\texttimes\,10-8, MOSTest identifies 347 genomic loci associated with regional brain morphology, more than any previous study, improving upon the discovery of established GWAS approaches more than threefold. Our findings implicate more than 5\% of all protein-coding genes and provide evidence for gene sets involved in neuron development and differentiation., Regional brain morphology has a complex genetic architecture. Here the authors present MOSTest, a multivariate statistical framework, apply it to UK Biobank data, and discover hundreds of loci associated with regional brain morphology.},
  pmcid = {PMC7360598},
  pmid = {32665545},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/van der Meer et al_2020_Understanding the genetic determinants of the brain with MOSTest.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@vandermeerUnderstandingGeneticDeterminants2020.md}
}

@article{vasilakiSpikeBasedReinforcementLearning2009,
  title = {Spike-{{Based Reinforcement Learning}} in {{Continuous State}} and {{Action Space}}: {{When Policy Gradient Methods Fail}}},
  shorttitle = {Spike-{{Based Reinforcement Learning}} in {{Continuous State}} and {{Action Space}}},
  author = {Vasilaki, Eleni and Fr{\'e}maux, Nicolas and Urbanczik, Robert and Senn, Walter and Gerstner, Wulfram},
  year = {2009},
  month = dec,
  journal = {PLOS Computational Biology},
  volume = {5},
  number = {12},
  pages = {e1000586},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000586},
  abstract = {Changes of synaptic connections between neurons are thought to be the physiological basis of learning. These changes can be gated by neuromodulators that encode the presence of reward. We study a family of reward-modulated synaptic learning rules for spiking neurons on a learning task in continuous space inspired by the Morris Water maze. The synaptic update rule modifies the release probability of synaptic transmission and depends on the timing of presynaptic spike arrival, postsynaptic action potentials, as well as the membrane potential of the postsynaptic neuron. The family of learning rules includes an optimal rule derived from policy gradient methods as well as reward modulated Hebbian learning. The synaptic update rule is implemented in a population of spiking neurons using a network architecture that combines feedforward input with lateral connections. Actions are represented by a population of hypothetical action cells with strong mexican-hat connectivity and are read out at theta frequency. We show that in this architecture, a standard policy gradient rule fails to solve the Morris watermaze task, whereas a variant with a Hebbian bias can learn the task within 20 trials, consistent with experiments. This result does not depend on implementation details such as the size of the neuronal populations. Our theoretical approach shows how learning new behaviors can be linked to reward-modulated plasticity at the level of single synapses and makes predictions about the voltage and spike-timing dependence of synaptic plasticity and the influence of neuromodulators such as dopamine. It is an important step towards connecting formal theories of reinforcement learning with neuronal and synaptic properties.},
  langid = {english},
  keywords = {Action potentials,Decision making,Excitatory postsynaptic potentials,Learning,Machine learning,Membrane potential,Neurons,Synapses},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FCS53MAY/Vasilaki et al_2009_Spike-Based Reinforcement Learning in Continuous State and Action Space.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SEE3FGLR/article.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 15 pages, 5 figures},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/32I4MU69/Vaswani et al_2017_Attention Is All You Need.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/BR25BCS9/1706.html}
}

@inproceedings{velusamyEvaluatingReinforcementLearning2019,
  title = {Evaluating {{Reinforcement Learning Methods}} for {{Bundle Routing Control}}},
  booktitle = {2019 {{IEEE Cognitive Communications}} for {{Aerospace Applications Workshop}} ({{CCAAW}})},
  author = {Velusamy, Gandhimathi and Lent, Ricardo},
  year = {2019},
  month = jun,
  pages = {1--4},
  doi = {10.1109/CCAAW.2019.8904909},
  abstract = {Cognitive networking applications continuously adapt actions according to observations of the environment and assigned performance goals. In this paper, one such cognitive networking application is evaluated where the aim is to route bundles over parallel links of different characteristics. Several machine learning algorithms may be suitable for the task. This research tested different reinforcement learning methods as potential enablers for this application: Q-Routing, Double Q-Learning, an actor-critic Learning Automata implementing the S-model, and the Cognitive Network Controller (CNC), which uses on a spiking neural network for Q-value prediction. All cases are evaluated under the same experimental conditions. Working with either a stable or time-varying environment with respect to the quality of the links, each routing method was evaluated with an identical number of bundle transmissions generated at a common rate. The measurements indicate that in general, the Cognitive Network Controller (CNC) produces better performance than the other methods followed by the Learning Automata. In the presented tests, the performance of Q-Routing and Double Q-Learning achieved similar performance to a non-learning round-robin approach. It is expect that these results will help to guide and improve the design of this and future cognitive networking applications.},
  keywords = {Cognitive Networking,Delays,Learning automata,Learning Automata,Neural networks,Neuromorphic Computing,Reinforcement learning,Reinforcement Learning,Routing,Spiking Neural Networks,Throughput,Time factors},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ET82DGDE/Velusamy_Lent_2019_Evaluating Reinforcement Learning Methods for Bundle Routing Control.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/GBENVDS9/8904909.html}
}

@article{verstraetenExperimentalUnificationReservoir2007,
  title = {An Experimental Unification of Reservoir Computing Methods},
  author = {Verstraeten, D. and Schrauwen, B. and D'Haene, M. and Stroobandt, D.},
  year = {2007},
  month = apr,
  journal = {Neural Networks},
  series = {Echo {{State Networks}} and {{Liquid State Machines}}},
  volume = {20},
  number = {3},
  pages = {391--403},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2007.04.003},
  abstract = {Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classification layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks.},
  langid = {english},
  keywords = {Chaos,Lyapunov exponent,Memory capability,Reservoir computing},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HW2DQXZY/An experimental unification of reservoir computing methods - 11.05.22.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MQ9U6RTX/Verstraeten et al_2007_An experimental unification of reservoir computing methods.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UTD9KLBH/S089360800700038X.html}
}

@inproceedings{vigneronCriticalSurveySTDP2020,
  title = {A Critical Survey of {{STDP}} in {{Spiking Neural Networks}} for {{Pattern Recognition}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Vigneron, Alex and Martinet, Jean},
  year = {2020},
  month = jul,
  pages = {1--9},
  issn = {2161-4407},
  doi = {10.1109/IJCNN48605.2020.9207239},
  abstract = {The bio-inspired concept of Spike-Timing-Dependent Plasticity (STDP) derived from neurobiology is increasingly used in Spiking Neural Networks (SNNs) nowadays. Mostly found in unsupervised learning, though recent work has shown its usefulness in supervised or reinforced paradigms too, STDP is a key element to understanding SNN architectures' learning process. This review introduces a categorisation of its several variants and discusses their specificities and applications, from a pattern recognition perspective. It gathers a variety of definitions used in machine learning for pattern recognition. It provides relevant information for research communities of various backgrounds looking for an overview of this field.},
  keywords = {Artificial Neural Networks,Bio-inspiration,Biological neural networks,Computer architecture,Encoding,Machine Learning,Neurons,Pattern recognition,Pattern Recognition,Spiking Neural Networks,STDP,Synapses,Unsupervised Learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Vigneron_Martinet_2020_A critical survey of STDP in Spiking Neural Networks for Pattern Recognition.pdf;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@vigneronCriticalSurveySTDP2020.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/vigneronCriticalSurveySTDP2020-mdnotes.md;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@vigneronCriticalSurveySTDP2020.md;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@vigneronCriticalSurveySTDP2020.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/U8N8BIXP/9207239.html}
}

@article{vincent-lamarreLearningLongTemporal2020,
  title = {Learning {{Long Temporal Sequences}} in {{Spiking Networks}} by {{Multiplexing Neural Oscillations}}},
  author = {{Vincent-Lamarre}, Philippe and Calderini, M. and Thivierge, J.},
  year = {2020},
  journal = {Frontiers in Computational Neuroscience},
  doi = {10.3389/fncom.2020.00078},
  abstract = {The model performs temporal rescaling of natural spoken words and exhibits sequential neural activity commonly found in experimental data involving temporal processing, suggesting that combining oscillatory neuronal inputs with different frequencies provides a key mechanism to generate precisely timed sequences of activity in recurrent circuits of the brain. Many cognitive and behavioral tasks\textemdash such as interval timing, spatial navigation, motor control, and speech\textemdash require the execution of precisely-timed sequences of neural activation that cannot be fully explained by a succession of external stimuli. We show how repeatable and reliable patterns of spatiotemporal activity can be generated in chaotic and noisy spiking recurrent neural networks. We propose a general solution for networks to autonomously produce rich patterns of activity by providing a multi-periodic oscillatory signal as input. We show that the model accurately learns a variety of tasks, including speech generation, motor control, and spatial navigation. Further, the model performs temporal rescaling of natural spoken words and exhibits sequential neural activity commonly found in experimental data involving temporal processing. In the context of spatial navigation, the model learns and replays compressed sequences of place cells and captures features of neural activity such as the emergence of ripples and theta phase precession. Together, our findings suggest that combining oscillatory neuronal inputs with different frequencies provides a key mechanism to generate precisely timed sequences of activity in recurrent circuits of the brain.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WH9X5XNY/Vincent-Lamarre et al_2020_Learning Long Temporal Sequences in Spiking Networks by Multiplexing Neural.pdf}
}

@article{virgiliog.SpikingNeuralNetworks2020,
  title = {Spiking {{Neural Networks}} Applied to the Classification of Motor Tasks in {{EEG}} Signals},
  author = {Virgilio G., Carlos D. and Sossa A., Juan H. and Antelis, Javier M. and Falc{\'o}n, Luis E.},
  year = {2020},
  month = feb,
  journal = {Neural Networks},
  volume = {122},
  pages = {130--143},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.09.037},
  langid = {english}
}

@article{voelkerDynamicalSystemsSpiking2019,
  title = {Dynamical {{Systems}} in {{Spiking Neuromorphic Hardware}}},
  author = {Voelker, Aaron Russell},
  year = {2019},
  month = may,
  publisher = {{University of Waterloo}},
  abstract = {Dynamical systems are universal computers. They can perceive stimuli, remember, learn from feedback, plan sequences of actions, and coordinate complex behavioural responses. The Neural Engineering Framework (NEF) provides a general recipe to formulate models of such systems as coupled sets of nonlinear differential equations and compile them onto recurrently connected spiking neural networks \textendash{} akin to a programming language for spiking models of computation. The Nengo software ecosystem supports the NEF and compiles such models onto neuromorphic hardware. In this thesis, we analyze the theory driving the success of the NEF, and expose several core principles underpinning its correctness, scalability, completeness, robustness, and extensibility. We also derive novel theoretical extensions to the framework that enable it to far more effectively leverage a wide variety of dynamics in digital hardware, and to exploit the device-level physics in analog hardware. At the same time, we propose a novel set of spiking algorithms that recruit an optimal nonlinear encoding of time, which we call the Delay Network (DN). Backpropagation across stacked layers of DNs dramatically outperforms stacked Long Short-Term Memory (LSTM) networks\textemdash a state-of-the-art deep recurrent architecture\textemdash in accuracy and training time, on a continuous-time memory task, and a chaotic time-series prediction benchmark. The basic component of this network is shown to function on state-of-the-art spiking neuromorphic hardware including Braindrop and Loihi. This implementation approaches the energy-efficiency of the human brain in the former case, and the precision of conventional computation in the latter case.},
  langid = {english},
  annotation = {Accepted: 2019-05-10T18:29:03Z},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3UHWEJC2/Voelker_2019_Dynamical Systems in Spiking Neuromorphic Hardware.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/F7YAT2JC/14625.html}
}

@article{voelkerImprovingSpikingDynamical2018,
  title = {Improving {{Spiking Dynamical Networks}}: {{Accurate Delays}}, {{Higher-Order Synapses}}, and {{Time Cells}}},
  shorttitle = {Improving {{Spiking Dynamical Networks}}},
  author = {Voelker, Aaron R. and Eliasmith, Chris},
  year = {2018},
  month = mar,
  journal = {Neural Computation},
  volume = {30},
  number = {3},
  pages = {569--609},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01046},
  abstract = {Researchers building spiking neural networks face the challenge of improving the biological plausibility of their model networks while maintaining the ability to quantitatively characterize network behavior. In this work, we extend the theory behind the neural engineering framework (NEF), a method of building spiking dynamical networks, to permit the use of a broad class of synapse models while maintaining prescribed dynamics up to a given order. This theory improves our understanding of how low-level synaptic properties alter the accuracy of high-level computations in spiking dynamical networks. For completeness, we provide characterizations for both continuous-time (i.e., analog) and discrete-time (i.e., digital) simulations. We demonstrate the utility of these extensions by mapping an optimal delay line onto various spiking dynamical networks using higher-order models of the synapse. We show that these networks nonlinearly encode rolling windows of input history, using a scale invariant representation, with accuracy depending on the frequency content of the input signal. Finally, we reveal that these methods provide a novel explanation of time cell responses during a delay task, which have been observed throughout hippocampus, striatum, and cortex.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@voelkerImprovingSpikingDynamical2018.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/voelkerImprovingSpikingDynamical2018-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8ZXSHF98/8294070.html}
}

@inproceedings{voelkerLegendreMemoryUnits2019,
  title = {Legendre {{Memory Units}}: {{Continuous-Time Representation}} in {{Recurrent Neural Networks}}},
  shorttitle = {Legendre {{Memory Units}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Voelker, Aaron and Kaji{\'c}, Ivana and Eliasmith, Chris},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  note = {\section{Annotations\\
(5/9/2022, 11:56:47 AM)}

\par
``, which owes its superior performance to a combination of memory cells and gating mechanisms that maintain and nonlinearly mix information over time.'' (Voelker et al., 2019, p. 1)
\par
``4 Characteristics of the LMU Linear-Nonlinear Processing Linear units maximize the information capacity of dynamical systems, while nonlinearities are required to compute useful functions across this information [19]. The LMU formalizes this linear-nonlinear trade-off by decoupling the functional role of d linear memory units from that of n nonlinear hidden units, and then using backpropagation to learn their coupling. Parameter-State Trade-offs One can increase d to improve the linear memory (m) capacity at the cost of a linear increase in the size of the encoding parameters, or, increase n to improve the complexity of nonlinear interactions with the memory (h) at the expense of a quadratic increase in the size of the recurrent kernels. Thus, d and n can be set independently to trade storage for parameters while balancing linear memory capacity with hidden nonlinear processing. Optimality and Uniqueness The memory cell is optimal in the sense of being derived from the Pad\'e [30] approximants of the delay line expanded about the zeroth frequency [38]. These approximants have been proven optimal for this purpose. Moreover, the phase space of the memory maps onto the unique set of orthogonal polynomials over [0, \texttheta ] (the shifted Legendre polynomials) up to a constant scaling factor [24]. Thus the LMU is provably optimal with respect to its continuoustime memory capacity, which provides a nontrivial starting point for backpropagation. To validate this characteristic, we reran the psMNIST benchmark with the diagonals of{\= } A perturbed by {$\in$} \{-0.01, -0.001, 0.001, 0.01\}. Despite retraining the network for each , this achieved sub-optimal test performance in each case, and resulted in chance-level performance for larger | |.'' (Voelker et al., 2019, p. 6)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@voelkerLegendreMemoryUnits2019.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/voelkerLegendreMemoryUnits2019-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/7F8ZW28Q/Voelker et al_2019_Legendre Memory Units.pdf}
}

@article{voelkerSpikePerformanceTraining2021,
  title = {A {{Spike}} in {{Performance}}: {{Training Hybrid-Spiking Neural Networks}} with {{Quantized Activation Functions}}},
  shorttitle = {A {{Spike}} in {{Performance}}},
  author = {Voelker, Aaron R. and Rasmussen, Daniel and Eliasmith, Chris},
  year = {2021},
  month = mar,
  journal = {arXiv:2002.03553 [cs, q-bio, stat]},
  eprint = {2002.03553},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  abstract = {The machine learning community has become increasingly interested in the energy efficiency of neural networks. The Spiking Neural Network (SNN) is a promising approach to energy-efficient computing, since its activation levels are quantized into temporally sparse, one-bit values (i.e., "spike" events), which additionally converts the sum over weight-activity products into a simple addition of weights (one weight for each spike). However, the goal of maintaining state-of-the-art (SotA) accuracy when converting a non-spiking network into an SNN has remained an elusive challenge, primarily due to spikes having only a single bit of precision. Adopting tools from signal processing, we cast neural activation functions as quantizers with temporally-diffused error, and then train networks while smoothly interpolating between the non-spiking and spiking regimes. We apply this technique to the Legendre Memory Unit (LMU) to obtain the first known example of a hybrid SNN outperforming SotA recurrent architectures -- including the LSTM, GRU, and NRU -- in accuracy, while reducing activities to at most 3.74 bits on average with 1.26 significant bits multiplying each weight. We discuss how these methods can significantly improve the energy efficiency of neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  note = {Comment: 8 pages, 7 page supplementary},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@voelkerSpikePerformanceTraining2021.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/voelkerSpikePerformanceTraining2021 - Comment 8 pages, 7 page supplementary.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/voelkerSpikePerformanceTraining2021-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3RNXY9TL/Voelker et al_2021_A Spike in Performance.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/UN8EPBJ3/2002.html}
}

@article{vogelsSignalPropagationLogic2005,
  title = {Signal {{Propagation}} and {{Logic Gating}} in {{Networks}} of {{Integrate-and-Fire Neurons}}},
  author = {Vogels, Tim P. and Abbott, L. F.},
  year = {2005},
  month = nov,
  journal = {Journal of Neuroscience},
  volume = {25},
  number = {46},
  pages = {10786--10795},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3508-05.2005},
  abstract = {Transmission of signals within the brain is essential for cognitive function, but it is not clear how neural circuits support reliable and accurate signal propagation over a sufficiently large dynamic range. Two modes of propagation have been studied: synfire chains, in which synchronous activity travels through feedforward layers of a neuronal network, and the propagation of fluctuations in firing rate across these layers. In both cases, a sufficient amount of noise, which was added to previous models from an external source, had to be included to support stable propagation. Sparse, randomly connected networks of spiking model neurons can generate chaotic patterns of activity. We investigate whether this activity, which is a more realistic noise source, is sufficient to allow for signal transmission. We find that, for rate-coded signals but not for synfire chains, such networks support robust and accurate signal reproduction through up to six layers if appropriate adjustments are made in synaptic strengths. We investigate the factors affecting transmission and show that multiple signals can propagate simultaneously along different pathways. Using this feature, we show how different types of logic gates can arise within the architecture of the random network through the strengthening of specific synapses.},
  chapter = {Behavioral/Systems/Cognitive},
  copyright = {Copyright \textcopyright{} 2005 Society for Neuroscience 0270-6474/05/2510786-10.00/0},
  langid = {english},
  pmid = {16291952},
  keywords = {integrate-and-fire neurons,logic gates,network models,propagation,rate coding,sensory processing,snn,software,synfire chains},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/I2J5BEB6/Vogels_Abbott_2005_Signal Propagation and Logic Gating in Networks of Integrate-and-Fire Neurons.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZWYYY7TI/10786.html}
}

@misc{voigtMetabolicFunctionalConnectivity2021,
  title = {Metabolic and Functional Connectivity Provide Unique and Complementary Insights into Cognition-Connectome Relationships},
  author = {Voigt, Katharina and Liang, Emma and Misic, Bratislav and Ward, Phillip and Egan, Gary and Jamadar, Sharna D.},
  year = {2021},
  month = sep,
  pages = {2021.09.06.459214},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.09.06.459214},
  abstract = {A major challenge in current cognitive neuroscience is how functional brain connectivity gives rise to human cognition. Functional magnetic resonance imaging (fMRI) describes brain connectivity based on cerebral oxygenation dynamics (hemodynamic connectivity), whereas [18 F]-fluorodeoxyglucose functional positron emission tomography (FDG-fPET) describes brain connectivity based on cerebral glucose uptake (metabolic connectivity), each providing a unique characterisation of the human brain. How these two modalities differ in their contribution to cognition and behaviour is unclear. We used simultaneous resting-state FDG-fPET/fMRI to investigate how hemodynamic connectivity and metabolic connectivity relate to cognitive function by applying partial least squares analyses. Results revealed that while for both modalities the frontoparietal anatomical subdivisions related the strongest to cognition, using hemodynamic measures this network expressed executive functioning, episodic memory, and depression, while for metabolic measures this network exclusively expressed executive functioning. These findings demonstrate the unique advantages that simultaneous FDG-PET/fMRI has to provide a comprehensive understanding of the neural mechanisms that underpin cognition and highlights the importance of multimodality imaging in cognitive neuroscience research.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Voigt et al_2021_Metabolic and functional connectivity provide unique and complementary insights.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6I35FZDB/2021.09.06.html}
}

@article{vonbertalanffyGeneralSystemsTheory2008,
  title = {General Systems Theory: {{Foundations}}, Developments, Applications},
  author = {{von Bertalanffy}, Ludwig and Sutherland, John W.},
  year = {2008},
  month = jul,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-4},
  number = {6},
  pages = {592--592},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9472},
  doi = {10.1109/tsmc.1974.4309376},
  abstract = {This is a PDF-only article. The first page of the PDF of this article appears below.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5A6CV9YL/von Bertalanffy, Sutherland - 2008 - General Systems Theory Foundations, Developments, Applications.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/M2NNMFWW/von Bertalanffy, Sutherland - 2008 - General Systems Theory Foundations, Developments, Applications.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@vonbertalanffyGeneralSystemsTheory2008.md}
}

@techreport{vonbertalanffyGeneralSystemTheory,
  title = {General {{System Theory Foundations}}, {{Development}}, {{Applications Revised Edition}}},
  author = {Von Bertalanffy, Ludwig and Braziller, George and York, New},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ALF44VZA/Von Bertalanffy, Braziller, York - Unknown - General System Theory Foundations, Development, Applications Revised Edition.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@vonbertalanffyGeneralSystemTheory.md}
}

@article{wagenaarExtremelyRichRepertoire2006,
  title = {An Extremely Rich Repertoire of Bursting Patterns during the Development of Cortical Cultures},
  author = {Wagenaar, Daniel A. and Pine, Jerome and Potter, Steve M.},
  year = {2006},
  month = feb,
  journal = {BMC Neuroscience},
  volume = {7},
  number = {1},
  pages = {11},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-7-11},
  abstract = {We have collected a comprehensive set of multi-unit data on dissociated cortical cultures. Previous studies of the development of the electrical activity of dissociated cultures of cortical neurons each focused on limited aspects of its dynamics, and were often based on small numbers of observed cultures. We followed 58 cultures of different densities \textendash{} 3000 to 50,000 neurons on areas of 30 to 75 mm2 \textendash{} growing on multi-electrode arrays (MEAs) during the first five weeks of their development.},
  keywords = {Burst Size,Cortical Culture,Dense Culture,Firing Rate,Spike Count},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PTZGMKIX/Wagenaar et al. - 2006 - An extremely rich repertoire of bursting patterns .pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@wagenaarExtremelyRichRepertoire2006.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JB4K5PEU/1471-2202-7-11.html}
}

@article{wangDiscoveringGridcellModels2016,
  title = {Discovering Grid-Cell Models through Evolutionary Computation},
  author = {Wang, Lin and Yang, Bo and Orchard, Jeff},
  year = {2016},
  journal = {2016 IEEE Congress on Evolutionary Computation, CEC 2016},
  pages = {4683--4690},
  publisher = {{IEEE}},
  issn = {9781509006229},
  doi = {10.1109/CEC.2016.7744388},
  abstract = {One of the main tasks in neuroscience research is to\textbackslash ninterpret the activity of neurons. Given some\textbackslash nneuroscientific data, such as spike trains, one tries\textbackslash nto decipher how the activity of the neurons relate to\textbackslash nthe outside world and/or the behaviour of the animal.\textbackslash nThe discovery of place cells and grid cells are great\textbackslash nexamples - discoveries that garnered a Nobel Prize in\textbackslash n2014. However, the spatial patterns exhibited by such\textbackslash ncells are only the beginning of our understanding of\textbackslash nspatial representation in the brain. In this paper, we\textbackslash napply an evolutionary algorithm to discover spatial\textbackslash npatterns exhibited in cells from the entorhinal cortex\textbackslash nto see (1) if we can automatically deduce an accurate\textbackslash nmodel for the hexagonal-grid pattern, and (2) if we can\textbackslash ndiscover a more general model that also incorporates\textbackslash ngrid-cell-like variants that have been observed, but\textbackslash nnot understood.},
  keywords = {Genetic Expression Programming,Grid Cell,Neuroscience,Particle Swarm Optimization},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SBJMX89K/Wang, Yang, Orchard - 2016 - Discovering grid-cell models through evolutionary computation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@wangDiscoveringGridcellModels2016.md}
}

@article{wangSupervisedLearningSpiking2020,
  title = {Supervised Learning in Spiking Neural Networks: {{A}} Review of Algorithms and Evaluations},
  shorttitle = {Supervised Learning in Spiking Neural Networks},
  author = {Wang, Xiangwen and Lin, Xianghong and Dang, Xiaochao},
  year = {2020},
  month = may,
  journal = {Neural Networks},
  volume = {125},
  pages = {258--280},
  issn = {08936080},
  doi = {10.1016/j.neunet.2020.02.011},
  langid = {english}
}

@article{wangThetaSequencesAre2015,
  title = {Theta Sequences Are Essential for Internally Generated Hippocampal Firing Fields},
  author = {Wang, Yingxue and Romani, Sandro and Lustig, Brian and Leonardo, Anthony and Pastalkova, Eva},
  year = {2015},
  journal = {Nature Neuroscience},
  volume = {18},
  number = {2},
  pages = {282--288},
  issn = {1546-1726 (Electronic)\textbackslash r1097-6256 (Linking)},
  doi = {10.1038/nn.3904},
  abstract = {Sensory cue inputs and memory-related internal brain activities govern the firing of hippocampal neurons, but which specific firing patterns are induced by either of the two processes remains unclear. We found that sensory cues guided the firing of neurons in rats on a timescale of seconds and supported the formation of spatial firing fields. Independently of the sensory inputs, the memory-related network activity coordinated the firing of neurons not only on a second-long timescale, but also on a millisecond-long timescale, and was dependent on medial septum inputs. We propose a network mechanism that might coordinate this internally generated firing. Overall, we suggest that two independent mechanisms support the formation of spatial firing fields in hippocampus, but only the internally organized system supports short-timescale sequential firing and episodic memory.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IYLH98IU/Wang et al. - 2015 - Theta sequences are essential for internally generated hippocampal firing fields.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@wangThetaSequencesAre2015.md}
}

@article{wardThalamicDynamicCore2011,
  title = {The Thalamic Dynamic Core Theory of Conscious Experience},
  author = {Ward, Lawrence M.},
  year = {2011},
  month = jun,
  journal = {Consciousness and Cognition},
  volume = {20},
  number = {2},
  pages = {464--486},
  doi = {10.1016/j.concog.2011.01.007},
  abstract = {I propose that primary conscious awareness arises from synchronized activity in dendrites of neurons in dorsal thalamic nuclei, mediated particularly by inhibitory interactions with thalamic reticular neurons. In support, I offer four evidential pillars: (1) consciousness is restricted to the results of cortical computations; (2) thalamus is the common locus of action of brain injury in vegetative state and of general anesthetics; (3) the anatomy and physiology of the thalamus imply a central role in consciousness; (4) neural synchronization is a neural correlate of consciousness. \textcopyright{} 2011 Elsevier Inc.},
  keywords = {Awareness,Consciousness,Cortico-thalamic,Dynamic core,NCC,Neural synchrony,Thalamo-cortical,Thalamus},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LMJABXAZ/Ward - 2011 - The thalamic dynamic core theory of conscious experience.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@wardThalamicDynamicCore2011.md}
}

@article{watanabeAsymptoticEquivalenceBayes2010,
  title = {Asymptotic {{Equivalence}} of {{Bayes Cross Validation}} and {{Widely Applicable Information Criterion}} in {{Singular Learning Theory}}},
  author = {Watanabe, Sumio},
  year = {2010},
  month = apr,
  abstract = {In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to \$2{$\lambda$}/n\$, where \${$\lambda\$$} is the real log canonical threshold and \$n\$ is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NBMM9G7Y/Watanabe - 2010 - Asymptotic Equivalence of Bayes Cross Validation a.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@watanabeAsymptoticEquivalenceBayes2010.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/3IW6S4H9/1004.html;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8ETXXPMG/1004.html}
}

@article{wattsCollectiveDynamicsSmallworld1998,
  title = {Collective Dynamics of `Small-World' Networks},
  author = {Watts, Duncan J. and Strogatz, Steven H.},
  year = {1998},
  month = jun,
  journal = {Nature},
  volume = {393},
  number = {6684},
  pages = {440--442},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/30918},
  abstract = {Networks of coupled dynamical systems have been used to model biological oscillators1,2,3,4, Josephson junction arrays5,6, excitable media7, neural networks8,9,10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks `rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them `small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.},
  copyright = {1998 Macmillan Magazines Ltd.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4QNGMHZI/Watts and Strogatz - 1998 - Collective dynamics of ‘small-world’ networks.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TZ5FGNV4/Watts and Strogatz - 1998 - Collective dynamics of ‘small-world’ networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@wattsCollectiveDynamicsSmallworld1998.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5S6PMCUX/30918..html}
}

@article{weerasingheEnsemblePlasticityNetwork2022,
  title = {Ensemble Plasticity and Network Adaptability in {{SNNs}}},
  author = {Weerasinghe, Mahima Milinda Alwis and Parry, D. and Wang, Grace I. and Whalley, Jacqueline},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2203.07039},
  abstract = {A novel ensemble learning method based on entropy and network activation is introduced, which is amalgamated with a spike-rate neuron pruning technique, operated exclusively using spiking activity to investigate spike regulation using ensemble learning in an information theoretic approach. Artificial Spiking Neural Networks (ASNNs) promise greater information processing efficiency because of discrete event-based (i.e., spike) computation. Several Machine Learning (ML) applications use biologically inspired plasticity mechanisms as unsupervised learning techniques to increase the robustness of ASNNs while preserving efficiency. Spike Time Dependent Plasticity (STDP) and Intrinsic Plasticity (IP) (i.e., dynamic spiking threshold adaptation) are two such mechanisms that have been combined to form an ensemble learning method. However, it is not clear how this ensemble learning should be regulated based on spiking activity. Moreover, previous studies have attempted threshold based synaptic pruning following STDP, to increase inference efficiency at the cost of performance in ASNNs. However, this type of structural adaptation, that employs individual weight mechanisms, does not consider spiking activity for pruning which is a better representation of input stimuli. In this study, we aimed to investigate spike regulation using ensemble learning in an information theoretic approach, to optimize ASNNs for spatiotemporal data classification. We envisaged that plasticity-based spike-regulation and spike-based pruning will result in ASSNs that perform better in low resource situations. In this paper, a novel ensemble learning method based on entropy and network activation is introduced, which is amalgamated with a spike-rate neuron pruning technique, operated exclusively using spiking activity. Two electroencephalography (EEG) datasets are used as the input for classification experiments with a three-layer feed forward ASNN trained using one-pass learning. During the learning process, we observed neurons assembling into a hierarchy of clusters based on spiking rate. It was discovered that pruning lower spike-rate neuron clusters resulted in increased generalization or a predictable decline in performance. Moreover, the networks demonstrated avalanche-like spiking activity under ensembled learning. The findings of this study draw attention to the ability of ensemble learning to push ASNNs towards criticality and of neuron pruning to allow ASNNs to `forget' unimportant information. The results of this work illustrate how plasticity-based ensemble learning in conjunction with pruning can lead to increased robustness and efficiency of ASNNs in ML applications.}
}

@article{weigtIdentificationDirectResidue2009,
  title = {Identification of Direct Residue Contacts in Protein-Protein Interaction by Message Passing},
  author = {Weigt, Martin and White, Robert A. and Szurmant, Hendrik and Hoch, James A. and Hwa, Terence},
  year = {2009},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {106},
  number = {1},
  pages = {67--72},
  publisher = {{National Academy of Sciences}},
  doi = {10.1073/pnas.0805923106},
  abstract = {Understanding the molecular determinants of specificity in protein-protein interaction is an outstanding challenge of postgenome biology. The availability of large protein databases generated from sequences of hundreds of bacterial genomes enables various statistical approaches to this problem. In this context covariance-based methods have been used to identify correlation between amino acid positions in interacting proteins. However, these methods have an important shortcoming, in that they cannot distinguish between directly and indirectly correlated residues. We developed a method that combines covariance analysis with global inference analysis, adopted from use in statistical physics. Applied to a set of {$>$}2,500 representatives of the bacterial two-component signal transduction system, the combination of covariance with global inference successfully and robustly identified residue pairs that are proximal in space without resorting to ad hoc tuning parameters, both for heterointeractions between sensor kinase (SK) and response regulator (RR) proteins and for homointeractions between RR proteins. The spectacular success of this approach illustrates the effectiveness of the global inference approach in identifying direct interaction based on sequence information alone. We expect this method to be applicable soon to interaction surfaces between proteins present in only 1 copy per genome as the number of sequenced genomes continues to expand. Use of this method could significantly increase the potential targets for therapeutic intervention, shed light on the mechanism of protein-protein interaction, and establish the foundation for the accurate prediction of interacting protein partners. \textcopyright{} 2008 by The National Academy of Sciences of the USA.},
  keywords = {Covariance,Global inference,Mutual information,Signal transduction,Two-component system},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/R9LYQ2AH/Weigt et al. - 2009 - Identification of direct residue contacts in protein-protein interaction by message passing.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@weigtIdentificationDirectResidue2009.md}
}

@article{weinsteinHighthroughputSequencingZebrafish2009,
  title = {High-Throughput Sequencing of the Zebrafish Antibody Repertoire},
  author = {Weinstein, Joshua A. and Jiang, Ning and White, Richard A. and Fisher, Daniel S. and Quake, Stephen R.},
  year = {2009},
  month = may,
  journal = {Science},
  volume = {324},
  number = {5928},
  pages = {807--810},
  publisher = {{Harvard Univ. Press}},
  doi = {10.1126/science.1170020},
  abstract = {Despite tremendous progress in understanding the nature of the immune system, the full diversity of an organism's antibody repertoire is unknown. We used high-throughput sequencing of the variable domain of the antibody heavy chain from 14 zebrafish to analyze VD] usage and antibody sequence. Zebrafish were found to use between 50 and 86\% of all possible VD] combinations and shared a similar frequency distribution, with some correlation of VD] patterns between individuals. Zebrafish antibodies retained a few thousand unique heavy chains that also exhibited a shared frequency distribution. We found evidence of convergence, in which different individuals made the same antibody. This approach provides insight into the breadth of the expressed antibody repertoire and immunological diversity at the level of an individual organism.},
  keywords = {11 Single-letter abbreviations for the amino acid residues are as follows: A; Ala,and Y; Tyr,C; Cys,D; Asp,E; Glu,F; Phe,G; Gly,H; His,I; Ile,K; Lys,L; Leu,M; Met,N; Asn,P; Pro,Q; Gln,R; Arg,S; Ser,T; Thr,V; Val,W; Trp},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/PKWDWXD4/Weinstein et al. - 2009 - High-throughput sequencing of the zebrafish antibody repertoire.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@weinsteinHighthroughputSequencingZebrafish2009.md}
}

@misc{weirStructuralFunctionalDynamics,
  title = {Structural and Functional Dynamics of Healthy and Perturbed Neural Networks in Vitro},
  author = {Weir, Janelle},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/B5GAN5BF/Project description_PhD Socrates.docx}
}

@article{wenningOutputVariabilityAnimals2018,
  title = {Output Variability across Animals and Levels in a Motor System},
  author = {Wenning, Angela and Norris, Brian J. and G{\"u}nay, Cengiz and Kueh, Daniel and Calabrese, Ronald L.},
  year = {2018},
  journal = {eLife},
  volume = {7},
  pages = {7--9},
  issn = {2059348242},
  doi = {10.7554/eLife.31123},
  abstract = {Rhythmic behaviors vary across individuals. We investigated the sources of this output variability across a motor system, from the central pattern generator (CPG) to the motor plant. In the bilaterally symmetric leech heartbeat system, the CPG orchestrates two coordinations in the bilateral hearts with different intersegmental phase relations ({$\Delta\phi$}) and periodic side-to-side switches. Population variability is large. We show that the system is precise within a coordination, that differences in repetitions of a coordination contribute little to population output variability, but that differences between bilaterally homologous cells may contribute to some of this variability. Nevertheless, much output variability is likely associated with genetic and life history differences among individuals. Variability of {$\Delta\phi$} were coordination-specific: similar at all levels in one, but significantly lower for the motor pattern than the CPG pattern in the other. Mechanisms that transform CPG output to motor neurons may limit output variability in the motor pattern.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TXJMRACY/Wenning et al. - 2018 - Output variability across animals and levels in a motor system.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@wenningOutputVariabilityAnimals2018.md}
}

@article{westExperiencingSliceSky2018,
  title = {Experiencing a {{Slice}} of the {{Sky}}: {{Immersive Rendering}} and {{Sonification}} of {{Antarctic Astronomy Data}}},
  shorttitle = {Experiencing a {{Slice}} of the {{Sky}}},
  author = {West, Ruth and Johnson, Violet and Yeh, I Chen and Thomas, Zach and Tarlton, Mike},
  year = {2018},
  month = jan,
  journal = {Electronic Imaging},
  volume = {2018},
  number = {3},
  pages = {449-1-449-10},
  doi = {10.2352/ISSN.2470-1173.2018.03.ERVR-449},
  abstract = {We are creating INSTRUMENT: One Antarctic Night, a performative, multi-participant, and reconfigurable virtual reality artwork. We describe development of a large scale immersive star field from data collected by the AST3 (Antarctic Survey Telescope) robotic telescopes at Dome A of 817,373 astronomical objects from within the Large Magellanic Cloud within a game-engine based virtual environment. Real-time database queries, selections, and filtering operations enable participants to collaboratively interact with the star field to create dataremixes from astronomical data. Additionally, they facilitate collaborative creation of a soundscape via ambisonic audio spatialization and interactive sonification of the data utilizing data-driven granular synthesis. We evaluate the scalability of our approach and demonstrate that it maintains interactive frame rates at datasets with millions of astronomical objects, with each object being both individually manipulable or selectable and manipulable within subsets. Our user interface/interaction prototypes include a controller-attached UI and a wave/ripple based interaction where users grab hold of the star field and propagate waves and ripples throughout the virtual world. Our work arises from the art-science practice of dataremix: the appropriation and recombination (remixing) of data in any and all states along the continuum of its transformation from raw to processed to create new meaning.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DNHMPUS6/West et al. - 2018 - Experiencing a Slice of the Sky Immersive Renderi.pdf}
}

@article{widloskiInferringCircuitMechanisms2018,
  title = {Inferring Circuit Mechanisms from Sparse Neural Recording and Global Perturbation in Grid Cells},
  author = {Widloski, John and Marder, Michael P and Fiete, Ila R},
  year = {2018},
  journal = {eLife},
  volume = {7},
  number = {2},
  pages = {1--27},
  doi = {10.7554/elife.33503},
  abstract = {A goal of systems neuroscience is to discover the circuit mechanisms underlying brain function. Despite experimental advances that enable circuit-wide neural recording, the problem remains open in part because solving the `inverse problem' of inferring circuity and mechanism by merely observing activity is hard. In the grid cell system, we show through modeling that a technique based on global circuit perturbation and examination of a novel theoretical object called the distribution of relative phase shifts (DRPS) could reveal the mechanisms of a cortical circuit at unprecedented detail using extremely sparse neural recordings. We establish feasibility, showing that the method can discriminate between recurrent versus feedforward mechanisms and amongst various recurrent mechanisms using recordings from a handful of cells. The proposed strategy demonstrates that sparse recording coupled with simple perturbation can reveal more about circuit mechanism than can full knowledge of network activity or the synaptic connectivity matrix.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/W2BM7KEX/Widloski, Marder, Fiete - 2018 - Inferring circuit mechanisms from sparse neural recording and global perturbation in grid cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@widloskiInferringCircuitMechanisms2018.md}
}

@article{widloskiModelGridCell2014,
  title = {A Model of Grid Cell Development through Spatial Exploration and Spike Time-Dependent Plasticity},
  author = {Widloski, John and Fiete, Ila R.},
  year = {2014},
  journal = {Neuron},
  volume = {83},
  number = {2},
  pages = {481--495},
  publisher = {{Elsevier Inc.}},
  doi = {10.1016/j.neuron.2014.06.018},
  abstract = {Grid cell responses develop gradually after eye opening, but little is known about the rules that govern this process. We present a biologically plausible model for the formation of a grid cell network. An asymmetric spike time-dependent plasticity rule acts upon an initially unstructured network of spiking neurons that receive inputs encoding animal velocity and location. Neurons develop an organized recurrent architecture based on the similarity of their inputs, interacting through inhibitory interneurons. The mature network can convert velocity inputs into estimates of animal location, showing that spatially periodic responses and the capacity of path integration can arise through synaptic plasticity, acting on inputs that display neither. The model provides numerous predictions about the necessity of spatial exploration for grid cell development, network topography, the maturation of velocity tuning and neural correlations, the abrupt transition to stable patterned responses, and possible mechanisms to set grid period across grid modules. \textcopyright{} 2014 Elsevier Inc.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LUU77W9M/Widloski, Fiete - 2014 - A model of grid cell development through spatial exploration and spike time-dependent plasticity.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@widloskiModelGridCell2014.md}
}

@article{widrichModernHopfieldNetworks2020,
  title = {Modern {{Hopfield Networks}} and {{Attention}} for {{Immune Repertoire Classification}}},
  author = {Widrich, Michael and Sch{\"a}fl, Bernhard and Ramsauer, Hubert and Pavlovi{\'c}, Milena and Gruber, Lukas and Holzleitner, Markus and Brandstetter, Johannes and Sandve, Geir Kjetil and Greiff, Victor and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.13505 [cs, q-bio, stat]},
  eprint = {2007.13505},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  abstract = {A central mechanism in machine learning is to identify, store, and recognize patterns. How to learn, access, and retrieve such patterns is crucial in Hopfield networks and the more recent transformer architectures. We show that the attention mechanism of transformer architectures is actually the update rule of modern Hopfield networks that can store exponentially many patterns. We exploit this high storage capacity of modern Hopfield networks to solve a challenging multiple instance learning (MIL) problem in computational biology: immune repertoire classification. Accurate and interpretable machine learning methods solving this problem could pave the way towards new vaccines and therapies, which is currently a very relevant research topic intensified by the COVID-19 crisis. Immune repertoire classification based on the vast number of immunosequences of an individual is a MIL problem with an unprecedentedly massive number of instances, two orders of magnitude larger than currently considered problems, and with an extremely low witness rate. In this work, we present our novel method DeepRC that integrates transformer-like attention, or equivalently modern Hopfield networks, into deep learning architectures for massive MIL such as immune repertoire classification. We demonstrate that DeepRC outperforms all other methods with respect to predictive performance on large-scale experiments, including simulated and real-world virus infection data, and enables the extraction of sequence motifs that are connected to a given disease class. Source code and datasets: https://github.com/ml-jku/DeepRC},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Statistics - Machine Learning},
  note = {Comment: 10 pages (+appendix); Source code and datasets: https://github.com/ml-jku/DeepRC},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/T59TREJ3/Widrich et al. - 2020 - Modern Hopfield Networks and Attention for Immune .pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@widrichModernHopfieldNetworks2020.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/NJMSC96I/2007.html}
}

@article{willettHighperformanceBraintotextCommunication2021,
  title = {High-Performance Brain-to-Text Communication via Handwriting},
  author = {Willett, Francis R. and Avansino, Donald T. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V.},
  year = {2021},
  month = may,
  journal = {Nature},
  volume = {593},
  number = {7858},
  pages = {249--254},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03506-2},
  abstract = {Brain\textendash computer interfaces (BCIs) can restore communication to people who have lost the ability to move or speak. So far, a major focus of BCI research has been on restoring gross motor skills, such as reaching and grasping1\textendash 5 or point-and-click typing with a computer cursor6,7. However, rapid sequences of highly dexterous behaviours, such as handwriting or touch typing, might enable faster rates of communication. Here we developed an intracortical BCI that decodes attempted handwriting movements from neural activity in the motor cortex and translates it to text in real time, using a recurrent neural network decoding approach. With this BCI, our study participant, whose hand was paralysed from spinal cord injury, achieved typing speeds of 90~characters per minute with 94.1\% raw accuracy online, and greater than 99\% accuracy offline with a general-purpose autocorrect. To our knowledge, these typing speeds exceed those reported for any other BCI, and are comparable to typical smartphone typing speeds of individuals in the age group of our participant (115~characters per minute)8. Finally, theoretical considerations explain why temporally complex movements, such as handwriting, may be fundamentally easier to decode than point-to-point movements. Our results open a new approach for BCIs and demonstrate the feasibility of accurately decoding rapid, dexterous movements years after paralysis.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Brain\textendash machine interface;Motor cortex Subject\_term\_id: brain-machine-interface;motor-cortex},
  file = {C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@willettHighperformanceBraintotextCommunication2021.md;C\:\\Users\\Max\\Insync\\m@tarlton.info\\Google Drive\\Zotero\\storage\\Willett et al_2021_High-performance brain-to-text communication via handwriting.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZXLN4U35/s41586-021-03506-2.html}
}

@article{williamsNeuralBurstCodes2021,
  title = {Neural Burst Codes Disguised as Rate Codes},
  author = {Williams, Ezekiel and Payeur, Alexandre and Gidon, Albert and Naud, Richard},
  year = {2021},
  month = aug,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {15910},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-95037-z},
  abstract = {The burst coding hypothesis posits that the occurrence of sudden high-frequency patterns of action potentials constitutes a salient syllable of the neural code. Many neurons, however, do not produce clearly demarcated bursts, an observation invoked to rule out the pervasiveness of this coding scheme across brain areas and cell types. Here we ask how detrimental ambiguous spike patterns, those that are neither clearly bursts nor isolated spikes, are for neuronal information transfer. We addressed this question using information theory and computational simulations. By quantifying how information transmission depends on firing statistics, we found that the information transmitted is not strongly influenced by the presence of clearly demarcated modes in the interspike interval distribution, a feature often used to identify the presence of burst coding. Instead, we found that neurons having unimodal interval distributions were still able to ascribe different meanings to bursts and isolated spikes. In this regime, information transmission depends on dynamical properties of the synapses as well as the length and relative frequency of bursts. Furthermore, we found that common metrics used to quantify burstiness were unable to predict the degree with which bursts could be used to carry information. Our results provide guiding principles for the implementation of coding strategies based on spike-timing patterns, and show that even unimodal firing statistics can be consistent with a bivariate neural code.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Applied mathematics,Computational science,Neural decoding,Neural encoding},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Williams et al_2021_Neural burst codes disguised as rate codes.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/L5A9C6IB/s41598-021-95037-z.html}
}

@article{williamsonBridgingLargescaleNeuronal2019,
  title = {Bridging Large-Scale Neuronal Recordings and Large-Scale Network Models Using Dimensionality Reduction},
  author = {Williamson, Ryan C. and Doiron, Brent and Smith, Matthew A. and Yu, Byron M.},
  year = {2019},
  journal = {Current Opinion in Neurobiology},
  volume = {55},
  pages = {40--47},
  publisher = {{Elsevier Ltd}},
  doi = {10.1016/j.conb.2018.12.009},
  abstract = {A long-standing goal in neuroscience has been to bring together neuronal recordings and neural network modeling to understand brain function. Neuronal recordings can inform the development of network models, and network models can in turn provide predictions for subsequent experiments. Traditionally, neuronal recordings and network models have been related using single-neuron and pairwise spike train statistics. We review here recent studies that have begun to relate neuronal recordings and network models based on the multi-dimensional structure of neuronal population activity, as identified using dimensionality reduction. This approach has been used to study working memory, decision making, motor control, and more. Dimensionality reduction has provided common ground for incisive comparisons and tight interplay between neuronal recordings and network models.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/CEWXU2GN/Williamson et al. - 2019 - Bridging large-scale neuronal recordings and large-scale network models using dimensionality reduction.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@williamsonBridgingLargescaleNeuronal2019.md}
}

@article{williamsSimpleStatisticalGradientfollowing1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  year = {1992},
  month = may,
  journal = {Machine Learning},
  volume = {8},
  number = {3},
  pages = {229--256},
  issn = {1573-0565},
  doi = {10.1007/BF00992696},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  langid = {english},
  keywords = {connectionist networks,gradient descent,mathematical analysis,Reinforcement learning},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MEIE9S2E/Williams_1992_Simple statistical gradient-following algorithms for connectionist.pdf}
}

@article{wilsonInfluenceMultipleTemporal2015,
  title = {The Influence of Multiple Temporal Memories in the Peak-Interval Procedure},
  author = {Wilson, A. George and Matell, Matthew S. and Crystal, Jonathon D.},
  year = {2015},
  month = jun,
  journal = {Learning \& Behavior},
  volume = {43},
  number = {2},
  pages = {153--162},
  issn = {1543-4508},
  doi = {10.3758/s13420-015-0169-y},
  abstract = {Memories for when an event has occurred are used to anticipate future occurrences of the event, but what happens when the event is equally likely to occur at two different times? In this study, one group of rats was always reinforced at 21~s on the peak-interval procedure (21-only group), whereas another group of rats was reinforced at either 8 or 21~s, which varied daily (8\textendash 21 group). At the beginning of each session, the behavior of the 8\textendash 21 group largely lacked temporal control, but by the end of the session, temporal control was reestablished. When both groups were reinforced at 21~s, the patterns of responding were indistinguishable after subjects in the 8\textendash 21 group had experienced 13 reinforcement trials. Finally, the reinforcement times of previous sessions affected the 8\textendash 21 group, such that subjects were biased depending on the reinforcement time of the prior session. These results show that when the reinforcement time is initially ambiguous, rats respond in a way that combines their expectations of both possibilities; then they incrementally adjust their responding as they receive more information, but still information from prior sessions biases their initial expectation for the reinforcement time. Combined, these results imply that rats are sensitive to the age of encoded temporal memories in an environment in which the reinforcement time is variable. How these results inform the scalar expectancy theory, the currently accepted model of interval-timing behavior, is discussed.},
  langid = {english},
  keywords = {Interval timing,Multiple memories,Peak-interval procedure,Rat,Temporal control,Temporal memory},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/S674AYV8/Wilson et al_2015_The influence of multiple temporal memories in the peak-interval procedure.pdf}
}

@article{wilsonLimitCycleDynamics2019,
  title = {Limit Cycle Dynamics Can Guide the Evolution of Gene Regulatory Networks towards Point Attractors},
  author = {Wilson, Stuart P. and James, Sebastian S. and Whiteley, Daniel J. and Krubitzer, Leah A.},
  year = {2019},
  month = nov,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {16750},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-53251-w},
  abstract = {Developmental dynamics in Boolean models of gene networks self-organize, either into point attractors (stable repeating patterns of gene expression) or limit cycles (stable repeating sequences of patterns), depending on the network interactions specified by a genome of evolvable bits. Genome specifications for dynamics that can map specific gene expression patterns in early development onto specific point attractor patterns in later development are essentially impossible to discover by chance mutation alone, even for small networks. We show that selection for approximate mappings, dynamically maintained in the states comprising limit cycles, can accelerate evolution by at least an order of magnitude. These results suggest that self-organizing dynamics that occur within lifetimes can, in principle, guide natural selection across lifetimes.},
  copyright = {2019 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Evolutionary theory;Gene regulatory networks Subject\_term\_id: evolutionary-theory;gene-regulatory-networks},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Wilson et al_2019_Limit cycle dynamics can guide the evolution of gene regulatory networks.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@wilsonLimitCycleDynamics2019.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/B4VQVVEK/s41598-019-53251-w.html}
}

@article{wimmerRewardLearningWorking2022,
  title = {Reward Learning and Working Memory: {{Effects}} of Massed versus Spaced Training and Post-Learning Delay Period},
  shorttitle = {Reward Learning and Working Memory},
  author = {Wimmer, G. Elliott and Poldrack, Russell A.},
  year = {2022},
  month = feb,
  journal = {Memory \& Cognition},
  volume = {50},
  number = {2},
  pages = {312--324},
  issn = {1532-5946},
  doi = {10.3758/s13421-021-01233-7},
  abstract = {Neuroscience research has illuminated the mechanisms supporting learning from reward feedback, demonstrating a critical role for the striatum and midbrain dopamine system. However, in humans, short-term working memory that is dependent on frontal and parietal cortices can also play an important role, particularly in commonly used paradigms in which learning is relatively condensed in time. Given the growing use of reward-based learning tasks in translational studies in computational psychiatry, it is important to understand the extent of the influence of~working memory and also~how core~gradual learning mechanisms can be better isolated. In our experiments, we manipulated the spacing between repetitions along with a post-learning delay preceding a test phase. We found that learning was slower for stimuli repeated after a long delay (spaced-trained) compared to those repeated immediately (massed-trained), likely reflecting the remaining contribution of feedback learning mechanisms when working memory is not available. For massed learning,~brief interruptions led to drops in subsequent performance,~and individual differences in working memory capacity positively correlated with overall~performance. Interestingly, when tested~after a delay period but not immediately, relative preferences decayed in the massed condition and increased in the spaced condition. Our results provide additional support for a large role of working memory in reward-based learning in temporally condensed designs. We suggest that spacing training within or between sessions is a promising approach to better isolate and understand mechanisms supporting gradual reward-based learning, with particular importance for understanding potential learning dysfunctions in addiction and psychiatric disorders.},
  langid = {english},
  keywords = {Decision-making,Habit,Reinforcement learning,Reward,Spacing,Working memory},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/L955HYWV/Wimmer_Poldrack_2022_Reward learning and working memory.pdf}
}

@article{woulfeNuclearBodiesNeurodegenerative2008,
  title = {Nuclear Bodies in Neurodegenerative Disease},
  author = {Woulfe, John},
  year = {2008},
  month = nov,
  journal = {Biochimica Et Biophysica Acta},
  volume = {1783},
  number = {11},
  pages = {2195--2206},
  issn = {0006-3002},
  doi = {10.1016/j.bbamcr.2008.05.005},
  abstract = {Neurodegenerative diseases are characterized by a relentlessly progressive loss of the functional and structural integrity of the central nervous system. In many cases, these diseases arise sporadically and the causes are unknown. The abnormal aggregation of protein within the cytoplasm or the nucleus of brain cells represents a unifying pathological feature of these diseases. There is increasing evidence for nuclear dysfunction in neurodegenerative diseases. How this relates to protein aggregation in the context of "cause and effect" remains to be determined in most cases. Co-ordinated nuclear function is predicated on the activity of distinct nuclear subdomains, or nuclear bodies, each responsible for a specific function. If nuclear dysfunction represents an important etiopathological feature in neurodegenerative disease, then this should be reflected by functional and/or morphological alterations in this nuclear compartmentalization. For most neurodegenerative diseases, evidence for nuclear dysfunction, with attendant consequences for nuclear architecture, is only beginning to emerge. In this review, I will discuss neurodegenerative diseases in the context of nuclear dysfunction and, more specifically, alterations in nuclear bodies. Although research in this field is in its infancy, identifying alterations in the nucleus in neurodegenerative disease has potentially profound implications for elucidating the pathogenesis of these disorders.},
  langid = {english},
  pmid = {18539152},
  keywords = {Animals,Cell Nucleus,Humans,Intranuclear Inclusion Bodies,Muscular Atrophy; Spinal,Neurodegenerative Diseases}
}

@article{wuContinuousAttractorNeural2016,
  title = {Continuous {{Attractor Neural Networks}}: {{Candidate}} of a {{Canonical Model}} for {{Neural Information Representation}}.},
  author = {Wu, Si and Wong, K Y Michael and Fung, C C Alan and Mi, Yuanyuan and Zhang, Wenhao},
  year = {2016},
  journal = {F1000Research},
  volume = {5},
  publisher = {{Faculty of 1000 Ltd}},
  doi = {10.12688/f1000research.7387.1},
  abstract = {Owing to its many computationally desirable properties, the model of continuous attractor neural networks (CANNs) has been successfully applied to describe the encoding of simple continuous features in neural systems, such as orientation, moving direction, head direction, and spatial location of objects. Recent experimental and computational studies revealed that complex features of external inputs may also be encoded by low-dimensional CANNs embedded in the high-dimensional space of neural population activity. The new experimental data also confirmed the existence of the M-shaped correlation between neuronal responses, which is a correlation structure associated with the unique dynamics of CANNs. This body of evidence, which is reviewed in this report, suggests that CANNs may serve as a canonical model for neural information representation.},
  keywords = {anticipative tracking,canonical model,Continuous Attractor Neural Network,multi-sensory information integration,Neural network},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/4UHWP3S8/Wu et al. - 2016 - Continuous Attractor Neural Networks Candidate of a Canonical Model for Neural Information Representation.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@wuContinuousAttractorNeural2016.md}
}

@article{xueSpikingRecurrentNeural2022,
  title = {Spiking {{Recurrent Neural Networks Represent Task-Relevant Neural Sequences}} in {{Rule-Dependent Computation}}},
  author = {Xue, Xiaohe and Wimmer, R. D. and Halassa, Michael M. and Chen, Zhe},
  year = {2022},
  journal = {Cognitive Computation},
  doi = {10.1007/s12559-022-09994-2},
  abstract = {Semantic Scholar extracted view of "Spiking Recurrent Neural Networks Represent Task-Relevant Neural Sequences in Rule-Dependent Computation" by Xiaohe Xue et al.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/RQ5QSZHD/Xue et al_2022_Spiking Recurrent Neural Networks Represent Task-Relevant Neural Sequences in.pdf}
}

@book{xuMachineLearningEcho2020,
  title = {Machine {{Learning With Echo State Networks}}},
  author = {Xu, Yuanchao},
  year = {2020},
  month = mar,
  abstract = {This report aims at presenting a detailed technical review of Recurrent Neural Networks (RNNs) and in particular Echo State Networks (ESNs), which are networks belonging to the general class of Reservoir Computing. A general overview of RNNs is presented, followed by a detailed discussion of Reservoir Computing and Echo State Networks. We discuss the working principle, construction details, applications and limitations of ESNs. Current research work in ESNs directed towards overcoming some of the limitations faced by researchers using ESNs, is reviewed. One major limitation of ESNs is their dependence on the hyper-parameters. The stable operating region of ESNs in the hyper-parameter space is quite narrow, which makes training these networks a challenging task. The paper reviewed proposes a novel model of ESNs, which prevents the network from entering chaotic regime. The paper also demonstrates the improved performance in the experiments performed over standard benchmarking datasets for nonlinear systems. Finally, we conclude with a discussion of renewed interest in ESNs, in particular, as a computational principle.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/SNWT9NDU/Xu_2020_Machine Learning With Echo State Networks.pdf}
}

@article{yangCanStrengthsAIC2005,
  title = {Can the Strengths of {{AIC}} and {{BIC}} Be Shared? {{A}} Conflict between Model Indentification and Regression Estimation},
  shorttitle = {Can the Strengths of {{AIC}} and {{BIC}} Be Shared?},
  author = {Yang, Yuhong},
  year = {2005},
  month = dec,
  journal = {Biometrika},
  volume = {92},
  number = {4},
  pages = {937--950},
  publisher = {{Oxford Academic}},
  issn = {0006-3444},
  doi = {10.1093/biomet/92.4.937},
  abstract = {Abstract. A traditional approach to statistical inference is to identify the true or best model first with little or no consideration of the specific goal of in},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LLDEZNQL/Yang - 2005 - Can the strengths of AIC and BIC be shared A conf.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@yangCanStrengthsAIC2005.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WZGIMWQM/389439.html}
}

@techreport{yangUnderstandingVariationalLower2017,
  title = {Understanding the {{Variational Lower Bound}}},
  author = {Yang, Xitong},
  year = {2017},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/5W4PP7DU/Yang - 2017 - Understanding the Variational Lower Bound.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@yangUnderstandingVariationalLower2017.md}
}

@misc{yanliangshi[yanliangshi]ExcitedShareOur2022,
  type = {Tweet},
  title = {Excited to Share Our New Preprint on How Network Dynamics and Structured Connectivity Jointly Define the Spatial and Temporal Profiles of Neural Correlations. {{Work}} w/ @roxana\_zeraati, @{{SelfOrgAnna}}, @{{EngelTatiana}}: {{https://arxiv.org/abs/2207.07930}} {{Here}} Is a \#tweeprint: {{https://t.co/f7BjGMJRlx}}},
  author = {{Yanliang Shi [@YanliangShi]}},
  year = {2022},
  month = jul,
  journal = {Twitter},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/P9UP9MB7/1549533278192943111.html}
}

@article{yazidiAchievingFairLoad2021,
  title = {Achieving {{Fair Load Balancing}} by {{Invoking}} a {{Learning Automata-Based Two-Time-Scale Separation Paradigm}}},
  author = {Yazidi, Anis and Hassan, Ismail and Hammer, Hugo L. and Oommen, B. John},
  year = {2021},
  month = aug,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {8},
  pages = {3444--3457},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3010888},
  abstract = {In this article, we consider the problem of load balancing (LB), but, unlike the approaches that have been proposed earlier, we attempt to resolve the problem in a fair manner (or rather, it would probably be more appropriate to describe it as an {$\epsilon$}-fair manner because, although the LB can, probably, never be totally fair, we achieve this by being ``as close to fair as possible''). The solution that we propose invokes a novel stochastic learning automaton (LA) scheme, so as to attain a distribution of the load to a number of nodes, where the performance level at the different nodes is approximately equal and each user experiences approximately the same Quality of the Service (QoS) irrespective of which node that he/she is connected to. Since the load is dynamically varying, static resource allocation schemes are doomed to underperform. This is further relevant in cloud environments, where we need dynamic approaches because the available resources are unpredictable (or rather, uncertain) by virtue of the shared nature of the resource pool. Furthermore, we prove here that there is a coupling involving LA's probabilities and the dynamics of the rewards themselves, which renders the environments to be nonstationary. This leads to the emergence of the so-called property of ``stochastic diminishing rewards.'' Our newly proposed novel LA algorithm {$\epsilon$}-optimally solves the problem, and this is done by resorting to a two-time-scale-based stochastic learning paradigm. As far as we know, the results presented here are of a pioneering sort, and we are unaware of any comparable results.},
  keywords = {Cloud computing,Continuous learning automaton (LA),Convergence,fair load balancing (LB),Heuristic algorithms,Learning automata,resource allocation,Resource management,Servers,Stochastic processes},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Yazidi et al_2021_Achieving Fair Load Balancing by Invoking a Learning Automata-Based.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/IN7TCSGB/9159930.html}
}

@article{yazidiSolvingStochasticNonlinear2018,
  title = {Solving Stochastic Nonlinear Resource Allocation Problems Using Continuous Learning Automata},
  author = {Yazidi, Anis and Hammer, Hugo L.},
  year = {2018},
  month = nov,
  journal = {Applied Intelligence},
  volume = {48},
  number = {11},
  pages = {4392--4411},
  issn = {1573-7497},
  doi = {10.1007/s10489-018-1201-7},
  abstract = {This paper deals with the Stochastic Non-linear Fractional Equality Knapsack (NFEK) problem which is a fundamental resource allocation problem based on incomplete and noisy information (Granmo and Oommen, Appl Intell 33(1):3\textendash 20, 2010, IEEE Trans Comput 59(4):545\textendash 560, 2010). The NFEK problem arises in many applications such as in web polling under polling constraints, and in constrained estimation. The primary contribution of this paper is a continuous Learning Automata (LA)-based, optimal, efficient and yet simple solution to the NFEK problem. Our solution is distinct from the first-reported optimal solution to the problem due to Granmo and Oommen (Appl Intell 33(1):3\textendash 20, 2010, IEEE Trans Comput 59(4):545\textendash 560, 2010) which resorts to utilizing multiple two-action discretized LA, organized in a hierarchical manner which comes with extra implementation and computational complexity. In this work, we present an optimal solution to the problem using a continuous LA which does not involve mapping the materials onto a binary hierarchy. As opposed to the traditional family of Reward-Inaction (R-I) LA, our scheme is modified in order to accommodate non-absorbing barriers, thus guaranteeing convergence to the optimal allocation. The experimental results that we have presented for numerous simulations demonstrate the efficiency of our scheme and its superiority compared to the state-of-the-art in terms of peak performance.},
  langid = {english},
  keywords = {Continuous learning automata,Resource allocation,Stochastic non-linear fractional equality knapsack},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Yazidi_Hammer_2018_Solving stochastic nonlinear resource allocation problems using continuous.pdf}
}

@article{yazidiTwotimeScaleLearning2019,
  title = {Two-Time Scale Learning Automata: An Efficient Decision Making Mechanism for Stochastic Nonlinear Resource Allocation},
  shorttitle = {Two-Time Scale Learning Automata},
  author = {Yazidi, Anis and Hammer, Hugo L. and Jonassen, Tore M.},
  year = {2019},
  month = sep,
  journal = {Applied Intelligence},
  volume = {49},
  number = {9},
  pages = {3392--3405},
  issn = {1573-7497},
  doi = {10.1007/s10489-019-01453-0},
  abstract = {The Stochastic Non-linear Fractional Equality Knapsack (NFEK) problem is a substantial resource allocation problem which admits a large set of applications such as web polling under polling constraints, and constrained estimation. The NFEK problem is usually solved by trial and error based on noisy feedback information from the environment. The available solutions to NFEK are based on the traditional family of Reward-Inaction Learning Automata (LA) scheme where the action probabilities are updated based on only the last feedback. Such an update form seems counterproductive for two reasons: 1) it only uses the last feedback and does not consider the whole history of the feedback and 2) it ignores updates whenever the last feedback does not correspond to a reward. In this paper, we rather suggest instead a learning solution that resorts to the whole history of feedback using the theory of two time-scale separation. Through comprehensive experimental results we show that the proposed solution is not only superior to the state-of-the-art in terms of peak performance but is also robust to the choice of the tuning parameters.},
  langid = {english},
  keywords = {Continuous learning automata,Decision making under uncertainty,Resource allocation,Stochastic non-linear fractional equality knapsack,Two-time scale},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Yazidi et al_2019_Two-time scale learning automata.pdf}
}

@techreport{yedidiaMERLAMITSUBISHIELECTRIC2000,
  title = {{{MERL-A MITSUBISHI ELECTRIC RESEARCH LABORATORY An Idiosyncratic Journey Beyond Mean Field Theory}}},
  author = {Yedidia, Jonathan S and 201 Broadway, Merl},
  year = {2000},
  institution = {{MIT Press}},
  abstract = {I try to clarify the relationships between different ways of deriving or correcting mean field theory, and present "translations" between the language of physicists and that of computer scientists. The connecting thread between the different methods described here is the Gibbs free energy. After introducing the inference problem we are interested in analyzing, I will define the Gibbs free energy, and describe how to derive a mean field approximation to it using a variational approach. I will then explain how one might re-derive and correct the mean field and TAP free energies using high temperature expansions with constrained one-node beliefs. I will explore the relationships between the high-temperature expansion approach, the Bethe approximation, and the belief propagation algorithm, and point out in particular the equivalence of the Bethe approximation and belief propagation. Finally, I will describe Kikuchi approximations to the Gibbs Free energy and advertise new belief propagation algorithms that efficiently compute beliefs equivalent to those obtained from the Kikuchi free energy.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DG73VIAT/Yedidia, 201 Broadway - 2000 - MERL-A MITSUBISHI ELECTRIC RESEARCH LABORATORY An Idiosyncratic Journey Beyond Mean Field Theory.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@yedidiaMERLAMITSUBISHIELECTRIC2000.md}
}

@article{yinComparisonIntervalTiming2014,
  title = {Comparison of Interval Timing Behaviour in Mice Following Dorsal or Ventral Hippocampal Lesions with Mice Having {$\delta$}-Opioid Receptor Gene Deletion},
  author = {Yin, Bin and Meck, Warren H.},
  year = {2014},
  month = mar,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {369},
  number = {1637},
  pages = {20120466},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2012.0466},
  abstract = {Mice with cytotoxic lesions of the dorsal hippocampus (DH) underestimated 15 s and 45 s target durations in a bi-peak procedure as evidenced by proportional leftward shifts of the peak functions that emerged during training as a result of decreases in both `start' and `stop' times. In contrast, mice with lesions of the ventral hippocampus (VH) displayed rightward shifts that were immediately present and were largely limited to increases in the `stop' time for the 45 s target duration. Moreover, the effects of the DH lesions were congruent with the scalar property of interval timing in that the 15 s and 45 s functions superimposed when plotted on a relative timescale, whereas the effects of the VH lesions violated the scalar property. Mice with DH lesions also showed enhanced reversal learning in comparison to control and VH lesioned mice. These results are compared with the timing distortions observed in mice lacking {$\delta$}-opioid receptors (Oprd1-/-) which were similar to mice with DH lesions. Taken together, these results suggest a balance between hippocampal\textendash striatal interactions for interval timing and demonstrate possible functional dissociations along the septotemporal axis of the hippocampus in terms of motivation, timed response thresholds and encoding in temporal memory.},
  keywords = {basal ganglia,hippocampal–striatal interactions,motivation,scalar property,temporal memory,time perception},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DPP7HVQW/Yin_Meck_2014_Comparison of interval timing behaviour in mice following dorsal or ventral.pdf}
}

@inproceedings{yinEffectiveEfficientComputation2020,
  title = {Effective and {{Efficient Computation}} with {{Multiple-timescale Spiking Recurrent Neural Networks}}},
  booktitle = {International {{Conference}} on {{Neuromorphic Systems}} 2020},
  author = {Yin, Bojian and Corradi, Federico and Boht{\'e}, Sander M.},
  year = {2020},
  month = jul,
  series = {{{ICONS}} 2020},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3407197.3407225},
  abstract = {The emergence of brain-inspired neuromorphic computing as a paradigm for edge AI is motivating the search for high-performance and efficient spiking neural networks to run on this hardware. However, compared to classical neural networks in deep learning, current spiking neural networks lack competitive performance in compelling areas. Here, for sequential and streaming tasks, we demonstrate how a novel type of adaptive spiking recurrent neural network (SRNN) is able to achieve state-of-the-art performance compared to other spiking neural networks and almost reach or exceed the performance of classical recurrent neural networks (RNNs) while exhibiting sparse activity. From this, we calculate a {$>$} 100x energy improvement for our SRNNs over classical RNNs on the harder tasks. To achieve this, we model standard and adaptive multiple-timescale spiking neurons as self-recurrent neural units, and leverage surrogate gradients and auto-differentiation in the PyTorch Deep Learning framework to efficiently implement backpropagation-through-time, including learning of the important spiking neuron parameters to adapt our spiking neurons to the tasks.},
  isbn = {978-1-4503-8851-1},
  keywords = {backpropagation through time,spiking neural networks,surrogate gradient},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/TDSZPKZR/Yin et al_2020_Effective and Efficient Computation with Multiple-timescale Spiking Recurrent.pdf}
}

@incollection{yinIntervaltimingProtocolsTheir2017,
  title = {Interval-Timing {{Protocols}} and {{Their Relevancy}} to the {{Study}} of {{Temporal Cognition}} and {{Neurobehavioral Genetics}}},
  booktitle = {Handbook of {{Neurobehavioral Genetics}} and {{Phenotyping}}},
  author = {Yin, Bin and Lusk, Nicholas A. and Meck, Warren H.},
  year = {2017},
  pages = {179--227},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118540770.ch8},
  abstract = {Humans and other animals can be shown to process temporal information as if they use an internal stopwatch that can be ``run'', ``paused'', and ``reset'' on command and whose speed of ``ticking'' is adjustable. In addition, interval-timing behavior can be separated into ``clock'', ``memory'', and ``decision'' stages of information processing such that one stage can be modified without changing the others. Moreover, interval-timing procedures can be used to diagnose the behavioral abnormalities associated with transgenic, ``knock-out'', and ``knock-down'' mouse models of human diseases. In conjunction with interval-timing tasks, evaluation of spatial memory and emotional regulation provides the necessary information for identifying the most-likely locus of behavioral deficits in genetically modified mice. Consequently, the timing and immersive memory and emotional regulation (TIMER) test battery outlined here is recommended as a tool for behavioral phenotyping.},
  chapter = {8},
  isbn = {978-1-118-54077-0},
  langid = {english},
  keywords = {acetylcholine,calcium-response factor (CaRF),dopamine,dorsal hippocampus,emotional regulation,frontal-striatal circuits,glutamate,internal clock,motivation,peak-interval procedure,serotonin,spatial memory,temporal memory,time perception,ventral hippocampus},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118540770.ch8},
  note = {\section{Annotations\\
(7/19/2022, 6:44:53 PM)}

\par
``, the ability of the brain to process time in the seconds-to-minutes range is a challenging problem given that the basic electrophysiological properties of neurons operate on a milliseconds time scale.'' (Yin et al., 2017, p. 191)
\par
``These cortical and thalamic neurons oscillate with a mean periodicity of 10 Hz (Llinas, 1993, 1998).'' (Yin et al., 2017, p. 191)
\par
``The striatal medium spiny neurons have been hypothesized to be capable of detecting and responding'' (Yin et al., 2017, p. 191)
\par
``to select patterns of cortical input. The particular pattern of excitatory input is selected by long-term potentiation and/or long-term depression which is believed to result from dopaminergic activity from the midbrain ventral tegmental area (VTA) and the SNC following the delivery of reinforcement. Additionally, these dopamine neurons have been shown to transfer their activation onset to the signals that predict subsequent reinforcement (e.g., Bermudez \& Schultz, 2014; Schultz et~al., 1993, 1997; Tomasi et~al., 2015).'' (Yin et al., 2017, p. 192) +
\par
``The above neurobiological properties of the cortico-striatal circuitry can be combined with a ``beat frequency'' model of timing (Miall, 1989) that suggests that after resetting a range of oscillatory inputs, a specific time can be encoded by selectively weighting which inputs are currently active at the criterion time.'' (Yin et al., 2017, p. 192)
\par
``Thus, the SBF model provides a manner to encode a relatively long interval with very short neuronal mechanisms using the concept of coincidence detection which has been hypothesized as a major contributor to information processing in the basal ganglia (e.g., Buhusi \& Oprisan, 2013; Gu et~al., 2015a; Houk, 1995; Matell \& Meck, 2004; Oprisan \& Buhusi, 2011, 2013, 2014).'' (Yin et al., 2017, p. 192)
\par
``Specifically, upon onset of a meaningful signal (e.g., a cue that predicts important outcomes), dopamine neurons fire in a burst pattern which transiently synchronizes the cortical and thalamic oscillations, as well as hyperpolarizing the striatal membrane, thereby resetting the integrating mechanism. The level and reliability of this phasic dopamine response can vary as a function of age/disease state (e.g., Parkinson's disease~\textendash ~see Gu et~al., 2015a; Jones \& Jahanshahi, 2014) or magnitude of reinforcement (e.g., Ludvig et~al., 2007), thereby producing a continuum of effects ranging from full reset/synchronization to partial reset/synchronization (Allman \& Meck, 2012; Cheng et~al., 2006; Kononowicz \& van Rijn, 2014; Kononowicz et~al., 2016).'' (Yin et al., 2017, p. 192)
\par
``Following this dopamine burst, the cortical and thalamic neurons begin to oscillate at their inherent periods with associated variability, leading to a gradual reduction in synchronization and the emergence of the scalar property of interval timing while also allowing particular oscillatory patterns of neural activity to become meaningful. Upon detection of a previously reinforced pattern of oscillatory input, via the crossing of a coherent activity threshold, an ensemble of striatal spiny neurons fire and thereby engender a response that the encoded time has been reached. This striatal activity passes out of the basal ganglia to the thalamus and from there back to the cortex and striatum, thereby impinging on the current oscillatory inputs, allowing alterations of timing and time perception (Gu et~ al., 2015b; Kononowicz et~al., 2016; Matell \& Meck, 2000, 2004; van Rijn et~al., 2011, 2014).'' (Yin et al., 2017, p. 192)
\par
``recent work has successfully identified two genotypes that have highly specific and opposing effects on the precision with which mice represent interval duration (Gallistel et~al., 2014'' (Yin et al., 2017, p. 194)
\par
``Genes that modify clock speed should be identifiable with the combined use of pharmacological agents known to increase (e.g., methamphetamine) or decrease (e.g., haloperidol) clock speed in wild-type animals. Changes in clock speed lead to the observed ``clock pattern'' induced by dopaminergic manipulations (e.g., Coull et~ al., 2011; Meck, 1983, 1996; Meck et~al., 2012a; Oprisan \& Buhusi, 2011).'' (Yin et al., 2017, p. 194)
\par
``It would, of course, be possible to selectively ``knock out'' the clock~\textendash ~an extreme form of slowing it down. Presumably tyrosine hydroxylase knock-out (-/-) mice would have a completely dysfunctional ``internal clock'', but this is hardly a selective manipulation unless it is limited to the SNC or the VTA (e.g., Suri et~al., 1993). Dopamine transporter (DAT) ``knock-out'' (KO) and ``knock-down'' (KD) mice as well as mice exhibiting striatal D2 receptor overexpression (Ward et~al., 2009) are also of great interest in this regard. These mice have been shown to demonstrate paradoxical effects to dopaminergic drugs and have become an important model of attention-deficit hyperactivity disorder, obsessive-compulsive disorder, and schizophrenia (e.g., \c{C}evik, 2003a, b; Gainetdinov et~al., 1999; Giros et~al., 1996; Gu et~al., 2011; Jones et~al., 1999; Meck et~al., 2012a; Ward et~al., 2012~\textendash ~see also Balc\i, et~al., 2013 for a discussion of the epistasis effects of dopamine genes on interval timing and reward magnitude in humans evaluated using the PI timing procedure).'' (Yin et al., 2017, p. 194)
\par
``The bi-peak and tri-peak procedures'' (Yin et al., 2017, p. 195)
\par
``The bi-peak and tri-peak procedures (e.g., Agostino et~al., 2013; Buhusi \& Meck, 2009b; Matell \& Meck, 1999; Matell et~al., 2004; Meck et~al., 2012a; Yin \& Meck, 2014) can also be used to examine the scalar variability of interval timing and if there is any deviance in the multiple peak times obtained from the same trial. In this case, it would be possible to dissect whether the influences of an experimental manipulation affect clock speed or K* proportionally or not, and determine whether the affected peak functions still obey the scalar variability, i.e., whether the variances come from the clock, memory, or decision thresholds (Meck, 1996; Yin \& Meck, 2014).'' (Yin et al., 2017, p. 195)
\par
``. As a consequence, although the experimenter may think that there is an operational difference between the bisection and bi-peak procedures there really is not in terms of the subject's timing behavior~\textendash ~except for how and/or when the data are recorded during the trial. The ``switch'' point in the bi-peak procedure that is equivalent to the point of bisection in the duration bisection procedure is illustrated in peak functions averaged over many trials by the point of intersection between the Gaussian-like functions associated with the shorter and longer durations as illustrated by Fig. 8.5B or in Yin and Meck (2014~\textendash ~fig. 1 for example).'' (Yin et al., 2017, p. 197) bi-peak, bisection task
\par
``Interestingly, a novel variant of the temporal bisection and bi-peak procedures in which subjects switch from the response associated with a "short" duration to an alternative response associated with a "long" duration has recently been developed (e.g., Balc\i, et~ al., 2008b, 2009b; Gallistel et~ al., 2014; Kheifets \& Gallistel, 2012; Maggi et~ al., 2014; Tosun et~al., 2016). In this "timed-switching protocol", subjects (in this case mice) are provided with two flanking hoppers, one associated with a "shorter" delay to reward availability and the other associated with a "longer" delay after which a nosepoke will result in food reward as illustrated in Fig. 8.4. The optimal behavior for the mouse is to begin poking its nose into the "shorter" hopper and if there is no food delivery within a certain time period,'' (Yin et al., 2017, p. 197) bi-peak, bisection task variant
\par
``switch to the "longer" hopper. The distribution of switch latencies depends on both the "shorter" and "longer" durations, and therefore, the estimated mean of the distribution of switch latencies measures the accuracy with which the subject can target the center of the "temporal goalposts" so to speak (i.e., the point of subjective equality, PSE, between the two anchor durations), while the coefficient of variation (CV) measures the precision with which it does so. Changes in the "switch point" can be analyzed by systematically varying the probability of reward at each hopper, thereby providing a highly efficient way of measuring temporal sensitivity and reward optimization in wild-type and mutant mice (e.g., Balc\i, et~al., 2009b; Maggi et~al., 2014).'' (Yin et al., 2017, p. 198) +
\par
``9) Modulation of peak times and peak rates under selective behavioral manipulations, such as reversal of temporal contingencies (e.g., Cheng et~al., 2007b; Yin \& Meck, 2014) and pre-feeding (e.g., Roberts, 1981; Yin, 2016). When using a bi-peak procedure, the contingency between specific inputs (e.g., the left or the right lever) and the corresponding tobe-timed durations can be reversed, thus allowing the mice to switch their internal representations of the learned durations with specific motor programs, which are reflected by gradually changing peak times of the two to-be-timed durations. Again, the rate of reversal learning as reflected by the peak times changing can be modulated by interactions of different genes or different brain areas. On the other hand, mice that are pre-fed prior to testing in the PI timing procedure typically lack motivation to respond vigorously for food reward, which is reflected by reduced peak rates. However, even under these conditions, they are still able to time quite accurately and display the normal peak function (Yin, 2016~\textendash ~see also Balc\i, et~al., 2010b). Consequently, the sensitivity of peak rate reduction to pre-feeding can provide a useful measure of the functional interactions between different genes or different brain circuits'' (Yin et al., 2017, p. 198)
\par
``11) Spatial-temporal integration can be studied by incorporating interval-timing components into a spatial mapping task, such as has been done in Buhusi et~al. (2013). Animals can be confined in an area for a fixed duration before entering spatial exploration, during which they spontaneously start timing immediately on being placed in confinement, or immediately upon entering the spatial maze, and the temporal cues are used to plan out the spatial action sequence based on past experiences (see Van der Meer et~al., 2010). In this way, if an animal actively integrates spatial-temporal information in planning action sequences, a change in the duration of confinement will have significant effects on the success rate of the spatial exploration task. A classic interval-timing task, such as the PI procedure introduced above, should accompany the spatial mapping task to determine whether the animal indeed has distortions in the interval timing abilities per se.'' (Yin et al., 2017, p. 198)
\par
``12) Last but not least, contextual calibration using Bayesian optimization of interval timing has recently been proposed (e.g., Cicchini et~al., 2012; Jazayeri \& Shadlen, 2010; Shi et~al., 2013). In the Bayesian model, the likelihood function, prior distribution, and loss function can be related to the three information-processing stages of the classic internal-clock model~\textendash ~clock, memory, and decision-making stages. Evidence has shown that Bayesian inference provides strong predictions and a sound theoretical basis for contextual calibration in time perception. Therefore, the application of Bayesian inference to interval-timing paradigms should provide a deeper understanding of the perceptual and decision-making capabilities of genetically modified mice.'' (Yin et al., 2017, p. 198)
\par
``we describe the test battery for timing and immersive memory and emotional regulation (TIMER) and its ability to fulfill the needs of most neurogenetic studies as outlined in Table 8.2'' (Yin et al., 2017, p. 199)
\par
``There are four main reasons why we think this test battery is a priority choice in evaluating cognition in genetically modified mice. (1) The test battery consists of both multiple-session training paradigms and single-session training paradigms, and includes almost all possible behavioral deficits that could be shown in relation to cognition. (2) Because the memory and emotional regulation components are also nested within the timing tasks, the cross-validation among the different components of the test battery can provide excellent structural~validity. (3) The test battery consists of tasks that depend both on specific brain regions and on interactions among brain regions involved in interval timing, including the prefrontal cortex, striatum, hippocampus, amygdala, and midbrain (Allman et~ al., 2014a; Coull et~ al., 2011; Jin et~ al., 2009; MacDonald et~al., 2014; Meck et~al., 2008, 2013; Merchant et~al., 2013; Xu et~al., 2014; Yin \& Meck, 2014); therefore, the test battery is sensitive to both gene dose effects and brain-region specific effects as a result of genetic modifications.( 4) Timing, memory, and emotional regulation are three fundamental aspects underlying consciousness (see Allman et~ al., 2014b; Yin et~al., 2016b): as long as an organism is able to detect something novel that changes in time (as well as duration itself ), and is able to record the changes in memory combined with emotional regulation that modulate its strength, it has consciousness. Therefore, the TIMER test battery can potentially specify genes that directly link to one of the most mysterious aspects of cognition~\textendash ~consciousness.'' (Yin et al., 2017, p. 199)
\par
(Yin et al., 2017, p. 199)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/ZGA6GLZV/Yin et al_2017_Interval-timing Protocols and Their Relevancy to the Study of Temporal.pdf}
}

@article{yonedaLatticeSystemFunctionally2017,
  title = {Lattice System of Functionally Distinct Cell Types in the Neocortex},
  author = {Yoneda, Taisuke and Maruoka, Hisato and Sakai, Seiichiro and Tsuruno, Shun and Nakagawa, Nao and Hosoya, Toshihiko},
  year = {2017},
  journal = {Science},
  volume = {358},
  number = {6363},
  pages = {610--615},
  doi = {10.1126/science.aam6125},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/MHI3EMSL/Yoneda et al. - 2017 - Lattice system of functionally distinct cell types in the neocortex.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@yonedaLatticeSystemFunctionally2017.md}
}

@article{yoonSpecificEvidenceLowdimensional2013,
  title = {Specific Evidence of Low-Dimensional Continuous Attractor Dynamics in Grid Cells},
  author = {Yoon, KiJung and Buice, Michael A and Barry, Caswell and Hayman, Robin and Burgess, Neil and Fiete, Ila R},
  year = {2013},
  month = aug,
  journal = {Nature Neuroscience},
  volume = {16},
  number = {8},
  pages = {1077--1084},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nn.3450},
  abstract = {In this study, the authors show that the spatial responses of populations of grid cells are constrained to a two-dimensional activity manifold, and the relationships between pairs of grid cells are resistant to perturbation. These findings provide evidence of low-dimensional continuous attractor dynamics in the network.},
  keywords = {Dynamical systems},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/DFPZXUFB/Yoon et al. - 2013 - Specific evidence of low-dimensional continuous attractor dynamics in grid cells.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@yoonSpecificEvidenceLowdimensional2013.md}
}

@article{yuanReinforcementLearningSpiking2019,
  title = {Reinforcement {{Learning}} in {{Spiking Neural Networks}} with {{Stochastic}} and {{Deterministic Synapses}}},
  author = {Yuan, Mengwen and Wu, Xi and Yan, Rui and Tang, Huajin},
  year = {2019},
  month = dec,
  journal = {Neural Computation},
  volume = {31},
  number = {12},
  pages = {2368--2389},
  publisher = {{MIT Press}},
  issn = {08997667},
  doi = {10.1162/neco_a_01238},
  abstract = {Though succeeding in solving various learning tasks, most existing reinforcement learning (RL) models have failed to take into account the complexity of synaptic plasticity in the neural system. Models implementing reinforcement learning with spiking neurons involve only a single plasticity mechanism. Here, we propose a neural realistic reinforcement learning model that coordinates the plasticities of two types of synapses: stochastic and deterministic. The plasticity of the stochastic synapse is achieved by the hedonistic rule through modulating the release probability of synaptic neurotransmitter, while the plasticity of the deterministic synapse is achieved by a variant of a reward-modulated spike-timing-dependent plasticity rule through modulating the synaptic strengths. We evaluate the proposed learning model on two benchmark tasks: learning a logic gate function and the 19-state random walk problem. Experimental results show that the coordination of diverse synaptic plasticities can make the RL model learn in a rapid and stable form.},
  keywords = {ACTION potentials,LOGIC circuits,NEUROPLASTICITY,RANDOM walks,REINFORCEMENT learning,SYNAPSES},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6CI52REZ/Yuan et al_2019_Reinforcement Learning in Spiking Neural Networks with Stochastic and.pdf}
}

@misc{yuMAPSNNMappingSpike2022,
  title = {{{MAP-SNN}}: {{Mapping Spike Activities}} with {{Multiplicity}}, {{Adaptability}}, and {{Plasticity}} into {{Bio-Plausible Spiking Neural Networks}}},
  shorttitle = {{{MAP-SNN}}},
  author = {Yu, Chengting and Du, Yangkai and Chen, Mufeng and Wang, Aili and Wang, Gaoang and Li, Erping},
  year = {2022},
  month = apr,
  number = {arXiv:2204.09893},
  eprint = {2204.09893},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2204.09893},
  abstract = {Spiking Neural Network (SNN) is considered more biologically realistic and power-efficient as it imitates the fundamental mechanism of the human brain. Recently, backpropagation (BP) based SNN learning algorithms that utilize deep learning frameworks have achieved good performance. However, bio-interpretability is partially neglected in those BP-based algorithms. Toward bio-plausible BP-based SNNs, we consider three properties in modeling spike activities: Multiplicity, Adaptability, and Plasticity (MAP). In terms of multiplicity, we propose a Multiple-Spike Pattern (MSP) with multiple spike transmission to strengthen model robustness in discrete time-iteration. To realize adaptability, we adopt Spike Frequency Adaption (SFA) under MSP to decrease spike activities for improved efficiency. For plasticity, we propose a trainable convolutional synapse that models spike response current to enhance the diversity of spiking neurons for temporal feature extraction. The proposed SNN model achieves competitive performances on neuromorphic datasets: N-MNIST and SHD. Furthermore, experimental results demonstrate that the proposed three aspects are significant to iterative robustness, spike efficiency, and temporal feature extraction capability of spike activities. In summary, this work proposes a feasible scheme for bio-inspired spike activities with MAP, offering a new neuromorphic perspective to embed biological characteristics into spiking neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6LN299DE/Yu et al_2022_MAP-SNN.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VBP27EMJ/2204.html}
}

@inproceedings{yuNeuralDynamicsOscillator2018,
  title = {A {{Neural Dynamics}} and {{Oscillator Interference Mixed Model}} of the {{Grid Cells}} for {{Spatial Recognition}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Advanced Control}}, {{Automation}} and {{Artificial Intelligence}} ({{ACAAI}} 2018)},
  author = {Yu, Naigong and Luo, Ziwei and Zhai, Yujia},
  year = {2018},
  month = mar,
  publisher = {{Atlantis Press}},
  address = {{Paris, France}},
  doi = {10.2991/acaai-18.2018.26},
  isbn = {978-94-6252-483-5},
  keywords = {CAN,Grid Cells,OI,Phase Reset.,Speed Perception},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/2KJBZDLD/Yu, Luo, Zhai - 2018 - A Neural Dynamics and Oscillator Interference Mixed Model of the Grid Cells for Spatial Recognition.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@yuNeuralDynamicsOscillator2018.md}
}

@article{yuSmallWorldNeuronal2008,
  title = {A {{Small World}} of {{Neuronal Synchrony}}},
  author = {Yu, Shan and Huang, Debin and Singer, Wolf and Nikolic{\textasciiacute}1, Danko Nikolic{\textasciiacute}1},
  year = {2008},
  journal = {Cerebral Cortex December},
  volume = {18},
  pages = {2891--2901},
  doi = {10.1093/cercor/bhn047},
  abstract = {A small-world network has been suggested to be an efficient solution for achieving both modular and global processing-a property highly desirable for brain computations. Here, we investigated functional networks of cortical neurons using correlation analysis to identify functional connectivity. To reconstruct the interaction network, we applied the Ising model based on the principle of maximum entropy. This allowed us to assess the interactions by measuring pairwise correlations and to assess the strength of coupling from the degree of synchrony. Visual responses were recorded in visual cortex of anesthetized cats, simultaneously from up to 24 neurons. First, pairwise correlations captured most of the patterns in the population's activity and, therefore, provided a reliable basis for the reconstruction of the interaction networks. Second, and most importantly, the resulting networks had small-world properties; the average path lengths were as short as in simulated random networks, but the clustering coefficients were larger. Neurons differed considerably with respect to the number and strength of interactions, suggesting the existence of ''hubs'' in the network. Notably, there was no evidence for scale-free properties. These results suggest that cortical networks are optimized for the coexistence of local and global computations: feature detection and feature integration or binding.},
  keywords = {Ising model,maximum entropy,orientation selectivity,parallel recording,scale free,visual cortex},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/U2M3HMTI/Yu et al. - 2008 - A Small World of Neuronal Synchrony.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@yuSmallWorldNeuronal2008.md}
}

@article{yusoffBiologicallyInspiredTemporal2012,
  title = {Biologically {{Inspired Temporal Sequence Learning}}},
  author = {Yusoff, Nooraini and Gr{\"u}ning, Andr{\'e}},
  year = {2012},
  month = jan,
  journal = {Procedia Engineering},
  series = {International {{Symposium}} on {{Robotics}} and {{Intelligent Sensors}} 2012 ({{IRIS}} 2012)},
  volume = {41},
  pages = {319--325},
  issn = {1877-7058},
  doi = {10.1016/j.proeng.2012.07.179},
  abstract = {We propose a temporal sequence learning model in spiking neural networks consisting of Izhikevich spiking neurons. In our reward-based learning model, we train a network to associate two stimuli with temporal delay and a target response. Learning rule is dependent on reward signals that modulate the weight changes derived from spike-timing dependent plasticity (STDP) function. The dynamic properties of our model can be attributed to the sparse and recurrent connectivity, synaptic transmission delays, background activity and inter-stimulus interval (ISI). We have tested the learning in visual recognition task, and temporal AND and XOR problems. The network can be trained to associate a stimulus pair with its target response and to discriminate the temporal sequence of the stimulus presentation.},
  langid = {english},
  keywords = {Reward-based learning,Spike-timing dependent plasticity,Spiking neural networks,Temporal sequence learning},
  file = {/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/@yusoffBiologicallyInspiredTemporal2012.md;/home/Mike/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/Zotero Papers/yusoffBiologicallyInspiredTemporal2012-mdnotes.md;/Users/michaejt/Insync/m@tarlton.info/Google Drive/Yusoff_Grüning_2012_Biologically Inspired Temporal Sequence Learning.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\Citations\\@yusoffBiologicallyInspiredTemporal2012.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/N546AYL8/S1877705812025659.html}
}

@article{yuVisibleMachineLearning2018,
  title = {Visible {{Machine Learning}} for {{Biomedicine}}},
  author = {Yu, Michael K. and Ma, Jianzhu and Fisher, Jasmin and Kreisberg, Jason F. and Raphael, Benjamin J. and Ideker, Trey},
  year = {2018},
  journal = {Cell},
  volume = {173},
  number = {7},
  pages = {1562--1565},
  publisher = {{Elsevier Inc.}},
  doi = {10.1016/j.cell.2018.05.056},
  abstract = {A major ambition of artificial intelligence lies in translating patient data to successful therapies. Machine learning models face particular challenges in biomedicine, however, including handling of extreme data heterogeneity and lack of mechanistic insight into predictions. Here, we argue for ``visible'' approaches that guide model structure with experimental biology. A major ambition of artificial intelligence lies in translating patient data to successful therapies. Machine learning models face particular challenges in biomedicine, however, including handling of extreme data heterogeneity and lack of mechanistic insight into predictions. Here, we argue for ``visible'' approaches that guide model structure with experimental biology.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/WDLFF394/Yu et al. - 2018 - Visible Machine Learning for Biomedicine.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@yuVisibleMachineLearning2018.md}
}

@article{zakharovEpisodicMemorySubjectiveTimescale2021,
  title = {Episodic {{Memory}} for {{Subjective-Timescale Models}}},
  author = {Zakharov, Alexey and Crosby, Matthew and Fountas, Z.},
  year = {2021},
  abstract = {Planning in complex environments requires reasoning over multi-step timescales. However, in model-based learning, an agent's model is more commonly defined over transitions between consecutive states. This leads to plans using intermediate states that are either unnecessary, or worse, introduce cumulative prediction errors. Inspired by the recent works on human time perception, we devise a novel approach for learning a transition dynamics model based on the sequences of episodic memories that define an agent's subjective timescale \textendash{} over which it learns world dynamics and over which future planning is performed. We analyse the emergent benefits of the subjective-timescale model (STM) by incorporating it into two disparate model-based methods \textendash{} Dreamer and deep active inference. Using 3D visual foraging tasks, we demonstrate that STM can systematically vary the temporal extent of its predictions and is more likely to predict future salient events (such as new objects coming into view). In comparison to the agents trained using objective timescales, STM agents also collect more rewards due to their ability to perform flexible planning and a more pronounced exploratory behaviour.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/FJV9H9G7/Zakharov et al. - Episodic Memory for Subjective-Timescale Models.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/8HQ9VR2W/41bea732244d5a4f51c97c549169c0ac23eb36da.html}
}

@article{zdeborovaStatisticalPhysicsInference2016,
  title = {Statistical Physics of Inference: Thresholds and Algorithms},
  author = {Zdeborov{\'a}, Lenka and Krzakala, Florent},
  year = {2016},
  journal = {Advances in Physics},
  doi = {10.1080/00018732.2016.1211393},
  abstract = {Many questions of fundamental interest in todays science can be formulated as inference problems: Some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. Connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which formulated as an inference problem we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic.},
  keywords = {Bayesian inference,belief propagation,compressed sensing,phase transitions in computer science,spin glass theory,stochastic block model},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HH7TXPMP/Zdeborová, Krzakala - 2016 - Statistical physics of inference thresholds and algorithms.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@zdeborovaStatisticalPhysicsInference2016.md}
}

@article{zengCognitiveMappingBased2017,
  title = {Cognitive Mapping Based on Conjunctive Representations of Space and Movement},
  author = {Zeng, Taiping and Si, Bailu},
  year = {2017},
  journal = {Frontiers in Neurorobotics},
  volume = {11},
  number = {NOV},
  pages = {1--16},
  doi = {10.3389/fnbot.2017.00061},
  abstract = {It is a challenge to build robust simultaneous localization and mapping (SLAM) system in dynamical large-scale environments. Inspired by recent findings in the entorhinal-hippocampal neuronal circuits, we propose a cognitive mapping model that includes continuous attractor networks of head-direction cells and conjunctive grid cells to integrate velocity information by conjunctive encodings of space and movement. Visual inputs from the local view cells in the model provide feedback cues to correct drifting errors of the attractors caused by the noisy velocity inputs. We demonstrate the mapping performance of the proposed cognitive mapping model on an open-source dataset of 66 km car journey in a 3 km \texttimes{} 1.6 km urban area. Experimental results show that the proposed model is robust in building a coherent semi-metric topological map of the entire urban area using a monocular camera, even though the image inputs contain various changes caused by different light conditions and terrains. The results in this study could inspire both neuroscience and robotic research to better understand the neural computational mechanisms of spatial cognition and to build robust robotic navigation systems in large-scale environments.},
  keywords = {Attractor dynamics,Bio-inspired robots,Cognitive map,Conjunctive grid cells,Head-direction cells,Medial entorhinal cortex,Monocular SLAM,Path integration},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/HRSM4ETA/Zeng, Si - 2017 - Cognitive mapping based on conjunctive representations of space and movement.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@zengCognitiveMappingBased2017.md}
}

@techreport{zengNetworkInferenceUsing2010,
  title = {Network Inference Using Asynchronously Updated Kinetic {{Ising Model}}},
  author = {Zeng, Hong-Li and Aurell, Erik and Alava, Mikko and Mahmoudi, Hamed},
  year = {2010},
  abstract = {Network structures are reconstructed from dynamical data by respectively naive mean field (nMF) and Thouless-Anderson-Palmer (TAP) approximations. For TAP approximation, we use two methods to reconstruct the network: a) iteration method; b) casting the inference formula to a set of cubic equations and solving it directly. We investigate inference of the asymmetric Sherrington-Kirkpatrick (S-K) model using asynchronous update. The solutions of the sets cubic equation depend of temperature T in the S-K model, and a critical temperature Tc is found around 2.1. For T {$<$} Tc, the solutions of the cubic equation sets are composed of 1 real root and two conjugate complex roots while for T {$>$} Tc there are three real roots. The iteration method is convergent only if the cubic equations have three real solutions. The two methods give same results when the iteration method is convergent. Compared to nMF, TAP is somewhat better at low temperatures, but approaches the same performance as temperature increase. Both methods behave better for longer data length, but for improvement arises, TAP is well pronounced.},
  keywords = {0230Mv,8710Mn,8975Fb,numbers: 0250Tt},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/G6T9ZRYQ/Zeng et al. - 2010 - Network inference using asynchronously updated kinetic Ising Model.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@zengNetworkInferenceUsing2010.md}
}

@article{zenkeSynapticPlasticityNeural2013,
  title = {Synaptic {{Plasticity}} in {{Neural Networks Needs Homeostasis}} with a {{Fast Rate Detector}}},
  author = {Zenke, Friedemann and Hennequin, Guillaume and Gerstner, Wulfram},
  year = {2013},
  month = nov,
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {11},
  pages = {e1003330},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003330},
  abstract = {Hebbian changes of excitatory synapses are driven by and further enhance correlations between pre- and postsynaptic activities. Hence, Hebbian plasticity forms a positive feedback loop that can lead to instability in simulated neural networks. To keep activity at healthy, low levels, plasticity must therefore incorporate homeostatic control mechanisms. We find in numerical simulations of recurrent networks with a realistic triplet-based spike-timing-dependent plasticity rule (triplet STDP) that homeostasis has to detect rate changes on a timescale of seconds to minutes to keep the activity stable. We confirm this result in a generic mean-field formulation of network activity and homeostatic plasticity. Our results strongly suggest the existence of a homeostatic regulatory mechanism that reacts to firing rate changes on the order of seconds to minutes.},
  langid = {english},
  keywords = {Action potentials,Homeostasis,Homeostatic mechanisms,Neural networks,Neuronal plasticity,Neurons,snn,software,Synapses,Synaptic plasticity},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/LJ4W6RCM/Zenke et al_2013_Synaptic Plasticity in Neural Networks Needs Homeostasis with a Fast Rate.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/94MUXC49/article.html}
}

@article{zenkeVisualizingJointFuture2021,
  title = {Visualizing a Joint Future of Neuroscience and Neuromorphic Engineering},
  author = {Zenke, Friedemann and Boht{\'e}, Sander M. and Clopath, Claudia and Com{\c s}a, Iulia M. and G{\"o}ltz, Julian and Maass, Wolfgang and Masquelier, Timoth{\'e}e and Naud, Richard and Neftci, Emre O. and Petrovici, Mihai A. and Scherr, Franz and Goodman, Dan F. M.},
  year = {2021},
  month = feb,
  journal = {Neuron},
  volume = {109},
  number = {4},
  pages = {571--575},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.01.009},
  abstract = {Recent research resolves the challenging problem of building biophysically plausible spiking neural models that are also capable of complex information processing. This advance creates new opportunities in neuroscience and neuromorphic engineering, which we discussed at an online focus meeting.},
  langid = {english},
  keywords = {SNN},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/@zenkeVisualizingJointFuture2021.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/05. Obsidian/Obsidian/oslomet/50 Reading/Zotero Papers/zenkeVisualizingJointFuture2021-zotero.md;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/Y6JF3CJG/Zenke et al_2021_Visualizing a joint future of neuroscience and neuromorphic engineering.pdf;/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/XLK4UUDJ/S089662732100009X.html}
}

@article{zhangRecentAdvancesNew2022,
  title = {Recent {{Advances}} and {{New Frontiers}} in {{Spiking Neural Networks}}},
  author = {Zhang, Duzhen and Zhang, Tielin and Jia, Shuncheng and Wang, Qing and Xu, Bo},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2204.07050},
  abstract = {The recent advances and new frontiers in SNNs from four major research topics are discussed, including essential elements (i.e., spiking neuron models, encoding methods, and topology structures), datasets, optimization algorithms, and software and hardware frameworks. In recent years, spiking neural networks (SNNs) have received extensive attention in the field of brain-inspired intelligence due to their rich spatially-temporal dynamics, various coding schemes, and event-driven characteristics that naturally fit the neuromorphic hardware. With the development of SNNs, brain-inspired intelligence, an emerging research field inspired by brain science achievements and aiming at artificial general intelligence, is becoming hot. In this paper, we review the recent advances and discuss the new frontiers in SNNs from four major research topics, including essential elements (i.e., spiking neuron models, encoding methods, and topology structures), datasets, optimization algorithms, and software and hardware frameworks. We hope our survey can help researchers understand SNNs better and inspire new works to advance this field.},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/6AFVJ6WX/Zhang et al_2022_Recent Advances and New Frontiers in Spiking Neural Networks.pdf}
}

@article{zhangSelfbackpropagationSynapticModifications2021,
  title = {Self-Backpropagation of Synaptic Modifications Elevates the Efficiency of Spiking and Artificial Neural Networks},
  author = {Zhang, Tielin and Cheng, Xiang and Jia, Shuncheng and Poo, M. and Zeng, Yi and Xu, Bo},
  year = {2021},
  journal = {Science advances},
  doi = {10.1126/sciadv.abh0146},
  abstract = {Description},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/AS8477Y9/Zhang et al_2021_Self-backpropagation of synaptic modifications elevates the efficiency of.pdf}
}

@article{zhangSkipConnectedSelfRecurrentSpiking2021,
  title = {Skip-{{Connected Self-Recurrent Spiking Neural Networks With Joint Intrinsic Parameter}} and {{Synaptic Weight Training}}},
  author = {Zhang, Wenrui and Li, Peng},
  year = {2021},
  journal = {undefined},
  abstract = {A new Backpropagation (BP) method, backpropagated intrinsic plasticity (BIP), is proposed to boost the performance of ScSr-SNNs further by training intrinsic model parameters based on a well-defined global loss function in addition to synaptic weight training. Abstract As an important class of spiking neural networks (SNNs), recurrent spiking neural networks (RSNNs) possess great computational power and have been widely used for processing sequential data like audio and text. However, most RSNNs suffer from two problems. First, due to the lack of architectural guidance, random recurrent connectivity is often adopted, which does not guarantee good performance. Second, training of RSNNs is in general challenging, bottlenecking achievable model accuracy. To address these problems, we propose a new type of RSNN, skip-connected self-recurrent SNNs (ScSr-SNNs). Recurrence in ScSr-SNNs is introduced by adding self-recurrent connections to spiking neurons. The SNNs with self-recurrent connections can realize recurrent behaviors similar to those of more complex RSNNs, while the error gradients can be more straightforwardly calculated due to the mostly feedforward nature of the network. The network dynamics is enriched by skip connections between nonadjacent layers. Moreover, we propose a new backpropagation (BP) method, backpropagated intrinsic plasticity (BIP), to boost the performance of ScSr-SNNs further by training intrinsic model parameters. Unlike standard intrinsic plasticity rules that adjust the neuron\&\#39;s intrinsic parameters according to neuronal activity, the proposed BIP method optimizes intrinsic parameters based on the backpropagated error gradient of a well-defined global loss function in addition to synaptic weight training. Based on challenging speech, neuromorphic speech, and neuromorphic image data sets, the proposed ScSr-SNNs can boost performance by up to 2.85\% compared with other types of RSNNs trained by state-of-the-art BP methods.},
  langid = {english},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/JAIJGHML/951af94d232450a7170d9c0b9d9baeb430194924.html}
}

@article{zhouEncodingTimeNeural2022,
  title = {Encoding Time in Neural Dynamic Regimes with Distinct Computational Tradeoffs},
  author = {Zhou, Shanglin and Masmanidis, S. and Buonomano, D.},
  year = {2022},
  journal = {PLoS computational biology},
  doi = {10.1371/journal.pcbi.1009271},
  abstract = {The results predict that apparently similar neural dynamic patterns at the population level can exhibit fundamentally different computational properties in regards to their ability to generalize to novel stimuli and their robustness to noise\textemdash and that these differences are associated with differences in network connectivity and distinct contributions of excitatory and inhibitory neurons. Converging evidence suggests the brain encodes time in dynamic patterns of neural activity, including neural sequences, ramping activity, and complex dynamics. Most temporal tasks, however, require more than just encoding time, and can have distinct computational requirements including the need to exhibit temporal scaling, generalize to novel contexts, or robustness to noise. It is not known how neural circuits can encode time and satisfy distinct computational requirements, nor is it known whether similar patterns of neural activity at the population level can exhibit dramatically different computational or generalization properties. To begin to answer these questions, we trained RNNs on two timing tasks based on behavioral studies. The tasks had different input structures but required producing identically timed output patterns. Using a novel framework we quantified whether RNNs encoded two intervals using either of three different timing strategies: scaling, absolute, or stimulus-specific dynamics. We found that similar neural dynamic patterns at the level of single intervals, could exhibit fundamentally different properties, including, generalization, the connectivity structure of the trained networks, and the contribution of excitatory and inhibitory neurons. Critically, depending on the task structure RNNs were better suited for generalization or robustness to noise. Further analysis revealed different connection patterns underlying the different regimes. Our results predict that apparently similar neural dynamic patterns at the population level (e.g., neural sequences) can exhibit fundamentally different computational properties in regards to their ability to generalize to novel stimuli and their robustness to noise\textemdash and that these differences are associated with differences in network connectivity and distinct contributions of excitatory and inhibitory neurons. We also predict that the task structure used in different experimental studies accounts for some of the experimentally observed variability in how networks encode time.},
  note = {\section{Annotations\\
(6/8/2022, 5:48:50 PM)}

\par
``Itisnot known how neural circuits can encode time and satisfy distinct computational requirements, nor isitknown whether similar patterns ofneural activity atthe population level can exhibit dramatically different computational orgeneraliza tion properties'' (Zhou et al., 2022, p. 1)
\par
``Under absolute timing the neurons would PLOS COMPUTATIONALBIOLOGY Distinct dynamic regimes for encoding time PLOS ComputationalBiology |https:/ /doi.org/10.1371/journal.pcbi.1009271 March 3,2022 2/29 analysis, decision topublish, orprepara tionofthe manuscript. Competing interests: Theauthors havedeclared thatnocompeting interests exist'' (Zhou et al., 2022, p. 2)
\par
``respond at the same moments in time during both the production of short and long intervals but additional neurons would be active during the long interval; in a temporal scaling scheme neurons encode the same relative time during both short and long intervals; and in astimulusspecific code, there would be unrelated patterns for each interval (e.g., entirely different neural sequences for the short and long interval).'' (Zhou et al., 2022, p. 3)
\par
``s [14,18,23]'' (Zhou et al., 2022, p. 3)
\par
``The RNNs were based on firing rate units with distinct populations of excitatory (80\%) and inhibitory (20\%) units.'' (Zhou et al., 2022, p. 3)
\par
(Zhou et al., 2022, p. 4)
\par
(Zhou et al., 2022, p. 4)
\par
(Zhou et al., 2022, p. 4)
\par
``77. Kim R, Sejnowski TJ. Strong inhibitory signalingunderlies stable temporal dynamics and working memory inspiking neural networks. Nature Neuroscience.2021; 24(1):129\textendash 39. https://doi.org/10.1038/ s41593-020-00753-w PMID: 33288909 78. Kim R, LiY, Sejnowski TJ. Simple framework for constructing functional spiking recurrent neural networks. Proc Natl Acad Sci USA. 2019; 116(45):22811\textendash 20. https://doi.org/10.1073/pnas.1905926116 PMID: 31636215; PubMed Central PMCID: PMC6842655.'' (Zhou et al., 2022, p. 29)},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/63A3SKVK/Zhou et al_2022_Encoding time in neural dynamic regimes with distinct computational tradeoffs.pdf}
}

@article{zilliModelsGridCell2012,
  title = {Models of {{Grid Cell Spatial Firing Published}} 2005\textendash 2011},
  author = {Zilli, Eric A.},
  year = {2012},
  journal = {Frontiers in Neural Circuits},
  volume = {6},
  number = {April},
  pages = {1--17},
  issn = {1662-5110 (Electronic)\textbackslash r1662-5110 (Linking)},
  doi = {10.3389/fncir.2012.00016},
  abstract = {Since the discovery of grid cells in rat entorhinal cortex, many models of their hexagonally arrayed spatial firing fields have been suggested. We review the models and organize them according to the mechanisms they use to encode position, update the positional code, read it out in the spatial grid pattern, and learn any patterned synaptic connections needed. We mention biological implementations of the models, but focus on the models on Marr's algorithmic level, where they are not things to individually prove or disprove, but rather are a valuable collection of metaphors of the grid cell system for guiding research that are all likely true to some degree, with each simply emphasizing different aspects of the system. For the convenience of interested researchers, MATLAB implementations of the discussed grid cell models are provided at ModelDB accession 144006 or http://people.bu.edu/zilli/gridmodels.html.},
  keywords = {medial temporal lobe,medial temporal lobe; path integration; place cell,path integration,place cell,ring attractor,self-organization},
  file = {/home/michaelt/Insync/m@tarlton.info/Google Drive/06. Zotero/storage/VYDZJQYM/Zilli - 2012 - Models of Grid Cell Spatial Firing Published 2005–2011.pdf;G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@zilliModelsGridCell2012.md}
}

@misc{zongLargescaleTwophotonCalcium2021,
  title = {Large-Scale Two-Photon Calcium Imaging in Freely Moving Mice},
  author = {Zong, Weijian and Obenhaus, Horst Andreas and Skyt{\o}en, Emilie Ranheim and Eneqvist, Hanna and de Jong, Nienke Laura and Jorge, Marina Rodrigues and Moser, May-Britt and Moser, Edvard Ingjald},
  year = {2021},
  month = sep,
  pages = {2021.09.20.461015},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2021.09.20.461015},
  abstract = {We developed a miniaturized two-photon microscope (MINI2P) for fast, high-resolution, multiplane calcium imaging of over 1,000 neurons at a time in freely moving mice. With a microscope weight below 3g and a highly flexible connection cable, MINI2P allowed imaging to proceed with no impediment of behavior in half-hour free-foraging trials compared to untethered, unimplanted animals. The improved cell yield was achieved through a new optical system design featuring an enlarged field of view (FOV) and a new micro-tunable lens with increased z-scanning range and speed that allowed for fast and stable imaging of multiple, interleaved planes as well as 3D functional imaging. A novel technique for successive imaging across multiple, adjacent FOVs enabled recordings from more than 10,000 neurons in the same animal. Large-scale proof-of-principle data were obtained from cell populations in visual cortex, medial entorhinal cortex, and hippocampus, revealing spatial tuning of cells in all areas, including visual cortex.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/michaejt/Insync/m@tarlton.info/Google Drive/Zong et al_2021_Large-scale two-photon calcium imaging in freely moving mice.pdf}
}

@inproceedings{zouLearningContinuousAttractor2017,
  title = {Learning a {{Continuous Attractor Neural Network}} from {{Real Images}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Zou, Xiaolong and Ji, Zilong and Liu, Xiao and Mi, Yuanyuan and Wong, K. Y.Michael and Wu, Si},
  year = {2017},
  volume = {10637 LNCS},
  pages = {622--631},
  doi = {10.1007/978-3-319-70093-9_66},
  abstract = {Continuous attractor neural networks(CANNs) have been widely used as a canonical model for neural information representation. It remains, however, unclear how the neural system acquires such a net- work structure in practice. In the present study, we propose a biological plausible scheme for the neural system to learn a CANN from real im- ages. The scheme contains two key issues. One is to generate high-level representations of objects, such that the correlation between neural rep- resentations reflects the sematic relationship between objects. We adopt a deep neural network trained by a large number of natural images to achieve this goal. The other is to learn correlated memory patterns in a recurrent neural network.We adopt a modified Hebb rule, which encodes the correlation between neural representations into the connection form of the network. We carry out a number of experiments to demonstrate that when the presented images are linked by a continuous feature, the neural system learns a CANN successfully, in term of that these images are stored as a continuous family of stationary states of the network, forming a sub-manifold of low energy in the network state space.},
  isbn = {978-3-319-70092-2},
  keywords = {Continuous attractor neural network,Correlated patterns,Deep neural network,Modified Hebb rule},
  file = {G\:\\My Drive\\Obsidian\\Obsidian\\Charlie Vault\\MDnotes\\@zouLearningContinuousAttractor2017.md}
}


