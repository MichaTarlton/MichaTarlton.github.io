---
type: reading
status: open
priority: p4
creationtag: 2022-05-31 13:28
title: A Review of Designs and Applications of Echo State Networks
citekey: "sunReviewDesignsApplications2020"
infotags: [ESN, reservoir]
---

Comments:: [[Sun 2020 - comments]]
Topics:: [[Sun 2020 - topics]]
Glossary:: [[Sun 2020 - glossary]]
Citation:: [[@sunReviewDesignsApplications2020]]

Going to merge this with my annotations. Standby.
---
# Comments
Is the ESN defined by the training only done on the readout weights?

So based on equation 13 or 14, I think it would be possible to local BP, or give a reward signal / prediction error online. How to assign credit still remains. Though by TDSP it would be by how close and to what degree the incoming spike is.

So perhaps if we are doing 3-factor learning, then you would have a global inhib/excit signal which acts as your third factor and then the learning gets assigned with typical learning rules. This is basically the same as what is on my whiteboard. 

# Extracts
*lol this is basically just straight latex rip of section 2*
## Definition 1 (Echo State Networks (ESNs)) 
Echo state networks are a type of fast and efficient recurrent neural network. A typical ESN consists of an input layer, a recurrent layer, called reservoir, with a large number of sparsely connected neurons, and an output layer. The connection weights of the input layer and the reservoir layer are fixed after initialization, and the output weights are trainable and obtained by solving a linear regression problem.

In an ESN, $u(t) \in R^{D \times 1}$ denotes the input value at time $t$ of the time series, $x(t) \in R^{N \times 1}$ denotes the state of the reservoir at time $t . y(t) \in R^{M \times 1}$ denotes the output value. $W_{i n} \in R^{N \times D}$ represents the connection weights between the input layer and the hidden layer, $W_{r e s} \in R^{N \times N}$ notes the connection weights in hidden layer. The state transition equation is in Equation 1, where $f$ is a nonlinear function such as tan and sigmoid.
$$
\begin{aligned}
&x(t)=f\left(W_{\text {in }} u(t)+W_{\text {res }} x(t-1)\right) \\
&y(t)=W_{\text {out }} x(t)
\end{aligned}
$$
Only parameters of readout weights $W_{\text {out }}$ are subject to training. Which can obtain a closed-form solution by extremely fast algorithms such as ridge regression. Assuming the internal states and desired outputs are stored in $X$ and $Y$. By training to find a solution to the least squares problem in Equation 2, the calculation formula of the readout weights $W_{o u t}$ is got in Equation $3 .$
$$
\begin{gathered}
\min _{W_{\text {out }}}\left\|W_{\text {out }} X-Y\right\|_{2}^{2} \\
W_{o u t}=Y \cdot X^{-1}
\end{gathered}
$$
Before training phase, there are three main hyper-parameters of ESNs need to initialize - $w^{i n}, \alpha$ and $\rho\left(W_{r e s}\right)$.
- $w^{i n}$ is an input-scaling parameter, the elements in $W_{i n}$ are commonly randomly initialized from a uniform distribution in $\left[-w^{i n}, w^{i n}\right]$
- $\alpha$ is the sparsity parameter of $W_{r e s}$, denoting the proportion of non-zero elements in matrix.
- $\rho\left(W_{r e s}\right)$ is the spectral radius parameter (the largest eigenvalue in absolute value) of $W_{r e s} . W_{\text {res }}$ initialize from a matrix $W$, where the elements of $W$ are generated randomly in $[-1,1]$ and $\lambda_{\max }(W)$ is the largest eigenvalue.
$$
W_{r e s}=\rho\left(W_{r e s}\right) \cdot \frac{W}{\lambda_{\max }(W)}
$$
The extreme training efficiency of the ESN approach derives from the fact that only the readout weights are trained, while the weights of input and reservoir are initialized under certain conditions and then are left untrained.
After training phase, ESNs can give predictions $\hat{Y}$ of new data $\hat{U}$ with the trained $W_{\text {out }}$ by Equation $5 .$
$$
\begin{aligned}
\hat{X}_{t} &=f\left(W_{i n} \hat{U}_{t}+W_{r e s} \hat{X}_{t-1}\right) \\
\hat{Y}_{t} &=W_{o u t} \hat{X}_{t}
\end{aligned}
$$
Assuming that the size of the reservoir are fixed by $\mathrm{N}$, The input time series data has T-length D-dimension. For the typical reservoir computing Equation 1, its complexity is computed in Equation $6 .$
$$
C_{\text {typical }}=\mathcal{O}\left(\alpha T N^{2}+T N D\right)
$$

A pivotal property of ESNs is [[Echo State Property (ESP)]], which characterizes valid ESN dynamics. ESP essentially describes that the states of reservoir should asymptotically depend only on the driving input signal, which means the state is an echo of the input, meanwhile, and the influence of initial conditions should progressively vanish with time.  ^ptk58d

Briefly, according to Jeagerâ€™s paper [6], the ESN is that every echo state vector $x$ is uniquely determined for every input sequence $u$. It implies that the nearby echo states have the similar input histories.

## Definition 2 (Echo State Property (ESP)) 
A network with state transition equation $F$ with has the echo state property if for each input sequence $U=[u(1), u(2), \ldots u(N)]$ and all couples of initial states $x, x^{\prime}$, it could hold the condition in Equation $7 .$
$$
\left\|F(U, x)-F\left(U, x^{\prime}\right)\right\| \rightarrow 0 \quad \text { as } \quad N \rightarrow \infty
$$
The existence of ESP are verified by a necessary condition and a sufficient condition with spectral radius $\rho\left(W_{\text {res }}\right)$ and the largest singular value $\bar{\sigma}\left(W_{r e s}\right)=\|W\|_{2}$ :
$$
\begin{aligned}
&\text { echo state property } \Rightarrow \rho\left(W_{\text {res }}\right)<1 \\
&\bar{\sigma}\left(W_{\text {res }}\right)<1 \Rightarrow \text { echo state property }
\end{aligned}
$$
Two conditions are commonly used in ESN literature for reservoir initialization [22]. 

In recent years, the conditions for the ESP have been further investigated and refined in successive contributions $[23,24,10,25,26]$. For example, based on the two basic conditions, Buehner et al. [23] provided a rigorous bound for guaranteeing asymptotic stability of ESN, which was required in the applications where stability is required. 

Authors extended L2-norm $\left\|W_{\text {res }}\right\|_{2}$ 
to the D-norm $\left\|W_{\text {res }}\right\|_{D}=\bar{\sigma}\left(D W_{r e s} D^{-1}\right)$. 

They connected two conditions of echo state property by $\left\|W_{r e s}\right\|_{D}=\rho\left(W_{r e s}\right)+\epsilon$ and the sufficient condition changes to Equation $9 .$
$$
\bar{\sigma} D\left(W_{r e s} D^{-1}\right)<1 \Rightarrow \text { echo state property }
$$
**Short-Term Memory (STM)** denotes the memory effects connected with the transient activation dynamics of networks.

**Memory capacity $(\mathrm{MC})$** is a quantitative measure of STM. MC is calculated by the determination coefficient of $W_{o u t}^{k}$, $$d\left[W_{o u t}^{k}\right]\left(u(n-k), y_{k}(n)\right)=\frac{c o v^{2}\left(u(n-k), u_{k}(n)\right)}{\sigma^{2}(u(n)] \sigma^{2}\left(y_{k}(n)\right)}$$ when training the ESN to generate at its output units delayed versions $u(n-k)$. The STM capacity of the network according to MC is described in Equation 10
$$
\begin{aligned}
M C &=\sum_{k=1}^{\infty} M C_{k} \\
M C_{k} &=\max _{W_{\text {out }}^{k}} d\left[W_{\text {out }}^{k}\right]\left(u(n-k), y_{k}(n)\right)
\end{aligned}
$$
## 2.2 Designs
### 2.2.1 Basic models
In 2002, Jeager [6] proposed and named the echo state networks (ESNs) at the first time. He introduced ESN as a computationally efficient and easy learning method based on artificial recurrent neural networks (RNNs). The schema of ESN is that we described in section 2.1. The ESN were suitable for process chaotic time series, abounded nonlinear dynamical systems of the sciences and engineering. 

In 2004, Jeager et al. [27] applied ESN for predicting a chaotic time series and the accuracy were improved by a factor of 2400 over previous techniques.
#### Leaky ESN
In 2007, Jeager et al. [28] proposed another important structure, **leaky-ESN**, formalized in Equation 11, where 
- $a \in[0,1]$ is the reservoir neuron's leaking rate and 
- $\gamma$ is the compound gain. 

**Leaky-ESN**'s reservoir used leaky integrator neurons and the leaking rate was determined and fixed after model selection. By the effect analysis and experiments in slow dynamic systems, noisy time series and time-warped dynamic patterns, leaky-ESN outperformed than ESN and could incorporate a time constant to act as a low-pass filter and the dynamics can be slowed down.
$$
x(t)=(1-a \gamma) x(t-1)+\gamma f\left(W_{i n} u(t)+W_{r e s} x(t-1)\right)
$$
#### Feedback ESN
ESNs with output feedbacks was also emerging. The hidden state is not only affected by the input of this time and the hidden state of the previous time, but also affected by the output of the previous time, shown as Figure 1 right. The state transition equation changed to Equation $12 .$
$$
x(t)=(1-a \gamma) x(t-1)+\gamma f\left(W_{i n} u(t)+W_{\text {res }} x(t-1)+W_{\text {out }}^{\text {back }} y(t-1)\right)
$$
Based on these original versions, many deformations have been proposed.

### 2.2.2 Designs of reservoir
#### Dynamic weights of reservoir
In the basic versions of ESNs, the weights in reservoir are fixed after initialization. However, the randomly initialized reservoir could not be always the most suitable one for the specific data and tasks, and it brings about complications with choosing starting parameters. Thus, the dynamic reservoir changing when processing data is a main focus when designing reservoir.

Mayer et al. [29] changed the random weights to adapted weights by a method of adapting the recurrent layer dynamics and self-prediction. The weights of reservoir changed to $\hat{W}_{r e s}$ by the output weight through output-prediction $W_{\text {out }}^{1} x_{t}=y(t)$ and self-prediction $W_{\text {out }}^{2} x_{t}=x_{t+1}$ in Equation 13, where $\alpha$ is constant parameter. By adding self-prediction, the modified ESN could moderate the noise sensitivity.
$$
\hat{W}_{r e s}=(1-\alpha) W_{r e s}+\alpha W_{\text {out }}^{2}
$$
Hajnal et al.[30] introduced a notion of critical echo state networks (CESN), re-setiting $W_{r e s}$ to $\hat{W}_{r e s}$ by Equation $14 .$ In this case, $\hat{W}_{r e s}$ were able to approximate non-periodic dynamical systems, because the hidden layer were embedded by the input matrix $W_{i n}$ and output matrix $W_{o u t}$, and they can modify finite cycles.
$$
\hat{W}_{r e s}=W_{r e s}+W_{i n} W_{o u t}
$$
Fan et al. [31] proposed principle neuron reinforcement (PNR) algorithm to alter the strength of internal synapses within the reservoir by modifying the reservoir connections towards the goal of optimizing the neuronal dynamics. The PNR altered the connection according to the output weights of a pre-training stage, shown in Equation $15 .$
$$
W_{r e s}(t)=\left(1+\rho\left(W_{r e s}\right)\right) W_{r e s}(t-1), \quad \text { if } \quad W_{o u t}>\xi
$$
Babinec et al. [32] optimized ESN by the updating of $W_{r e s}$ with Anti-Oja's learning (AO) by Equation 16, where $\eta$ is a small positive constant. $\mathrm{AO}$ changed the weights by decreasing correlation between neurons. It increased the diversity between hidden neurons and the internal state dynamics became much richer.
$$
\begin{aligned}
&W_{r e s}(n+1)=W_{r e s}(n)-\Delta W_{r e s}(n) \\
&\Delta W_{r e s}(n)=\eta y(n)\left(x(n)-y(n) W_{r e s}(n)\right)
\end{aligned}
$$
Boedecker et al. [33] derived a new unsupervised learning rule based on intrinsic plasticity (IP) [34] to design weights of reservoir adapting dependency online. Then, Equation 1 changed to Equation 17 . Where $a$ is the gain vector and c is the bias vector. And the objective in Equation 2 changed to Kullback-Leibler divergence $D_{K L}=\sum p(y) \log \frac{p(y)}{\bar{p}(y)}$. Where $p(y)$ denotes the sampled output distribution of a reservoir neuron and $\bar{p}(y)$ is the desired output distribution with Laplace distribution $\bar{p}(y)=\frac{1}{2 \sigma} e^{\left(-\frac{|x-\mu|}{\sigma}\right)}$.
$$
y(t)=f\left(\operatorname{diag}(a) W_{r e s} x(t-1)+\operatorname{diag}(a) W_{i n} u(t)+c\right)
$$

#### Different connection modes of reservoir.
There are also many other designs about the dynamic connections. For example, Qiao et al. [35] proposed an adaptive lasso ESN (ALESN) to solve the problem of collinearity in randomly and sparsely connected reservoir. Wang et al. [36] used local plasticity rules, that allow different neurons to use different parameters to learn local features, to replace the same type of plasticity rule used in the unmodified ESN, that makes the connections between neurons remain to be global. Babinec et al. [37] designed gating ESNs by combining some local expert ESNs with different spectral radius and used the them of local ESN with best prediction result to train gating ESN, shown in the first structure of Figure 2 .

There are three basic connections of states: 1) delay line reservoir (DLR), 2) delay line reservoir with feedback connections (DLRB) and 3) simple cycle reservoir (SCR) shown in Figure 2. In [38], authors tested the the minimal complexity of reservoir construction for obtaining competitive models and the memory capacity of such simplified reservoirs. They finally concluded that the memory capacity of SCR can be made arbitrarily close to the proved optimal value.

Besides the basic connections, Sun et al. $[39,40]$ proposed the adjacent-feedback loop reservoir (ALR) based on SCR. ALR had a deterministic reservoir structure, in which the reservoir units were connected orderly in the loop manner and were also connected to the preceding one by adjacent feedback. In [41], authors also designed a circle reservoir structure with wavelet-neurons. To evaluate the different connections of reservoirs,

For designing the size and topology of multiple reservoirs automatically, Growing ESN (GESN) [42, 43] was proposed. It owned a growing reservoir with multiple sub-reservoirs by adding hidden units to the network group by group based on block matrix theory. And in [44], authors introduced using PSO and SVD algorithms to pre-train a growing ESN with multiple sub-reservoirs by optimizing singular values.


#### Modes of multiple reservoirs.
The above structures were built on the single reservoir, in practice, ESN can be extended with more reservoirs to achieve higher accuracy.

Chen et al. [45] designed shared reservoir modular ESN (SRMESN) to first divided the neural state space into several modules of subspaces with independent output weight and then put the data belonging to each subspace into the same reservoir. Which means the ESNs own $n$ reservoirs rather than only one, shown in Figure 2 . Many designs were proposed based on this. In [46], the inputs of this structure were utilized different wavelets of sequence; In [47], authors built a stacking ESN by stacking ensemble of reservoir computing learners based that structure; In [48], variational mode decomposition (VMD) was used to pick data as the input of different reservoirs.

Mean while, another structure is shown in 2 , the output $y(t)$ could also calculated from the multiple hidden states from time 1 to $t[49]$. However, the linear nature of the output layer may limit the capability of exploring the available infomation, since higher-order statistics of the signals are not taken into account. There are also other deigns of the output layer. In [50], authors used the nonlinear readout to represent the output probabilities; In [51], authors presented a Volterra filter structure and used principal component analysis to reduce the number of effective signals transmitted to the output layer.

Gallicchio and Micheli [8] pointed out that mapping the states of reservoir of ESN to the high-dimensional feature space was beneficial to both memory and good nonlinear mapping capabilities of the network, and proposed to extend the ESN with a random static nonlinear hidden layer. This type of structural variant is collectively referred to as $\varphi$-ESN, shown in Figure 2. In [52], An orthogonal least squares $\varphi$-ESN (OLS- $\varphi$-ESN) was proposed for generating the nodes of the extended static nonlinear hidden layer by applying incremental randomized learning method.

There are also other designs based on multiple reservoirs. In [53], authors proposed Broad-ESN through the unsupervised learning algorithm of restricted Boltzmann machine (RBM) to determine the number of reservoirs and aimed to fully reflect dynamic characteristics of a class of multivariate time series. Xue et al. [54] proposed decoupled ESN (DESN), used lateral inhibition unit to decouple the representation of state before input into multiple reservoirs. This structure have better robustness with respect to the random generation of reservoir weight matrix and feedback connections.

#### Analysis of short-term memory in reservoirs.
One of the most important properties of ESN is short-term memory, first introduced in [55]. And all of the basic ESN including the above reservoir designs worked by modeling it.

The topological structure of the reservoir relates to STM. Ma et al. [56] first tested it. They concluded that the reservoir time steps ago. They also designed a model to convert $r$ to $W_{\text {res }}$.

Different inputs also influence the STM. Barancok et al. [57] calculated memory capacity (MC) of the networks for various input data, both random and structured, and showed that for uniformly distributed input data, the higher interval shift lead to higher MC; for structured data, depending on data properties.

Mean while, the hyper-parameters $w^{i n}, \rho\left(W_{r e s}\right), \alpha$ can also affect STM. Farkas et al. [58] systematically analyzed the $\mathrm{MC}$ from the perspective of that three hyper-parameters and their relations. They concluded that the optimal $w^{i n}$ value





---
![[@sunReviewDesignsApplications2020]]
