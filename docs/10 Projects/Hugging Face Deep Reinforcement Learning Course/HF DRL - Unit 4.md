---
type: log
status: active
priority: p2
project: HF-DRL-course
creationtag: 2023-01-11 18:50
infotags: [course, RL, DRL, dev, programming, Q-learning, temporal difference]
people:
date:
---
web:: https://huggingface.co/deep-rl-course/unit4

# POLICY GRADIENT WITH PYTORCH

In value-based methods, theÂ policyÂ **Ï€**Â only exists because of the action value estimates since the policy is just a function(for instance, greedy-policy) that will select the action with the highest value given a state.

But, with policy-based methods, we want to optimize the policy directlyÂ **without having an intermediate step of learning a value function.**

So today,Â **weâ€™ll learn about ==policy-based methods== and study a subset of these methods called ==policy gradient==**. Then weâ€™ll implement our first policy gradient algorithm called ==Monte CarloÂ **Reinforce==** or just [[REINFORCE]] [^2]Â from scratch using PyTorch. Then, weâ€™ll test its robustness using the CartPole-v1 and PixelCopter environments.

# What are the policy-based methods?

The main goal of Reinforcement learning is toÂ **find the optimal policyÂ \pi^{*}Ï€âˆ—Â that will maximize the expected cumulative reward**. Because Reinforcement Learning is based on theÂ _reward hypothesis_:Â **all goals can be described as the maximization of the expected cumulative reward.**

## Value-based, Policy-based, and Actor-critic methods

We studied in the first unit, that we had two methods to find (most of the time approximate) this optimal policyÂ \pi^{*}Ï€âˆ—.

-   InÂ _value-based methods_, we learn a value function.
    -   The idea is that an optimal value function leads to an optimal policyÂ \pi^{*}Ï€âˆ—.
    -   Our objective is toÂ **minimize the loss between the predicted and target value**Â to approximate the true action-value function.
    -   We have a policy, but itâ€™s implicit since itÂ **was generated directly from the value function**. For instance, in Q-Learning, we defined an epsilon-greedy policy.
-   On the other hand, inÂ _policy-based methods_, we directly learn to approximateÂ \pi^{*}Ï€âˆ—Â without having to learn a value function.
    
    -   The idea isÂ **to parameterize the policy**. For instance, using a neural networkÂ \pi_\thetaÏ€Î¸â€‹, this policy will output a probability distribution over actions (stochastic policy).
![[image-20230112101717652.png]]

-   Our objective then is **to maximize the performance of the parameterized policy using gradient ascent**.
-   To do that, we control the parameter Î¸Î¸ that will affect the distribution of actions over a state.

![[image-20230112101749251.png]]

Consequently, thanks to policy-based methods, we can directly optimize our policy Ï€Î¸Ï€Î¸â€‹ to output a probability distribution over actions Ï€Î¸(aâˆ£s)Ï€Î¸â€‹(aâˆ£s) that leads to the best cumulative return. To do that, we define an objective function J(Î¸)J(Î¸), that is, the expected cumulative reward, and we **want to find Î¸Î¸ that maximizes this objective function**.

## The difference between policy-based and policy-gradient methods

Policy-gradient methods, what weâ€™re going to study in this unit, is a subclass of policy-based methods. In policy-based methods, the optimization is most of the time _on-policy_ since for each update, we only use data (trajectories) collected **by our most recent version of** Ï€Î¸Ï€Î¸â€‹.

The difference between these two methods **lies on how we optimize the parameter** Î¸Î¸:

-   In _policy-based methods_, we search directly for the optimal policy. We can optimize the parameter Î¸Î¸ **indirectly** by maximizing the local approximation of the objective function with techniques like hill climbing, simulated annealing, or evolution strategies.
-   In _policy-gradient methods_, because weâ€™re a subclass of the policy-based methods, we search directly for the optimal policy. But we optimize the parameter Î¸Î¸ **directly** by performing the gradient ascent on the performance of the objective function J(Î¸)J(Î¸).

Before diving more into how works policy-gradient methods (the objective function, policy gradient theorem, gradient ascent, etc.), letâ€™s study the advantages and disadvantages of policy-based methods.

# The advantages and disadvantages of policy-gradient methods
## Advantages
### The simplicity of integration

We can estimate the policy directly without storing additional data (action values).

### Policy-gradient methods can learn a stochastic policy

Policy-gradient methods canÂ **learn a stochastic policy while value functions canâ€™t**.

This has two consequences:

1.  We **donâ€™t need to implement an exploration/exploitation trade-off by hand**. Since we output a probability distribution over actions, the agent exploresÂ **the state space without always taking the same trajectory.**
    
2.  We also get rid of the problem of **[[perceptual aliasing]]**. Perceptual aliasing is when two states seem (or are) the same but need different actions.
Letâ€™s take an example: we have an intelligent vacuum cleaner whose goal is to suck the dust and avoid killing the hamsters.

![Hamster 1](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster1.jpg)

Our vacuum cleaner can only perceive where the walls are.

The problem is that theÂ **two rose cases are aliased states because the agent perceives an upper and lower wall for each**.

![Hamster 1](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster2.jpg)

Under a deterministic policy, the policy either will move right when in a red state or move left.Â **Either case will cause our agent to get stuck and never suck the dust**.

Under a value-based Reinforcement learning algorithm, we learn aÂ **quasi-deterministic policy**Â (â€œgreedy epsilon strategyâ€). Consequently, our agent canÂ **spend a lot of time before finding the dust**.

On the other hand, an optimal stochastic policyÂ **will randomly move left or right in rose states**. Consequently,Â **it will not be stuck and will reach the goal state with a high probability**.

### Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces

The problem with Deep Q-learning is that theirÂ **predictions assign a score (maximum expected future reward) for each possible action**, at each time step, given the current state.

But what if we have an infinite possibility of actions?

For instance, with a self-driving car, at each state, you can have a (near) infinite choice of actions (turning the wheel at 15Â°, 17.2Â°, 19,4Â°, honking, etc.).Â **Weâ€™ll need to output a Q-value for each possible action**! AndÂ **taking the max action of a continuous output is an optimization problem itself**!

Instead, with policy-gradient methods, we output aÂ **probability distribution over actions.**

### Policy-gradient methods have better convergence properties

In value-based methods, we use an aggressive operator toÂ **change the value function: we take the maximum over Q-estimates**. Consequently, the action probabilities may change dramatically for an arbitrarily small change in the estimated action values if that change results in a different action having the maximal value.

For instance, if during the training, the best action was left (with a Q-value of 0.22) and the training step after itâ€™s right (since the right Q-value becomes 0.23), we dramatically changed the policy since now the policy will take most of the time right instead of left.

On the other hand, in policy-gradient methods, stochastic policy action preferences (probability of taking action)Â **change smoothly over time**.
## Disadvantages

Naturally, policy-gradient methods also have some disadvantages:

-   **Frequently, policy-gradient converges on a local maximum instead of a global optimum.**
-   Policy-gradient goes slower,Â **step by step: it can take longer to train (inefficient).**
-   Policy-gradient can have high variance. Weâ€™ll see in actor-critic unit why and how we can solve this problem.

ğŸ‘‰ If you want to go deeper into the advantages and disadvantages of policy-gradient methods,Â [you can check this video](https://youtu.be/y3oqOjHilio).

# Diving deeper into policy-gradient methods

## Getting the big picture

We just learned that policy-gradient methods aim to find parametersÂ \thetaÎ¸Â thatÂ **maximize the expected return**.

The idea is that we have aÂ _parameterized stochastic policy_. In our case, a neural network outputs a probability distribution over actions. The probability of taking each action is also calledÂ _action preference_.

If we take the example of CartPole-v1:

-   As input, we have a state.
-   As output, we have a probability distribution over actions at that state.
![[image-20230112122414811.png]]

Our goal with policy-gradient is toÂ **control the probability distribution of actions**Â by tuning the policy such thatÂ **good actions (that maximize the return)Â are sampled more frequently in the future.**Â Each time the agent interacts with the environment, we tweak the parameters such that good actions will be sampled more likely in the future.

ButÂ **how are we going to optimize the weights using the expected return**?

The idea is that weâ€™re going toÂ **let the agent interact during an episode**. And if we win the episode, we consider that each action taken was good and must be more sampled in the future since they lead to win.

So for each state-action pair, we want to increase theÂ P(a|s)P(aâˆ£s): the probability of taking that action at that state. Or decrease if we lost.

The Policy-gradient algorithm (simplified) looks like this:

![[image-20230112122500047.png]]

## Diving deeper into policy-gradient methods

We have our stochastic policyÂ \piÏ€Â which has a parameterÂ \thetaÎ¸. ThisÂ \piÏ€, given a state,Â **outputs a probability distribution of actions**.

![[image-20230112122519941.png]]

WhereÂ \pi_\theta(a_t|s_t)Ï€Î¸â€‹(atâ€‹âˆ£stâ€‹)Â is the probability of the agent selecting actionÂ a_tatâ€‹Â from stateÂ s_tstâ€‹Â given our policy.

**But how do we know if our policy is good?**Â We need to have a way to measure it. To know that, we define a score/objective function calledÂ J(\theta)J(Î¸).


### The objective function

TheÂ _objective function_Â gives us theÂ **performance of the agent**Â given a trajectory (state action sequence without considering reward (contrary to an episode)), and it outputs theÂ _expected cumulative reward_.
![[image-20230112122539704.png]]

-   TheÂ _expected return_Â (also called expected cumulative reward), is the weighted average (where the weights are given byÂ P(\tau;\theta)P(Ï„;Î¸)Â of all possible values that the returnÂ R(\tau)R(Ï„)Â can take.
-   R(Ï„)Â : Return from an arbitrary trajectory. To take this quantity and use it to calculate the expected return, we need to multiply it by the probability of each possible trajectory.
-   P(\tau;\theta)P(Ï„;Î¸)Â : Probability of each possible trajectoryÂ \tauÏ„Â (that probability depends onÂ \thetaÎ¸Â since it defines the policy that it uses to select the actions of the trajectory which as an impact of the states visited).
![[image-20230112122911013.png]]

-   J(Î¸)Â : Expected return, we calculate it by summing for all trajectories, the probability of taking that trajectory givenÂ \thetaÎ¸, and the return of this trajectory.

Our objective then is to maximize the expected cumulative reward by findingÂ \thetaÎ¸Â that will output the best action probability distributions:
![[image-20230112122935065.png]]

## Gradient Ascent and the Policy-gradient Theorem

Policy-gradient is an optimization problem: we want to find the values ofÂ \thetaÎ¸Â that maximize our objective functionÂ J(\theta)J(Î¸), we need to useÂ **gradient-ascent**. Itâ€™s the inverse ofÂ _gradient-descent_Â since it gives the direction of the steepest increase ofÂ J(\theta)J(Î¸).

(If you need a refresher on the difference between gradient descent and gradient ascentÂ [check this](https://www.baeldung.com/cs/gradient-descent-vs-ascent)Â andÂ [this](https://stats.stackexchange.com/questions/258721/gradient-ascent-vs-gradient-descent-in-logistic-regression)).

Our update step for gradient-ascent is:

$$\theta \leftarrow \theta + \alpha * \nabla_\theta J(\theta)$$Î¸â†Î¸+Î±âˆ—âˆ‡Î¸â€‹J(Î¸)

We can repeatedly apply this update state in the hope thatÂ \thetaÎ¸Â converges to the value that maximizesÂ J(\theta)J(Î¸).

However, we have two problems to obtain the derivative ofÂ J(\theta)J(Î¸):

1.  We canâ€™t calculate the true gradient of the objective function since it would imply calculating the probability of each possible trajectory which is computationally super expensive. We want then toÂ **calculate a gradient estimation with a sample-based estimate (collect some trajectories)**.
2. We have another problem that I detail in the next optional section. To differentiate this objective function, we need to differentiate the state distribution, called Markov Decision Process dynamics. This is attached to the environment. It gives us the probability of the environment going into the next state, given the current state and the action taken by the agent. The problem is that we canâ€™t differentiate it because we might not know about it.

Fortunately weâ€™re going to use a solution called the Policy Gradient Theorem that will help us to reformulate the objective function into a differentiable function that does not involve the differentiation of the state distribution.

![[image-20230112132323188.png]]

If you want to understand how we derivate this formula that we will use to approximate the gradient, check the next (optional) section.

## The Reinforce algorithm (Monte Carlo Reinforce)

The Reinforce algorithm, also called Monte-Carlo policy-gradient, is a policy-gradient algorithm thatÂ **uses an estimated return from an entire episode to update the policy parameter**Â \thetaÎ¸:

In a loop:

-   Use the policyÂ $\pi_\theta$â€‹Â to collect an episodeÂ $\tau$
-   Use the episode to estimate the gradientÂ $\hat{g} = \nabla_\theta J(\theta)$ (see eq below)
-   Update the weights of the policy:Â $\theta \leftarrow \theta + \alpha \hat{g}$
![[image-20230112141344806.png]]

The interpretation we can make is this one:
-   $\nabla_\theta log \pi_\theta(a_t|s_t)$Â is the direction ofÂ **steepest increase of the (log) probability**Â of selecting action at from state $s_t$. This tells usÂ **how we should change the weights of policy**Â if we want to increase/decrease the log probability of selecting actionÂ $a_t$â€‹Â at stateÂ $s_t$â€‹.
-   $R(\tau)$: is the scoring function:
    -   If the return is high, it willÂ **push up the probabilities**Â of the (state, action) combinations.
    -   Else, if the return is low, it willÂ **push down the probabilities**Â of the (state, action) combinations.

We can alsoÂ **collect multiple episodes (trajectories)**Â to estimate the gradient:
![[image-20230112141540649.png]]

# (Optional) the Policy Gradient Theorem

In this optional section where weâ€™reÂ **going to study how we differentiate the objective function that we will use to approximate the policy gradient**.

Letâ€™s first recap our different formulas:

1.  The Objective function
![[image-20230112141603785.png]]

2.  The probability of a trajectory (given that action comes fromÂ $\pi_\theta$â€‹):
![[image-20230112141649739.png]]

So we have:
$$
\nabla_\theta J(\theta)=\nabla_\theta \sum_\tau P(\tau ; \theta) R(\tau)
$$
We can rewrite the gradient of the sum as the sum of the gradient:
$$
=\sum_\tau \nabla_\theta P(\tau ; \theta) R(\tau)
$$
We then multiply every term in the sum by $\frac{P(\tau ; \theta)}{P(\tau ; \theta)}$ (which is possible since it's = 1)
$$
\begin{aligned}
& =\sum_\tau \frac{P(\tau ; \theta)}{P(\tau ; \theta)} \nabla_\theta P(\tau ; \theta) R(\tau) \\
& \text { We can simplify further this since } \frac{P(\tau ; \theta)}{P(\tau ; \theta)} \nabla_\theta P(\tau ; \theta)=P(\tau ; \theta) \frac{\nabla_\theta P(\tau ; \theta)}{P(\tau ; \theta)} \\
& =\sum_\tau P(\tau ; \theta) \frac{\nabla_\theta P(\tau ; \theta)}{P(\tau ; \theta)} R(\tau)
\end{aligned}
$$
We can then use the ==[[derivative log trick]] (also called likelihood ratio trick or REINFORCE trick==), a simple rule in calculus that implies that $\nabla_x \log f(x)=\frac{\nabla_x f(x)}{f(x)}$
So given we have $\frac{\nabla_\theta P(\tau ; \theta)}{P(\tau ; \theta)}$ we transform it as $\nabla_\theta \log P(\tau \mid \theta)$
So this is our likelihood policy gradient:
$$
\nabla_\theta J(\theta)=\sum_\tau P(\tau ; \theta) \nabla_\theta \log P(\tau ; \theta) R(\tau)
$$
Thanks for this new formula, we can estimate the gradient using trajectory samples (we can approximate the likelihood ratio policy gradient with sample-based estimate if you prefer).
$\nabla_\theta J(\theta)=\frac{1}{m} \sum_{i=1}^m \nabla_\theta \log P\left(\tau^{(i)} ; \theta\right) R\left(\tau^{(i)}\right)$ where each $\tau^{(i)}$ is a sampled trajectory.

---

But we still have some mathematics work to do there: we need to simplify $\nabla_\theta \log P(\tau \mid \theta)$
We know that:
$$
\nabla_\theta \log P\left(\tau^{(i)} ; \theta\right)=\nabla_\theta \log \left[\mu\left(s_0\right) \prod_{t=0}^H P\left(s_{t+1}^{(i)} \mid s_t^{(i)}, a_t^{(i)}\right) \pi_\theta\left(a_t^{(i)} \mid s_t^{(i)}\right)\right]
$$
Where $\mu\left(s_0\right)$ is the initial state distribution and $P\left(s_{t+1}^{(i)} \mid s_t^{(i)}, a_t^{(i)}\right)$ is the state transition dynamics of the MDP.

---

==We know that the log of a product is equal to the sum of the logs==:

$$
\nabla_\theta \log P\left(\tau^{(i)} ; \theta\right)=\nabla_\theta \log \mu\left(s_0\right)+\nabla_\theta \sum_{t=0}^H \log P\left(s_{t+1}^{(i)} \mid s_t^{(i)} a_t^{(i)}\right)+\nabla_\theta \sum_{t=0}^H \log \pi_\theta\left(a_t^{(i)} \mid s_t^{(i)}\right)
$$
---

Since neither initial state distribution or state transition dynamics of the MDP are dependent of $\theta$, the derivate of both terms are 0 . So we can remove them:
Since: $\nabla_\theta \sum_{t=0}^H \log P\left(s_{t+1}^{(i)} \mid s_t^{(i)} a_t^{(i)}\right)=0$ and $\nabla_\theta \mu\left(s_0\right)=0$
$$
\nabla_\theta \log P\left(\tau^{(i)} ; \theta\right)=\nabla_\theta \sum_{t=0}^H \log \pi_\theta\left(a_t^{(i)} \mid s_t^{(i)}\right)
$$
---

We can rewrite the gradient of the sum as the sum of gradients:
$$
\nabla_\theta \log P\left(\tau^{(i)} ; \theta\right)=\sum_{t=0}^H \nabla_\theta \log \pi_\theta\left(a_t^{(i)} \mid s_t^{(i)}\right)
$$
So, the final formula for estimating the policy gradient is:
$$
\nabla_\theta J(\theta)=\hat{g}=\frac{1}{m} \sum_{i=1}^m \sum_{t=0}^H \nabla_\theta \log \pi_\theta\left(a_t^{(i)} \mid s_t^{(i)}\right) R\left(\tau^{(i)}\right)
$$
# Coding
See the dedicated note: [[HF DRL Coding - Unit 4]]
