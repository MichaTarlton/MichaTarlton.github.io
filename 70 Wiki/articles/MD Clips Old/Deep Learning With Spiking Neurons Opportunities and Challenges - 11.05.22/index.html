
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Pfeiffer, Michael, Pfeil, Thomas">
      
      
      
        <link rel="prev" href="../Computational%20roles%20of%20plastic%20probabilistic%20synapses%20-%2010.05.22/">
      
      
        <link rel="next" href="../Design%20of%20deep%20echo%20state%20networks%20-%2009.05.22/">
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.6">
    
    
      
        <title>Frontiers | Deep Learning With Spiking Neurons: Opportunities and Challenges | Neuroscience - obsidian-mkdocs template</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.ded33207.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="pink" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#frontiers-deep-learning-with-spiking-neurons-opportunities-and-challenges-neuroscience" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="obsidian-mkdocs template" class="md-header__button md-logo" aria-label="obsidian-mkdocs template" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            obsidian-mkdocs template
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Frontiers | Deep Learning With Spiking Neurons: Opportunities and Challenges | Neuroscience
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="pink" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="pink" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="obsidian-mkdocs template" class="md-nav__button md-logo" aria-label="obsidian-mkdocs template" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    obsidian-mkdocs template
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          10 Projects
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          10 Projects
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/10%20Projects/" class="md-nav__link">
        10 Projects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Active%20Projects%20Overview/" class="md-nav__link">
        Active Projects Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/All%20Projects%20Overview/" class="md-nav__link">
        All Projects Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Open%20Projects%20Overview/" class="md-nav__link">
        Open Projects Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Projects%20Overview%20Template/" class="md-nav__link">
        Projects Overview Template
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Projects%20Table/" class="md-nav__link">
        Projects Table
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7" id="__nav_2_7_label" tabindex="0">
          1 Main Research
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7">
          <span class="md-nav__icon md-icon"></span>
          1 Main Research
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_1" id="__nav_2_7_1_label" tabindex="0">
          Global Trigger Local Back Propagation
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_1">
          <span class="md-nav__icon md-icon"></span>
          Global Trigger Local Back Propagation
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Global%20Trigger%20Local%20Back-Propagation/Global%20Trigger%20Local%20Back-Propagation/" class="md-nav__link">
        Global Trigger Local Back Propagation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_2" id="__nav_2_7_2_label" tabindex="0">
          Make the fucking timing figure
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_2">
          <span class="md-nav__icon md-icon"></span>
          Make the fucking timing figure
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Make%20the%20fucking%20timing%20figure/Figure%20Descriptions/" class="md-nav__link">
        Figure Descriptions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Make%20the%20fucking%20timing%20figure/Make%20the%20fucking%20timing%20figure/" class="md-nav__link">
        Make the fucking timing figure
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_3" id="__nav_2_7_3_label" tabindex="0">
          PhD Proposal
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_3">
          <span class="md-nav__icon md-icon"></span>
          PhD Proposal
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/Nicolai%27s%20Comments%20on%20Draft%205/" class="md-nav__link">
        Nicolai's Comments on Draft 5
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20Proposal%20tasks/" class="md-nav__link">
        PhD Proposal tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20Proposal/" class="md-nav__link">
        PhD Proposal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Outline/" class="md-nav__link">
        PhD proposal Outline
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_3_5" id="__nav_2_7_3_5_label" tabindex="0">
          PhD proposal Drafts
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_7_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_3_5">
          <span class="md-nav__icon md-icon"></span>
          PhD proposal Drafts
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20%20Draft%208%20Research%20Plan/" class="md-nav__link">
        PhD Proposal  Draft 8 Research Plan
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%201/" class="md-nav__link">
        1 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%202%20-%20cut%20and%20paste%20palette/" class="md-nav__link">
        1 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%202/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%204/" class="md-nav__link">
        PhD Proposal Draft 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%205%20CUT%20AND%20PASTE%20PALETTE/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%205.1/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%205/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%206.1/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%206/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%207%20latex%20conversion/" class="md-nav__link">
        PhD Proposal Draft 7 latex conversion
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%207/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Mega%20Frakenstein%20Collage%20Draft%203/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20draft%202%20-%20outline/" class="md-nav__link">
        1 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20proposal%20Drafts/" class="md-nav__link">
        PhD proposal Drafts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/Research%20Question%20Section%20Draft%202/" class="md-nav__link">
        Research Question Section Draft 2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_4" id="__nav_2_7_4_label" tabindex="0">
          Recreating SBF Model
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_4">
          <span class="md-nav__icon md-icon"></span>
          Recreating SBF Model
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Abstract%20for%20SBFA%20-%2027.03.23/" class="md-nav__link">
        Abstract for SBFA   27.03.23
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Brief%20Overview%20of%20SBF%20Model/" class="md-nav__link">
        Brief Overview of SBF Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Outline%20of%20SBF%20Model%20for%20Supervision%20Meeting/" class="md-nav__link">
        Outline of SBF Model for Supervision Meeting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Outline%20of%20SBF%20Model/" class="md-nav__link">
        Outline of SBF Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Recreating%20SBF%20Model/" class="md-nav__link">
        Recreating SBF Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_5" id="__nav_2_7_5_label" tabindex="0">
          SBF   Automata Experiment
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_5">
          <span class="md-nav__icon md-icon"></span>
          SBF   Automata Experiment
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Algorithm%20Outline/" class="md-nav__link">
        Algorithm Outline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Experiment%20Environment%20Variants/" class="md-nav__link">
        Experiment Environment Variants
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/First%20python%20implementation%20-%20conditions%20and%20results/" class="md-nav__link">
        First python implementation   conditions and results
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Pseudocode%20for%20Modular%20Arrangement/" class="md-nav__link">
        Pseudocode for Modular Arrangement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Pseudocode%20for%20SBF%20Automata/" class="md-nav__link">
        Pseudocode for SBF Automata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBF%20-%20Automata%20-%20Design%201/" class="md-nav__link">
        The SBF Automata Design
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBF%20-%20Automata%20Algorithm%20Formalized/" class="md-nav__link">
        SBF   Automata Algorithm Formalized
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBF%20-%20Automata%20Experiment/" class="md-nav__link">
        SBF   Automata Experiment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20-%20Normalized%20Automata%20Vote/" class="md-nav__link">
        SBFA   Normalized Automata Vote
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20Development%20Log/" class="md-nav__link">
        SBFA Development Log
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20Experiment%20Development/" class="md-nav__link">
        SBFA Experiment Development
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20Experiment%20Results%2010.02.23/" class="md-nav__link">
        SBFA Experiment Results 10.02.23
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Weighted%20Majority%20Vote%20Version%2027.01.23/" class="md-nav__link">
        Weighted Majority Vote Version [[27.01.23]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_5_14" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_5_14" id="__nav_2_7_5_14_label" tabindex="0">
          Ooman Collab
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_7_5_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_5_14">
          <span class="md-nav__icon md-icon"></span>
          Ooman Collab
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Ooman%20Collab/Ooman%20Collab/" class="md-nav__link">
        Formerly the Automata Oscilllators Project
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_6" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_6" id="__nav_2_7_6_label" tabindex="0">
          SNN Models
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_6">
          <span class="md-nav__icon md-icon"></span>
          SNN Models
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SNN%20Models/SNN%20Models%20/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_7" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_7" id="__nav_2_7_7_label" tabindex="0">
          Survey Paper
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_7">
          <span class="md-nav__icon md-icon"></span>
          Survey Paper
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Survey%20Paper/Survey%20Paper%20Tasks/" class="md-nav__link">
        Tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Survey%20Paper/Survey%20Paper/" class="md-nav__link">
        Survey Paper
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_7_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_7_3" id="__nav_2_7_7_3_label" tabindex="0">
          How to Conduct Survey
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_7_7_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_7_3">
          <span class="md-nav__icon md-icon"></span>
          How to Conduct Survey
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Survey%20Paper/How%20to%20Conduct%20Survey/How%20to%20Conduct%20Survey%20/" class="md-nav__link">
        How to Conduct Survey 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_8" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_8" id="__nav_2_7_8_label" tabindex="0">
          Timing Tasks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_8">
          <span class="md-nav__icon md-icon"></span>
          Timing Tasks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Timing%20Tasks/Timing%20Tasks%20Research/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8" >
      
      
      
        <label class="md-nav__link" for="__nav_2_8" id="__nav_2_8_label" tabindex="0">
          2 Alt. Research
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_8">
          <span class="md-nav__icon md-icon"></span>
          2 Alt. Research
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_8_1" id="__nav_2_8_1_label" tabindex="0">
          Stable Diffusion Project
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_8_1">
          <span class="md-nav__icon md-icon"></span>
          Stable Diffusion Project
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/2%20Alt.%20Research/Stable%20Diffusion%20Project/Stable%20Diffusion%20Project/" class="md-nav__link">
        Stable Diffusion Project
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_8_2" id="__nav_2_8_2_label" tabindex="0">
          Vicente Collab
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_8_2">
          <span class="md-nav__icon md-icon"></span>
          Vicente Collab
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/2%20Alt.%20Research/Vicente%20Collab/Vicente%20Collab/" class="md-nav__link">
        Vicente Collab
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_9" >
      
      
      
        <label class="md-nav__link" for="__nav_2_9" id="__nav_2_9_label" tabindex="0">
          3 PhD Administrative Stuff
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_9">
          <span class="md-nav__icon md-icon"></span>
          3 PhD Administrative Stuff
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/Applying%20for%20trip%20expenses/" class="md-nav__link">
        How to apply for trip expenses
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/PhD%20Administrative%20Stuff/" class="md-nav__link">
        [Resources for Ph.D. candidates at TKD](https://ansatt.oslomet.no/en/ressursside-for-phd-kandidater-ved-tkd1)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/PhD%20Courses/" class="md-nav__link">
        PhD Courses
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/welcome%20letter/" class="md-nav__link">
        ![[Velkomstbrev ph.d.-kandidater internt - engelsk 1.pdf]]
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10" id="__nav_2_10_label" tabindex="0">
          4 Conferences & Schools
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10">
          <span class="md-nav__icon md-icon"></span>
          4 Conferences & Schools
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_1" id="__nav_2_10_1_label" tabindex="0">
          DeepLearn 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_1">
          <span class="md-nav__icon md-icon"></span>
          DeepLearn 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/DeepLearn%202022%20Travel/" class="md-nav__link">
        DeepLearn 2022 budget proposal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/DeepLearn%202022/" class="md-nav__link">
        DeepLearn 2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/DeepLearn%20Lecture%20Selections/" class="md-nav__link">
        [[DeepLearn 2022]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/Sean%20Meyn%20-%20DeepLearn%202022%20summer%20-%2025.07.22/" class="md-nav__link">
        Sean Meyn   DeepLearn 2022 summer   25.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_1_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_1_5" id="__nav_2_10_1_5_label" tabindex="0">
          Schuller
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_10_1_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_1_5">
          <span class="md-nav__icon md-icon"></span>
          Schuller
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/schuller/Bjorn%20Schuller%20-%20DeepLearn%202022%20summer%20-%2025.07.22/" class="md-nav__link">
        Motivations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_2" id="__nav_2_10_2_label" tabindex="0">
          MLSS^N 2022 Krakow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_2">
          <span class="md-nav__icon md-icon"></span>
          MLSS^N 2022 Krakow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/MLSSN%20Review/" class="md-nav__link">
        MLSSN Review
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/MLSS%5EN%202022%20Krakow/" class="md-nav__link">
        Dataview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Our%20Contribution%20to%20Continual%20Learning/" class="md-nav__link">
        PDF
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_2_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_2_4" id="__nav_2_10_2_4_label" tabindex="0">
          Talk Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_10_2_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_2_4">
          <span class="md-nav__icon md-icon"></span>
          Talk Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Andrea%20Tagliasacchi/" class="md-nav__link">
        Andrea Tagliasacchi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Andrew%20Saxe/" class="md-nav__link">
        Andrew Saxe
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Ewa%20Szczurek/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Friedemann%20Zenke%20MLSSN%20Lecture%20-%2030.06.22/" class="md-nav__link">
        Lecture Material
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Grabaska-Barwinska%20MLSSN%20Lecture%20-%2001.07.22/" class="md-nav__link">
        A rapid and efficient learning rule
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Jan%20Chorowski%20MLSSN%20%20-%2002.07.22/" class="md-nav__link">
        Jan Chorowski MLSSN    02.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Joao%20Henriques/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Joao%20Sacremento%20Lecture%20-%2029.06.22/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Piotr%20Milos%20Lecture%20MLSS%20-%2002.07.22/" class="md-nav__link">
        Piotr Milos Lecture MLSS   02.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Poster%20Session%20notes/" class="md-nav__link">
        Poster Session notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Rafal%20Bogcaz/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Razvan%20Pascnau%20Lecture%20-%2027.06.22/" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Rianne%20Van%20Den%20Berg%20Lecture/" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Tomasz%20Trzcinski%20MLSS%20-%2002.07.22/" class="md-nav__link">
        Tomasz Trzcinski MLSS   02.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Zieba%20MLSSN%20Lecture%20-%2001.07.22/" class="md-nav__link">
        Discrete normalizing flows
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11" id="__nav_2_11_label" tabindex="0">
          5 Presentations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11">
          <span class="md-nav__icon md-icon"></span>
          5 Presentations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/5%20Presentations/" class="md-nav__link">
        5 Presentations
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_2" id="__nav_2_11_2_label" tabindex="0">
          Present on SNN MAB
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_2">
          <span class="md-nav__icon md-icon"></span>
          Present on SNN MAB
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Present%20on%20SNN%20MAB/Present%20on%20SNN%20MAB/" class="md-nav__link">
        Present on SNN MAB
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_3" id="__nav_2_11_3_label" tabindex="0">
          Presenting on Petter 2018
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_3">
          <span class="md-nav__icon md-icon"></span>
          Presenting on Petter 2018
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20on%20Petter%202018/Petter%202018%20Slides%20-%20Draft%201/" class="md-nav__link">
        Petter 2018 Slides   Draft 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20on%20Petter%202018/Petter%202018%20Slides%20-%20Draft%202/" class="md-nav__link">
        Petter 2018 Slides   Draft 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20on%20Petter%202018/Presenting%20on%20Petter%202018/" class="md-nav__link">
        Presenting on Petter 2018
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_4" id="__nav_2_11_4_label" tabindex="0">
          Presenting to Ooman
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_4">
          <span class="md-nav__icon md-icon"></span>
          Presenting to Ooman
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20to%20Ooman/Presenting%20to%20Oomman/" class="md-nav__link">
        RL, SNNs, and Representing time
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_5" id="__nav_2_11_5_label" tabindex="0">
          Templates
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_5">
          <span class="md-nav__icon md-icon"></span>
          Templates
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/templates/tpl/" class="md-nav__link">
        Tpl
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/templates/tpl2/" class="md-nav__link">
        Tpl2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12" id="__nav_2_12_label" tabindex="0">
          6 Teaching
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12">
          <span class="md-nav__icon md-icon"></span>
          6 Teaching
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_1" id="__nav_2_12_1_label" tabindex="0">
          ACIT4420   Python Programming Course Autumn 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_1">
          <span class="md-nav__icon md-icon"></span>
          ACIT4420   Python Programming Course Autumn 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4420%20-%20Python%20Programming%20Course%20Autumn%202022/ACIT4420%20-%20Python%20Programming%20Course%20Autumn%202022/" class="md-nav__link">
        ACIT4420 - Python Scripting
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_2" id="__nav_2_12_2_label" tabindex="0">
          ACIT4620   Comp. Intel Course Autmun 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_2">
          <span class="md-nav__icon md-icon"></span>
          ACIT4620   Comp. Intel Course Autmun 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/" class="md-nav__link">
        [[ACIT4620 Log]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/ACIT4620%20Log/" class="md-nav__link">
        [[04.10.22]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/Recommended%20Changes%20to%20Course/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_3" id="__nav_2_12_3_label" tabindex="0">
          DATA3900 Bachelors Thesis Supervision
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_3">
          <span class="md-nav__icon md-icon"></span>
          DATA3900 Bachelors Thesis Supervision
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/DATA%203900%20Group%20-%20Microsoft%20Norge%20AS/" class="md-nav__link">
        DATA 3900 Group   Microsoft Norge AS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/DATA3900%20Bachelors%20Thesis%20Supervision/" class="md-nav__link">
        DATA3900 Bachelors Thesis Supervision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/Group%20-%20Intility%20AS/" class="md-nav__link">
        Group   Intility AS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/MSFT%20Bacheloroppgave/" class="md-nav__link">
        MSFT Bacheloroppgave
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_4" id="__nav_2_12_4_label" tabindex="0">
          TA courses Autumn 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_4">
          <span class="md-nav__icon md-icon"></span>
          TA courses Autumn 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/TA%20courses%20Autumn%202022/TA%20courses%20Autumn%202022%20/" class="md-nav__link">
        TA Course for Fall 2022
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13" id="__nav_2_13_label" tabindex="0">
          7 Obsidian Vault Meta
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_13_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13">
          <span class="md-nav__icon md-icon"></span>
          7 Obsidian Vault Meta
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Body%20of%20Knowledge%20Without%20Organs/" class="md-nav__link">
        Body of Knowledge without Organs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Obsidian%20Vault%20Meta/" class="md-nav__link">
        Obsidian Vault Meta
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Obsidian%20web%20browsing%20-12.01.23/" class="md-nav__link">
        Obsidian web browsing  12.01.23
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/obsidian%20meta%20log/" class="md-nav__link">
        Obsidian meta log
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13_5" id="__nav_2_13_5_label" tabindex="0">
          Better Reading
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_13_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13_5">
          <span class="md-nav__icon md-icon"></span>
          Better Reading
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Better%20Reading/Better%20Reading%20%20/" class="md-nav__link">
        Better Reading  
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13_6" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13_6" id="__nav_2_13_6_label" tabindex="0">
          Showing off my obsidian wokflow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_13_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13_6">
          <span class="md-nav__icon md-icon"></span>
          Showing off my obsidian wokflow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Showing%20off%20my%20obsidian%20wokflow/Showing%20off%20my%20obsidian%20wokflow/" class="md-nav__link">
        Showing off my obsidian wokflow
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13_7" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13_7" id="__nav_2_13_7_label" tabindex="0">
          Tracking People
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_13_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13_7">
          <span class="md-nav__icon md-icon"></span>
          Tracking People
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Tracking%20People/Tracking%20People/" class="md-nav__link">
        Tracking People
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_14" >
      
      
      
        <label class="md-nav__link" for="__nav_2_14" id="__nav_2_14_label" tabindex="0">
          AI Movie Series
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_14">
          <span class="md-nav__icon md-icon"></span>
          AI Movie Series
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI%20Movie%20Series/AI%20Movie%20Series/" class="md-nav__link">
        AI Movie Series
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_15" >
      
      
      
        <label class="md-nav__link" for="__nav_2_15" id="__nav_2_15_label" tabindex="0">
          AI Art Fair Project
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_15_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_15">
          <span class="md-nav__icon md-icon"></span>
          AI Art Fair Project
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Art%20Fair%20Project/AI-Art%20Fair%20Project/" class="md-nav__link">
        AI Art Fair Project
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_16" >
      
      
      
        <label class="md-nav__link" for="__nav_2_16" id="__nav_2_16_label" tabindex="0">
          AI Lab Retreat Dec. 5 7 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_16_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_16">
          <span class="md-nav__icon md-icon"></span>
          AI Lab Retreat Dec. 5 7 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/AI-Lab%20Retreat%20Dec.%205-7%202022/" class="md-nav__link">
        AI Lab Retreat Dec. 5 7 2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Contributions%20Challenge/" class="md-nav__link">
        Contributions Challenge
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Free%20writing%20test%202/" class="md-nav__link">
        Free writing assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Group%20collaboration%20for%20living%20documents/" class="md-nav__link">
        A New Publication Method for a Future of Shared Knowledge
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Problem%20challenge/" class="md-nav__link">
        So here is a problem
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/test%20free%20writing/" class="md-nav__link">
        Free writing exercises
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_17" >
      
      
      
        <label class="md-nav__link" for="__nav_2_17" id="__nav_2_17_label" tabindex="0">
          AI Lab Wiki
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_17_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_17">
          <span class="md-nav__icon md-icon"></span>
          AI Lab Wiki
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Wiki/AI-Lab%20Wiki%20/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_18" >
      
      
      
        <label class="md-nav__link" for="__nav_2_18" id="__nav_2_18_label" tabindex="0">
          Hugging Face Deep Reinforcement Learning Course
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_18_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_18">
          <span class="md-nav__icon md-icon"></span>
          Hugging Face Deep Reinforcement Learning Course
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%201/" class="md-nav__link">
        RL Framework
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%202/" class="md-nav__link">
        HF DRL   Unit 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%203/" class="md-nav__link">
        HF DRL   Unit 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%204/" class="md-nav__link">
        HF DRL   Unit 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%205/" class="md-nav__link">
        HF DRL   Unit 5
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20Coding%20-%20Unit%204/" class="md-nav__link">
        HF DRL Coding   Unit 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/" class="md-nav__link">
        Hugging Face Deep Reinforcement Learning Course
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_19" >
      
      
      
        <label class="md-nav__link" for="__nav_2_19" id="__nav_2_19_label" tabindex="0">
          IKT460 G Reinforcement Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_19_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_19">
          <span class="md-nav__icon md-icon"></span>
          IKT460 G Reinforcement Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/IKT460%20Lecture%201/" class="md-nav__link">
        IKT460 Lecture 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/IKT460%20Lecture%202/" class="md-nav__link">
        Top Comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/IKT460-G%20Reinforcement%20Learning/" class="md-nav__link">
        IKT460 G Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/Project%20Proposal%20Outline/" class="md-nav__link">
        Project Proposal Outline
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_20" >
      
      
      
        <label class="md-nav__link" for="__nav_2_20" id="__nav_2_20_label" tabindex="0">
          Lab Social Media
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_20_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_20">
          <span class="md-nav__icon md-icon"></span>
          Lab Social Media
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Lab%20Social%20Media/Lab%20Social%20Media/" class="md-nav__link">
        Lab Social Media
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_21" >
      
      
      
        <label class="md-nav__link" for="__nav_2_21" id="__nav_2_21_label" tabindex="0">
          Laptop Environemnt
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_21_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_21">
          <span class="md-nav__icon md-icon"></span>
          Laptop Environemnt
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Laptop%20Environemnt/Laptop%20Environment/" class="md-nav__link">
        Laptop Environment
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_22" >
      
      
      
        <label class="md-nav__link" for="__nav_2_22" id="__nav_2_22_label" tabindex="0">
          Learning Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_22_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_22">
          <span class="md-nav__icon md-icon"></span>
          Learning Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Learning%20Python/Learning%20Python/" class="md-nav__link">
        Learning Python
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_23" >
      
      
      
        <label class="md-nav__link" for="__nav_2_23" id="__nav_2_23_label" tabindex="0">
          Making Posters
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_23_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_23">
          <span class="md-nav__icon md-icon"></span>
          Making Posters
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Making%20Posters/Making%20Posters/" class="md-nav__link">
        Making Posters
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Making%20Posters/Neuromorphics%20Poster%20-%20Staging/" class="md-nav__link">
        Neuromorphics Poster   Staging
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Making%20Posters/Neuromorphics%20Poster%202nd%20draft/" class="md-nav__link">
        Title
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_24" >
      
      
      
        <label class="md-nav__link" for="__nav_2_24" id="__nav_2_24_label" tabindex="0">
          New Business
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_24_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_24">
          <span class="md-nav__icon md-icon"></span>
          New Business
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/New%20Business/New%20Business%20/" class="md-nav__link">
        New Business 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_25" >
      
      
      
        <label class="md-nav__link" for="__nav_2_25" id="__nav_2_25_label" tabindex="0">
          Norwegian Courses A1 and A2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_25_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_25">
          <span class="md-nav__icon md-icon"></span>
          Norwegian Courses A1 and A2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Norwegian%20Courses%20A1%20and%20A2/Norwegian%20Courses%20A1%20and%20A2%20/" class="md-nav__link">
        Norwegian Courses A1 and A2 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_26" >
      
      
      
        <label class="md-nav__link" for="__nav_2_26" id="__nav_2_26_label" tabindex="0">
          Obsidian Addons and Todos
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_26_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_26">
          <span class="md-nav__icon md-icon"></span>
          Obsidian Addons and Todos
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/Example%20Slides%20-%20Miniml/" class="md-nav__link">
        Example Slides   Miniml
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/Example%20Slides/" class="md-nav__link">
        Example Slides
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/MD%20Slides/" class="md-nav__link">
        MD Slides
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/Obsidian%20Addons%20and%20Todos/" class="md-nav__link">
        Obsidian Addons and Todos
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_27" >
      
      
      
        <label class="md-nav__link" for="__nav_2_27" id="__nav_2_27_label" tabindex="0">
          PENG9560 Topics in Artificial Intelligence and Machine Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_27_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_27">
          <span class="md-nav__icon md-icon"></span>
          PENG9560 Topics in Artificial Intelligence and Machine Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Mandatory%20Assignment%20for%20Module%20I%20-%20Computational%20Intelligence/" class="md-nav__link">
        Mandatory Assignment for Module I   Computational Intelligence
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%201/" class="md-nav__link">
        PENG9560 Assignment Draft 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%202/" class="md-nav__link">
        PENG9560 Assignment Draft 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%203/" class="md-nav__link">
        PENG9560 Assignment Draft 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%204/" class="md-nav__link">
        PENG9560 Assignment Draft 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Outline%20Assignment%201/" class="md-nav__link">
        Drafts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/" class="md-nav__link">
        PENG9560 Topics in Artificial Intelligence and Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_27_8" >
      
      
      
        <label class="md-nav__link" for="__nav_2_27_8" id="__nav_2_27_8_label" tabindex="0">
          Presentation
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_27_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_27_8">
          <span class="md-nav__icon md-icon"></span>
          Presentation
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20-%20Draft%201%20-%20PENG9560/" class="md-nav__link">
        Presentation   Draft 1   PENG9560
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20-%20Draft%202%20-%20PENG9560/" class="md-nav__link">
        Presentation   Draft 2   PENG9560
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20-%20Draft%203%20-%20PENG9560/" class="md-nav__link">
        Presentation   Draft 3   PENG9560
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20Outline%20-%20PENG9560/" class="md-nav__link">
        Presentation Outline   PENG9560
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_28" >
      
      
      
        <label class="md-nav__link" for="__nav_2_28" id="__nav_2_28_label" tabindex="0">
          Video   How Critical is Brain Criticality
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_28_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_28">
          <span class="md-nav__icon md-icon"></span>
          Video   How Critical is Brain Criticality
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Video%20-%20How%20Critical%20is%20Brain%20Criticality/Video%20-%20How%20Critical%20is%20Brain%20Criticality/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_29" >
      
      
      
        <label class="md-nav__link" for="__nav_2_29" id="__nav_2_29_label" tabindex="0">
          Video Production
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_29_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_29">
          <span class="md-nav__icon md-icon"></span>
          Video Production
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Video%20Production/Video%20Production%20/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_30" >
      
      
      
        <label class="md-nav__link" for="__nav_2_30" id="__nav_2_30_label" tabindex="0">
          Workplan 2022 23 for Gustavo
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_30_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_30">
          <span class="md-nav__icon md-icon"></span>
          Workplan 2022 23 for Gustavo
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Workplan%202022-23%20for%20Gustavo/Workplan%202022-23%20for%20Gustavo/" class="md-nav__link">
        Workplan 2022 23 for Gustavo
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          50 Reading
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          50 Reading
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/50%20Reading/" class="md-nav__link">
        50 Reading
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          PDF Searches
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          PDF Searches
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/PDF%20Searches/" class="md-nav__link">
        Searching in pdfs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/fixed%20interval%201/" class="md-nav__link">
        Paton_Buonomano_2018_The Neural Basis of Timing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/fixed%20interval/" class="md-nav__link">
        Fixed interval
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/interval/" class="md-nav__link">
        Interval
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/time%20interval/" class="md-nav__link">
        Time interval
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          Reading Lists
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          Reading Lists
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Lists/Recent%20Reading/" class="md-nav__link">
        Recent Reading
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
          Reading Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          Reading Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Gerstner%20et%20al.%202018/" class="md-nav__link">
        Gerstner et al. 2018
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mehonic%202022/" class="md-nav__link">
        Mehonic 2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Oomen%202017/" class="md-nav__link">
        Oomen 2017
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Parent%202004/" class="md-nav__link">
        Parent 2004
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Payeur%20et%20al.%202021/" class="md-nav__link">
        Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_6" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_6" id="__nav_3_4_6_label" tabindex="0">
          Buzski and Vrslakos (2023)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_6">
          <span class="md-nav__icon md-icon"></span>
          Buzski and Vrslakos (2023)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Brain%20rhythms%20have%20come%20of%20age%20-%20comments/" class="md-nav__link">
        Brain rhythms have come of age   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Brain%20rhythms%20have%20come%20of%20age%20-%20glossary/" class="md-nav__link">
        Brain rhythms have come of age   glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Brain%20rhythms%20have%20come%20of%20age%20-%20topics/" class="md-nav__link">
        Brain rhythms have come of age   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/" class="md-nav__link">
        Buzski and Vrslakos (2023)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_7" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_7" id="__nav_3_4_7_label" tabindex="0">
          Chen 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_7">
          <span class="md-nav__icon md-icon"></span>
          Chen 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022%20-%20comments/" class="md-nav__link">
        Chen 2022   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022%20-%20glossary/" class="md-nav__link">
        Formulae
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022%20-%20topics/" class="md-nav__link">
        Chen 2022   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022/" class="md-nav__link">
        Deep Reinforcement Learning with Spiking Q-learning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_8" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_8" id="__nav_3_4_8_label" tabindex="0">
          Granmo 2010
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_8">
          <span class="md-nav__icon md-icon"></span>
          Granmo 2010
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Granmo%202010/Granmo%202010%20Comments/" class="md-nav__link">
        Personal Summary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Granmo%202010/Granmo%202010%20Topics/" class="md-nav__link">
        Learning Automata (LA)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Granmo%202010/Granmo%202010/" class="md-nav__link">
        Granmo 2010
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_9" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_9" id="__nav_3_4_9_label" tabindex="0">
          Hartcher O'Brien, Brighouse, Levitan (2016)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_9">
          <span class="md-nav__icon md-icon"></span>
          Hartcher O'Brien, Brighouse, Levitan (2016)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29%20-%20comments/" class="md-nav__link">
        Hartcher O'Brien, Brighouse, Levitan (2016)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29%20-%20glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29%20-%20topics/" class="md-nav__link">
        Hartcher O'Brien, Brighouse, Levitan (2016)   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/" class="md-nav__link">
        Hartcher O'Brien, Brighouse, Levitan (2016)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_10" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_10" id="__nav_3_4_10_label" tabindex="0">
          Hasegawa and Sakata 2015
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_10">
          <span class="md-nav__icon md-icon"></span>
          Hasegawa and Sakata 2015
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hasegawa%20and%20Sakata%202015/Hasegawa%20and%20Sakata%202015/" class="md-nav__link">
        Hasegawa and Sakata 2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hasegawa%20and%20Sakata%202015/Hasegawa%2C%20and%20Sakata%202015%20-%20comments/" class="md-nav__link">
        Hasegawa, and Sakata 2015   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hasegawa%20and%20Sakata%202015/Hasegawa%2C%20and%20Sakata%202015%20-%20topics/" class="md-nav__link">
        Hasegawa, and Sakata 2015   topics
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_11" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_11" id="__nav_3_4_11_label" tabindex="0">
          Mello 2016
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_11">
          <span class="md-nav__icon md-icon"></span>
          Mello 2016
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016%20-%20Glossary/" class="md-nav__link">
        ABBREVIATIONS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016%20-%20Topics/" class="md-nav__link">
        Mello 2016   Topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016%20-%20comments/" class="md-nav__link">
        Mello 2016   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016/" class="md-nav__link">
        Mello 2016
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_12" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_12" id="__nav_3_4_12_label" tabindex="0">
          Mello, Soares, Paton 2015
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_12">
          <span class="md-nav__icon md-icon"></span>
          Mello, Soares, Paton 2015
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015%20-%20comments/" class="md-nav__link">
        Mello, Soares, Paton 2015   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015%20-%20glossary/" class="md-nav__link">
        Mello, Soares, Paton 2015   glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015%20-%20topics/" class="md-nav__link">
        Mello, Soares, Paton 2015   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015/" class="md-nav__link">
        Mello, Soares, Paton 2015
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_13" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_13" id="__nav_3_4_13_label" tabindex="0">
          OByrne & Jerbi (2022)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_13_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_13">
          <span class="md-nav__icon md-icon"></span>
          OByrne & Jerbi (2022)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/O%27Byrne%20%26%20Jerbi%20%282022%29%20-%20comments/" class="md-nav__link">
        O'Byrne & Jerbi (2022)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/O%27Byrne%20%26%20Jerbi%20%282022%29%20-%20topics/" class="md-nav__link">
        Topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/" class="md-nav__link">
        OByrne & Jerbi (2022)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_14" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_14" id="__nav_3_4_14_label" tabindex="0">
          Petter, Gershman, Meck (2018)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_14">
          <span class="md-nav__icon md-icon"></span>
          Petter, Gershman, Meck (2018)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29%20-%20comments/" class="md-nav__link">
        Petter, Gershman, Meck (2018)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29%20-%20glossary/" class="md-nav__link">
        Pavlovian Conditioning Protocol
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29%20-%20topics/" class="md-nav__link">
        Reward prediction error (RPE)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29/" class="md-nav__link">
        Petter, Gershman, Meck (2018)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_15" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_15" id="__nav_3_4_15_label" tabindex="0">
          Ponulak 2011
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_15_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_15">
          <span class="md-nav__icon md-icon"></span>
          Ponulak 2011
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Ponulak%202011/Ponulak%202011%20-%20comments/" class="md-nav__link">
        Ponulak 2011   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Ponulak%202011/Ponulak%202011%20-%20topics/" class="md-nav__link">
        Ponulak 2011   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Ponulak%202011/Ponulak%202011/" class="md-nav__link">
        Introduction to spiking neural networks: Information processing, learning and applications
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_16" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_16" id="__nav_3_4_16_label" tabindex="0">
          Sun 2020
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_16_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_16">
          <span class="md-nav__icon md-icon"></span>
          Sun 2020
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020%20-%20comments/" class="md-nav__link">
        Sun 2020   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020%20-%20glossary/" class="md-nav__link">
        Sun 2020   glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020%20-%20topics/" class="md-nav__link">
        Sun 2020   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020/" class="md-nav__link">
        A Review of Designs and Applications of Echo State Networks
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_17" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_17" id="__nav_3_4_17_label" tabindex="0">
          Voelkner 2019
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_17_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_17">
          <span class="md-nav__icon md-icon"></span>
          Voelkner 2019
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Voelkner%202019/Voelkner%202019%20-%20comments/" class="md-nav__link">
        Voelkner 2019   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Voelkner%202019/Voelkner%202019%20-%20topics/" class="md-nav__link">
        Voelkner 2019   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Voelkner%202019/Voelkner%202019/" class="md-nav__link">
        Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_18" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_18" id="__nav_3_4_18_label" tabindex="0">
          Yazidi 2021
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_18_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_18">
          <span class="md-nav__icon md-icon"></span>
          Yazidi 2021
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yazidi%202021/Yazidi%202021%20-%20Comments/" class="md-nav__link">
        Yazidi 2021   Comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yazidi%202021/Yazidi%202021%20-%20Topics/" class="md-nav__link">
        load balancing (LB)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yazidi%202021/Yazidi%202021/" class="md-nav__link">
        Yazidi 2021
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_19" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_19" id="__nav_3_4_19_label" tabindex="0">
          Yin 2017
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_19_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_19">
          <span class="md-nav__icon md-icon"></span>
          Yin 2017
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%202017/Yin%202017%20-%20comments/" class="md-nav__link">
        Yin 2017   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%202017/Yin%202017%20-%20topics/" class="md-nav__link">
        Yin 2017   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%202017/Yin%202017/" class="md-nav__link">
        Yin 2017
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_20" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_20" id="__nav_3_4_20_label" tabindex="0">
          Yin et al. (2022)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_20_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_20">
          <span class="md-nav__icon md-icon"></span>
          Yin et al. (2022)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29%20-%20comments/" class="md-nav__link">
        Yin et al. (2022)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29%20-%20glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29%20-%20topics/" class="md-nav__link">
        Topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29/" class="md-nav__link">
        Yin et al. (2022)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
          Videos
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          Videos
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Cosyne%202022%20Tutorial%20on%20Spiking%20Neural%20Networks/" class="md-nav__link">
        Cosyne 2022 Tutorial on Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Eliasmith%20%20-%20Spiking%20Neural%20Networks%20for%20More%20Efficient%20AI%20Algorithms/" class="md-nav__link">
        https://www.youtube.com/watch?v=PeW-TN3P1hk
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Hopfield%20Networks%20is%20All%20You%20Need/" class="md-nav__link">
        Hopfield Networks is All You Need
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/JNS%20Lecture%20Will%20Dabney%20-%20A%20Distributional%20Code%20for%20Value%20in%20Dopamine-Based%20Reinforcement%20Learning/" class="md-nav__link">
        A Distributional Code for Value in Dopamine-Based Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Neural%20representations%20of%20time%2C%20space%20and%20other%20continuous%20variables/" class="md-nav__link">
        Neural representations of time, space and other continuous variables
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
      
      
      
        <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
          Zotero Papers
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          Zotero Papers
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Zotero%20Papers/Zotero%20Papers/" class="md-nav__link">
        Zotero Papers
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
      
      
      
        <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
          Citations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_7">
          <span class="md-nav__icon md-icon"></span>
          Citations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/%40pateriaHierarchicalReinforcementLearning2021/" class="md-nav__link">
        @pateriaHierarchicalReinforcementLearning2021
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/%40voelkerLegendreMemoryUnits2019/" class="md-nav__link">
        @voelkerLegendreMemoryUnits2019
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20Anand%20Subramoney%2C%20Franz%20Scherr%2C%20Guillaume%20Bellec%2C%20Elias%20Hajek%2C%20Darjan%20Salaj%2C%20Robert%20Legenstein%2C%20Wolfgang%20Maass%20-%20/" class="md-nav__link">
        Citation Anand Subramoney, Franz Scherr, Guillaume Bellec, Elias Hajek, Darjan Salaj, Robert Legenstein, Wolfgang Maass   
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20Eleni%20Vasilaki%2C%20Nicolas%20Fr%C3%A9maux%2C%20Robert%20Urbanczik%2C%20Walter%20Senn%2C%20Wulfram%20Gerstner%20-%202009/" class="md-nav__link">
        Citation Eleni Vasilaki, Nicolas Frmaux, Robert Urbanczik, Walter Senn, Wulfram Gerstner   2009
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20Peter%20Diehl%2C%20Matthew%20Cook%20-%202015/" class="md-nav__link">
        Citation Peter Diehl, Matthew Cook   2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20SlowProcessesNeurons/" class="md-nav__link">
        Citation SlowProcessesNeurons
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
      
      
      
        <label class="md-nav__link" for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
          Tweets
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_8">
          <span class="md-nav__icon md-icon"></span>
          Tweets
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/tweets/Twitter%20-%20VnVrinda%20-%20How%20to%20search%20and%20read%20papers%20-%2011.07.22/" class="md-nav__link">
        Twitter   VnVrinda   How to search and read papers   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/tweets/Twitter%20-%20YanliangShi%20-%2020.07.22/" class="md-nav__link">
        Twitter   YanliangShi   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/tweets/Twitter%20-%20hisspikeness%20-%2023.07.22/" class="md-nav__link">
        Twitter   hisspikeness   23.07.22
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
      
      
      
        <label class="md-nav__link" for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
          Zot2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_9">
          <span class="md-nav__icon md-icon"></span>
          Zot2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40abelesCorticonicsNeuralCircuits1991/" class="md-nav__link">
        Corticonics: Neural Circuits of the Cerebral Cortex
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40allmanPathophysiologicalDistortionsTime2012/" class="md-nav__link">
        Pathophysiological distortions in time perception and timed performance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40balciPeakIntervalProcedure2020/" class="md-nav__link">
        The Peak Interval Procedure in Rodents: A Tool for Studying the Neurobiological Basis of Interval Timing and Its Alterations in Models of Human Disease
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40beggsCortexCriticalPoint2022/" class="md-nav__link">
        The Cortex and the Critical Point: Understanding the Power of Emergence.
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bellecSolutionLearningDilemma2020/" class="md-nav__link">
        A solution to the learning dilemma for recurrent networks of spiking neurons
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bendorBiasingContentHippocampal2012/" class="md-nav__link">
        Biasing the content of hippocampal replay during sleep
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bengioBiologicallyPlausibleDeep2016/" class="md-nav__link">
        Towards Biologically Plausible Deep Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40biTimeRepresentationNeural2019/" class="md-nav__link">
        Time representation in neural network models trained to perform interval timing tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40botvinickDeepReinforcementLearning2020/" class="md-nav__link">
        Deep Reinforcement Learning and its Neuroscientific Implications
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40brahlekTransportPropertiesTopological2015/" class="md-nav__link">
        @brahlekTransportPropertiesTopological2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bretteSimulationNetworksSpiking2007/" class="md-nav__link">
        Simulation of networks of spiking neurons: A review of tools and strategies
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bronsteinGeometricDeepLearning2021/" class="md-nav__link">
        Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40buzsakiBrainRhythmsHave2023/" class="md-nav__link">
        Brain rhythms have come of age
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40caoExplanatoryModelsNeuroscience2021/" class="md-nav__link">
        Explanatory models in neuroscience: Part 1 -- taking mechanistic abstraction seriously
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40carvalhoTemporalBisectionProcedure2019/" class="md-nav__link">
        Temporal Bisection Procedure
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40chenDeepReinforcementLearning2022/" class="md-nav__link">
        Deep Reinforcement Learning with Spiking Q-learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40chilkuriParallelizingLegendreMemory2021/" class="md-nav__link">
        Parallelizing Legendre Memory Unit Training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40cramerSurrogateGradientsAnalog2022/" class="md-nav__link">
        Surrogate gradients for analog neuromorphic computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40cruzActionSuppressionReveals2022/" class="md-nav__link">
        Action suppression reveals opponent parallel control via striatal circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40deverettIntervalTimingDeep2019/" class="md-nav__link">
        Interval timing in deep reinforcement learning agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40erdemExploringRelationshipsEffort2020/" class="md-nav__link">
        Exploring relationships between effort, motion, and sound in new musical instruments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40eshraghianIntroduction2022/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40fangwei123456SpikingJelly2022/" class="md-nav__link">
        SpikingJelly
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40fremauxNeuromodulatedSpikeTimingDependentPlasticity2016/" class="md-nav__link">
        Neuromodulated Spike-Timing-Dependent Plasticity, and Theory of Three-Factor Learning Rules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40furberSpiNNakerProject2014/" class="md-nav__link">
        The SpiNNaker Project
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gallonialessandroSNUFA2022Behavioral/" class="md-nav__link">
        SNUFA 2022 - Behavioral Timescale Synaptic Plasticity (BTSP) for credit assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gaoCorticalColumnWholebrain/" class="md-nav__link">
        Cortical column and whole-brain imaging with molecular contrast and nanoscale resolution
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gershmanReinforcementLearningEpisodic2017/" class="md-nav__link">
        Reinforcement learning and episodic memory in humans and animals: an integrative framework
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gershmanTimeRepresentationReinforcement2014/" class="md-nav__link">
        Time representation in reinforcement learning models of the basal ganglia
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gerstnerEligibilityTracesPlasticity2018/" class="md-nav__link">
        @gerstnerEligibilityTracesPlasticity2018
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40goudarEncodingSensoryMotor2018/" class="md-nav__link">
        Encoding sensory and motor patterns as time-invariant trajectories in recurrent neural networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40grondinTimingTimePerception2010/" class="md-nav__link">
        Timing and time perception: A review of recent behavioral and neuroscience findings and theoretical directions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40guOscillatoryMultiplexingNeural2015a/" class="md-nav__link">
        Oscillatory multiplexing of neural population codes for interval timing and working memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hamedaniDeepSpikingDelayed2020/" class="md-nav__link">
        Deep Spiking Delayed Feedback Reservoirs and Its Application in Spectrum Sensing of MIMO-OFDM Dynamic Spectrum Sharing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hardyEncodingTimeFeedforward2018/" class="md-nav__link">
        Encoding Time in Feedforward Trajectories of a Recurrent Neural Network Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hartcher-obrienSingleMechanismAccount2016/" class="md-nav__link">
        A single mechanism account of duration and rate processing via the pacemaker-accumulator and beat frequency models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hasegawaModelMultisecondTiming2015/" class="md-nav__link">
        A model of multisecond timing behaviour under peak-interval procedures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hesselRainbowCombiningImprovements2017/" class="md-nav__link">
        Rainbow: Combining Improvements in Deep Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40howardEfficientNeuralComputation2019/" class="md-nav__link">
        Efficient Neural Computation in the Laplace Domain
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40jazayeriNeuralMechanismSensing2015/" class="md-nav__link">
        @jazayeriNeuralMechanismSensing2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40jensenSelfOrganizedCriticalityEmergent1998/" class="md-nav__link">
        Self-Organized Criticality: Emergent Complex Behavior in Physical and Biological Systems
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40jirsaEntropyFreeEnergy2022/" class="md-nav__link">
        @jirsaEntropyFreeEnergy2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40khajehabdollahiWhenBeCritical2022/" class="md-nav__link">
        When to Be Critical? Performance and Evolvability in Different Regimes of Neural Ising Agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40kononowiczTimingTimePerception2018/" class="md-nav__link">
        Timing and Time Perception
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40kumarSpikingActivityPropagation2010/" class="md-nav__link">
        Spiking activity propagation in neuronal networks: reconciling different perspectives on neural coding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40lentResourceSelectionCognitive2018/" class="md-nav__link">
        Resource Selection in Cognitive Networks With Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40liMiceInferProbabilistic2013/" class="md-nav__link">
        Mice infer probabilistic models for timing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40liuHumanLevelControlDirectly2022/" class="md-nav__link">
        Human-Level Control Through Directly Trained Deep Spiking $Q$-Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40liuMultiobjectiveReinforcementLearning2015/" class="md-nav__link">
        Multiobjective Reinforcement Learning: A Comprehensive Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40loefflerModularityMultitaskingNeuromemristive2021a/" class="md-nav__link">
        Modularity and multitasking in neuro-memristive reservoir networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40maassNetworksSpikingNeurons1997/" class="md-nav__link">
        Networks of spiking neurons: The third generation of neural network models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40marsiliQuantifyingRelevanceLearning2022/" class="md-nav__link">
        Quantifying Relevance in Learning and Inference
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40matellNeuropsychologicalMechanismsInterval2000/" class="md-nav__link">
        Neuropsychological mechanisms of interval timing behavior
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40mehonicBraininspiredComputingNeeds2022/" class="md-nav__link">
        Brain-inspired computing needs a master plan
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40melloNeuralBehavioralMechanisms2016/" class="md-nav__link">
        Neural and Behavioral Mechanisms of Interval Timing in the Striatum
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40melloScalablePopulationCode2015/" class="md-nav__link">
        A Scalable Population Code for Time in the Striatum
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40mnihHumanlevelControlDeep2015/" class="md-nav__link">
        Human-level control through deep reinforcement learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40montanoGridgraphModelingEmergent2022/" class="md-nav__link">
        Grid-graph modeling of emergent neuromorphic dynamics and heterosynaptic plasticity in memristive nanonetworks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40montemurroPhaseofFiringCodingNatural2008/" class="md-nav__link">
        Phase-of-Firing Coding of Natural Visual Stimuli in Primary Visual Cortex
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40murrayLearningMultipleVariablespeed2017/" class="md-nav__link">
        Learning multiple variable-speed sequences in striatum via cortical tutoring
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40naudBurstdependentSynapticPlasticity/" class="md-nav__link">
        Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40neftciSurrogateGradientLearning2019/" class="md-nav__link">
        Surrogate Gradient Learning in Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40nicolaSupervisedLearningSpiking2017/" class="md-nav__link">
        Supervised learning in spiking neural networks with FORCE training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40obyrneHowCriticalBrain2022/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40oneillPlayItAgain2010/" class="md-nav__link">
        Play it again: reactivation of waking experience and memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40parisottoStabilizingTransformersReinforcement2020/" class="md-nav__link">
        Stabilizing Transformers for Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40patelImprovedRobustnessReinforcement2019/" class="md-nav__link">
        Improved robustness of reinforcement learning policies upon conversion to spiking neuronal network platforms applied to Atari Breakout game
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40patonNeuralBasisTiming2018/" class="md-nav__link">
        The Neural Basis of Timing: Distributed Mechanisms for Diverse Functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40payeurBurstdependentSynapticPlasticity2021/" class="md-nav__link">
        Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40petterIntegratingModelsInterval2018/" class="md-nav__link">
        Integrating Models of Interval Timing and Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40petterTemporalProcessingIntrinsic2016/" class="md-nav__link">
        Temporal Processing by Intrinsic Neural Network Dynamics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pfeifferDeepLearningSpiking2018/" class="md-nav__link">
        Deep Learning With Spiking Neurons: Opportunities and Challenges
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40phuongFormalAlgorithmsTransformers2022/" class="md-nav__link">
        Formal Algorithms for Transformers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoAssessingRobustnessCritical2022/" class="md-nav__link">
        Assessing the robustness of critical behavior in stochastic cellular automata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoBidirectionalLearningRobust2019%20%282%29/" class="md-nav__link">
        Bidirectional Learning for Robust Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoBidirectionalLearningRobust2019/" class="md-nav__link">
        Bidirectional Learning for Robust Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoGeneralRepresentationDynamical2019/" class="md-nav__link">
        A general representation of dynamical systems for reservoir computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40ponulakIntroductionSpikingNeural2011/" class="md-nav__link">
        Introduction to spiking neural networks: Information processing, learning and applications
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40princeCurrentStateFuture2022/" class="md-nav__link">
        Current State and Future Directions for Learning in Biological Recurrent Neural Networks: A Perspective Piece
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40ramsauerHopfieldNetworksAll2020/" class="md-nav__link">
        Hopfield Networks is All You Need
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40rolnickExperienceReplayContinual2019a/" class="md-nav__link">
        Experience Replay for Continual Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40rossbroichFluctuationdrivenInitializationSpiking2022/" class="md-nav__link">
        Fluctuation-driven initialization for spiking neural network training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40rueckauerConversionContinuousValuedDeep2017/" class="md-nav__link">
        Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40schmittNeuromorphicHardwareLoop2017/" class="md-nav__link">
        Neuromorphic hardware in the loop: Training a deep spiking network on the BrainScaleS wafer-scale system
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40shankarScaleInvariantInternalRepresentation2012/" class="md-nav__link">
        A Scale-Invariant Internal Representation of Time
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40shiScalarTimingTheory2022/" class="md-nav__link">
        Beyond Scalar Timing Theory: Integrating Neural Oscillators with Computational Accessibility in Memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40shiSpatialTemporalCorrelations2022/" class="md-nav__link">
        Spatial and temporal correlations in neural networks with structured connectivity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40songRewardbasedTrainingRecurrent2017/" class="md-nav__link">
        Reward-based training of recurrent neural networks for cognitive and value-based tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40spinolaWhatHumansLearn2013/" class="md-nav__link">
        What do humans learn in a double, temporal bisection task: Absolute or relative stimulus durations?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40sunReviewDesignsApplications2020/" class="md-nav__link">
        A Review of Designs and Applications of Echo State Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40sungSimultaneousEmulationSynaptic2022/" class="md-nav__link">
        Simultaneous emulation of synaptic and intrinsic plasticity using a memristive synapse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40suvrathanSTDPDiverseFunctionally2019a/" class="md-nav__link">
        Beyond STDPtowards diverse and functionally relevant plasticity rules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40swearingenPatternRespondingPeakInterval2010/" class="md-nav__link">
        The Pattern of Responding in the Peak-Interval Procedure with Gaps: An Individual-Trials Analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tallotNeuralEncodingTime2020/" class="md-nav__link">
        Neural encoding of time in the animal brain
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tanStrategyBenchmarkConverting2020/" class="md-nav__link">
        Strategy and Benchmark for Converting Deep Q-Networks to Event-Driven Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tangDeepReinforcementLearning2020/" class="md-nav__link">
        Deep Reinforcement Learning with Population-Coded Spiking Neural Network for Continuous Control
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tavanaeiDeepLearningSpiking2019/" class="md-nav__link">
        Deep learning in spiking neural networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tekiPersistenceMemoryHow2017/" class="md-nav__link">
        The Persistence of Memory: How the Brain Encodes Time in Memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tello-ramosTimePlaceLearning2015/" class="md-nav__link">
        Timeplace learning in wild, free-living hummingbirds
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40varellaModelPeakintervalTask2019/" class="md-nav__link">
        A model for the peak-interval task based on neural oscillation-delimited states
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40vasilakiSpikeBasedReinforcementLearning2009/" class="md-nav__link">
        Spike-Based Reinforcement Learning in Continuous State and Action Space: When Policy Gradient Methods Fail
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40vigneronCriticalSurveySTDP2020/" class="md-nav__link">
        A critical survey of STDP in Spiking Neural Networks for Pattern Recognition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40voelkerDynamicalSystemsSpiking2019/" class="md-nav__link">
        Dynamical Systems in Spiking Neuromorphic Hardware
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40voelkerLegendreMemoryUnits2019/" class="md-nav__link">
        Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40voelkerSpikePerformanceTraining2021/" class="md-nav__link">
        A Spike in Performance: Training Hybrid-Spiking Neural Networks with Quantized Activation Functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40vogelsSignalPropagationLogic2005/" class="md-nav__link">
        Signal Propagation and Logic Gating in Networks of Integrate-and-Fire Neurons
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wangConvergentEfficientDeep2022/" class="md-nav__link">
        Convergent and Efficient Deep Q Network Algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wangDeepReinforcementLearning2022/" class="md-nav__link">
        Deep Reinforcement Learning: A Survey
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wengTianshouHighlyModularized2022/" class="md-nav__link">
        Tianshou: a Highly Modularized Deep Reinforcement Learning Library
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wiesnerMeasuringComplexity2020/" class="md-nav__link">
        Measuring complexity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40williamsNeuralBurstCodes2021/" class="md-nav__link">
        Neural burst codes disguised as rate codes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wilsonInfluenceMultipleTemporal2015/" class="md-nav__link">
        The influence of multiple temporal memories in the peak-interval procedure
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wimmerRewardLearningWorking2022/" class="md-nav__link">
        Reward learning and working memory: Effects of massed versus spaced training and post-learning delay period
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinAccurateOnlineTraining2022/" class="md-nav__link">
        Accurate online training of dynamical spiking neural networks through Forward Propagation Through Time
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinEffectiveEfficientComputation2020/" class="md-nav__link">
        Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinIntervaltimingProtocolsTheir2017/" class="md-nav__link">
        Interval-timing Protocols and Their Relevancy to the Study of Temporal Cognition and Neurobehavioral Genetics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinOscillationCoincidenceDetectionModels2022/" class="md-nav__link">
        Oscillation/Coincidence-Detection Models of Reward-Related Timing in Corticostriatal Circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zenkeSuperSpikeSupervisedLearning2018/" class="md-nav__link">
        SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zenkeSynapticPlasticityNeural2013/" class="md-nav__link">
        Synaptic Plasticity in Neural Networks Needs Homeostasis with a Fast Rate Detector
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zenkeVisualizingJointFuture2021/" class="md-nav__link">
        Visualizing a joint future of neuroscience and neuromorphic engineering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zhouEncodingTimeNeural2022/" class="md-nav__link">
        Encoding time in neural dynamic regimes with distinct computational tradeoffs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/Diehl%20and%20Cook%202015/" class="md-nav__link">
        Unsupervised learning of digit recognition using spike-timing-dependent plasticity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/goldwasserPlantingUndetectableBackdoors2022/" class="md-nav__link">
        goldwasserPlantingUndetectableBackdoors2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/wiesnerMeasuringComplexity2020/" class="md-nav__link">
        ['Measuring complexity']
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          70 Wiki
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          70 Wiki
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
          Articles
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          Articles
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../A%20Comprehensive%20Guide%20to%20Convolutional%20Neural%20Networks%20%E2%80%94%20the%20ELI5%20way%20-%2003.04.22/" class="md-nav__link">
        A Comprehensive Guide to Convolutional Neural Networks  the ELI5 way   03.04.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Strange%20Illusion%20Shows%20The%20Human%20Brain%20Mess%20With%20Time%20to%20Maintain%20Our%20Expectations%20-%2008.04.22/" class="md-nav__link">
        Strange Illusion Shows The Human Brain Mess With Time to Maintain Our Expectations   08.04.22
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_3" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4_1_3" id="__nav_4_1_3_label" tabindex="0">
          MD Clips Old
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_3_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4_1_3">
          <span class="md-nav__icon md-icon"></span>
          MD Clips Old
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20Neural%20Mechanism%20for%20Sensing%20and%20Reproducing%20a%20Time%20Interval%20-%2023.06.22/" class="md-nav__link">
        A Neural Mechanism for Sensing and Reproducing a Time Interval   23.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20Neural%20Mechanism%20for%20Sensing%20and%20Reproducing%20a%20Time%20Interval%20-%20PubMed%20-%2023.06.22/" class="md-nav__link">
        A Neural Mechanism for Sensing and Reproducing a Time Interval   PubMed   23.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20Scalable%20Population%20Code%20for%20Time%20in%20the%20Striatum%20-%2008.06.22/" class="md-nav__link">
        A Scalable Population Code for Time in the Striatum - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20model%20of%20multisecond%20timing%20behaviour%20under%20peak-interval%20procedures%20-%2011.07.22/" class="md-nav__link">
        A model of multisecond timing behaviour under peak interval procedures   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20review%20of%20learning%20in%20biologically%20plausible%20spiking%20neural%20networks%20-%2011.05.22/" class="md-nav__link">
        A review of learning in biologically plausible spiking neural networks - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Action%20suppression%20reveals%20opponent%20parallel%20control%20via%20striatal%20circuits%20-%2012.07.22/" class="md-nav__link">
        Action suppression reveals opponent parallel control via striatal circuits   12.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../An%20experimental%20unification%20of%20reservoir%20computing%20methods%20-%2011.05.22/" class="md-nav__link">
        An experimental unification of reservoir computing methods - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Computational%20roles%20of%20plastic%20probabilistic%20synapses%20-%2010.05.22/" class="md-nav__link">
        Computational roles of plastic probabilistic synapses - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Frontiers | Deep Learning With Spiking Neurons: Opportunities and Challenges | Neuroscience
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Frontiers | Deep Learning With Spiking Neurons: Opportunities and Challenges | Neuroscience
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#excerpt" class="md-nav__link">
    Excerpt
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1. Introduction
  </a>
  
    <nav class="md-nav" aria-label="1. Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-what-is-a-deep-spiking-neural-network" class="md-nav__link">
    1.1. What Is a Deep Spiking Neural Network?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-advantages-of-deep-snns" class="md-nav__link">
    1.2. Advantages of Deep SNNs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-limitations-of-deep-snns" class="md-nav__link">
    1.3. Limitations of Deep SNNs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-inference-with-deep-snns" class="md-nav__link">
    2. Inference With Deep SNNs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-training-of-deep-snns" class="md-nav__link">
    3. Training of Deep SNNs
  </a>
  
    <nav class="md-nav" aria-label="3. Training of Deep SNNs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-binary-deep-neural-networks" class="md-nav__link">
    3.1. Binary Deep Neural Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-conversion-of-deep-neural-networks" class="md-nav__link">
    3.2. Conversion of Deep Neural Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-training-of-constrained-networks" class="md-nav__link">
    3.3. Training of Constrained Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-supervised-learning-with-spikes" class="md-nav__link">
    3.4. Supervised Learning With Spikes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-local-learning-rules" class="md-nav__link">
    3.5. Local Learning Rules
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-neuromorphic-hardware" class="md-nav__link">
    4. Neuromorphic Hardware
  </a>
  
    <nav class="md-nav" aria-label="4. Neuromorphic Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-inference-on-neuromorphic-hardware" class="md-nav__link">
    4.1. Inference on Neuromorphic Hardware
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-on-chip-learning" class="md-nav__link">
    4.2. On-Chip Learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-applications" class="md-nav__link">
    5. Applications
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-discussion" class="md-nav__link">
    6. Discussion
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#author-contributions" class="md-nav__link">
    Author Contributions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#funding" class="md-nav__link">
    Funding
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conflict-of-interest-statement" class="md-nav__link">
    Conflict of Interest Statement
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#acknowledgments" class="md-nav__link">
    Acknowledgments
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Design%20of%20deep%20echo%20state%20networks%20-%2009.05.22/" class="md-nav__link">
        Design of deep echo state networks - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Encoding%20sensory%20and%20motor%20patterns%20as%20time-invariant%20trajectories%20in%20recurrent%20neural%20networks%20-%2020.07.22/" class="md-nav__link">
        Encoding sensory and motor patterns as time invariant trajectories in recurrent neural networks   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Ensemble%20Coding%20of%20Vocal%20Control%20in%20Birdsong%20-%2021.06.22/" class="md-nav__link">
        Ensemble Coding of Vocal Control in Birdsong   21.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Hopfield%20network%20-%20Scholarpedia%20-%2022.06.22/" class="md-nav__link">
        Hopfield network   Scholarpedia   22.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20Review%20Articles%20-%2009.06.22/" class="md-nav__link">
        How to Review Articles   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20conduct%20a%20review%20-%2009.06.22/" class="md-nav__link">
        How to conduct a review   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20write%20a%20review%20article%20%20Writing%20your%20paper%20%20Author%20Services%20-%2009.06.22/" class="md-nav__link">
        How to write a review article  Writing your paper  Author Services   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20write%20a%20superb%20literature%20review%20-%2009.06.22/" class="md-nav__link">
        How to write a superb literature review
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Human-level%20control%20through%20deep%20reinforcement%20learning%20-%2013.05.22/" class="md-nav__link">
        Human-level control through deep reinforcement learning | Nature
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Integrating%20Models%20of%20Interval%20Timing%20and%20Reinforcement%20Learning%20-%2011.07.22/" class="md-nav__link">
        Integrating Models of Interval Timing and Reinforcement Learning   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../L0%20Norm%2C%20L1%20Norm%2C%20L2%20Norm%20%26%20L-Infinity%20Norm%20-%20Sara%20Iris%20Garcia%20-%20Medium%20-%2013.06.22/" class="md-nav__link">
        L0 Norm, L1 Norm, L2 Norm & L Infinity Norm   Sara Iris Garcia   Medium   13.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Learning%20multiple%20variable-speed%20sequences%20in%20striatum%20via%20cortical%20tutoring%20-%2020.07.22/" class="md-nav__link">
        Learning multiple variable speed sequences in striatum via cortical tutoring   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Learning%20offline%20memory%20replay%20in%20biological%20and%20artificial%20reinforcement%20learning%20-%2010.05.22/" class="md-nav__link">
        Learning offline: memory replay in biological and artificial reinforcement learning - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Local%20Lyapunov%20exponents%20of%20deep%20echo%20state%20networks%20-%2009.05.22/" class="md-nav__link">
        Local Lyapunov exponents of deep echo state networks - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Pad%C3%A9%20approximant%20-%20Scholarpedia%20-%2009.05.22/" class="md-nav__link">
        Pad approximant   Scholarpedia   09.05.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Peer%20Review%20Writing%20Guide%20-%20Reviewer%20Resources%20-%20Volunteer%20-%2009.06.22/" class="md-nav__link">
        Peer Review Writing Guide   Reviewer Resources   Volunteer   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../RNN%2C%20LSTM%20%26%20GRU%20-%2009.05.22/" class="md-nav__link">
        RNN, LSTM & GRU
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Reinforcement%20learning%20and%20episodic%20memory%20in%20humans%20and%20animals%20an%20integrative%20framework%20-%2020.07.22/" class="md-nav__link">
        Reinforcement learning and episodic memory in humans and animals an integrative framework   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Reinforcement%20learning%20with%20Marr%20-%2020.07.22/" class="md-nav__link">
        Reinforcement learning with Marr   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Research%20Guides%20Publishing%20in%20the%20Sciences%20How%20to%20Write%20a%20Scientific%20Literature%20Review%20-%2009.06.22/" class="md-nav__link">
        Research Guides Publishing in the Sciences How to Write a Scientific Literature Review   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Reservoir%20computing%20and%20extreme%20learning%20machines%20for%20non-linear%20time-series%20data%20analysis%20-%2011.05.22/" class="md-nav__link">
        Reservoir computing and extreme learning machines for non-linear time-series data analysis - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Reservoir%20computing%20approaches%20to%20recurrent%20neural%20network%20training%20-%2009.05.22/" class="md-nav__link">
        Reservoir computing approaches to recurrent neural network training - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Scientists%20discover%20how%20the%20brain%20keeps%20the%20urge%20to%20act%20in%20check%20%20Champalimaud%20Foundation%20-%2012.07.22/" class="md-nav__link">
        Scientists discover how the brain keeps the urge to act in check  Champalimaud Foundation   12.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Spiking%20Neural%20Networks%20and%20online%20learning%20An%20overview%20and%20perspectives%20-%2010.05.22/" class="md-nav__link">
        Spiking Neural Networks and online learning: An overview and perspectives - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action%20-%2006.05.22/" class="md-nav__link">
        Stateactionrewardstateaction   06.05.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Temporal%20Bisection%20Procedure%20-%2020.06.22/" class="md-nav__link">
        Temporal Bisection Procedure   20.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../The%20Pattern%20of%20Responding%20in%20the%20Peak-Interval%20Procedure%20with%20Gaps%20An%20Individual-Trials%20Analysis%20-%2011.07.22/" class="md-nav__link">
        The Pattern of Responding in the Peak Interval Procedure with Gaps An Individual Trials Analysis   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../The%20influence%20of%20multiple%20temporal%20memories%20in%20the%20peak-interval%20procedure%20-%2011.07.22%20%281%29/" class="md-nav__link">
        The influence of multiple temporal memories in the peak interval procedure   11.07.22 (1)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../The%20influence%20of%20multiple%20temporal%20memories%20in%20the%20peak-interval%20procedure%20-%2011.07.22/" class="md-nav__link">
        The influence of multiple temporal memories in the peak interval procedure   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Time%20representation%20in%20reinforcement%20learning%20models%20of%20the%20basal%20ganglia%20-%2020.07.22/" class="md-nav__link">
        Time representation in reinforcement learning models of the basal ganglia   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Unified%20pre-%20and%20postsynaptic%20long-term%20plasticity%20enables%20reliable%20and%20flexible%20learning%20-%2011.05.22/" class="md-nav__link">
        Unified pre- and postsynaptic long-term plasticity enables reliable and flexible learning | eLife
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../What%20do%20humans%20learn%20in%20a%20double%2C%20temporal%20bisection%20task%20Absolute%20or%20relative%20stimulus%20durations%20-%2020.06.22/" class="md-nav__link">
        What do humans learn in a double, temporal bisection task Absolute or relative stimulus durations   20.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Wikipedia%20-%20Adiabatic%20process%20-%2022.06.22/" class="md-nav__link">
        Wikipedia   Adiabatic process   22.06.22
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_4" id="__nav_4_1_4_label" tabindex="0">
          MD webclips
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_4">
          <span class="md-nav__icon md-icon"></span>
          MD webclips
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_4_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_4_1" id="__nav_4_1_4_1_label" tabindex="0">
          MD Clips New
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_4_1_4_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_4_1">
          <span class="md-nav__icon md-icon"></span>
          MD Clips New
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../MD_webclips/MD_Clips_New/Surrogate%20gradients%20for%20analog%20neuromorphic%20computing%20-%2006.12.22/" class="md-nav__link">
        Surrogate gradients for analog neuromorphic computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../MD_webclips/MD_Clips_New/What%20is%20the%20Difference%20Between%20Gradient%20Descent%20and%20Gradient%20Ascent%20-%2012.01.23/" class="md-nav__link">
        What is the Difference Between Gradient Descent and Gradient Ascent?
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_5" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_5" id="__nav_4_1_5_label" tabindex="0">
          Markdown Clips
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_5">
          <span class="md-nav__icon md-icon"></span>
          Markdown Clips
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips/A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning%20-%20MachineLearningMastery.com%20-%2008.02.23.md/" class="md-nav__link">
        A Gentle Introduction to Cross-Entropy for Machine Learning - MachineLearningMastery.com
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips/Human-level%20control%20through%20deep%20reinforcement%20learning%20-%2020.02.23.md/" class="md-nav__link">
        Human-level control through deep reinforcement learning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_6" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_6" id="__nav_4_1_6_label" tabindex="0">
          Markdown Clips old
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_6">
          <span class="md-nav__icon md-icon"></span>
          Markdown Clips old
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/A%20model%20for%20the%20peak-interval%20task%20based%20on%20neural%20oscillation-delimited%20states%20-%2017.11.22.md/" class="md-nav__link">
        A model for the peak-interval task based on neural oscillation-delimited states
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/A%20single%20mechanism%20account%20of%20duration%20and%20rate%20processing%20via%20the%20pacemaker-accumulator%20and%20beat%20frequency%20models%20-%2016.11.22.md/" class="md-nav__link">
        A single mechanism account of duration and rate processing via the pacemaker-accumulator and beat frequency models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Beyond%20STDP%20%E2%80%94%20towards%20diverse%20and%20functionally%20relevant%20plasticity%20rules%20-%2012.10.22/" class="md-nav__link">
        Beyond STDPtowards diverse and functionally relevant plasticity rules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Geometric%20foundations%20of%20Deep%20Learning%20-%20Towards%20Data%20Science%20-%2006.10.22/" class="md-nav__link">
        Geometric foundations of Deep Learning - Towards Data Science
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Grid-graph%20modeling%20of%20emergent%20neuromorphic%20dynamics%20and%20heterosynaptic%20plasticity%20in%20memristive%20nanonetworks%20-%2007.10.22/" class="md-nav__link">
        Grid-graph modeling of emergent neuromorphic dynamics and heterosynaptic plasticity in memristive nanonetworks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/How%20critical%20is%20brain%20criticality%20-%20%20Original/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/How%20critical%20is%20brain%20criticality%20-%2020.10.22/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/How%20critical%20is%20brain%20criticality%20-%2022.10.22/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Modularity%20and%20multitasking%20in%20neuro-memristive%20reservoir%20networks%20-%2007.10.22/" class="md-nav__link">
        Modularity and multitasking in neuro-memristive reservoir networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Oscillatory%20multiplexing%20of%20neural%20population%20codes%20for%20interval%20timing%20and%20working%20memory%20-%2021.11.22.md/" class="md-nav__link">
        Oscillatory multiplexing of neural population codes for interval timing and working memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/The%20Persistence%20of%20Memory%20How%20the%20Brain%20Encodes%20Time%20in%20Memory%20-%2021.11.22.md/" class="md-nav__link">
        The Persistence of Memory: How the Brain Encodes Time in Memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Timing%20Intervals%20Using%20Population%20Synchrony%20and%20Spike%20Timing%20Dependent%20Plasticity%20-%2017.11.22.md/" class="md-nav__link">
        Timing Intervals Using Population Synchrony and Spike Timing Dependent Plasticity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Weighted%20Majority%20Algorithm%20-%20A%20beautiful%20algorithm%20for%20Learning%20from%20Experts/" class="md-nav__link">
        Weighted Majority Algorithm: A beautiful algorithm for Learning from Experts
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
      
      
      
        <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          Pages
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          Pages
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Apical%20Dendrite/" class="md-nav__link">
        Apical Dendrite
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Echo%20State%20Networks/" class="md-nav__link">
        Echo State Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Grid%20Cell%20-%20Time%20Cell%20Interactions/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Hierarchical%20Reinforcement%20Learning/" class="md-nav__link">
        Hierarchical Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Legendre%20Memory%20Unit%20%28LMU%29/" class="md-nav__link">
        Legendre Memory Unit (LMU)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Liquid%20State%20Machine/" class="md-nav__link">
        Liquid State Machine
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Recurrent%20Neural%20Network%20%28RNN%29/" class="md-nav__link">
        Nomenclature around degree of recurrence or moment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Reservoir%20Computing/" class="md-nav__link">
        Reservoir Computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Spiking%20Neural%20Networks%20%28SNN%29/" class="md-nav__link">
        Spiking Neural Networks (SNN)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Striatal%20Beat%20Frequency%20Model/" class="md-nav__link">
        Striatal Beat Frequency Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Striatal%20Beat%20Frequency/" class="md-nav__link">
        Striatal Beat Frequency
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Three-Factor%20Learning%20Rule/" class="md-nav__link">
        Search
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
      
      
      
        <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
          People
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          People
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Anna%20Levina/" class="md-nav__link">
        Anna Levina
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Friedemann%20Zenke/" class="md-nav__link">
        Friedemann Zenke
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Jan%20Tore%20L%C3%B8nning/" class="md-nav__link">
        Jan Tore Lnning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Jeffrey%20Allan%20Lugowe/" class="md-nav__link">
        Jeffrey Allan Lugowe
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Kai%20Olav%20Ellefsen/" class="md-nav__link">
        Kai Olav Ellefsen
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Katalin%20Vertes/" class="md-nav__link">
        Katalin Vertes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Klas%20Henning%20Pettersen/" class="md-nav__link">
        Klas Henning Pettersen
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Sven%20Peter%20N%C3%A4sholm/" class="md-nav__link">
        Sven Peter Nsholm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/people/" class="md-nav__link">
        In this folder
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
          Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/ADAM%20Optimizer/" class="md-nav__link">
        ADAM Optimizer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Adiabatic%20Process/" class="md-nav__link">
        Adiabatic Process
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Bellman%20Equation/" class="md-nav__link">
        From [[HF DRL - Unit 2]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Chaos/" class="md-nav__link">
        Chaos
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Complexity/" class="md-nav__link">
        Reading
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Continuous%20Markov%20Decision%20Process/" class="md-nav__link">
        Continuous Markov Decision Process
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Convolutional%20Neural%20Network%20%28CNN%29/" class="md-nav__link">
        Convolutional Neural Network (CNN)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Cortico-basal%20ganglia-thalamo-cortical%20loop/" class="md-nav__link">
        Cortico basal ganglia thalamo cortical loop
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Credit%20Assignment%20Problem/" class="md-nav__link">
        Credit Assignment Problem
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Criticality/" class="md-nav__link">
        Criticality
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Cross%20Entropy/" class="md-nav__link">
        Cross Entropy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Deep%20Q-Learning/" class="md-nav__link">
        Deep Q Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Double%20DQN/" class="md-nav__link">
        Double DQN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Dyna-Q/" class="md-nav__link">
        Dyna Q
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Echo%20State%20Property%20%28ESP%29/" class="md-nav__link">
        Echo State Property (ESP)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Eligibility%20Trace/" class="md-nav__link">
        Commentary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/End-to-End%20Reinforcement-Learning/" class="md-nav__link">
        End to End Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Epsilon%20Decay/" class="md-nav__link">
        -Decay Strategies
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Ergodicity/" class="md-nav__link">
        Ergodicity / Ergodic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Excitatory%20-%20Inhibitory%20Oscillation%20%28EIO-SBF%29/" class="md-nav__link">
        Excitatory   Inhibitory Oscillation (EIO SBF)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Excitatory%20-%20Inhibitory%20postsynaptic%20potential%20%28EPSP%29%20-%20%28IPSP%29/" class="md-nav__link">
        Excitatory   Inhibitory postsynaptic potential (EPSP)   (IPSP)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Firing-Rate%20RNN%20Model/" class="md-nav__link">
        Firing Rate RNN Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Fixed%20Interval%20%28FI%29%20procedure/" class="md-nav__link">
        Main
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Geometric%20Deep%20Learning/" class="md-nav__link">
        Geometric Deep Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Impairment%20effect/" class="md-nav__link">
        Impairment effect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Interval%20Timing/" class="md-nav__link">
        Interval Timing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Izhikevich%20%20simple%20spiking%20neurons/" class="md-nav__link">
        Izhikevich  simple spiking neurons
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Kernel/" class="md-nav__link">
        Kernel
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/L2-Norm/" class="md-nav__link">
        L2 Norm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Leaky%20Integrate-and-Fire/" class="md-nav__link">
        Leaky Integrate and Fire
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Long%20Short-Term%20Memory%20%28LSTM%29/" class="md-nav__link">
        Long Short Term Memory (LSTM)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Marr%E2%80%99s%20three%20levels%20of%20analysis/" class="md-nav__link">
        Marrs three levels of analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Membrane%20Time%20Constant/" class="md-nav__link">
        Membrane Time Constant
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Multi-Armed%20Bandit%20Problem/" class="md-nav__link">
        Multi Armed Bandit Problem
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Multiplicative%20Weights%20Update%20Algorithm/" class="md-nav__link">
        About
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neural%20Cellular%20Automata/" class="md-nav__link">
        Neural Cellular Automata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neural%20Oscillations/" class="md-nav__link">
        Gamma
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neuromorphics/" class="md-nav__link">
        Neuromorphics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neuroscientific%20AI/" class="md-nav__link">
        Neuroscientific AI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Nomenclature%20for%20Experiments/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Novelty%20Signal/" class="md-nav__link">
        Novelty Signal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Nullcline/" class="md-nav__link">
        Nullcline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/On%20Off%20Policy/" class="md-nav__link">
        On Off Policy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Optuna/" class="md-nav__link">
        Optuna
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Pad%C3%A9%20approximants/" class="md-nav__link">
        Pad approximants
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Pareto%20Optimality/" class="md-nav__link">
        Pareto Optimality
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Peak%20Interval%20%28PI%29%20procedure/" class="md-nav__link">
        Peak Interval (PI) procedure
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Physical%20Symbol%20System%20Hypothesis%20%28PSSH%29/" class="md-nav__link">
        Physical Symbol System Hypothesis (PSSH)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Power%20Law%20distribution/" class="md-nav__link">
        Power Law distribution
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Q-Learning%20v.%20SARSA/" class="md-nav__link">
        Q Learning v. SARSA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Q-Learning/" class="md-nav__link">
        Q Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/REINFORCE/" class="md-nav__link">
        REINFORCE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Randomness%2C%20Disorder%2C%20and%20Noise/" class="md-nav__link">
        Randomness, Disorder, and Noise
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/ReLU/" class="md-nav__link">
        ReLU
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Relevance/" class="md-nav__link">
        Relevance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Reward%20Prediction%20Error/" class="md-nav__link">
        Reward Prediction Error
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/SARSA%20Algorithm/" class="md-nav__link">
        SARSA Algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Scalar%20Expectancy%20Theory/" class="md-nav__link">
        Scalar Expectancy Theory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Semi-Markov%20Decision%20Process%20%28SMDP%29/" class="md-nav__link">
        Semi Markov Decision Process (SMDP)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Separation%20Property/" class="md-nav__link">
        Separation Property
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Softmax/" class="md-nav__link">
        Softmax
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Spike%20Response%20Model%20neuron/" class="md-nav__link">
        Extracts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Stochastic%20Gradient%20Descent/" class="md-nav__link">
        Stochastic Gradient Descent
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Striatal%20Beat%20Frequency%20model%20%28SBF%29/" class="md-nav__link">
        Referenced in:
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Surrogate%20Gradient%20Learning/" class="md-nav__link">
        Surrogate Gradient Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Target%20v%20behavior%20Policy/" class="md-nav__link">
        Target v behavior Policy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Temporal%20Bisection%20Task/" class="md-nav__link">
        Temporal Bisection Task
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Temporal%20Difference%20%28TD%29/" class="md-nav__link">
        Temporal Difference (TD)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Three-Factor%20Learning%20Rule/" class="md-nav__link">
        Three Factor Learning Rule
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Three-Factor%20Learning/" class="md-nav__link">
        Three Factor Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Tigerprop/" class="md-nav__link">
        Tigerprop
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Time%20Invariance/" class="md-nav__link">
        Definition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Timing%20Tasks/" class="md-nav__link">
        Timing Tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Topics%20-%20Striatal%20Beat%20Frequency%20model%20%28SBF%29/" class="md-nav__link">
        Referenced in:
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Vogels-Abbott%20Benchmark/" class="md-nav__link">
        Vogels Abbott Benchmark
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Weber%27s%20Law/" class="md-nav__link">
        (Webers law):
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Weighted%20majority%20algorithm/" class="md-nav__link">
        Weighted majority algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Wilson-Cowan%20rate%20model/" class="md-nav__link">
        Wilson Cowan rate model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/behavior%20theory%20of%20timing%20%28BeT%29/" class="md-nav__link">
        [[@hasegawaModelMultisecondTiming2015|Takayuki Hasegawa, Shogo Sakata 2015]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/bias-variance%20trade-off/" class="md-nav__link">
        Bias variance trade off
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/catastrophic%20forgetting/" class="md-nav__link">
        Catastrophic forgetting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/central%20tendency%20effects/" class="md-nav__link">
        Central tendency effects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/derivative%20log%20trick/" class="md-nav__link">
        Derivative log trick
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/discount%20factor/" class="md-nav__link">
        Discount factor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/e-prop/" class="md-nav__link">
        E prop
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/learned%20embedding/" class="md-nav__link">
        Learned embedding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/local%20plasticity/" class="md-nav__link">
        Local plasticity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/multiple%20clock%20or%20flexible%20clock%20hypothesis/" class="md-nav__link">
        Multiple clock or flexible clock hypothesis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/pacemaker%20accumulator%20models%20%28PA%29/" class="md-nav__link">
        Reading
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/perceptual%20aliasing/" class="md-nav__link">
        Perceptual aliasing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/primitive%20actions/" class="md-nav__link">
        Primitive actions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/rescaling/" class="md-nav__link">
        Rescaling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/stride/" class="md-nav__link">
        Stride
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/temporal%20abstraction/" class="md-nav__link">
        Temporal abstraction
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          AZ. Assets
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          AZ. Assets
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../AZ.%20Assets/Outline%20or%20Goals%20Draft/" class="md-nav__link">
        Outline or Goals Draft
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          Features
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Features
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Features/LaTeX%20Math%20Support/" class="md-nav__link">
        LaTeX Math Support
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Features/Mermaid%20Diagrams/" class="md-nav__link">
        Mermaid diagrams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Features/Text%20Formatting/" class="md-nav__link">
        Text Formatting
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          Topic 1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Topic 1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Topic%201/Note%201/" class="md-nav__link">
        Note 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Topic%201/Note%202/" class="md-nav__link">
        Note 2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#excerpt" class="md-nav__link">
    Excerpt
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1. Introduction
  </a>
  
    <nav class="md-nav" aria-label="1. Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-what-is-a-deep-spiking-neural-network" class="md-nav__link">
    1.1. What Is a Deep Spiking Neural Network?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-advantages-of-deep-snns" class="md-nav__link">
    1.2. Advantages of Deep SNNs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-limitations-of-deep-snns" class="md-nav__link">
    1.3. Limitations of Deep SNNs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-inference-with-deep-snns" class="md-nav__link">
    2. Inference With Deep SNNs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-training-of-deep-snns" class="md-nav__link">
    3. Training of Deep SNNs
  </a>
  
    <nav class="md-nav" aria-label="3. Training of Deep SNNs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-binary-deep-neural-networks" class="md-nav__link">
    3.1. Binary Deep Neural Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-conversion-of-deep-neural-networks" class="md-nav__link">
    3.2. Conversion of Deep Neural Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-training-of-constrained-networks" class="md-nav__link">
    3.3. Training of Constrained Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-supervised-learning-with-spikes" class="md-nav__link">
    3.4. Supervised Learning With Spikes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-local-learning-rules" class="md-nav__link">
    3.5. Local Learning Rules
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-neuromorphic-hardware" class="md-nav__link">
    4. Neuromorphic Hardware
  </a>
  
    <nav class="md-nav" aria-label="4. Neuromorphic Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-inference-on-neuromorphic-hardware" class="md-nav__link">
    4.1. Inference on Neuromorphic Hardware
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-on-chip-learning" class="md-nav__link">
    4.2. On-Chip Learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-applications" class="md-nav__link">
    5. Applications
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-discussion" class="md-nav__link">
    6. Discussion
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#author-contributions" class="md-nav__link">
    Author Contributions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#funding" class="md-nav__link">
    Funding
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conflict-of-interest-statement" class="md-nav__link">
    Conflict of Interest Statement
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#acknowledgments" class="md-nav__link">
    Acknowledgments
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="frontiers-deep-learning-with-spiking-neurons-opportunities-and-challenges-neuroscience">Frontiers | Deep Learning With Spiking Neurons: Opportunities and Challenges | Neuroscience<a class="headerlink" href="#frontiers-deep-learning-with-spiking-neurons-opportunities-and-challenges-neuroscience" title="Permanent link">&para;</a></h1>
<blockquote>
<h2 id="excerpt">Excerpt<a class="headerlink" href="#excerpt" title="Permanent link">&para;</a></h2>
<p>Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years...</p>
</blockquote>
<hr />
<h2 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h2>
<p>Training and inference with deep neural networks (DNNs), commonly known as deep learning (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B90">LeCun et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B167">Schmidhuber, 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B53">Goodfellow et al., 2016</a>), has contributed to many of the spectacular success stories of artificial intelligence (AI) in recent years (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B54">Goodfellow et al., 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B3">Amodei et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B59">He et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B174">Silver et al., 2016</a>). Models of cortical hierarchies from neuroscience have strongly inspired the architectural principles behind DNNs (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B44">Fukushima, 1988</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B160">Riesenhuber and Poggio, 1999</a>), but at the implementation level, only marginal similarities between brain-like computation and analog neural networks (ANNs) as used in AI applications can be recognized. One obvious difference is that neurons in ANNs are mostly non-linear but continuous function approximators that operate on a common clock cycle, whereas biological neurons compute with asynchronous <em>spikes</em> that signal the occurrence of some characteristic event by digital and temporally precise action potentials. In recent years, researchers from the domains of machine learning, computational neuroscience, neuromorphic engineering, and embedded systems design have tried to bridge the gap between the big success of DNNs in AI applications and the promise of spiking neural networks (SNNs) (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B101">Maass, 1997</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B152">Ponulak and Kasinski, 2011</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B55">Grning and Bohte, 2014</a>). This promise of SNNs results from their favorable properties exhibited in real neural circuits like brains, such as analog computation, low power consumption, fast inference, event-driven processing, online learning, and massive parallelism. Furthermore, event-based vision and audio sensors (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B96">Lichtsteiner et al., 2008</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B153">Posch et al., 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B100">Liu et al., 2015</a>) have reached an increasingly mature level, and deep SNNs are one of the most promising concepts for processing such inputs efficiently (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B187">Tavanaei et al., 2018</a>). This line of research has coincided with an increased interest in efficient hardware implementations for conventional DNNs, since the massive hunger for computational resources has turned out to be a major obstacle as deep learning makes its way toward real-world applications such as automated driving, robotics, or the internet of things (IoT). Concepts such as so-called <em>binary networks</em>, which allow in-memory computations, share a binary and potentially sparse communication scheme with SNNs. However, such networks are typically executed in a synchronized manner, which is different from the event-driven (asynchronous) mode of execution in SNNs. Consequently, a fruitful interdisciplinary exchange of ideas to build neuromorphic systems for these concepts is taking place.</p>
<p>In this review, we provide an overview of several key ideas behind deep SNNs, and discuss challenges and limitations of SNNs compared to their ANN counterparts, as well as opportunities for future applications, in particular in conjunction with novel computing models and hardware currently being developed. This article is structured as follows: Section 2 discusses the preparation of input and output in order to perform inference with deep SNNs. In section 3, we give an overview of how deep SNNs can be trained, how this is connected to training conventional DNNs, and how to possibly learn on spike level. Section 4 discusses efficient implementations of deep SNNs on neuromorphic hardware and their limitations, as well as highlights similarities to hardware-efficient solutions for conventional DNNs. In section 5, we present possible use cases of deep SNNs, and argue that their strengths are complementary to those of conventional DNNs. Finally, section 6 provides a discussion of the state-of-the-art, and gives an outlook on promising research directions.</p>
<h3 id="11-what-is-a-deep-spiking-neural-network">1.1. What Is a Deep Spiking Neural Network?<a class="headerlink" href="#11-what-is-a-deep-spiking-neural-network" title="Permanent link">&para;</a></h3>
<p>Neural networks are typically called <em>deep</em> in case they have at least two hidden layers computing non-linear transformations of the input. In this article, we consider only feed-forward networks, which compute a mapping from input to output (for an example see Figure <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#F1">1A</a>), and do not address recurrent neural networks. Our definition includes multi-layer fully-connected networks, convolutional neural networks (CNNs; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B89">LeCun and Bengio, 1995</a>), deep belief networks (DBNs; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B61">Hinton et al., 2006</a>), deep autoencoders, and many more.</p>
<p><a href="https://www.frontiersin.org/files/Articles/409662/fnins-12-00774-HTML/image_m/fnins-12-00774-g001.jpg"><img alt="www.frontiersin.org" src="https://www.frontiersin.org/files/Articles/409662/fnins-12-00774-HTML/image_t/fnins-12-00774-g001.gif" /></a></p>
<p><strong>Figure 1</strong>. Comparison of deep spiking neural networks (SNNs) to conventional deep neural networks (DNNs). <strong>(A)</strong> Example of a deep network with two hidden layers. Here, exemplarily a fully-connected network is shown. Neurons are depicted with circles, connections with lines. <strong>(B)</strong> Time-stepped layer-by-layer computation of activations in a conventional DNN with step duration <em>T</em>. The activation values of neurons (rectangles) are exemplarily visualized with different gray values. The output of the network, e.g. categories in the case of a classification task, is only available after all layers are completely processed. <strong>(C)</strong> Like <strong>(B)</strong>, but with binarized activations. <strong>(D)</strong> The activity of a deep SNN showing a fast and asynchronous propagation of spikes through the layers of the network. <strong>(E)</strong> The membrane potential of the neuron highlighted in green in <strong>(D)</strong>. When the membrane potential (green) crosses the threshold (black dashed line) a spike is emitted and the membrane potential is reset. <strong>(F)</strong> The first spike in the output layer (red arrow in <strong>D</strong>) rapidly estimates the category (assuming a classification task) of the input. The accuracy of this estimation increases over time with the occurrence of more spikes (red line and <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B34">Diehl et al., 2015</a>). In contrast, the time-stepped synchronous operation mode of DNNs results in later, but potentially more accurate classifications compared to SNNs (blue dashed line and red arrows in <strong>B,C</strong>).</p>
<p>Spiking neural networks were originally studied as models of biological information processing (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B52">Gerstner and Kistler, 2002</a>), in which neurons exchange information via spikes (for an example, see Figure <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#F1">1D</a>). It is assumed that all spikes are stereotypical events, and, consequently, the processing of information is reduced to two factors: first, the timing of spikes, e.g., firing frequencies, relative timing of pre- and postsynaptic spikes, and particular firing patterns. Second, the identity of the synapses used, i.e., which neurons are connected, whether the synapse is excitatory or inhibitory, the synaptic strength, and possible short-term plasticity or modulatory effects. Depending on the level of detail of the simulation neurons are either point neurons in which arriving spikes immediately change their (somatic) membrane potentials, or are modeled as multi-compartment models with complex spatial (dendritic) structure, such that dendritic currents can interact before the somatic potential is modified. Different spiking neuron models such as the integrate-and-fire, spike response, or Hodgkin-Huxley model describe the evolution of the membrane potential and spike generation in different levels of detail. Typically, the membrane potential integrates currents from arriving spikes and generates a new spike whenever some threshold is crossed (Figure <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#F1">1E</a>). Once a spike is generated, it is sent via the axon to all connected neurons with a small axonal delay and the membrane potential is reset toward a given baseline.</p>
<p>The most direct connection between analog and spiking neural networks is made by considering the activation of an analog neuron as the equivalent of the firing rate of a spiking neuron assuming a steady state. Many models of neuronal measurements have used such rate codes to explain computational processes in brains (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B66">Hubel and Wiesel, 1959</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B159">Rieke, 1999</a>). However, spiking neuron models can also model more complex processes that depend on the relative timing between spikes (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B57">Gtig, 2014</a>) or on timing relative to some reference signal, such as network oscillations (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B115">Montemurro et al., 2008</a>). Temporal codes are of high importance in biology where even a single spike or small temporal variations of single neuron firing may trigger different reactions (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B51">Gerstner et al., 1996</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B180">Stemmler, 1996</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B159">Rieke, 1999</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B103">Machens et al., 2003</a>), because often decisions have to be made before a reliable estimate of a spike rate can be computed.</p>
<p>Besides the biologically motivated definition of SNNs, there is a more pragmatic application-oriented view coming from the field of neuromorphic engineering, where SNNs are often called <em>event-based</em> instead of spiking (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B100">Liu et al., 2015</a>). Here, an event is a digital packet of information, which is characterized by its origin and destination address, a timestamp, and - in contrast to biologically motivated SNNsmay carry a few bits of payload information. The origin of this view is the address event representation (AER) protocol (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B104">Mahowald, 1994</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B16">Boahen, 2000</a>), which is used to connect, e.g., event-based sensors (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B96">Lichtsteiner et al., 2008</a>) via digital interconnect to neuromorphic chips (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B70">Indiveri et al., 2011</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B2">Amir et al., 2017</a>) or digital post-processing hardware (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B46">Furber et al., 2014</a>). Event-based vision sensors use the payload bits to distinguish visual ON or OFF events, but the payload can also be used to send any other type of relevant information to the postsynaptic targets potentially computing more sophisticated functions than simple integrate-and-fire (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B179">Stefanini et al., 2014</a>).</p>
<h3 id="12-advantages-of-deep-snns">1.2. Advantages of Deep SNNs<a class="headerlink" href="#12-advantages-of-deep-snns" title="Permanent link">&para;</a></h3>
<p>A motivation for studying SNNs is that brains exhibit a remarkable cognitive performance in real-world tasks. With ongoing efforts toward improving our understanding of brain-like computation, there are expectations that models staying closer to biology will also come closer to achieving natural intelligence than more abstract models, or at least will have greater computational efficiency.</p>
<p>SNNs are ideally suited for processing spatio-temporal event-based information from neuromorphic sensors, which are themselves power efficient. The sensors record temporally precise information from the environment and SNNs can utilize efficient temporal codes in their computations as well (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B117">Mostafa, 2018</a>). This processing of information is also event-driven meaning that whenever there is little or no information recorded the SNN does not compute much, but when sudden bursts of activity are recorded, the SNN will create more spikes. Under the assumption that typically information from the outside world is sparse, this results in a highly power-efficient way of computing. In addition, using time domain input is additional valuable information compared to frame-driven approaches, where an artificial time step imposed by the sensor is introduced. This can lead to efficient computation of features such as optical flow (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B13">Benosman et al., 2014</a>) or stereo disparity (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B137">Osswald et al., 2017</a>), and in combination with learning rules sensitive to spike timing leads to more data-efficient training (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B141">Panda et al., 2017</a>).</p>
<p>In deep SNNs, the asynchronous data-driven mode of computing leads to fast propagation of salient information through multiple layers of the network. To best exploit this effect in practice, SNNs should be run on neuromorphic hardware. In combination with an event-based sensor, this results in <em>pseudo-simultaneous</em> information processing (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B41">Farabet et al., 2012</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B24">Camuas-Mesa et al., 2014</a>), which means that a first approximate output of the final layer is available immediately after recording the first input spikes. This is true even for multi-layer networks, because spikes begin to propagate immediately to higher layers as soon as the lower layer provides sufficient activity (Figure <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#F1">1D</a>). It is not necessary to wait for the complete input sequence to finish, which is in contrast to conventional DNNs, where all layers need to be fully updated before the final output can be computed (Figures <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#F1">1B,C</a>). The initial output spikes are necessarily based on incomplete information, hence it has been shown that deep SNNs improve their classification performance the longer they are given time to process more spikes of their input (Figure <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#F1">1F</a>). SNNs can also be trained specifically to reduce the latency of approximate inference (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B128">Neil et al., 2016a</a>).</p>
<p>SNNs are the preferred computational model to exploit highly energy-efficient neuromorphic hardware devices, which support the data-driven processing mode, and keep computations local, thereby avoiding expensive memory access operations.</p>
<h3 id="13-limitations-of-deep-snns">1.3. Limitations of Deep SNNs<a class="headerlink" href="#13-limitations-of-deep-snns" title="Permanent link">&para;</a></h3>
<p>One of the biggest drawbacks of deep SNNs is that despite recent progress (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B162">Rueckauer et al., 2017</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B171">Sengupta et al., 2018</a>) their accuracy on typical benchmarks such as MNIST (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B91">Lecun et al., 1998</a>), CIFAR (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B83">Krizehvsky and Hinton, 2009</a>), or ImageNet (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B163">Russakovsky et al., 2015</a>) do not reach the same levels as their machine learning counterparts. To some extent, this can be attributed to the nature of these benchmarks, which are on conventional frame-based images. Thus, some form of conversion from images into spike trains is required that is typically lossy and inefficient. Another limiting factor is the lack of training algorithms that make specific use of the capabilities of spiking neurons, e.g., efficient time codes. Instead, most approaches use rate-based approximations of conventional DNNs, which means that no accuracy gains can be expected. Deep SNNs might still be useful in such scenarios, because approximate results might be obtained faster and more efficiently than on conventional systems, especially if the SNN is run on neuromorphic hardware. Training algorithms for SNNs are also more difficult to design and analyze, because of the asynchronous and discontinuous way of computing, which makes a direct application of successful backpropagation techniques as used for DNNs difficult.</p>
<p>The performance of SNNs on conventional AI benchmarks should only be seen as a proof-of-concept, but not as the ultimate research goal. If spiking networks model biology, then we should expect them to be optimized for the behaviorally most relevant tasks, such as making decisions based on continuous input streams while moving in the real world. Image classification corresponds to the task of classifying a random image suddenly flashed on the retina, without any supporting context. While brains are able to solve such tasks (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B190">Thorpe et al., 1996</a>), they are certainly not optimized for it. We are currently lacking both good benchmark datasets and evaluation metrics that could measure efficient real-world performance. One fruitful direction is the collection of dynamic vision sensor (DVS) benchmarks (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B134">Orchard et al., 2015a</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B173">Serrano-Gotarredona and Linares-Barranco, 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B63">Hu et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B99">Liu et al., 2016</a>), in particular for relevant use cases such as automated driving (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B15">Binas et al., 2017</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B176">Sironi et al., 2018</a>).</p>
<h2 id="2-inference-with-deep-snns">2. Inference With Deep SNNs<a class="headerlink" href="#2-inference-with-deep-snns" title="Permanent link">&para;</a></h2>
<p>Before diving into the discussion of how to train deep SNNs, we briefly discuss inference with fully trained deep SNNs, i.e., the transformation of input signals to output signals. Whereas updates between hidden layers are straightforward (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B52">Gerstner and Kistler, 2002</a>), the input and output layers deserve special attention.</p>
<p>In the ideal case, the input of deep SNNs are already spike trains, e.g., from neuromorphic sensors. However, in many cases, especially when using conventional benchmark datasets, some form of conversion from the input signal into spike trains is necessary. The most widely used method is for each pixel to translate real-valued input such as gray levels or color intensities into spike trains drawn from Poisson processes with proportional firing rates (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B132">O'Connor et al., 2013</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B25">Cao et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B34">Diehl et al., 2015</a>). This implies that only average firing rates are important for classification and information of precise timing is neglected. Although this is clearly a sub-optimal use of SNNs, the method is effective in practice and can be realized in hardware (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B127">Neil and Liu, 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B181">Stromatias et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B168">Schmitt et al., 2017</a>).</p>
<p>Alternative codes that enable an efficient use of spike times have only recently been introduced to spiking deep networks. <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B79">Kheradpisheh et al. (2018)</a> use a rank-order code in which every neuron can fire at most once. <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B118">Mostafa et al. (2017a)</a> propose a very sparse and efficient temporal code, in which the output of a neuron is the time of its first spike. Such codes drastically reduce the number of spikes sent through the network, and training can be achieved via backpropagation (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B117">Mostafa, 2018</a>) or STDP (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B79">Kheradpisheh et al., 2018</a>). <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B136">Orchard et al. (2015b)</a> used the timing of spikes to determine maxima in pooling operations. Temporal codes are very efficient, fast, and map well to hardware, but they have so far not been able to match the state-of-the-art accuracy of ANNs or rate-based SNNs. In order to tune the trade-off between rate-based and temporal coding <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B87">Lagorce et al. (2017)</a> and <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B176">Sironi et al. (2018)</a> propose to use time surfaces around event input as hierarchical features.</p>
<p>During inference, hidden layers are updated by sending spikes from pre- to postsynaptic neurons. In deep SNNs, simple and efficient models for membrane potential updating and spike generation are typically preferred over more biologically plausible ones (for examples see section 1). One spike from a presynaptic neuron triggers updates of many postsynaptic neurons depending on the number of outgoing connections.</p>
<p>In the output layer of deep SNNs another conversion takes place. Assuming we are dealing with classification tasks, spike trains need to be converted into categories. The simplest form of output code is to report the class corresponding to the neuron with the highest firing rate over some time period or over a fixed number of total output spikes. An extreme case is to report the neuron firing first as the output class, which typically achieves already good performance (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B136">Orchard et al., 2015b</a>). However, <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B34">Diehl et al. (2015)</a> have shown that the classification accuracy increases with the number of output spikes taken into account. Furthermore, SNNs can be specifically optimized to report correct output spikes as early as possible (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B128">Neil et al., 2016a</a>). Instead of using the output of single neurons, larger populations of neurons can be used to reduce the variance of the output, or temporal smoothing may be performed.</p>
<h2 id="3-training-of-deep-snns">3. Training of Deep SNNs<a class="headerlink" href="#3-training-of-deep-snns" title="Permanent link">&para;</a></h2>
<p>Conventional deep learning is relying on stochastic gradient descent and error backpropagation, which requires differentiable activation functions. Consequently, in order to reduce activations to binary values, often also interpreted as spikes, modifications are required. Such <em>binary networks</em> share the discontinuous nature of spikes, but not the asynchronous operation mode with SNNs. The integration of the timing of spikes into the training process, only required for asynchronous SNNs, requires additional effort. Five main strategies for training deep SNNs have been developed over the past years. In this section, we will briefly review these approaches and discuss their advantages and disadvantages:</p>
<p>1. Binarization of ANNs: Conventional DNNs are trained with binary activations, but maintain their synchronous mode of information processing.</p>
<p>2. Conversion from ANNs: Conventional DNNs are trained with backpropagation, and then all analog neurons are converted into spiking ones.</p>
<p>3. Training of constrained networks: Before conversion, conventional DNN training methods are used together with constraints that model the properties of the spiking neuron models.</p>
<p>4. Supervised learning with spikes: Directly training SNNs using variations of error backpropagation.</p>
<p>5. Local learning rules at synapses, such as STDP (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B14">Bi and Poo, 1998</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B177">Song et al., 2000</a>), are used for more biologically realistic training.</p>
<h3 id="31-binary-deep-neural-networks">3.1. Binary Deep Neural Networks<a class="headerlink" href="#31-binary-deep-neural-networks" title="Permanent link">&para;</a></h3>
<p>A simple method to convert ANNs into networks using spikes for communication is to binarize activations for efficient inference (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B65">Hubara et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B81">Kim and Smaragdis, 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B157">Rastegari et al., 2016</a>). Binarized networks propagate information in a synchronized way and layer-by-layer like in conventional DNNs, which does not allow for asynchronous information processing and fast propagation of most salient features as in SNNs (compare <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#F1">Figures 1C,D</a>). However, binarization makes network execution on event-based neuromorphic systems energy-efficient due to sparse activations and computation on demand (see section 4). Furthermore, the computational costs on conventional hardware, like CPUs and GPUs, are also decreased by binarization, since the memory bandwidth as well as the complexity of multiply-add operations are reduced (for a review see <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B183">Sze et al., 2017</a>), and weight kernels can be re-used within the same network (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B65">Hubara et al., 2016</a>). In an extreme case, if both activations and weights are binarized, multiply-add operations can be reduced to bitwise XNORs and bit counting (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B157">Rastegari et al., 2016</a>, note that binarized means values in {1, 1} for this example). In networks with binary activations, weights are usually also quantized to lower-bit representations, and hence, in the following, we review training methods for networks with binary activations and low-bit weights.</p>
<p>To obtain good test performance, binarization of a network (e.g., by rounding) after its training with floating point activations is usually not sufficient (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B76">Judd et al., 2015</a>). Instead, networks have to be trained with binarized activations from scratch. Two different approaches are commonly used: training with deterministic and stochastic methods. Deterministic methods usually apply <em>straight-through-estimators</em> (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B9">Bengio et al., 2013</a>) to approximate non-differentiable activation functions during backpropagation and accumulate gradients on so-called <em>shadow weights</em> (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B30">Courbariaux et al., 2015</a>). Activations and shadow weights are quantized during the forward pass, while during the backward pass gradients are calculated by assuming that both activations and weights are continuous values. Weight updates are accumulated in shadow weights, which allows quantized weights to change their value in the forward pass in the presence of only very small weight changes during individual training steps. A commonly used stochastic method is <em>expectation backpropagation</em>, where neuron activations and synaptic weights are represented by probability distributions updated by backpropagation (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B178">Soudry et al., 2014</a>). <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B39">Esser et al. (2015)</a> adapted this method and showed that ensembles of networks with deterministic binary activations and their ternary weights randomly drawn from the learned probability distributions achieve comparable results to unconstrained networks on classification tasks.</p>
<p>A number of methods exist that improve test performance specifically for networks with binary activations. These methods include normalization of activations (e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B65">Hubara et al., 2016</a>), modifications of regularizers (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B185">Tang et al., 2017</a>), gradual transitions from soft to hard binarizations (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B195">Wu et al., 2015</a>), adding noise on activations and weights (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B111">Merolla et al., 2016</a>), and knowledge distillation (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B193">Wu et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B150">Polino et al., 2018</a>). Nevertheless, networks with binary activations usually show degraded test performance, which can partly be compensated by modifying the network structure, e.g., increasing their width (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B114">Mishra et al., 2017</a>). Input and output layers, as well as first and last weight kernels are often not binarized, because only little additional computational resources are needed and otherwise the test performance is likely to significantly drop (e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B65">Hubara et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B157">Rastegari et al., 2016</a>). Binarization also improves the robustness of networks against adversarial examples (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B49">Galloway et al., 2018</a>) and other distortions (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B111">Merolla et al., 2016</a>).</p>
<p><a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B65">Hubara et al. (2016)</a> achieve good results on classification tasks by modifying the binarization scheme of activations <em>a</em> from spikes (<em>a</em>  {0, 1}) to <em>a</em>  {1, 1}, thereby improving the convergence properties (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B95">Li et al., 2017</a>). Although events with payload (here with size 1 bit; see also section 1.1) could be used for communication in order to implement the latter scheme on neuromorphic hardware, communication is not sparse anymore. It is also possible to use payloads with multiple bits per spike to simulate low-precision networks (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B29">Courbariaux et al., 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B32">Deng et al., 2018</a>), but this approach is outside the scope of the present review.</p>
<p>In summary, binary networks offer efficient inference, which often comes at the cost of slight performance degradations. In addition, the learning process usually takes longer than for unconstrained networks, since the training methods are more complex, intermediate results need to be tracked with floating point precision, and networks are potentially larger. As the need for energy-efficient conventional deep networks increases, binary networks are an active and important research topic independent of their connection to SNNs.</p>
<h3 id="32-conversion-of-deep-neural-networks">3.2. Conversion of Deep Neural Networks<a class="headerlink" href="#32-conversion-of-deep-neural-networks" title="Permanent link">&para;</a></h3>
<p>To circumvent the problems of gradient descent in spiking networks, conventionally trained DNNs can be converted into deep SNNs by adapting weights and parameters of the spiking neurons. The goal is to achieve the same input-output mapping with a deep SNN as the original DNN. This mapping, however, does not only include the neural network itself, but also the input- and output-encoding, as discussed in section 2.</p>
<p>Conversion approaches were initially developed to process data from event-based sensors with convolutional networks. Whereas, early attempts used manually programmed convolution kernels on spike train inputs (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B172">Serrano-Gotarredona et al., 2009</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B145">Prez-Carrasco et al., 2013</a>) introduced the first systematic way to map conventionally learned CNNs to SNNs. Their conversion approach and that of almost all others follows the idea of rate-coding, such that activations of analog neurons are translated into firing rates of spiking ones. Weights are rescaled according to the parameters of spiking neurons such as leak rates or refractory times. These parameters are not present in conventional CNNs, and need to be set as hyperparameters before conversion. An alternative method is to use the <em>Neural Engineering Framework</em> (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B37">Eliasmith and Anderson, 2004</a>) to convert restricted Boltzmann machines into spiking networks (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B38">Eliasmith et al., 2012</a>). An approach for converting recurrent neural networks under constraints of a neuromorphic platform was presented by <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B35">Diehl et al. (2016)</a>.</p>
<p>The main advantage of the conversion approach is that the full toolkit of deep learning can be exploited, meaning that state-of-the-art deep networks for classification tasks can be straightforwardly converted into SNNs (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B64">Hu et al., 2018</a>). For most methods, the original DNNs can be trained without considering the later conversion into SNNs. Once the parameters of the DNN are known, conversion into an SNN usually consists only of parsing and simple transformations, and thus adds only negligible training overhead. Network conversion has set most benchmark records in terms of accuracy for SNNs, with negligible deviations in accuracy from the underlying DNNs (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B34">Diehl et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B171">Sengupta et al., 2018</a>). It is even possible to provide performance guarantees that can quantify the expected deviations in accuracy (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B162">Rueckauer et al., 2017</a>).</p>
<p>Conversion from conventional networks into SNNs, however, comes with its flaws: first of all, not all ANNs can easily be converted into SNNs. One major obstacle is that in ANNs it does not matter if activations are negative, whereas firing rates in SNNs are always positive. In principle, spiking neurons can be divided into excitatory and inhibitory neurons, i.e., neurons with exclusively positive or negative synapses, respectively. However, compared to these biologically realistic SNNs, ANNs can switch the sign of their activation between different inputs. One possible solution, first suggested by <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B145">Prez-Carrasco et al. (2013)</a>, is to have two spiking neurons for each ANN neuron, one for either positive or negative activations, but mutually exclusive. This problem has gotten less severe with the dominance of rectified linear-unit (ReLU) (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B123">Nair and Hinton, 2010</a>) activation functions in deep learning, because then activations are either zero or positive, and can thus easily be translated into firing rates (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B25">Cao et al., 2015</a>). Sigmoid activation functions were used in <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B145">Prez-Carrasco et al. (2013)</a>, but their non-linearity requires additional approximations and introduces additional errors compared to the mainly linear ReLU. Negative activations are a specific problem for softmax-layers at the output, but <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B162">Rueckauer et al. (2017)</a> presented a practical solution to this problem.</p>
<p>Another limitation of most conversion approaches for CNNs is that max-pooling operations, which are common in state-of-the-art analog deep networks, are difficult to realize in the spiking setting (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B197">Yu et al., 2002</a>). The main problem is that the maximum operation is non-linear, and cannot be computed on a spike-by-spike basis. Most approaches (e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B25">Cao et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B34">Diehl et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B171">Sengupta et al., 2018</a>) circumvented this problem by replacing all pooling operations with average pooling, which is easy to implement in SNNs as a linear operation, but leads to a drop of accuracy. A simple mechanism for max-pooling is presented in <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B162">Rueckauer et al. (2017)</a>, where output units contain gating functions that only let spikes from the maximally firing neuron pass, and all other spikes are discarded. This allows a non-linear pooling operation, and contributes to better accuracy of SNNs. Max operations can also be implemented with latency codes (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B136">Orchard et al., 2015b</a>), but this is not directly compatible with rate codes typically used for conversion.</p>
<p>While in ReLU networks activations of a layer can be linearly rescaled without changing the final class output by scaling all weights in this layer, SNNs are not immune to such rescaling of weights. Neurons with low firing rates are more susceptible to noisy firing rates and temporal jitter of spikes, which increases the variance of each estimate and elongates the time until a reliable estimate can be formed. Too high firing rates can also be an issue, especially if predicted firing rates exceed the maximum firing rate determined by the neuron parameters. <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B34">Diehl et al. (2015)</a> improved the performance of deep SNNs significantly by propagating a subset of training examples through the network, observing the firing rates in each layer, and rescaling the input weights to each layer such that a target rate is reached. <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B162">Rueckauer et al. (2017)</a> and <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B171">Sengupta et al. (2018)</a> extended this mechanism and improved the results for very deep networks by increasing the robustness against outliers and accounting for the actual firing rates during weight normalization, respectively.</p>
<p>Conversion and weight normalization may come at the cost of more spikes being produced, and thus less energy efficient classification. The trade-off between latency and accuracy in SNNs (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B34">Diehl et al., 2015</a>) allows compensating this effect by training deep SNNs to achieve a target performance level as early as possible (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B128">Neil et al., 2016a</a>). Nevertheless, ANNs converted into SNNs using rate codes are in general not particularly efficient in terms of spikes being produced, because multiple spikes are necessary to represent one real-valued activation. In the worst case the SNN might need more spike operations than the ANN needs multiply-adds. However, spike operations are cheaper than real-valued matrix multiplications, and can be implemented on very efficient neuromorphic hardware (section 4).</p>
<p>In order to address the inefficiencies of conversion approaches based on rate codes, an important direction of research investigates the use of alternative spike codes based on the timing information. This is particularly important when inputs come from event-based sensors, and therefore naturally contain precise timing information. The HFirst model (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B136">Orchard et al., 2015b</a>) introduced a spiking adaptation of an HMAX hierarchical network (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B160">Riesenhuber and Poggio, 1999</a>) with predefined Gabor filters. A temporal winner-take-all mechanism replaces the computation of rate maxima, and thus simplifies the classification. Time surface features (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B87">Lagorce et al., 2017</a>) capture the local spatio-temporal dynamics around events by computing a continuous-valued feature for small spatial and temporal windows around each event, which describes the spiking activity of nearby events. The HATS method (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B176">Sironi et al., 2018</a>) divides the image into a regular grid of cells, and smoothes all time surfaces within each cell and specified time window. A histogram of averaged time surfaces is formed, and the resulting feature vector is fed to a standard classifier, such as an SVM or a neural network. Approaches such as HATS are tailored for neuromorphic vision sensors, and have no equivalent in conventional deep learning or computer vision. <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B198">Zambrano and Bohte (2016)</a> introduces an asynchronous pulsed Sigma-Delta coding scheme for SNNs, which maintains the accuracy of the underlying ANN, but utilizes far fewer spikes than other ANN-to-SNN conversion methods.</p>
<h3 id="33-training-of-constrained-networks">3.3. Training of Constrained Networks<a class="headerlink" href="#33-training-of-constrained-networks" title="Permanent link">&para;</a></h3>
<p>Whereas the conversion approaches presented in section 3.2 start from fully trained ANNs, and then convert these networks into SNNs, <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B39">Esser et al. (2015)</a> coined the term <em>constrain-then-train</em> for approaches that include constraints due to the properties of spiking neurons or the target hardware already during the training process. Conventional learning rules for ANNs, such as backpropagation, are applied to learn the optimal weights under constraints of the spiking model. After training, a conversion into an SNN is performed, where the parameters of the constrained ANN model are directly used as parameters of the SNN without further weight scaling. There is a fine line between conversion and constrain-then-train methods, since conversion algorithms also put some constraints on the ANN model, e.g., they demand the use of ReLU activation functions or zero biases, and constrain-then-train models also convert ANNs into SNNs. The main difference is that conversion methods train the ANN just once and then map the weights for arbitrarily specified parameters of spiking neurons, whereas for constrain-then-train methods the ANN is trained for one specific setting of spiking neuron model parameters. If later these spiking neuron parameters should change then a complete retraining of the constrained ANN is required, which is not necessary for conversion methods. Constrain-then-train models have the potential to adapt better to the target platform than converted models, because the ANN training already considers specifics of the final SNN. As a result, constrain-then-train methods often yield better accuracy than generic conversion methods, at the expense of more complicated ANN training.</p>
<p>Constrain-then-train methods need to transform spiking neuron models into a continuous-valued and differentiable form that can be trained via backpropagation. In <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B39">Esser et al. (2015)</a> a training network was introduced which used continuous valued weights and activations constrained to be in the range [0, 1]. Such a network after training yields values representing probabilities of spikes occurring or binary synapses being on. The learned probabilities are used to generate samples of deployment networks with low-bit synapses, matching the constraints of the TrueNorth target platform (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B112">Merolla et al., 2014</a>). This results in highly accurate classifiers for MNIST, at very low energy costs (see also section 4). This approach was improved and extended to multi-chip setups in <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B40">Esser et al. (2016)</a>.</p>
<p>For more realistic models of spiking neurons it is often possible to derive or approximate a transfer function that relates a constant input current and neuron parameters (e.g., refractory period, reset voltage, etc.) to an average firing rate (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B52">Gerstner and Kistler, 2002</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B22">Burkitt, 2006</a>). This will typically result in a non-differentiable function, because output spikes will only be generated after the input current crossed a threshold. Instead of ignoring the more complex activation functions of spiking neurons and simply using ReLUs, constrain-then-train approaches typically introduce various smoothing approximations to model spike generation and the variability of spike times and rates more accurately (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B132">O'Connor et al., 2013</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B68">Hunsberger and Eliasmith, 2016</a>). The key idea is modeling this variability, e.g., noisy firing rates and jitter on spike times, in the input spike trains to obtain a differentiable activation function that enable training with backpropagation. After training, all neurons are turned into spiking ones using the model parameters set before training and the newly learned weights. The goal is to have the rate-coded SNN perform similar to the ANN resulting from the constrained learning process.</p>
<p>One of the first successful applications of deep SNNs was presented by <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B132">O'Connor et al. (2013)</a>, who used the so-called <em>Siegert approximation</em> (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B77">Jug et al., 2012</a>) to train a spiking DBN. Their training did not involve backpropagation, but contrastive divergence (CD) learning (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B61">Hinton et al., 2006</a>). However, the key ideas remain the same. For CD learning in SNNs, firing rates are used as proxies for activation probabilities that are normalized into the range [0, 1]. This transformation can be easily achieved by assuming that the inverse of the refractory period yields the maximum firing rate, and therefore also the maximum activation during training with CD. The different layers of the DBN are trained sequentially, and after training all neurons in the network are converted into spiking neurons, using the parameters defined before or during training. The Siegert approximation provides more accurate predictions of the output firing rate of leaky integrate-and-fire (I&amp;F) neurons, if the inputs are Poisson spike trains, rather than constant currents. A conventional DBN trained with Siegert approximation can thus be converted almost loss-less into a spiking DBN. Interestingly, the DBN trained on conventional MNIST also performs well when being fed inputs from an event-based vision sensor recording MNIST digits. Despite the mismatch to the assumed Poisson distribution, visual recognition with high accuracy can be performed in real-time.</p>
<p>A similar concept was used in <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B67">Hunsberger and Eliasmith (2015)</a>, where a spiking CNN was trained with backpropagation, using a so-called <em>soft leaky I</em>&amp;<em>F</em>. This neuron model employs a smoothed and therefore differentiable version of the I&amp;F transfer function, which can be used for gradient descent, and additionally adds noise to the training process. <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B68">Hunsberger and Eliasmith (2016)</a> show competitive results with this approach on a number of benchmarks, including the challenging ImageNet dataset.</p>
<h3 id="34-supervised-learning-with-spikes">3.4. Supervised Learning With Spikes<a class="headerlink" href="#34-supervised-learning-with-spikes" title="Permanent link">&para;</a></h3>
<p>Whereas constrain-then-train methods reduce the training of SNNs to training methods of conventional ANNs, various approaches have been proposed that directly introduce supervised learning on the level of spikes. These approaches do not necessarily aim for biological plausibility, which is the goal of approaches using local learning with STDP, as discussed in section 3.5. Instead, supervised training methods with spikes typically use variants of backpropagation to train deep SNNs. The obvious advantage of spike-based learning rules for SNNs is that they are not constrained to mean-rate codes, but can learn to utilize spatio-temporal patterns in spike trains, which can arise in inputs from event-based sensors. This might come at the cost of longer training times, because fully spiking simulations are computationally more expensive than simulations of conventional DNNs, although the number of spikes needed is typically lower than in rate-coded simulations of SNNs.</p>
<p>There are several supervised learning methods for spiking networks that work only for single layers, such as ReSuMe (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B151">Ponulak and Kasiski, 2010</a>) or the Tempotron (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B58">Gtig and Sompolinsky, 2006</a>). The focus of this review is on deep spiking networks, so we discuss in the following only methods that implement some form of backpropagation to train multiple layers. Another important distinction is the nature of the target signal: whereas some methods (e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B17">Bohte et al., 2002</a>) require a target spike train and during training try to reproduce the temporal pattern for a given input, for most other methods presented here (e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B93">Lee et al., 2016</a>) it is sufficient if a target label is provided. The training goal can then either be to have the correct output neuron firing more than all others, or having the correct output neuron firing first (e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B117">Mostafa, 2018</a>). Intermediate forms, such as defining a regular target spike train for the correct class as in <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B84">Kulkarni and Rajendran (2018)</a> are also possible.</p>
<p>The key for many spike-based learning rules for multi-layer SNNs is to find a real-valued and almost-everywhere differentiable proxy, on which backpropagation can be performed. The earliest attempts at training multi-layer SNNs fall into this category, most notably SpikeProp (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B17">Bohte et al., 2002</a>) and variants (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B170">Schrauwen and Van Campenhout, 2004</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B109">McKennoch et al., 2006</a>). SpikeProp derives a backpropagation rule for spike times in the output layer, and <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B19">Booij and tat Nguyen (2005)</a> showed an extension to patterns of multiple spikes. However, SpikeProp has not been applied to problems at the scale of modern deep learning applications, yet, because this method is computationally expensive.</p>
<p>Recently, <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B93">Lee et al. (2016)</a> have introduced a spike-based backpropagation rule that can train deep SNNs for conventional classification tasks (given only labels, but not target spike trains) directly from spike signals. The key trick is to perform stochastic gradient descent on real-valued membrane potentials. Discontinuities at the times of spikes are handled via low-pass filtering before they are used for backpropagation. Together with a variety of optimizations, this method achieves state-of-the-art results for deep SNNs on tasks such as MNIST, and its event-based counterpart N-MNIST (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B134">Orchard et al., 2015a</a>). (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B84">Kulkarni and Rajendran, 2018</a>) follow a similar approach and achieved similar results, but minimized the distance between network output and a regular firing target spike train for the desired output neuron, instead of the squared distance between normalized network output and one-hot labels.</p>
<p><a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B133">O'Connor and Welling (2016)</a> propose a spiking network that approximates a deep MLP with ReLU activations using signed spikes. During training backpropagation operates on collected spike statistics. Similarly, <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B182">Stromatias et al. (2017)</a> bin spike trains, and fine-tune output layers of deep SNNs by performing gradient descent on these real-valued histogram bins. A different approach is taken in <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B117">Mostafa (2018)</a>, where the time of first spike for each neuron is used as its activation value during training. This results in sparsely firing SNNs that are able to utilize temporal patterns in input sequences. A spatio-temporal backpropagation rule for SNNs is derived in <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B194">Wu et al. (2017)</a>. The authors are separating spatial input signals, which come from other neurons, from temporal dynamics arising from the spiking behavior of the neuron itself. Their results consistently show improvements over methods unaware of the temporal aspects. An interesting hybrid model has recently been proposed by <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B74">Jin et al. (2018)</a>, which uses a backpropagation rule for a rate-coded error signal on a longer "macro" time-scale, and combines this with an update on a shorter "micro" time-scale which captures individual spike effects. The method leads to state-of-the-art results on static MNIST and N-MNIST (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B134">Orchard et al., 2015a</a>).</p>
<p>Overall, the past years have clearly yielded an increasing number of spike-based learning approaches for supervised training, occasionally outperforming approaches based on conversion alone. Their benefits for machine learning tasks on neuromorphic sensor data is still not fully explored, but potentially even greater performance gains could be achieved by exploiting temporal codes that deviate from pure rate models.</p>
<h3 id="35-local-learning-rules">3.5. Local Learning Rules<a class="headerlink" href="#35-local-learning-rules" title="Permanent link">&para;</a></h3>
<p>It is of great interest for neuroscience to understand how hierarchically organized neural networks can be trained with local learning rules such as STDP (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B106">Markram et al., 1997</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B14">Bi and Poo, 1998</a>) or Hebbian learning (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B60">Hebb, 1949</a>). For practical applications, the use of local learning rules is very attractive, because it would allow very hardware-efficient ways of training DNNs. In addition, spike-timing dependent learning allows detecting spatio-temporal patterns as features.</p>
<p>The main obstacle for the use of purely local learning rules in deep networks is the difficulty to perform backpropagation of supervised error signals. An error signal might only be available at the output layer, and since the information flow in biological axons is assumed to be uni-directional, it is unclear how error information can reach lower levels of the hierarchy at all. Typical feed-forward architectures used in machine learning are not capable of providing the necessary training information to synapses learning with local rules. Hence, most of the following studies investigating local learning in hierarchies introduce recurrent feedback connections that modulate learning in lower layers.</p>
<p>One insight from biology is that feedback connections, i.e., connections projecting from higher to lower layers, are common and important in hierarchically organized networks for information processing, such as the cortex (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B105">Markov et al., 2014</a>). Furthermore, feedback connections could provide training signals in the framework of <em>predictive coding</em> (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B156">Rao and Ballard, 1999</a>). Another interesting perspective is that random back-projections of error signals are sufficient to train lower layers (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B97">Lillicrap et al., 2016</a>). This concept has been recently demonstrated for deep SNNs (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B125">Neftci et al., 2017</a>), and for networks with spiking multi-compartment neurons (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B56">Guerguiev et al., 2017</a>). Although the performance does not match that of conventional machine learning techniques (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B6">Bartunov et al., 2018</a>), this approach is a proof-of-concept that biologically plausible training of deep SNNs with local learning rules is possible. In particular, random backpropagation is one possible solution for training spiking and biologically realistic networks with backpropagation, despite deviating from the requirements of classical backpropagation, namely precise calculation of real-valued gradients, a separation and synchronization of forward- and backward-passes, and symmetry of weights in both directions (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B8">Bengio et al., 2015a</a>). <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B119">Mostafa et al. (2017b)</a> show how feature hierarchies can be trained with local errors from random auxiliary classifiers, and how training can work despite the asynchronous updates found in SNNs. Another recent line of research has established links between inference in energy-based networks and backpropagation (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B11">Bengio et al., 2017</a>). A proposal for solving the credit assignment problem by using multiple layers with local rules was made, and it was shown that early steps of inference in this iterative method in autoencoder-like models yield activation changes in hidden layers that approximate backpropagation. Furthermore, the required updates are compatible with STDP, and could thus be implemented in a biologically plausible way (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B10">Bengio et al., 2015b</a>).</p>
<p>In general, the function of STDP is highly dependent on the network architecture, in which it is applied. In competitive networks, STDP is capable of solving unsupervised learning tasks such as clustering (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B108">Masquelier et al., 2009</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B130">Nessler et al., 2013</a>). This is encouraging, since recent work (e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B27">Coates et al., 2011</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B36">Dundar et al., 2015</a>) has shown that competitive convolutional networks can be trained with unsupervised learning of filters. If spiking neurons are connected according to the structure of restricted Boltzmann machines, contrastive divergence can be approximated in an event-based fashion (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B124">Neftci et al., 2014</a>, <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B126">2016</a>). In principle, this can be extended to multi-layer DBNs, but then training occurs only within each layer. Spiking CNNs (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B79">Kheradpisheh et al., 2018</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B92">Lee et al., 2018</a>) and autoencoders (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B138">Panda and Roy, 2016</a>) can be trained layer-by-layer with unsupervised STDP, assuming that weight updates for identical kernels are shared between their applications to different spatial locations, and single-layer supervised frame-based learning is used for the output layer (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B182">Stromatias et al., 2017</a>). <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B120">Mozafari et al. (2018)</a> add multiple layers with reward-modulated STDP to such networks to obtain fully spiking supervised training. Recently, in order to simultaneously train all layers with STDP within a deep network <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B189">Thiele et al. (2018)</a> introduced neurons with two integrate-and-fire units decoupling learning with STDP and inference. These approaches discussed above, however, do not reach the accuracy of conventional deep neural networks trained with backpropagation.</p>
<h2 id="4-neuromorphic-hardware">4. Neuromorphic Hardware<a class="headerlink" href="#4-neuromorphic-hardware" title="Permanent link">&para;</a></h2>
<p>There is a big discrepancy between the promise of efficient computing with SNNs and the actual implementation on currently available computing hardware. Simulating SNNs on von Neumann machines is typically inefficient, since asynchronous network activity leads to a quasi-random access of synaptic weights in time and space. Ideally, each spiking neuron is an own processor in a network without central clock, which is the design principle of <em>neuromorphic</em> platforms. The highly parallel structure, sparse communication, and in-memory computation proposed by SNNs stands in contrast to the sequential and central processing of data constrained by the memory wall between processor and memory on CPUs and GPUs. The computational efficiency of SNNs can be observed in brains that can solve complex tasks while requiring less power than a dim light bulb (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B88">Laughlin and Sejnowski, 2003</a>). To close the gap in terms of energy-efficiency between simulations of SNNs and biological SNNs in the last decade several neuromorphic hardware systems were developed which are optimized for execution of SNNs (see Table <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#T1">1</a>; for a review of technical specifications see <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B45">Furber, 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B175">Singh Thakur et al., 2018</a>). The energy-efficiency of neuromorphic systems makes them ideal candidates for embedded devices subject to power constraints, e.g., mobile phones, mobile and aerial robots, and internet of things (IoT) devices. Furthermore, neuromorphic devices could be utilized in data centers to reduce the cost of cloud applications relying on neural networks.</p>
<p><a href="https://www.frontiersin.org/files/Articles/409662/fnins-12-00774-HTML/image_m/fnins-12-00774-t001.jpg"><img alt="www.frontiersin.org" src="https://www.frontiersin.org/files/Articles/409662/fnins-12-00774-HTML/image_t/fnins-12-00774-t001.gif" /></a></p>
<p><strong>Table 1</strong>. This table lists built neuromorphic systems for which results with deep SNNs on classification tasks have been shown (for extended lists of hardware systems that may potentially be used for deep SNNs see, e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B71">Indiveri and Liu, 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B99">Liu et al., 2016</a>).</p>
<p>Inspired by biology, neuromorphic devices share the locality of data to reduce on-chip traffic, mostly reflected by using spikes for communication and limiting the fan-in of neurons. The massive parallelism of neuromorphic devices manifests itself in the physical representation of neurons and synapses on hardware inspired by the seminal study of <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B110">Mead (1990)</a> (for a review, see <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B70">Indiveri et al., 2011</a>). Analog neuromorphic systems, which implement functionalities of spiking neurons and synapses with analog electronic circuits, usually have a one-to-one mapping between neurons and synapses in the network description and on hardware. In contrast, digital systems implement the parallel structure less fine-grained by grouping and virtualizing neurons on <em>cores</em> (hundreds for the TrueNorth and thousands for the SpiNNaker system, see also Table <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#T1">1</a>). However, compared to the extensive virtualization on CPUs and GPUs, i.e., the total number of neurons in a network divided by the number of cores, the virtualization on neuromorphic systems is rather low. This leads to less flexibility in terms of connectivity and size of networks, and thus hardware demonstrations that show functional deep SNNs are few. All hardware systems listed in Table <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#T1">1</a> share an asynchronous computation scheme that enables computation on demand and reduces power consumption in case of low network activity.</p>
<p>In principle, neuromorphic hardware could be used for both training and inference of SNNs. While original and constrained DNNs (section 3.3) can usually be trained on GPUs and are then converted to SNNs (section 3.2), spike-based training (section 3.4) and especially local learning rules (section 3.5) are computationally more expensive on von Neumann machines and, hence, could highly benefit from hardware acceleration. However, so far, spike-based training and local learning rules have not been shown for competitive deep networks. Rapid developments in this area of research makes it difficult to build dedicated hardware for training, since their design and production is time-consuming and costly (see also section 6).</p>
<h3 id="41-inference-on-neuromorphic-hardware">4.1. Inference on Neuromorphic Hardware<a class="headerlink" href="#41-inference-on-neuromorphic-hardware" title="Permanent link">&para;</a></h3>
<p>Once the parameters of SNNs are obtained by any of the training methods reviewed in section 3, usually these networks have to be adapted to the specific hardware system to be used for inference. Analog neuromorphic systems suffering from parameter variation may require cumbersome fine-tuning of pre-trained networks with the hardware system in-the-loop (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B168">Schmitt et al., 2017</a>). This is not always practical, because the re-configuration of neuromorphic systems is often slow compared to, for example, CPUs and GPUs. Another common approach to improve the test performance is to incorporate hardware constraints like, for example, limited counts of incoming connections and quantized weights into the training process (section 3.3). Once parameters are trained and the device is configured, inference is usually fast and energy-efficient due to their optimization for spike input and output. To our knowledge only for the TrueNorth, SpiNNaker and BrainScaleS hardware system results were shown, in which deep SNNs on silicon chips were used for classification tasks with the complexity of at least MNIST (for hardware specifications and classification performances, see Table <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#T1">1</a>). For other promising neuromorphic systems no results for deep SNNs are shown yet (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B142">Park et al., 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B98">Lin et al., 2018</a>), or the presented neuron and synapse count is too small to show competitive results (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B147">Pfeil et al., 2013a</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B169">Schmuker et al., 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B69">Indiveri et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B154">Qiao et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B116">Moradi et al., 2017</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B146">Petrovici et al., 2017</a>). Prototypical software implementations and field-programmable gate array (FPGA) systems are not considered in this study. As an exception, we would like to mention the novel Intel Loihi chip (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B31">Davies et al., 2018</a>), for which results of a single layer network on preprocessed MNIST images on a prototypical FPGA implementation are shown (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B98">Lin et al., 2018</a>). Once commissioned, Loihi's large number of neurons, their connectivity and configurability, and on-chip learning capabilities could be a good basis to enable deep networks on raw image data. Table <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#T1">1</a> shows deep SNNs on the SpiNNaker and BrainScaleS systems that approximate multi-layer perceptrons (MLPs) and rate-based deep belief networks (DBNs), respectively, showing network activity like exemplarily plotted in Figure <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#F1">1D</a>. In contrast, deep CNNs are binarized for their execution on the TrueNorth system (compare to Figure <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#F1">1C</a>). This means that neuron activations on TrueNorth are represented by single spikes and each neuron in a network is stateless and fires at most once for each input. In other words, spikes do not contain temporal information anymore, but the high throughput makes inference energy-efficient.</p>
<p>Are the presented neuromorphic systems more power-efficient than GPUs? The answer to this question very much depends on the chosen benchmark task, and we can give only approximate numbers for frame-based classification tasks (for further discussions see section 6). Since power measurements on modern mobile GPUs (Nvidia Tegra X1) are only reported for large networks (AlexNet) on comparably large images from the ImageNet dataset (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B131">NVIDIA Corporation, 2015</a>), and power numbers of the most efficient neuromorphic system are recorded for custom networks on smaller images from the CIFAR10 dataset (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B40">Esser et al., 2016</a>), a straight-forward comparison is not possible. However, if we assume a linear decrease in the number of operations with the area of the input image, which is approximately true for convolutional networks, the reported energy of 76mJ for GPUs to process an image of size 224  224 scales down to approximately 2mJ for an image from the CIFAR10 dataset with size 32  32. This energy consumption is approximately one order of magnitude higher than for the most power-efficient neuromorphic solution, i.e., binarized DNNs on the TrueNorth system (for numbers see Table <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#T1">1</a>). Since the energy consumption of most neuromorphic systems is dominated by that of synaptic events, i.e., communication and processing of spikes, higher benefits are expected for models that exploit sparse temporal codes, rather than rate-based models.</p>
<h3 id="42-on-chip-learning">4.2. On-Chip Learning<a class="headerlink" href="#42-on-chip-learning" title="Permanent link">&para;</a></h3>
<p>Although unified methods to train SNNs are still missing, the SpiNNaker and BrainScaleS hardware systems implement spike-timing-dependent plasticity (STDP), a local unsupervised learning rule inspired by biology. Synaptic weights are updated by means of local correlations between pre- and postsynaptic activity (see also section 3.5). Neuromorphic systems are valuable tools to investigate such local learning rules, because the training of networks with STDP often requires long simulations of SNNs in terms of biological time, and neuromorphic systems usually accelerate such simulations compared to conventional computers. The BrainScaleS system (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B165">Schemmel et al., 2010</a>) and its successor (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B1">Aamir et al., 2018</a>) is an especially promising candidate for on-chip learning due to its acceleration of up to a factor of 10000 compared to biological real time, but so far STDP is only shown for small networks on a prototype chip (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B149">Pfeil et al., 2013b</a>) and shows significant parameter variation due to imperfections in the production process (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B148">Pfeil et al., 2012</a>). In addition, <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B43">Friedmann et al. (2017)</a> investigated the integration of on-chip plasticity processors into the BrainScaleS system to modulate STDP based on the model of neuromodulators in biology (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B143">Pawlak et al., 2010</a>) allowing for supervised training. Although the implementation of STDP is in terms of chip area costly for the presented neuromorphic systems, novel electronic components, so called memristors, may allow for much higher densities of plastic synapses (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B75">Jo et al., 2010</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B164">Saghi et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B20">Boyn et al., 2017</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B23">Burr et al., 2017</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B144">Pedretti et al., 2017</a>).</p>
<h2 id="5-applications">5. Applications<a class="headerlink" href="#5-applications" title="Permanent link">&para;</a></h2>
<p>With the current big success of deep learning in conventional machine learning it is tempting to view deep SNNs exclusively as a more efficient replacement of conventional DNNs. This view is reflected in the way deep SNNs are benchmarked against conventional machine learning approaches by their classification accuracy on standard datasets such as MNIST or CIFAR. Such comparisons are certainly important, because they show that SNNs can be powerful classifiers in the classical machine learning setup. However, entirely focusing on accuracy can easily become misleading when it comes to the potential advantages of SNNs, namely efficiency and low latency. Achieving state-of-the-art accuracy with rate-based networks often comes at the cost of having very high firing rates and long integration times to obtain reliable results (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B162">Rueckauer et al., 2017</a>).</p>
<p>Using temporal codes (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B118">Mostafa et al., 2017a</a>) is an attractive alternative, but, so far, such approaches have not reached state-of-the-art accuracy. The ability to utilize the information of precise timing is a feature than only SNNs - whether deep or not - offer, but which we think has not been investigated and exploited enough. Temporal codes allow to represent features of the input in precise spike patterns of small groups of neurons (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B57">Gtig, 2014</a>), and there is no more need to precisely estimate firing rates first (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B50">Gardner et al., 2015</a>). Information in SNNs might also be encoded in spike times relative to background oscillations, which has been shown to benefit learning in recurrent networks (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B129">Neil et al., 2016b</a>), and allows to encode multiple features in parallel (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B78">Kayser et al., 2009</a>). The use of precise timing information always carries the risk of being susceptible to noise and temporal jitter, but the approaches mentioned above exhibit quite high degrees of noise robustness. Several promising, but not necessarily deep, temporal coding and learning schemes have been proposed (e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B58">Gtig and Sompolinsky, 2006</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B186">Tapson et al., 2013</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B85">Lagorce and Benosman, 2015</a>), but no applications at the scale that conventional deep learning is addressing have been demonstrated, yet.</p>
<p>The greatest impact of deep SNNs is expected in the processing of inputs from event-based sensors, because only SNNs are able to fully exploit the precise temporal information such sensors offer. To stimulate research in this direction, we and others from the neuromorphic engineering community have argued that new benchmarks are necessary, which do not carry the legacy of evaluation in conventional machine learning or computer vision (e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B72">Iyer et al., 2018</a>). Instead, they should be specifically designed to show the advantages of the neuromorphic approach (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B134">Orchard et al., 2015a</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B184">Tan et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B5">Barranco et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B63">Hu et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B99">Liu et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B15">Binas et al., 2017</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B122">Mueggler et al., 2017</a>). Such datasets have only recently become available, but already had a beneficial effect on the fair comparison between different SNN approaches. Although this is a step into the right direction, it is still problematic that most of these datasets are event-based variants of conventional classification tasks, such as MNIST digits recorded with a dynamic vision sensor. In order to fully exploit the strengths of the neuromorphic approach, we suggest that a careful analysis of use cases is necessary. We propose that there are at least two classes of use cases that should be investigated deeper: First, the case where neuromorphic sensors provide additional features (e.g., precise timing and low latency) that can be exploited by a deep SNN. Second the case where low-power and low-latency aspects of deep SNNs really make a difference in real-world applications.</p>
<p>We are currently observing the interesting trend that event-based vision becomes increasingly interesting for research communities rooted in classical computer vision and robotics. Advantages of using event-based sensors have been demonstrated for diverse applications such as tracking (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B121">Mueggler et al., 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B86">Lagorce et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B48">Gallego et al., 2018</a>), stereo vision (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B161">Rogister et al., 2012</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B137">Osswald et al., 2017</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B107">Martel et al., 2018</a>), optical flow estimation (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B13">Benosman et al., 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B4">Bardow et al., 2016</a>), gesture recognition (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B94">Lee et al., 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B2">Amir et al., 2017</a>), scene reconstruction (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B26">Carneiro et al., 2013</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B80">Kim et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B158">Rebecq et al., 2017</a>), or SLAM (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B192">Weikersdorfer et al., 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B191">Vidal et al., 2018</a>). All of these applications benefit from the high speed and the high dynamic range of spike-based sensors to solve tasks, such as high-speed localization and navigation, which are very hard with conventional vision sensors. However, only few of these approaches use SNNs for event-based post-processing, or run on neuromorphic hardware. A notable exception is the gesture recognition system in <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B2">Amir et al. (2017)</a>, which was designed to highlight the benefits of combining a dynamic vision sensor with the TrueNorth processing chip. We think there is great potential for fully event-based sensing and processing systems, and given the success of conventional deep learning, deep SNNs on neuromorphic hardware platforms seem like an obvious choice. Initial demonstrations on simpler classification tasks are encouraging (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B135">Orchard et al., 2013</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B112">Merolla et al., 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B127">Neil and Liu, 2014</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B181">Stromatias et al., 2015</a>), but more research is needed to create deep SNNs specifically tailored for event-based sensor input.</p>
<p>Conventional machine learning has realized that a co-development of algorithms and hardware is necessary by moving toward low-bit precision or binary networks. The same is true in the neuromorphic domain, and further adaptations of vision sensors to the capabilities of post-processing systems have a great potential. Once the performance of deep SNNs is good enough, neuromorphic hardware implementations could become the method of choice for applications, wherever low power is of particular importance. Besides battery powered robots and embedded devices, a particularly interesting application field is brain-machine interfaces (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B33">Dethier et al., 2013</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B28">Corradi and Indiveri, 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B18">Boi et al., 2016</a>), where small size, low energy consumption, low heat dissipation, robustness, and the ability to decode in real-time are important. The fact that SNNs can process recorded biological spikes without further transformation adds to the appeal of such systems. The field of automated driving is expected to become another major application area, where the focus is less on low power, but on enhancing safety critical functions by exploiting speed and dynamic range of neuromorphic sensors (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B15">Binas et al., 2017</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B176">Sironi et al., 2018</a>). On-board, SNNs can process sensor information in real-time, potentially improving emergency brake assistants, which have to deal with challenging light conditions as well as suddenly appearing road users, or providing reliable perception during evasive high-speed maneuvers.</p>
<p>Finally, deep SNNs and their hardware implementations will continue to be used as models of computation in biological neural circuits, and thus form a valuable tool for Computational Neuroscience (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B175">Singh Thakur et al., 2018</a>). Hardware platforms such as SpiNNaker (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B46">Furber et al., 2014</a>), Neurogrid (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B12">Benjamin et al., 2014</a>), TrueNorth (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B112">Merolla et al., 2014</a>), and BrainScaleS (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B165">Schemmel et al., 2010</a>) have shown great potential to accelerate large-scale brain simulations. Recently discovered analogies of real neural representations in the cortex to those learned in deep networks (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B82">Kriegeskorte, 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B196">Yamins and DiCarlo, 2016</a>) have increased the interest of the neuroscience community in deep learning, and deep SNNs could become an interesting tool to study the interplay of neuronal structure, plasticity, and spiking dynamics in large-scale simulations.</p>
<h2 id="6-discussion">6. Discussion<a class="headerlink" href="#6-discussion" title="Permanent link">&para;</a></h2>
<p>Advances in deep SNNs have helped closing the performance gap to conventional DNNs. However, the promise of low-power inference is not fulfilled yet, since network conversion (section 3.2) and training of constrained networks (section 3.3) result in spike-based networks that encode information mostly in their neurons' mean firing rates, but do not exploit the potential of encoding information in the timing of single spikes. Although these networks achieve a remarkable performance on various benchmark datasets, the average firing rates of their neurons are comparably high for static input images, and hence their energy-efficiency on neuromorphic systems is not significantly better than for conventional DNNs on GPUs (section 4). To reduce firing rates and increase energy-efficiency spike-based training methods (section 3.4) and local learning rules (section 3.5) have become increasingly popular research topics. Their accuracy on machine learning benchmarks is not quite at the level of converted networks, but recent approaches by <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B93">Lee et al. (2016)</a> or <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B74">Jin et al. (2018)</a> could partly close the gap. Besides, the choice of benchmarks that usually consist of frame-based images converted to spiking representations (section 2) puts spike-based rules at a disadvantage. Finding local learning rules that can achieve the same performance as backpropagation would be a result with great implications beyond machine learning applications, since it could possibly explain how brains can learn their remarkable capabilities with the constraints for information and error signal routing imposed by biology (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B8">Bengio et al., 2015a</a>).</p>
<p>We have argued in section 5 that further opportunities for deep SNNs will arise when appropriate benchmark datasets recorded with event-based sensors become available. The rise of deep learning has largely been driven by the availability of large common benchmarks such as ImageNet (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B163">Russakovsky et al., 2015</a>). Similarly large and challenging neuromorphic datasets are not available, yet, but we see a positive trend and increased awareness of the community. First benchmarks for real-world applications in automated driving (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B15">Binas et al., 2017</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B176">Sironi et al., 2018</a>) and robotics (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B122">Mueggler et al., 2017</a>) have been released, and together with convincing results on problems where conventional systems struggle (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B80">Kim et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B191">Vidal et al., 2018</a>), we expect that this will lead to increasing demand for efficient event-based post-processing systems. Fully event-based systems are not only energy-efficient, but could also better exploit the rich temporal dynamics of the real world than frame-based approaches, which artificially introduce time steps through sensing or processing components. For agents interacting with the real world, temporal information on different time-scales plays an important role, because critical situations require short reaction times hardly accessible by frame-based perception. Deep SNNs have the important property of providing good early estimates, which improve when given more processing time (see Figure <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#F1">1F</a>). Although mechanisms to provide early estimations are also proposed for conventional DNNs (e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B188">Teerapittayanon et al., 2016</a>), their implementations are rather artificial and not as seamlessly integrated as in SNNs. <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B42">Fischer et al. (2018)</a> proposed a hybrid solution between conventional DNNs and deep SNNs, called <em>streaming rollouts</em>, which are conventional synchronous DNNs that share a dense temporal integration and fast response times with deep SNNs.</p>
<p>A future direction of research may be the incorporation of recurrence into deep SNNs improving the storage and integration of temporal information. Recurrent SNNs have shown remarkable performance in sequence recognition (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B199">Zhang et al., 2015</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B140">Panda and Srinivasa, 2018</a>) and generation tasks (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B155">Rajan et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B139">Panda and Roy, 2017</a>). In these cases, instead of a deep or structured recurrent architecture the networks were configured as <em>liquid state machines</em> (<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B102">Maass et al., 2002</a>), which consist of a reservoir of randomly and recurrently connected neurons, followed by a linear readout. Recent work (e.g., <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B35">Diehl et al., 2016</a>; <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B7">Bellec et al., 2018</a>) have shown how standard recurrent network architectures such as long short-term memory networks (LSTMs, <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B62">Hochreiter and Schmidhuber, 1997</a>) can be ported into the spiking domain, whereas <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B129">Neil et al. (2016b)</a> have shown a way to process event-based data with otherwise standard recurrent networks. Combining recurrent architectures with the intrinsic short-time memory of spiking neurons appears as a promising route for efficiently solving real-world pattern recognition tasks.</p>
<p>As deep SNNs become larger and capable of solving tasks that are more complex, training time is likely to become a bottleneck due to the more complex training methods compared to conventional DNNs, as well as inefficient spiking simulations on conventional computing platforms. It is therefore important to advance neuromorphic hardware systems for large-scale deep SNNs, and not only consider energy-efficient inference, but also training. Efficient training can be either realized via on-chip learning rules like STDP as discussed in section 4.2, by using neuromorphic systems in-the-loop, i.e., computing weight updates on the host computer and then re-configuring the hardware system, or by hybrid solutions. However, contemporary neuromorphic systems share a comparably low bandwidth to the host computer, usually sufficient for spike input and output, but inappropriate for a frequent re-configuration of the device. This is why the development and investigation of hierarchies of learning rules both on algorithmic and hardware level, ranging from in-memory plasticity rules like STDP to global reward signals, would be a valuable topic for future studies. <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B43">Friedmann et al. (2017)</a> and <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full?source=post_page---------------------------#B98">Lin et al. (2018)</a> already proposed architectures that go into this direction, and it will be interesting to see first large-scale experimental results and further developments in the near future. Although such systems may allow for the exploration of networks with a size and complexity not accessible with current hardware systems, their development is time consuming, costly, and will most likely not offer the flexibility to catch up with the latest algorithmic developments. Compared to digital systems, analog systems promise a higher energy-efficiency. However, the training of analog systems requires additional efforts (see section 4.1) and the short- and long-term variations in their parameters and computations, e.g., caused by temperature fluctuations, pose great challenges.</p>
<h2 id="author-contributions">Author Contributions<a class="headerlink" href="#author-contributions" title="Permanent link">&para;</a></h2>
<p>All authors listed have made a substantial, direct and intellectual contribution to the work, and approved it for publication.</p>
<h2 id="funding">Funding<a class="headerlink" href="#funding" title="Permanent link">&para;</a></h2>
<p>This publication has received funding from the European Union's Horizon 2020 research innovation programme under grant agreement 732642 (ULPEC project).</p>
<h2 id="conflict-of-interest-statement">Conflict of Interest Statement<a class="headerlink" href="#conflict-of-interest-statement" title="Permanent link">&para;</a></h2>
<p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
<h2 id="acknowledgments">Acknowledgments<a class="headerlink" href="#acknowledgments" title="Permanent link">&para;</a></h2>
<p>We would like to thank David Stckel, Volker Fischer, and Andre Guntoro for critical reading and helpful discussions.</p>
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<p>Aamir, S. A., Stradmann, Y., Mller, P., Pehle, C., Hartel, A., Grbl, A., et al. (2018). An accelerated LIF neuronal network array for a large scale mixed-signal neuromorphic architecture. <em>arXiv [Preprint]. arXiv:1804.01906</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=S.+A.+Aamir&amp;author=Y.+Stradmann&amp;author=P.+M%C3%BCller&amp;author=C.+Pehle&amp;author=A.+Hartel&amp;author=A.+Gr%C3%BCbl+&amp;publication_year=2018&amp;title=An+accelerated+LIF+neuronal+network+array+for+a+large+scale+mixed-signal+neuromorphic+architecture&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1804.01906">Google Scholar</a></p>
<p>Amir, A., Taba, B., Berg, D., Melano, T., McKinstry, J., Nolfo, C. D., et al. (2017). A low power, fully event-based gesture recognition system, in <em>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> (Honolulu, HI: CVPR), 73887397.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Amir&amp;author=B.+Taba&amp;author=D.+Berg&amp;author=T.+Melano&amp;author=J.+McKinstry&amp;author=C.+D.+Nolfo+&amp;publication_year=2017&amp;title=%E2%80%9CA+low+power,+fully+event-based+gesture+recognition+system,%E2%80%9D&amp;pages=7388-7397">Google Scholar</a></p>
<p>Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenberg, E., Case, C., et al. (2016). Deep speech 2 : end-to-end speech recognition in English and Mandarin, in <em>Proceedings of The 33rd International Conference on Machine Learning</em>, volume 48 of <em>Proceedings of Machine Learning Research</em>, eds M. F. Balcan and K. Q. Weinberger (New York, NY: PMLR),173182.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=D.+Amodei&amp;author=S.+Ananthanarayanan&amp;author=R.+Anubhai&amp;author=J.+Bai&amp;author=E.+Battenberg&amp;author=C.+Case+&amp;publication_year=2016&amp;title=%E2%80%9CDeep+speech+2+%3A+end-to-end+speech+recognition+in+English+and+Mandarin,%E2%80%9D&amp;pages=173-182">Google Scholar</a></p>
<p>Bardow, P., Davison, A. J., and Leutenegger, S. (2016). Simultaneous optical flow and intensity estimation from an event camera, in <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> (Las Vegas, NV: CVPR), 884892.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=P.+Bardow&amp;author=A.+J.+Davison&amp;author=S.+Leutenegger+&amp;publication_year=2016&amp;title=%E2%80%9CSimultaneous+optical+flow+and+intensity+estimation+from+an+event+camera,%E2%80%9D&amp;pages=884-892">Google Scholar</a></p>
<p>Barranco, F., Fermuller, C., Aloimonos, Y., and Delbruck, T. (2016). A dataset for visual navigation with neuromorphic methods. <em>Front. Neurosci.</em> 10:49. doi: 10.3389/fnins.2016.00049</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=26941595">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2016.00049">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=F.+Barranco&amp;author=C.+Fermuller&amp;author=Y.+Aloimonos&amp;author=T.+Delbruck+&amp;publication_year=2016&amp;title=A+dataset+for+visual+navigation+with+neuromorphic+methods&amp;journal=Front.+Neurosci.&amp;volume=10&amp;pages=49">Google Scholar</a></p>
<p>Bartunov, S., Santoro, A., Richards, B. A., Hinton, G. E., and Lillicrap, T. (2018). Assessing the scalability of biologically-motivated deep learning algorithms and architectures. <em>arXiv [Preprint]. arXiv:1807.04587</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=S.+Bartunov&amp;author=A.+Santoro&amp;author=B.+A.+Richards&amp;author=G.+E.+Hinton&amp;author=T.+Lillicrap+&amp;publication_year=2018&amp;title=Assessing+the+scalability+of+biologically-motivated+deep+learning+algorithms+and+architectures&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1807.04587">Google Scholar</a></p>
<p>Bellec, G., Salaj, D., Subramoney, A., Legenstein, R., and Maass, W. (2018). Long short-term memory and learning-to-learn in networks of spiking neurons. <em>arXiv [Preprint]. arXiv:1803.09574</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=G.+Bellec&amp;author=D.+Salaj&amp;author=A.+Subramoney&amp;author=R.+Legenstein&amp;author=W.+Maass+&amp;publication_year=2018&amp;title=Long+short-term+memory+and+learning-to-learn+in+networks+of+spiking+neurons&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1803.09574">Google Scholar</a></p>
<p>Bengio, Y., Lee, D.-H., Bornschein, J., Mesnard, T., and Lin, Z. (2015a). Towards biologically plausible deep learning. <em>arXiv [Preprint]. arXiv:1502.04156</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=Y.+Bengio&amp;author=D.+-H.+Lee&amp;author=J.+Bornschein&amp;author=T.+Mesnard&amp;author=Z.+Lin+&amp;publication_year=2015a&amp;title=Towards+biologically+plausible+deep+learning&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1502.04156">Google Scholar</a></p>
<p>Bengio, Y., Lonard, N., and Courville, A. (2013). Estimating or propagating gradients through stochastic neurons for conditional computation. <em>arXiv [Preprint]. arXiv:1308.3432</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=Y.+Bengio&amp;author=N.+L%C3%A9onard&amp;author=A.+Courville+&amp;publication_year=2013&amp;title=Estimating+or+propagating+gradients+through+stochastic+neurons+for+conditional+computation&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1308.3432">Google Scholar</a></p>
<p>Bengio, Y., Mesnard, T., Fischer, A., Zhang, S., and Wu, Y. (2015b). STDP as presynaptic activity times rate of change of postsynaptic activity. <em>arXiv [Preprint]. arXiv:1509.05936</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=Y.+Bengio&amp;author=T.+Mesnard&amp;author=A.+Fischer&amp;author=S.+Zhang&amp;author=Y.+Wu+&amp;publication_year=2015b&amp;title=STDP+as+presynaptic+activity+times+rate+of+change+of+postsynaptic+activity&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1509.05936">Google Scholar</a></p>
<p>Bengio, Y., Mesnard, T., Fischer, A., Zhang, S., and Wu, Y. (2017). STDP-compatible approximation of backpropagation in an energy-based model. <em>Neural Comput.</em> 29, 555577. doi: 10.1162/NECO_a_00934</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=28095200">PubMed Abstract</a> | <a href="https://doi.org/10.1162/NECO_a_00934">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Y.+Bengio&amp;author=T.+Mesnard&amp;author=A.+Fischer&amp;author=S.+Zhang&amp;author=Y.+Wu+&amp;publication_year=2017&amp;title=STDP-compatible+approximation+of+backpropagation+in+an+energy-based+model&amp;journal=Neural+Comput.&amp;volume=29&amp;pages=555-577">Google Scholar</a></p>
<p>Benjamin, B. V., Gao, P., McQuinn, E., Choudhary, S., Chandrasekaran, A. R., Bussat, J.-M., et al. (2014). Neurogrid: a mixed-analog-digital multichip system for large-scale neural simulations. <em>Proc. IEEE</em> 102, 699716. doi: 10.1109/JPROC.2014.2313565</p>
<p><a href="https://doi.org/10.1109/JPROC.2014.2313565">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=B.+V.+Benjamin&amp;author=P.+Gao&amp;author=E.+McQuinn&amp;author=S.+Choudhary&amp;author=A.+R.+Chandrasekaran&amp;author=J.+-M.+Bussat+&amp;publication_year=2014&amp;title=Neurogrid%3A+a+mixed-analog-digital+multichip+system+for+large-scale+neural+simulations&amp;journal=Proc.+IEEE&amp;volume=102&amp;pages=699-716">Google Scholar</a></p>
<p>Benosman, R., Clercq, C., Lagorce, X., Ieng, S. H., and Bartolozzi, C. (2014). Event-based visual flow. <em>IEEE Trans. Neural Netw. Learn. Syst.</em> 25, 407417. doi: 10.1109/TNNLS.2013.2273537</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=24807038">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TNNLS.2013.2273537">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+Benosman&amp;author=C.+Clercq&amp;author=X.+Lagorce&amp;author=S.+H.+Ieng&amp;author=C.+Bartolozzi+&amp;publication_year=2014&amp;title=Event-based+visual+flow&amp;journal=IEEE+Trans.+Neural+Netw.+Learn.+Syst.&amp;volume=25&amp;pages=407-417">Google Scholar</a></p>
<p>Bi, G. Q., and Poo, M. M. (1998). Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type. <em>J. Neurosci.</em> 18, 1046410472.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=9852584">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Q.+Bi&amp;author=M.+M.+Poo+&amp;publication_year=1998&amp;title=Synaptic+modifications+in+cultured+hippocampal+neurons%3A+dependence+on+spike+timing,+synaptic+strength,+and+postsynaptic+cell+type&amp;journal=J.+Neurosci.&amp;volume=18&amp;pages=10464-10472">Google Scholar</a></p>
<p>Binas, J., Neil, D., Liu, S.-C., and Delbruck, T. (2017). DDD17: End-to-end DAVIS driving dataset. <em>arXiv [Preprint]. arXiv:1711.01458</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+Binas&amp;author=D.+Neil&amp;author=S.+-C.+Liu&amp;author=T.+Delbruck+&amp;publication_year=2017&amp;title=DDD17%3A+End-to-end+DAVIS+driving+dataset&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1711.01458">Google Scholar</a></p>
<p>Boahen, K. A. (2000). Point-to-point connectivity between neuromorphic chips using address events. <em>IEEE Trans. Circuits Syst. II Analog Digital Signal Process.</em> 47, 416434. doi: 10.1109/82.842110</p>
<p><a href="https://doi.org/10.1109/82.842110">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=K.+A.+Boahen+&amp;publication_year=2000&amp;title=Point-to-point+connectivity+between+neuromorphic+chips+using+address+events&amp;journal=IEEE+Trans.+Circuits+Syst.+II+Analog+Digital+Signal+Process.&amp;volume=47&amp;pages=416-434">Google Scholar</a></p>
<p>Bohte, S. M., Kok, J. N., and Poutr, H. L. (2002). Error-backpropagation in temporally encoded networks of spiking neurons. <em>Neurocomputing</em> 48, 1737. doi: 10.1016/S0925-2312(01)00658-0</p>
<p><a href="https://doi.org/10.1016/S0925-2312(01)00658-0">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+M.+Bohte&amp;author=J.+N.+Kok&amp;author=H.+L.+Poutr+&amp;publication_year=2002&amp;title=Error-backpropagation+in+temporally+encoded+networks+of+spiking+neurons&amp;journal=Neurocomputing&amp;volume=48&amp;pages=17-37">Google Scholar</a></p>
<p>Boi, F., Moraitis, T., De Feo, V., Diotalevi, F., Bartolozzi, C., Indiveri, G., et al. (2016). A bidirectional brain-machine interface featuring a neuromorphic hardware decoder. <em>Front. Neurosci.</em> 10:563. doi: 10.3389/fnins.2016.00563</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=28018162">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2016.00563">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=F.+Boi&amp;author=T.+Moraitis&amp;author=V.+De+Feo&amp;author=F.+Diotalevi&amp;author=C.+Bartolozzi&amp;author=G.+Indiveri+&amp;publication_year=2016&amp;title=A+bidirectional+brain-machine+interface+featuring+a+neuromorphic+hardware+decoder&amp;journal=Front.+Neurosci.&amp;volume=10&amp;pages=563">Google Scholar</a></p>
<p>Booij, O., and tat Nguyen, H. (2005). A gradient descent rule for spiking neurons emitting multiple spikes. <em>Inform. Process. Lett.</em> 95, 552558. doi: 10.1016/j.ipl.2005.05.023</p>
<p><a href="https://doi.org/10.1016/j.ipl.2005.05.023">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=O.+Booij&amp;author=H.+tat+Nguyen+&amp;publication_year=2005&amp;title=A+gradient+descent+rule+for+spiking+neurons+emitting+multiple+spikes&amp;journal=Inform.+Process.+Lett.&amp;volume=95&amp;pages=552-558">Google Scholar</a></p>
<p>Boyn, S., Grollier, J., Lecerf, G., Xu, B., Locatelli, N., Fusil, S., et al. (2017). Learning through ferroelectric domain dynamics in solid-state synapses. <em>Nat. Commun.</em> 8:14736. doi: 10.1038/ncomms14736</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=28368007">PubMed Abstract</a> | <a href="https://doi.org/10.1038/ncomms14736">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Boyn&amp;author=J.+Grollier&amp;author=G.+Lecerf&amp;author=B.+Xu&amp;author=N.+Locatelli&amp;author=S.+Fusil+&amp;publication_year=2017&amp;title=Learning+through+ferroelectric+domain+dynamics+in+solid-state+synapses&amp;journal=Nat.+Commun.&amp;volume=8&amp;pages=14736">Google Scholar</a></p>
<p>Brderle, D., Petrovici, M. A., Vogginger, B., Ehrlich, M., Pfeil, T., Millner, S., et al. (2011). A comprehensive workflow for general-purpose neural modeling with highly configurable neuromorphic hardware systems. <em>Biol. Cybern.</em> 104, 263296. doi: 10.1007/s00422-011-0435-9</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=21618053">PubMed Abstract</a> | <a href="https://doi.org/10.1007/s00422-011-0435-9">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Br%C3%BCderle&amp;author=M.+A.+Petrovici&amp;author=B.+Vogginger&amp;author=M.+Ehrlich&amp;author=T.+Pfeil&amp;author=S.+Millner+&amp;publication_year=2011&amp;title=A+comprehensive+workflow+for+general-purpose+neural+modeling+with+highly+configurable+neuromorphic+hardware+systems&amp;journal=Biol.+Cybern.&amp;volume=104&amp;pages=263-296">Google Scholar</a></p>
<p>Burr, G. W., Shelby, R. M., Sebastian, A., Kim, S., Kim, S., Sidler, S., et al. (2017). Neuromorphic computing using non-volatile memory. <em>Adv. Phys. X</em> 2, 89124. doi: 10.1080/23746149.2016.1259585</p>
<p><a href="https://doi.org/10.1080/23746149.2016.1259585">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+W.+Burr&amp;author=R.+M.+Shelby&amp;author=A.+Sebastian&amp;author=S.+Kim&amp;author=S.+Kim&amp;author=S.+Sidler+&amp;publication_year=2017&amp;title=Neuromorphic+computing+using+non-volatile+memory&amp;journal=Adv.+Phys.+X&amp;volume=2&amp;pages=89-124">Google Scholar</a></p>
<p>Camuas-Mesa, L. A., Serrano-Gotarredona, T., and Linares-Barranco, B. (2014). Event-driven sensing and processing for high-speed robotic vision, in <em>Proceedings of the 2014 IEEE Biomedical Circuits and Systems Conference (BioCAS)</em> (Lausanne: BioCAS), 516519.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=L.+A.+Camu%C3%B1as-Mesa&amp;author=T.+Serrano-Gotarredona&amp;author=B.+Linares-Barranco+&amp;publication_year=2014&amp;title=%E2%80%9CEvent-driven+sensing+and+processing+for+high-speed+robotic+vision,%E2%80%9D&amp;pages=516-519">Google Scholar</a></p>
<p>Cao, Y., Chen, Y., and Khosla, D. (2015). Spiking deep convolutional neural networks for energy-efficient object recognition. <em>Int. J. Comput. Vision</em> 113, 5466. doi: 10.1007/s11263-014-0788-3</p>
<p><a href="https://doi.org/10.1007/s11263-014-0788-3">CrossRef Full Text</a></p>
<p>Coates, A., Ng, A., and Lee, H. (2011). An analysis of single-layer networks in unsupervised feature learning, in <em>Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</em>, volume 15 of <em>Proceedings of Machine Learning Research</em>, eds G. Gordon, D. Dunson, and M. Dudk (Fort Lauderdale, FL: PMLR), 215223.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Coates&amp;author=A.+Ng&amp;author=H.+Lee+&amp;publication_year=2011&amp;title=%E2%80%9CAn+analysis+of+single-layer+networks+in+unsupervised+feature+learning,%E2%80%9D&amp;pages=215-223">Google Scholar</a></p>
<p>Corradi, F., and Indiveri, G. (2015). A neuromorphic event-based neural recording system for smart brain-machine-interfaces. <em>IEEE Trans. Biomed. Circuits Syst.</em> 9, 699709. doi: 10.1109/TBCAS.2015.2479256</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=26513801">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TBCAS.2015.2479256">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=F.+Corradi&amp;author=G.+Indiveri+&amp;publication_year=2015&amp;title=A+neuromorphic+event-based+neural+recording+system+for+smart+brain-machine-interfaces&amp;journal=IEEE+Trans.+Biomed.+Circuits+Syst.&amp;volume=9&amp;pages=699-709">Google Scholar</a></p>
<p>Courbariaux, M., Bengio, Y., and David, J.-P. (2014). Training deep neural networks with low precision multiplications. <em>arXiv [Preprint]. arXiv:1412.7024</em>.</p>
<p>Courbariaux, M., Bengio, Y., and David, J.-P. (2015). BinaryConnect: training deep neural networks with binary weights during propagations, in <em>Advances in Neural Information Processing Systems 28</em>, eds C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (Montreal, QC: NIPS), 31233131.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=M.+Courbariaux&amp;author=Y.+Bengio&amp;author=J.+-P.+David+&amp;publication_year=2015&amp;title=%E2%80%9CBinaryConnect%3A+training+deep+neural+networks+with+binary+weights+during+propagations,%E2%80%9D&amp;pages=3123-3131">Google Scholar</a></p>
<p>Davies, M., Srinivasa, N., Lin, T. H., Chinya, G., Cao, Y., Choday, S. H., et al. (2018). Loihi: a neuromorphic manycore processor with on-chip learning. <em>IEEE Micro</em> 38, 8299. doi: 10.1109/MM.2018.112130359</p>
<p><a href="https://doi.org/10.1109/MM.2018.112130359">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Davies&amp;author=N.+Srinivasa&amp;author=T.+H.+Lin&amp;author=G.+Chinya&amp;author=Y.+Cao&amp;author=S.+H.+Choday+&amp;publication_year=2018&amp;title=Loihi%3A+a+neuromorphic+manycore+processor+with+on-chip+learning&amp;journal=IEEE+Micro&amp;volume=38&amp;pages=82-99">Google Scholar</a></p>
<p>Deng, L., Jiao, P., Pei, J., Wu, Z., and Li, G. (2018). Gxnor-net: training deep neural networks with ternary weights and activations without full-precision memory under a unified discretization framework. <em>Neural Netw.</em> 100, 4958. doi: 10.1016/j.neunet.2018.01.010</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=29471195">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.neunet.2018.01.010">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=L.+Deng&amp;author=P.+Jiao&amp;author=J.+Pei&amp;author=Z.+Wu&amp;author=G.+Li+&amp;publication_year=2018&amp;title=Gxnor-net%3A+training+deep+neural+networks+with+ternary+weights+and+activations+without+full-precision+memory+under+a+unified+discretization+framework&amp;journal=Neural+Netw.&amp;volume=100&amp;pages=49-58">Google Scholar</a></p>
<p>Dethier, J., Nuyujukian, P., Ryu, S. I., Shenoy, K. V., and Boahen, K. (2013). Design and validation of a real-time spiking-neural-network decoder for brainmachine interfaces. <em>J. Neural Eng.</em> 10:036008. doi: 10.1088/1741-2560/10/3/036008</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=23574919">PubMed Abstract</a> | <a href="https://doi.org/10.1088/1741-2560/10/3/036008">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Dethier&amp;author=P.+Nuyujukian&amp;author=S.+I.+Ryu&amp;author=K.+V.+Shenoy&amp;author=K.+Boahen+&amp;publication_year=2013&amp;title=Design+and+validation+of+a+real-time+spiking-neural-network+decoder+for+brain%E2%80%93machine+interfaces&amp;journal=J.+Neural+Eng.&amp;volume=10&amp;pages=036008">Google Scholar</a></p>
<p>Diehl, P. U., Neil, D., Binas, J., Cook, M., Liu, S. C., and Pfeiffer, M. (2015). Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing, in <em>2015 International Joint Conference on Neural Networks (IJCNN)</em> (Killarney: IJCNN), 18.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=P.+U.+Diehl&amp;author=D.+Neil&amp;author=J.+Binas&amp;author=M.+Cook&amp;author=S.+C.+Liu&amp;author=M.+Pfeiffer+&amp;publication_year=2015&amp;title=%E2%80%9CFast-classifying,+high-accuracy+spiking+deep+networks+through+weight+and+threshold+balancing,%E2%80%9D&amp;pages=1-8">Google Scholar</a></p>
<p>Diehl, P. U., Zarrella, G., Cassidy, A., Pedroni, B. U., and Neftci, E. (2016). Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware, in <em>2016 IEEE International Conference on Rebooting Computing (ICRC)</em> (San Diego, CA: ICRC), 18.</p>
<p>Dundar, A., Jin, J., and Culurciello, E. (2015). Convolutional clustering for unsupervised learning. <em>arXiv [Preprint]. arXiv:1511.06241</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Dundar&amp;author=J.+Jin&amp;author=E.+Culurciello+&amp;publication_year=2015&amp;title=Convolutional+clustering+for+unsupervised+learning&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1511.06241">Google Scholar</a></p>
<p>Eliasmith, C., and Anderson, C. H. (2004). <em>Neural Engineering: Computation, Representation, and Dynamics in Neurobiological Systems</em>. Cambridge: MIT Press.</p>
<p>Eliasmith, C., Stewart, T. C., Choo, X., Bekolay, T., DeWolf, T., Tang, Y., et al. (2012). A large-scale model of the functioning brain. <em>Science</em> 338, 12021205. doi: 10.1126/science.1225266</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=23197532">PubMed Abstract</a> | <a href="https://doi.org/10.1126/science.1225266">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=C.+Eliasmith&amp;author=T.+C.+Stewart&amp;author=X.+Choo&amp;author=T.+Bekolay&amp;author=T.+DeWolf&amp;author=Y.+Tang+&amp;publication_year=2012&amp;title=A+large-scale+model+of+the+functioning+brain&amp;journal=Science&amp;volume=338&amp;pages=1202-1205">Google Scholar</a></p>
<p>Esser, S. K., Appuswamy, R., Merolla, P., Arthur, J. V., and Modha, D. S. (2015). Backpropagation for energy-efficient neuromorphic computing, in <em>Advances in Neural Information Processing Systems 28</em>, eds C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (Montreal, QC: NIPS), 11171125.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=27651489">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+K.+Esser&amp;author=R.+Appuswamy&amp;author=P.+Merolla&amp;author=J.+V.+Arthur&amp;author=D.+S.+Modha+&amp;publication_year=2015&amp;title=%E2%80%9CBackpropagation+for+energy-efficient+neuromorphic+computing,%E2%80%9D&amp;pages=1117-1125">Google Scholar</a></p>
<p>Esser, S. K., Merolla, P. A., Arthur, J. V., Cassidy, A. S., Appuswamy, R., Andreopoulos, A., et al. (2016). Convolutional networks for fast, energy-efficient neuromorphic computing. <em>Proc. Natl. Acad. Sci. U.S.A.</em> 113, 1144111446. doi: 10.1073/pnas.1604850113</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=27651489">PubMed Abstract</a> | <a href="https://doi.org/10.1073/pnas.1604850113">CrossRef Full Text</a></p>
<p>Farabet, C., Paz, R., Prez-Carrasco, J., Zamarreo, C., Linares-Barranco, A., LeCun, Y., et al. (2012). Comparison between frame-constrained fix-pixel-value and frame-free spiking-dynamic-pixel convNets for visual processing. <em>Front. Neurosci.</em> 6:32. doi: 10.3389/fnins.2012.00032</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=22518097">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2012.00032">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=C.+Farabet&amp;author=R.+Paz&amp;author=J.+P%C3%A9rez-Carrasco&amp;author=C.+Zamarre%C3%B1o&amp;author=A.+Linares-Barranco&amp;author=Y.+LeCun+&amp;publication_year=2012&amp;title=Comparison+between+frame-constrained+fix-pixel-value+and+frame-free+spiking-dynamic-pixel+convNets+for+visual+processing&amp;journal=Front.+Neurosci.&amp;volume=6&amp;pages=32">Google Scholar</a></p>
<p>Fischer, V., Pfeil, T., and Koehler, J. M. (2018). The streaming rollout of deep networks - towards fully model-parallel execution, in <em>Advances in Neural Information Processing Systems</em> (Montreal, QC: NIPS).</p>
<p>Friedmann, S., Schemmel, J., Grbl, A., Hartel, A., Hock, M., and Meier, K. (2017). Demonstrating hybrid learning in a flexible neuromorphic hardware system. <em>IEEE Trans. Biomed. Circuits Syst.</em> 11, 128142. doi: 10.1109/TBCAS.2016.2579164</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=28113678">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TBCAS.2016.2579164">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Friedmann&amp;author=J.+Schemmel&amp;author=A.+Gr%C3%BCbl&amp;author=A.+Hartel&amp;author=M.+Hock&amp;author=K.+Meier+&amp;publication_year=2017&amp;title=Demonstrating+hybrid+learning+in+a+flexible+neuromorphic+hardware+system&amp;journal=IEEE+Trans.+Biomed.+Circuits+Syst.&amp;volume=11&amp;pages=128-142">Google Scholar</a></p>
<p>Fukushima, K. (1988). Neocognitron: a hierarchical neural network capable of visual pattern recognition. <em>Neural Netw.</em> 1, 119130.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=K.+Fukushima+&amp;publication_year=1988&amp;title=Neocognitron%3A+a+hierarchical+neural+network+capable+of+visual+pattern+recognition&amp;journal=Neural+Netw.&amp;volume=1&amp;pages=119-130">Google Scholar</a></p>
<p>Furber, S. B., Galluppi, F., Temple, S., and Plana, L. A. (2014). The SpiNNaker project. <em>Proc. IEEE</em> 102, 652665. doi: 10.1109/JPROC.2014.2304638</p>
<p><a href="https://doi.org/10.1109/JPROC.2014.2304638">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+B.+Furber&amp;author=F.+Galluppi&amp;author=S.+Temple&amp;author=L.+A.+Plana+&amp;publication_year=2014&amp;title=The+SpiNNaker+project&amp;journal=Proc.+IEEE&amp;volume=102&amp;pages=652-665">Google Scholar</a></p>
<p>Furber, S. B., Lester, D. R., Plana, L. A., Garside, J. D., Painkras, E., Temple, S., et al. (2013). Overview of the SpiNNaker system architecture. <em>IEEE Trans. Comput.</em> 62, 24542467. doi: 10.1109/TC.2012.142</p>
<p><a href="https://doi.org/10.1109/TC.2012.142">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+B.+Furber&amp;author=D.+R.+Lester&amp;author=L.+A.+Plana&amp;author=J.+D.+Garside&amp;author=E.+Painkras&amp;author=S.+Temple+&amp;publication_year=2013&amp;title=Overview+of+the+SpiNNaker+system+architecture&amp;journal=IEEE+Trans.+Comput.&amp;volume=62&amp;pages=2454-2467">Google Scholar</a></p>
<p>Gallego, G., Lund, J. E., Mueggler, E., Rebecq, H., Delbruck, T., and Scaramuzza, D. (2018). Event-based, 6-DOF camera tracking from photometric depth maps. <em>IEEE Trans. Pattern Anal. Mach. Intel.</em> 40, 24022412.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=29990121">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Gallego&amp;author=J.+E.+Lund&amp;author=E.+Mueggler&amp;author=H.+Rebecq&amp;author=T.+Delbruck&amp;author=D.+Scaramuzza+&amp;publication_year=2018&amp;title=Event-based,+6-DOF+camera+tracking+from+photometric+depth+maps&amp;journal=IEEE+Trans.+Pattern+Anal.+Mach.+Intel.&amp;volume=40&amp;pages=2402-2412">Google Scholar</a></p>
<p>Galloway, A., Taylor, G. W., and Moussa, M. (2018). Attacking binarized neural networks. <em>arXiv [Preprint]. arXiv:1711.00449</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Galloway&amp;author=G.+W.+Taylor&amp;author=M.+Moussa+&amp;publication_year=2018&amp;title=Attacking+binarized+neural+networks&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1711.00449">Google Scholar</a></p>
<p>Gardner, B., Sporea, I., and Grning, A. (2015). Encoding spike patterns in multilayer spiking neural networks. <em>arXiv [Preprint]. arXiv:1503.09129</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=B.+Gardner&amp;author=I.+Sporea&amp;author=A.+Gr%C3%BCning+&amp;publication_year=2015&amp;title=Encoding+spike+patterns+in+multilayer+spiking+neural+networks&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1503.09129">Google Scholar</a></p>
<p>Gerstner, W., Kempter, R., van Hemmen, J. L., and Wagner, H. (1996). A neuronal learning rule for sub-millisecond temporal coding. <em>Nature</em> 383:76.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=8779718">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=W.+Gerstner&amp;author=R.+Kempter&amp;author=J.+L.+van+Hemmen&amp;author=H.+Wagner+&amp;publication_year=1996&amp;title=A+neuronal+learning+rule+for+sub-millisecond+temporal+coding&amp;journal=Nature&amp;volume=383&amp;pages=76">Google Scholar</a></p>
<p>Gerstner, W., and Kistler, W. M. (2002). <em>Spiking Neuron Models: Single Neurons, Populations, Plasticity</em>. Cambridge: Cambridge University Press.</p>
<p>Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y. (2016). <em>Deep Learning, Vol. 1.</em> Cambridge: MIT Press.</p>
<p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al. (2014). Generative adversarial nets, in <em>Advances in Neural Information Processing Systems 27</em>, eds Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (Montreal, QC: NIPS), 26722680.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=I.+Goodfellow&amp;author=J.+Pouget-Abadie&amp;author=M.+Mirza&amp;author=B.+Xu&amp;author=D.+Warde-Farley&amp;author=S.+Ozair+&amp;publication_year=2014&amp;title=%E2%80%9CGenerative+adversarial+nets,%E2%80%9D&amp;pages=2672-2680">Google Scholar</a></p>
<p>Grning, A., and Bohte, S. M. (2014). Spiking neural networks: Principles and challenges, in <em>European Symposium on Artificial Neural Networks (ESANN), Computational Intelligence and Machine Learning</em> (Bruges: ESANN).</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition, in <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> (Las Vegas, NV: CVPR), 770778.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=K.+He&amp;author=X.+Zhang&amp;author=S.+Ren&amp;author=J.+Sun+&amp;publication_year=2016&amp;title=%E2%80%9CDeep+residual+learning+for+image+recognition,%E2%80%9D&amp;pages=770-778">Google Scholar</a></p>
<p>Hebb, D. O. (1949). <em>The Organization of Behavior: A Neuropsychological Theory</em>. Hoboken, NJ: John Wiley and Sons.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=D.+O.+Hebb+&amp;publication_year=1949&amp;title=The+Organization+of+Behavior%3A+A+Neuropsychological+Theory">Google Scholar</a></p>
<p>Hu, Y., Liu, H., Pfeiffer, M., and Delbruck, T. (2016). DVS benchmark datasets for object tracking, action recognition, and object recognition. <em>Front. Neurosci.</em> 10:405. doi: 10.3389/fnins.2016.00405</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=27630540">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2016.00405">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Y.+Hu&amp;author=H.+Liu&amp;author=M.+Pfeiffer&amp;author=T.+Delbruck+&amp;publication_year=2016&amp;title=DVS+benchmark+datasets+for+object+tracking,+action+recognition,+and+object+recognition&amp;journal=Front.+Neurosci.&amp;volume=10&amp;pages=405">Google Scholar</a></p>
<p>Hu, Y., Tang, H., Wang, Y., and Pan, G. (2018). Spiking deep residual network. <em>arXiv [Preprint]. arXiv:1805.01352</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=Y.+Hu&amp;author=H.+Tang&amp;author=Y.+Wang&amp;author=G.+Pan+&amp;publication_year=2018&amp;title=Spiking+deep+residual+network&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1805.01352">Google Scholar</a></p>
<p>Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y. (2016). Binarized neural networks, in <em>Advances in Neural Information Processing Systems 29</em>, eds D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (Barcelona: NIPS), 41074115.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=I.+Hubara&amp;author=M.+Courbariaux&amp;author=D.+Soudry&amp;author=R.+El-Yaniv&amp;author=Y.+Bengio+&amp;publication_year=2016&amp;title=%E2%80%9CBinarized+neural+networks,%E2%80%9D&amp;pages=4107-4115">Google Scholar</a></p>
<p>Hubel, D. H., and Wiesel, T. N. (1959). Receptive fields of single neurones in the cat's striate cortex. <em>J. Physiol.</em> 148, 574591.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=14403679">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+H.+Hubel&amp;author=T.+N.+Wiesel+&amp;publication_year=1959&amp;title=Receptive+fields+of+single+neurones+in+the+cat%27s+striate+cortex&amp;journal=J.+Physiol.&amp;volume=148&amp;pages=574-591">Google Scholar</a></p>
<p>Hunsberger, E., and Eliasmith, C. (2015). Spiking deep networks with LIF neurons. <em>arXiv [Preprint]. arXiv:1510.08829</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=E.+Hunsberger&amp;author=C.+Eliasmith+&amp;publication_year=2015&amp;title=Spiking+deep+networks+with+LIF+neurons&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1510.08829">Google Scholar</a></p>
<p>Hunsberger, E., and Eliasmith, C. (2016). Training spiking deep networks for neuromorphic hardware. <em>arXiv [Preprint]. arXiv:1611.05141</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=E.+Hunsberger&amp;author=C.+Eliasmith+&amp;publication_year=2016&amp;title=Training+spiking+deep+networks+for+neuromorphic+hardware&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1611.05141">Google Scholar</a></p>
<p>Indiveri, G., Corradi, F., and Qiao, N. (2015). Neuromorphic architectures for spiking deep neural networks, in <em>2015 IEEE International Electron Devices Meeting (IEDM)</em> (Washington, DC: IEDM), 2.12.4.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=G.+Indiveri&amp;author=F.+Corradi&amp;author=N.+Qiao+&amp;publication_year=2015&amp;title=%E2%80%9CNeuromorphic+architectures+for+spiking+deep+neural+networks,%E2%80%9D&amp;pages=2.1-2.4">Google Scholar</a></p>
<p>Indiveri, G., Linares-Barranco, B., Hamilton, T. J., Van Schaik, A., Etienne-Cummings, R., Delbruck, T., et al. (2011). Neuromorphic silicon neuron circuits. <em>Front. Neurosci.</em> 5:73. doi: 10.3389/fnins.2011.00073</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=21747754">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2011.00073">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Indiveri&amp;author=B.+Linares-Barranco&amp;author=T.+J.+Hamilton&amp;author=A.+Van+Schaik&amp;author=R.+Etienne-Cummings&amp;author=T.+Delbruck+&amp;publication_year=2011&amp;title=Neuromorphic+silicon+neuron+circuits&amp;journal=Front.+Neurosci.&amp;volume=5&amp;pages=73">Google Scholar</a></p>
<p>Indiveri, G., and Liu, S. C. (2015). Memory and information processing in neuromorphic systems. <em>Proc. IEEE</em> 103, 13791397. doi: 10.1109/JPROC.2015.2444094</p>
<p><a href="https://doi.org/10.1109/JPROC.2015.2444094">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Indiveri&amp;author=S.+C.+Liu+&amp;publication_year=2015&amp;title=Memory+and+information+processing+in+neuromorphic+systems&amp;journal=Proc.+IEEE&amp;volume=103&amp;pages=1379-1397">Google Scholar</a></p>
<p>Iyer, L. R., Chua, Y., and Li, H. (2018). Is Neuromorphic MNIST neuromorphic? Analyzing the discriminative power of neuromorphic datasets in the time domain. <em>arXiv [Preprint]. arXiv:1807.01013</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=L.+R.+Iyer&amp;author=Y.+Chua&amp;author=H.+Li+&amp;publication_year=2018&amp;title=Is+Neuromorphic+MNIST+neuromorphic%3F+Analyzing+the+discriminative+power+of+neuromorphic+datasets+in+the+time+domain&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1807.01013">Google Scholar</a></p>
<p>Jin, X., Rast, A., Galluppi, F., Davies, S., and Furber, S. (2010). Implementing spike-timing-dependent plasticity on SpiNNaker neuromorphic hardware, in <em>The 2010 International Joint Conference on Neural Networks (IJCNN)</em> (Barcelona: IJCNN), 18.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=X.+Jin&amp;author=A.+Rast&amp;author=F.+Galluppi&amp;author=S.+Davies&amp;author=S.+Furber+&amp;publication_year=2010&amp;title=%E2%80%9CImplementing+spike-timing-dependent+plasticity+on+SpiNNaker+neuromorphic+hardware,%E2%80%9D&amp;pages=1-8">Google Scholar</a></p>
<p>Jin, Y., Li, P., and Zhang, W. (2018). Hybrid macro/micro level backpropagation for training deep spiking neural networks. <em>arXiv [Preprint]. arXiv:1805.07866</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=Y.+Jin&amp;author=P.+Li&amp;author=W.+Zhang+&amp;publication_year=2018&amp;title=Hybrid+macro%2Fmicro+level+backpropagation+for+training+deep+spiking+neural+networks&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1805.07866">Google Scholar</a></p>
<p>Jo, S. H., Chang, T., Ebong, I., Bhadviya, B. B., Mazumder, P., and Lu, W. (2010). Nanoscale memristor device as synapse in neuromorphic systems. <em>Nano Lett.</em> 10:12971301. doi: 10.1021/nl904092h</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=20192230">PubMed Abstract</a> | <a href="https://doi.org/10.1021/nl904092h">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+H.+Jo&amp;author=T.+Chang&amp;author=I.+Ebong&amp;author=B.+B.+Bhadviya&amp;author=P.+Mazumder&amp;author=W.+Lu+&amp;publication_year=2010&amp;title=Nanoscale+memristor+device+as+synapse+in+neuromorphic+systems&amp;journal=Nano+Lett.&amp;volume=10&amp;pages=1297-1301">Google Scholar</a></p>
<p>Judd, P., Albericio, J., Hetherington, T., Aamodt, T., Enright Jerger, N., Urtasun, R., et al. (2015). Reduced-precision strategies for bounded memory in deep neural nets. <em>arXiv [Preprint]. arXiv:1511.05236</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=P.+Judd&amp;author=J.+Albericio&amp;author=T.+Hetherington&amp;author=T.+Aamodt&amp;author=N.+Enright+Jerger&amp;author=R.+Urtasun+&amp;publication_year=2015&amp;title=Reduced-precision+strategies+for+bounded+memory+in+deep+neural+nets&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1511.05236">Google Scholar</a></p>
<p>Jug, F., Lengler, J., Krautz, C., and Steger, A. (2012). <em>Spiking Networks and Their Rate-Based Equivalents: Does It Make Sense To Use Siegert Neurons?</em> Zurich: Swiss Society for Neuroscience.</p>
<p>Kayser, C., Montemurro, M. A., Logothetis, N. K., and Panzeri, S. (2009). Spike-phase coding boosts and stabilizes information carried by spatial and temporal spike patterns. <em>Neuron</em> 61, 597608. doi: 10.1016/j.neuron.2009.01.008</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=19249279">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.neuron.2009.01.008">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=C.+Kayser&amp;author=M.+A.+Montemurro&amp;author=N.+K.+Logothetis&amp;author=S.+Panzeri+&amp;publication_year=2009&amp;title=Spike-phase+coding+boosts+and+stabilizes+information+carried+by+spatial+and+temporal+spike+patterns&amp;journal=Neuron&amp;volume=61&amp;pages=597-608">Google Scholar</a></p>
<p>Kheradpisheh, S. R., Ganjtabesh, M., Thorpe, S. J., and Masquelier, T. (2018). STDP-based spiking deep convolutional neural networks for object recognition. <em>Neural Netw.</em> 99, 5667. doi: 10.1016/j.neunet.2017.12.005</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=29328958">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.neunet.2017.12.005">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+R.+Kheradpisheh&amp;author=M.+Ganjtabesh&amp;author=S.+J.+Thorpe&amp;author=T.+Masquelier+&amp;publication_year=2018&amp;title=STDP-based+spiking+deep+convolutional+neural+networks+for+object+recognition&amp;journal=Neural+Netw.&amp;volume=99&amp;pages=56-67">Google Scholar</a></p>
<p>Kim, H., Leutenegger, S., and Davison, A. J. (2016). Real-time 3D reconstruction and 6-DoF tracking with an event camera, in <em>European Conference on Computer Vision</em> (Amsterdam: Springer), 349364.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=H.+Kim&amp;author=S.+Leutenegger&amp;author=A.+J.+Davison+&amp;publication_year=2016&amp;title=%E2%80%9CReal-time+3D+reconstruction+and+6-DoF+tracking+with+an+event+camera,%E2%80%9D&amp;pages=349-364">Google Scholar</a></p>
<p>Kim, M., and Smaragdis, P. (2016). Bitwise neural networks. <em>arXiv [Preprint]. arXiv:1601.06071</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=M.+Kim&amp;author=P.+Smaragdis+&amp;publication_year=2016&amp;title=Bitwise+neural+networks&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1601.06071">Google Scholar</a></p>
<p>Kriegeskorte, N. (2015). Deep neural networks: a new framework for modeling biological vision and brain information processing. <em>Annu. Rev. Vision Sci.</em> 1, 417446. doi: 10.1146/annurev-vision-082114-035447</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=28532370">PubMed Abstract</a> | <a href="https://doi.org/10.1146/annurev-vision-082114-035447">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=N.+Kriegeskorte+&amp;publication_year=2015&amp;title=Deep+neural+networks%3A+a+new+framework+for+modeling+biological+vision+and+brain+information+processing&amp;journal=Annu.+Rev.+Vision+Sci.&amp;volume=1&amp;pages=417-446">Google Scholar</a></p>
<p>Krizehvsky, A., and Hinton, G. (2009): <em>Learning Multiple Layers of Features from Tiny Images</em>. Technical Report, University of Toronto. Vol. 1.</p>
<p>Kulkarni, S. R., and Rajendran, B. (2018). Spiking neural networks for handwritten digit recognitionsupervised learning and network optimization. <em>Neural Netw.</em> 103, 118127. doi: 10.1016/j.neunet.2018.03.019</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=29674234">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.neunet.2018.03.019">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+R.+Kulkarni&amp;author=B.+Rajendran+&amp;publication_year=2018&amp;title=Spiking+neural+networks+for+handwritten+digit+recognition%E2%80%94supervised+learning+and+network+optimization&amp;journal=Neural+Netw.&amp;volume=103&amp;pages=118-127">Google Scholar</a></p>
<p>Lagorce, X., and Benosman, R. (2015). STICK: spike time interval computational kernel, a framework for general purpose computation using neurons, precise timing, delays, and synchrony. <em>Neural Comput.</em> 27, 22612317. doi: 10.1162/NECO_a_00783</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=26378879">PubMed Abstract</a> | <a href="https://doi.org/10.1162/NECO_a_00783">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=X.+Lagorce&amp;author=R.+Benosman+&amp;publication_year=2015&amp;title=STICK%3A+spike+time+interval+computational+kernel,+a+framework+for+general+purpose+computation+using+neurons,+precise+timing,+delays,+and+synchrony&amp;journal=Neural+Comput.&amp;volume=27&amp;pages=2261-2317">Google Scholar</a></p>
<p>Lagorce, X., Meyer, C., Ieng, S. H., Filliat, D., and Benosman, R. (2015). Asynchronous event-based multikernel algorithm for high-speed visual features tracking. <em>IEEE Trans Neural Netw. Learn. Syst.</em> 26, 17101720. doi: 10.1109/TNNLS.2014.2352401</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=25248193">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TNNLS.2014.2352401">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=X.+Lagorce&amp;author=C.+Meyer&amp;author=S.+H.+Ieng&amp;author=D.+Filliat&amp;author=R.+Benosman+&amp;publication_year=2015&amp;title=Asynchronous+event-based+multikernel+algorithm+for+high-speed+visual+features+tracking&amp;journal=IEEE+Trans+Neural+Netw.+Learn.+Syst.&amp;volume=26&amp;pages=1710-1720">Google Scholar</a></p>
<p>Lagorce, X., Orchard, G., Galluppi, F., Shi, B. E., and Benosman, R. B. (2017). HOTS: a hierarchy of event-based time-surfaces for pattern recognition. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em> 39, 13461359. doi: 10.1109/TPAMI.2016.2574707</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=27411216">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TPAMI.2016.2574707">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=X.+Lagorce&amp;author=G.+Orchard&amp;author=F.+Galluppi&amp;author=B.+E.+Shi&amp;author=R.+B.+Benosman+&amp;publication_year=2017&amp;title=HOTS%3A+a+hierarchy+of+event-based+time-surfaces+for+pattern+recognition&amp;journal=IEEE+Trans.+Pattern+Anal.+Mach.+Intell.&amp;volume=39&amp;pages=1346-1359">Google Scholar</a></p>
<p>LeCun, Y., and Bengio, Y. (1995). <em>The Handbook of Brain Theory and Neural Networks.</em> chapter Convolutional Networks for Images, Speech, and Time Series. Cambridge, MA: MIT Press.</p>
<p>Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. <em>Proc. IEEE</em> 86, 22782324.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=Y.+Lecun&amp;author=L.+Bottou&amp;author=Y.+Bengio&amp;author=P.+Haffner+&amp;publication_year=1998&amp;title=Gradient-based+learning+applied+to+document+recognition&amp;journal=Proc.+IEEE&amp;volume=86&amp;pages=2278-2324">Google Scholar</a></p>
<p>Lee, C., Srinivasan, G., Panda, P., and Roy, K. (2018). Deep spiking convolutional neural network trained with unsupervised spike timing dependent plasticity. <em>IEEE Trans. Cognit. Develop. Syst.</em> 1. doi: 10.1109/TCDS.2018.2833071</p>
<p><a href="https://doi.org/10.1109/TCDS.2018.2833071">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=C.+Lee&amp;author=G.+Srinivasan&amp;author=P.+Panda&amp;author=K.+Roy+&amp;publication_year=2018&amp;title=Deep+spiking+convolutional+neural+network+trained+with+unsupervised+spike+timing+dependent+plasticity&amp;journal=IEEE+Trans.+Cognit.+Develop.+Syst">Google Scholar</a></p>
<p>Lee, J. H., Delbruck, T., Pfeiffer, M., Park, P. K., Shin, C.-W., Ryu, H., et al. (2014). Real-time gesture interface based on event-driven processing from stereo silicon retinas. <em>IEEE Trans. Neural Netw. Learn. Syst.</em> 25, 22502263. doi: 10.1109/TNNLS.2014.2308551</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=25420246">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TNNLS.2014.2308551">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+H.+Lee&amp;author=T.+Delbruck&amp;author=M.+Pfeiffer&amp;author=P.+K.+Park&amp;author=C.+-W.+Shin&amp;author=H.+Ryu+&amp;publication_year=2014&amp;title=Real-time+gesture+interface+based+on+event-driven+processing+from+stereo+silicon+retinas&amp;journal=IEEE+Trans.+Neural+Netw.+Learn.+Syst.&amp;volume=25&amp;pages=2250-2263">Google Scholar</a></p>
<p>Li, H., De, S., Xu, Z., Studer, C., Samet, H., and Goldstein, T. (2017). Training quantized nets: A deeper understanding, in <em>Advances in Neural Information Processing Systems 30</em>, eds. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Long Beach, CA), 58115821.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=H.+Li&amp;author=S.+De&amp;author=Z.+Xu&amp;author=C.+Studer&amp;author=H.+Samet&amp;author=T.+Goldstein+&amp;publication_year=2017&amp;title=%E2%80%9CTraining+quantized+nets%3A+A+deeper+understanding,%E2%80%9D&amp;pages=5811-5821">Google Scholar</a></p>
<p>Lichtsteiner, P., Posch, C., and Delbruck, T. (2008). A 128  128 120 dB 15s latency asynchronous temporal contrast vision sensor. <em>IEEE J. Solid State Circuits</em> 43, 566576. doi: 10.1109/JSSC.2007.914337</p>
<p><a href="https://doi.org/10.1109/JSSC.2007.914337">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=P.+Lichtsteiner&amp;author=C.+Posch&amp;author=T.+Delbruck+&amp;publication_year=2008&amp;title=A+128+%C3%97+128+120+dB+15%CE%BCs+latency+asynchronous+temporal+contrast+vision+sensor&amp;journal=IEEE+J.+Solid+State+Circuits&amp;volume=43&amp;pages=566-576">Google Scholar</a></p>
<p>Lillicrap, T. P., Cownden, D., Tweed, D. B., and Akerman, C. J. (2016). Random synaptic feedback weights support error backpropagation for deep learning. <em>Nat. Commun.</em> 7:13276. doi: 10.1038/ncomms13276</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=27824044">PubMed Abstract</a> | <a href="https://doi.org/10.1038/ncomms13276">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=T.+P.+Lillicrap&amp;author=D.+Cownden&amp;author=D.+B.+Tweed&amp;author=C.+J.+Akerman+&amp;publication_year=2016&amp;title=Random+synaptic+feedback+weights+support+error+backpropagation+for+deep+learning&amp;journal=Nat.+Commun.&amp;volume=7&amp;pages=13276">Google Scholar</a></p>
<p>Lin, C. K., Wild, A., Chinya, G. N., Cao, Y., Davies, M., Lavery, D. M., et al. (2018). Programming spiking neural networks on Intel's Loihi. <em>Computer</em> 51, 5261. doi: 10.1109/MC.2018.157113521</p>
<p><a href="https://doi.org/10.1109/MC.2018.157113521">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=C.+K.+Lin&amp;author=A.+Wild&amp;author=G.+N.+Chinya&amp;author=Y.+Cao&amp;author=M.+Davies&amp;author=D.+M.+Lavery+&amp;publication_year=2018&amp;title=Programming+spiking+neural+networks+on+Intel%27s+Loihi&amp;journal=Computer&amp;volume=51&amp;pages=52-61">Google Scholar</a></p>
<p>Liu, Q., Pineda-Garca, G., Stromatias, E., Serrano-Gotarredona, T., and Furber, S. B. (2016). Benchmarking spike-based visual recognition: a dataset and evaluation. <em>Front. Neurosci.</em> 10:496. doi: 10.3389/fnins.2016.00496</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=27853419">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2016.00496">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Q.+Liu&amp;author=G.+Pineda-Garc%C3%ADa&amp;author=E.+Stromatias&amp;author=T.+Serrano-Gotarredona&amp;author=S.+B.+Furber+&amp;publication_year=2016&amp;title=Benchmarking+spike-based+visual+recognition%3A+a+dataset+and+evaluation&amp;journal=Front.+Neurosci.&amp;volume=10&amp;pages=496">Google Scholar</a></p>
<p>Liu, S.-C., Delbruck, T., Indiveri, G., Whatley, A., and Douglas, R. (2015). <em>Event-based neuromorphic systems</em>. John Wiley &amp; Sons.</p>
<p>Maass, W. (1997). Networks of spiking neurons: the third generation of neural network models. <em>Neural Netw.</em> 10, 16591671.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=W.+Maass+&amp;publication_year=1997&amp;title=Networks+of+spiking+neurons%3A+the+third+generation+of+neural+network+models&amp;journal=Neural+Netw.&amp;volume=10&amp;pages=1659-1671">Google Scholar</a></p>
<p>Maass, W., Natschlger, T., and Markram, H. (2002). Real-time computing without stable states: a new framework for neural computation based on perturbations. <em>Neural Comput.</em> 14, 25312560. doi: 10.1162/089976602760407955</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=12433288">PubMed Abstract</a> | <a href="https://doi.org/10.1162/089976602760407955">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=W.+Maass&amp;author=T.+Natschl%C3%A4ger&amp;author=H.+Markram+&amp;publication_year=2002&amp;title=Real-time+computing+without+stable+states%3A+a+new+framework+for+neural+computation+based+on+perturbations&amp;journal=Neural+Comput.&amp;volume=14&amp;pages=2531-2560">Google Scholar</a></p>
<p>Machens, C. K., Schtze, H., Franz, A., Kolesnikova, O., Stemmler, M. B., Ronacher, B., et al. (2003). Single auditory neurons rapidly discriminate conspecific communication signals. <em>Nat. Neurosci.</em> 6, 341. doi: 10.1038/nn1036</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=12652305">PubMed Abstract</a> | <a href="https://doi.org/10.1038/nn1036">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=C.+K.+Machens&amp;author=H.+Sch%C3%BCtze&amp;author=A.+Franz&amp;author=O.+Kolesnikova&amp;author=M.+B.+Stemmler&amp;author=B.+Ronacher+&amp;publication_year=2003&amp;title=Single+auditory+neurons+rapidly+discriminate+conspecific+communication+signals&amp;journal=Nat.+Neurosci.&amp;volume=6&amp;pages=341">Google Scholar</a></p>
<p>Mahowald, M. (1994). <em>An Analog VLSI System for Stereoscopic Vision, Vol. 265</em>. New York, NY: Springer US.</p>
<p>Markov, N. T., Vezoli, J., Chameau, P., Falchier, A., Quilodran, R., Huissoud, C., et al. (2014). Anatomy of hierarchy: feedforward and feedback pathways in macaque visual cortex. <em>J. Comp. Neurol.</em> 522, 225259. doi: 10.1002/cne.23458</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=23983048">PubMed Abstract</a> | <a href="https://doi.org/10.1002/cne.23458">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=N.+T.+Markov&amp;author=J.+Vezoli&amp;author=P.+Chameau&amp;author=A.+Falchier&amp;author=R.+Quilodran&amp;author=C.+Huissoud+&amp;publication_year=2014&amp;title=Anatomy+of+hierarchy%3A+feedforward+and+feedback+pathways+in+macaque+visual+cortex&amp;journal=J.+Comp.+Neurol.&amp;volume=522&amp;pages=225-259">Google Scholar</a></p>
<p>Markram, H., Lbke, J., Frotscher, M., and Sakmann, B. (1997). Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs. <em>Science</em> 275, 213215.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=8985014">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=H.+Markram&amp;author=J.+L%C3%BCbke&amp;author=M.+Frotscher&amp;author=B.+Sakmann+&amp;publication_year=1997&amp;title=Regulation+of+synaptic+efficacy+by+coincidence+of+postsynaptic+APs+and+EPSPs&amp;journal=Science&amp;volume=275&amp;pages=213-215">Google Scholar</a></p>
<p>Martel, J. N. P., Mller, J., Conradt, J., and Sandamirskaya, Y. (2018). An active approach to solving the stereo matching problem using event-based sensors, in <em>2018 IEEE International Symposium on Circuits and Systems (ISCAS)</em> (Florence: ISCAS), 15.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+N.+P.+Martel&amp;author=J.+M%C3%BCller&amp;author=J.+Conradt&amp;author=Y.+Sandamirskaya+&amp;publication_year=2018&amp;title=%E2%80%9CAn+active+approach+to+solving+the+stereo+matching+problem+using+event-based+sensors,%E2%80%9D&amp;pages=1-5">Google Scholar</a></p>
<p>McKennoch, S., Liu, D., and Bushnell, L. G. (2006). Fast modifications of the SpikeProp algorithm, in <em>The 2006 IEEE International Joint Conference on Neural Network Proceedings</em> (Vancouver, BC: IJCNN), 39703977.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=S.+McKennoch&amp;author=D.+Liu&amp;author=L.+G.+Bushnell+&amp;publication_year=2006&amp;title=%E2%80%9CFast+modifications+of+the+SpikeProp+algorithm,%E2%80%9D&amp;pages=3970-3977">Google Scholar</a></p>
<p>Mead, C. (1990). Neuromorphic electronic systems. <em>Proc. IEEE</em> 78, 16291636.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=C.+Mead+&amp;publication_year=1990&amp;title=Neuromorphic+electronic+systems&amp;journal=Proc.+IEEE&amp;volume=78&amp;pages=1629-1636">Google Scholar</a></p>
<p>Merolla, P., Appuswamy, R., Arthur, J., Esser, S. K., and Modha, D. (2016). Deep neural networks are robust to weight binarization and other non-linear distortions. <em>arXiv [Preprint]. arXiv:1606.01981</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=P.+Merolla&amp;author=R.+Appuswamy&amp;author=J.+Arthur&amp;author=S.+K.+Esser&amp;author=D.+Modha+&amp;publication_year=2016&amp;title=Deep+neural+networks+are+robust+to+weight+binarization+and+other+non-linear+distortions&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1606.01981">Google Scholar</a></p>
<p>Merolla, P. A., Arthur, J. V., Alvarez-Icaza, R., Cassidy, A. S., Sawada, J., Akopyan, F., et al. (2014). A million spiking-neuron integrated circuit with a scalable communication network and interface. <em>Science</em> 345, 668673. doi: 10.1126/science.1254642</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=25104385">PubMed Abstract</a> | <a href="https://doi.org/10.1126/science.1254642">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=P.+A.+Merolla&amp;author=J.+V.+Arthur&amp;author=R.+Alvarez-Icaza&amp;author=A.+S.+Cassidy&amp;author=J.+Sawada&amp;author=F.+Akopyan+&amp;publication_year=2014&amp;title=A+million+spiking-neuron+integrated+circuit+with+a+scalable+communication+network+and+interface&amp;journal=Science&amp;volume=345&amp;pages=668-673">Google Scholar</a></p>
<p>Mikaitis, M., Pineda Garca, G., Knight, J. C., and Furber, S. B. (2018). Neuromodulated synaptic plasticity on the SpiNNaker neuromorphic system. <em>Front. Neurosci.</em> 12:105. doi: 10.3389/fnins.2018.00105</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=29535600">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2018.00105">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Mikaitis&amp;author=G.+Pineda+Garca&amp;author=J.+C.+Knight&amp;author=S.+B.+Furber+&amp;publication_year=2018&amp;title=Neuromodulated+synaptic+plasticity+on+the+SpiNNaker+neuromorphic+system&amp;journal=Front.+Neurosci.&amp;volume=12&amp;pages=105">Google Scholar</a></p>
<p>Mishra, A., Nurvitadhi, E., Cook, J. J., and Marr, D. (2017). WRPN: Wide reduced-precision networks. <em>arXiv [Preprint]. arXiv:1709.01134</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Mishra&amp;author=E.+Nurvitadhi&amp;author=J.+J.+Cook&amp;author=D.+Marr+&amp;publication_year=2017&amp;title=WRPN%3A+Wide+reduced-precision+networks&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1709.01134">Google Scholar</a></p>
<p>Montemurro, M. A., Rasch, M. J., Murayama, Y., Logothetis, N. K., and Panzeri, S. (2008). Phase-of-firing coding of natural visual stimuli in primary visual cortex. <em>Curr. Biol.</em> 18, 375380. doi: 10.1016/j.cub.2008.02.023</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=18328702">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.cub.2008.02.023">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+A.+Montemurro&amp;author=M.+J.+Rasch&amp;author=Y.+Murayama&amp;author=N.+K.+Logothetis&amp;author=S.+Panzeri+&amp;publication_year=2008&amp;title=Phase-of-firing+coding+of+natural+visual+stimuli+in+primary+visual+cortex&amp;journal=Curr.+Biol.&amp;volume=18&amp;pages=375-380">Google Scholar</a></p>
<p>Moradi, S., Qiao, N., Stefanini, F., and Indiveri, G. (2017). A scalable multicore architecture with heterogeneous memory structures for dynamic neuromorphic asynchronous processors (dynaps), in <em>IEEE Transactions on Biomedical Circuits and Systems</em> (Turin: bioCAS).</p>
<p>Mostafa, H. (2018). Supervised learning based on temporal coding in spiking neural networks. <em>IEEE Trans. Neural Netw. Learn. Syst.</em> 29, 32273235.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=28783639">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=H.+Mostafa+&amp;publication_year=2018&amp;title=Supervised+learning+based+on+temporal+coding+in+spiking+neural+networks&amp;journal=IEEE+Trans.+Neural+Netw.+Learn.+Syst.&amp;volume=29&amp;pages=3227-3235">Google Scholar</a></p>
<p>Mostafa, H., Pedroni, B. U., Sheik, S., and Cauwenberghs, G. (2017a). Fast classification using sparsely active spiking networks, in <em>2017 IEEE International Symposium on Circuits and Systems (ISCAS)</em> (Baltimore, MD: ISCAS), 14.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=H.+Mostafa&amp;author=B.+U.+Pedroni&amp;author=S.+Sheik&amp;author=G.+Cauwenberghs+&amp;publication_year=2017a&amp;title=%E2%80%9CFast+classification+using+sparsely+active+spiking+networks,%E2%80%9D&amp;pages=1-4">Google Scholar</a></p>
<p>Mostafa, H., Ramesh, V., and Cauwenberghs, G. (2017b). Deep supervised learning using local errors. <em>arXiv [Preprint]. arXiv:1711.06756</em>.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=30233295">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=H.+Mostafa&amp;author=V.+Ramesh&amp;author=G.+Cauwenberghs+&amp;publication_year=2017b&amp;title=Deep+supervised+learning+using+local+errors&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1711.06756">Google Scholar</a></p>
<p>Mozafari, M., Ganjtabesh, M., Nowzari-Dalini, A., Thorpe, S. J., and Masquelier, T. (2018). Combining STDP and Reward-Modulated STDP in Deep Convolutional Spiking Neural Networks for Digit Recognition. <em>arXiv [Preprint]. arXiv:1804.00227</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=M.+Mozafari&amp;author=M.+Ganjtabesh&amp;author=A.+Nowzari-Dalini&amp;author=S.+J.+Thorpe&amp;author=T.+Masquelier+&amp;publication_year=2018&amp;title=Combining+STDP+and+Reward-Modulated+STDP+in+Deep+Convolutional+Spiking+Neural+Networks+for+Digit+Recognition&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1804.00227">Google Scholar</a></p>
<p>Mueggler, E., Huber, B., and Scaramuzza, D. (2014). Event-based, 6-DOF pose tracking for high-speed maneuvers, in <em>2014 IEEE/RSJ International Conference on Intelligent Robots and Systems</em> (Chicago, IL: IROS), 27612768.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=E.+Mueggler&amp;author=B.+Huber&amp;author=D.+Scaramuzza+&amp;publication_year=2014&amp;title=%E2%80%9CEvent-based,+6-DOF+pose+tracking+for+high-speed+maneuvers,%E2%80%9D&amp;pages=2761-2768">Google Scholar</a></p>
<p>Mueggler, E., Rebecq, H., Gallego, G., Delbruck, T., and Scaramuzza, D. (2017). The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and SLAM. <em>The International Journal of Robotics Research</em>, 36(2):142149.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=E.+Mueggler&amp;author=H.+Rebecq&amp;author=G.+Gallego&amp;author=T.+Delbruck&amp;author=D.+Scaramuzza+&amp;publication_year=2017&amp;title=The+event-camera+dataset+and+simulator%3A+Event-based+data+for+pose+estimation,+visual+odometry,+and+SLAM&amp;journal=The+International+Journal+of+Robotics+Research&amp;volume=36&amp;issue=2&amp;pages=142-149">Google Scholar</a></p>
<p>Nair, V., and Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann machines, in <em>Proceedings of the 27th International Conference on Machine Learning (ICML-10)</em>, eds J. Frnkranz and T. Joachims (Haifa: Omni Press), 807814.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=V.+Nair&amp;author=G.+E.+Hinton+&amp;publication_year=2010&amp;title=%E2%80%9CRectified+linear+units+improve+restricted+Boltzmann+machines,%E2%80%9D&amp;pages=807-814">Google Scholar</a></p>
<p>Neftci, E., Das, S., Pedroni, B., Kreutz-Delgado, K., and Cauwenberghs, G. (2014). Event-driven contrastive divergence for spiking neuromorphic systems. <em>Front. Neurosci.</em> 7:272. doi: 10.3389/fnins.2013.00272</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=24574952">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2013.00272">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=E.+Neftci&amp;author=S.+Das&amp;author=B.+Pedroni&amp;author=K.+Kreutz-Delgado&amp;author=G.+Cauwenberghs+&amp;publication_year=2014&amp;title=Event-driven+contrastive+divergence+for+spiking+neuromorphic+systems&amp;journal=Front.+Neurosci.&amp;volume=7&amp;pages=272">Google Scholar</a></p>
<p>Neftci, E. O., Augustine, C., Paul, S., and Detorakis, G. (2017). Event-driven random back-propagation: enabling neuromorphic deep learning machines. <em>Front. Neurosci.</em> 11:324. doi: 10.3389/fnins.2017.00324</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=28680387">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2017.00324">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=E.+O.+Neftci&amp;author=C.+Augustine&amp;author=S.+Paul&amp;author=G.+Detorakis+&amp;publication_year=2017&amp;title=Event-driven+random+back-propagation%3A+enabling+neuromorphic+deep+learning+machines&amp;journal=Front.+Neurosci.&amp;volume=11&amp;pages=324">Google Scholar</a></p>
<p>Neftci, E. O., Pedroni, B. U., Joshi, S., Al-Shedivat, M., and Cauwenberghs, G. (2016). Stochastic synapses enable efficient brain-inspired learning machines. <em>Front. Neurosci.</em> 10:241. doi: 10.3389/fnins.2016.00241</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=27445650">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2016.00241">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=E.+O.+Neftci&amp;author=B.+U.+Pedroni&amp;author=S.+Joshi&amp;author=M.+Al-Shedivat&amp;author=G.+Cauwenberghs+&amp;publication_year=2016&amp;title=Stochastic+synapses+enable+efficient+brain-inspired+learning+machines&amp;journal=Front.+Neurosci.&amp;volume=10&amp;pages=241">Google Scholar</a></p>
<p>Neil, D., and Liu, S. C. (2014). Minitaur, an event-driven FPGA-based spiking network accelerator. <em>IEEE Trans. Very Large Scale Integr. Syst.</em> 22, 26212628. doi: 10.1109/TVLSI.2013.2294916</p>
<p><a href="https://doi.org/10.1109/TVLSI.2013.2294916">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Neil&amp;author=S.+C.+Liu+&amp;publication_year=2014&amp;title=Minitaur,+an+event-driven+FPGA-based+spiking+network+accelerator&amp;journal=IEEE+Trans.+Very+Large+Scale+Integr.+Syst.&amp;volume=22&amp;pages=2621-2628">Google Scholar</a></p>
<p>Neil, D., Pfeiffer, M., and Liu, S.-C. (2016a). Learning to be efficient: Algorithms for training low-latency, low-compute deep spiking neural networks, in <em>Proceedings of the 31st Annual ACM Symposium on Applied Computing</em>, SAC '16 (New York, NY:ACM), 293298.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=D.+Neil&amp;author=M.+Pfeiffer&amp;author=S.+-C.+Liu+&amp;publication_year=2016a&amp;title=%E2%80%9CLearning+to+be+efficient%3A+Algorithms+for+training+low-latency,+low-compute+deep+spiking+neural+networks,%E2%80%9D&amp;journal=Proceedings+of+the+31st+Annual+ACM+Symposium+on+Applied+Computing&amp;pages=293-298">Google Scholar</a></p>
<p>Neil, D., Pfeiffer, M., and Liu, S.-C. (2016b). Phased LSTM: Accelerating recurrent network training for long or event-based sequences, in <em>Advances in Neural Information Processing Systems 29</em>, eds D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (Barcelona: NIPS), 38823890.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=D.+Neil&amp;author=M.+Pfeiffer&amp;author=S.+-C.+Liu+&amp;publication_year=2016b&amp;title=%E2%80%9CPhased+LSTM%3A+Accelerating+recurrent+network+training+for+long+or+event-based+sequences,%E2%80%9D&amp;pages=3882-3890">Google Scholar</a></p>
<p>Nessler, B., Pfeiffer, M., Buesing, L., and Maass, W. (2013). Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity. <em>PLoS Comput. Biol.</em> 9:e1003037. doi: 10.1371/journal.pcbi.1003037</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=23633941">PubMed Abstract</a> | <a href="https://doi.org/10.1371/journal.pcbi.1003037">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=B.+Nessler&amp;author=M.+Pfeiffer&amp;author=L.+Buesing&amp;author=W.+Maass+&amp;publication_year=2013&amp;title=Bayesian+computation+emerges+in+generic+cortical+microcircuits+through+spike-timing-dependent+plasticity&amp;journal=PLoS+Comput.+Biol.&amp;volume=9&amp;pages=e1003037">Google Scholar</a></p>
<p>O'Connor, P., Neil, D., Liu, S. C., Delbruck, T., and Pfeiffer, M. (2013). Real-time classification and sensor fusion with a spiking deep belief network. <em>Front. Neurosci.</em> 7:178. doi: 10.3389/fnins.2013.00178</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=24115919">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2013.00178">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=P.+O%27Connor&amp;author=D.+Neil&amp;author=S.+C.+Liu&amp;author=T.+Delbruck&amp;author=M.+Pfeiffer+&amp;publication_year=2013&amp;title=Real-time+classification+and+sensor+fusion+with+a+spiking+deep+belief+network&amp;journal=Front.+Neurosci.&amp;volume=7&amp;pages=178">Google Scholar</a></p>
<p>O'Connor, P., and Welling, M. (2016). Deep spiking networks. <em>arXiv [Preprint]. arXiv:1602.08323</em>.</p>
<p>Orchard, G., Jayawant, A., Cohen, G. K., and Thakor, N. (2015a). Converting static image datasets to spiking neuromorphic datasets using saccades. <em>Front. Neurosci.</em> 9:437. doi: 10.3389/fnins.2015.00437</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=26635513">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2015.00437">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Orchard&amp;author=A.+Jayawant&amp;author=G.+K.+Cohen&amp;author=N.+Thakor+&amp;publication_year=2015a&amp;title=Converting+static+image+datasets+to+spiking+neuromorphic+datasets+using+saccades&amp;journal=Front.+Neurosci.&amp;volume=9&amp;pages=437">Google Scholar</a></p>
<p>Orchard, G., Martin, J. G., Vogelstein, R. J., and Etienne-Cummings, R. (2013). Fast neuromimetic object recognition using FPGA outperforms GPU implementations. <em>IEEE Trans. Neural Netw. Learn. Syst.</em> 24, 12391252. doi: 10.1109/TNNLS.2013.2253563</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=24808564">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TNNLS.2013.2253563">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Orchard&amp;author=J.+G.+Martin&amp;author=R.+J.+Vogelstein&amp;author=R.+Etienne-Cummings+&amp;publication_year=2013&amp;title=Fast+neuromimetic+object+recognition+using+FPGA+outperforms+GPU+implementations&amp;journal=IEEE+Trans.+Neural+Netw.+Learn.+Syst.&amp;volume=24&amp;pages=1239-1252">Google Scholar</a></p>
<p>Orchard, G., Meyer, C., Etienne-Cummings, R., Posch, C., Thakor, N., and Benosman, R. (2015b). HFirst: A temporal approach to object recognition. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em> 37, 20282040. doi: 10.1109/TPAMI.2015.2392947</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=26353184">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TPAMI.2015.2392947">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Orchard&amp;author=C.+Meyer&amp;author=R.+Etienne-Cummings&amp;author=C.+Posch&amp;author=N.+Thakor&amp;author=R.+Benosman+&amp;publication_year=2015b&amp;title=HFirst%3A+A+temporal+approach+to+object+recognition&amp;journal=IEEE+Trans.+Pattern+Anal.+Mach.+Intell.&amp;volume=37&amp;pages=2028-2040">Google Scholar</a></p>
<p>Osswald, M., Ieng, S.-H., Benosman, R., and Indiveri, G. (2017). A spiking neural network model of 3D perception for event-based neuromorphic stereo vision systems. <em>Sci. Rep.</em> 7:40703. doi: 10.1038/srep40703</p>
<p><a href="https://doi.org/10.1038/srep40703">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Osswald&amp;author=S.+-H.+Ieng&amp;author=R.+Benosman&amp;author=G.+Indiveri+&amp;publication_year=2017&amp;title=A+spiking+neural+network+model+of+3D+perception+for+event-based+neuromorphic+stereo+vision+systems&amp;journal=Sci.+Rep.&amp;volume=7&amp;pages=40703">Google Scholar</a></p>
<p>Panda, P., and Roy, K. (2016). Unsupervised regenerative learning of hierarchical features in spiking deep networks for object recognition, in <em>2016 International Joint Conference on Neural Networks (IJCNN)</em> (Vancouver, BC: IJCNN), 299306.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=P.+Panda&amp;author=K.+Roy+&amp;publication_year=2016&amp;title=%E2%80%9CUnsupervised+regenerative+learning+of+hierarchical+features+in+spiking+deep+networks+for+object+recognition,%E2%80%9D&amp;pages=299-306">Google Scholar</a></p>
<p>Panda, P., and Roy, K. (2017). Learning to generate sequences with combination of hebbian and non-hebbian plasticity in recurrent spiking neural networks. <em>Front. Neurosci.</em> 11:693. doi: 10.3389/fnins.2017.00693</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=29311774">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2017.00693">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=P.+Panda&amp;author=K.+Roy+&amp;publication_year=2017&amp;title=Learning+to+generate+sequences+with+combination+of+hebbian+and+non-hebbian+plasticity+in+recurrent+spiking+neural+networks&amp;journal=Front.+Neurosci.&amp;volume=11&amp;pages=693">Google Scholar</a></p>
<p>Panda, P., and Srinivasa, N. (2018). Learning to recognize actions from limited training examples using a recurrent spiking neural model. <em>Front. Neurosci.</em> 12:126. doi: 10.3389/fnins.2018.00126</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=29551962">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2018.00126">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=P.+Panda&amp;author=N.+Srinivasa+&amp;publication_year=2018&amp;title=Learning+to+recognize+actions+from+limited+training+examples+using+a+recurrent+spiking+neural+model&amp;journal=Front.+Neurosci.&amp;volume=12&amp;pages=126">Google Scholar</a></p>
<p>Panda, P., Srinivasan, G., and Roy, K. (2017). Convolutional spike timing dependent plasticity based feature learning in spiking neural networks. <em>arXiv [Preprint]. arXiv:1703.03854</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=P.+Panda&amp;author=G.+Srinivasan&amp;author=K.+Roy+&amp;publication_year=2017&amp;title=Convolutional+spike+timing+dependent+plasticity+based+feature+learning+in+spiking+neural+networks&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1703.03854">Google Scholar</a></p>
<p>Park, J., Ha, S., Yu, T., Neftci, E., and Cauwenberghs, G. (2014). A 65k-neuron 73-Mevents/s 22-pJ/event asynchronous micro-pipelined integrate-and-fire array transceiver, in <em>2014 IEEE Biomedical Circuits and Systems Conference (BioCAS) Proceedings</em> (Lausanne: BioCAS), 675678.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+Park&amp;author=S.+Ha&amp;author=T.+Yu&amp;author=E.+Neftci&amp;author=G.+Cauwenberghs+&amp;publication_year=2014&amp;title=%E2%80%9CA+65k-neuron+73-Mevents%2Fs+22-pJ%2Fevent+asynchronous+micro-pipelined+integrate-and-fire+array+transceiver,%E2%80%9D&amp;pages=675-678">Google Scholar</a></p>
<p>Pawlak, V., Wickens, J., Kirkwood, A., and Kerr, J. (2010). Timing is not everything: neuromodulation opens the STDP gate. <em>Front. Synapt. Neurosci.</em> 2:146. doi: 10.3389/fnsyn.2010.00146</p>
<p><a href="https://doi.org/10.3389/fnsyn.2010.00146">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=V.+Pawlak&amp;author=J.+Wickens&amp;author=A.+Kirkwood&amp;author=J.+Kerr+&amp;publication_year=2010&amp;title=Timing+is+not+everything%3A+neuromodulation+opens+the+STDP+gate&amp;journal=Front.+Synapt.+Neurosci.&amp;volume=2&amp;pages=146">Google Scholar</a></p>
<p>Pedretti, G., Milo, V., Ambrogio, S., Carboni, R., Bianchi, S., Calderoni, A., et al. (2017). Memristive neural network for on-line learning and tracking with brain-inspired spike timing dependent plasticity. <em>Sci. Rep.</em> 7:5288. doi: 10.1038/s41598-017-05480-0</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=28706303">PubMed Abstract</a> | <a href="https://doi.org/10.1038/s41598-017-05480-0">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Pedretti&amp;author=V.+Milo&amp;author=S.+Ambrogio&amp;author=R.+Carboni&amp;author=S.+Bianchi&amp;author=A.+Calderoni+&amp;publication_year=2017&amp;title=Memristive+neural+network+for+on-line+learning+and+tracking+with+brain-inspired+spike+timing+dependent+plasticity&amp;journal=Sci.+Rep.&amp;volume=7&amp;pages=5288">Google Scholar</a></p>
<p>Prez-Carrasco, J. A., Zhao, B., Serrano, C., Acha, B., Serrano-Gotarredona, T., Chen, S., et al. (2013). Mapping from frame-driven to frame-free event-driven vision systems by low-rate rate coding and coincidence processingapplication to feedforward convNets. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em> 35, 27062719. doi: 10.1109/TPAMI.2013.71</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=24051730">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TPAMI.2013.71">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+A.+P%C3%A9rez-Carrasco&amp;author=B.+Zhao&amp;author=C.+Serrano&amp;author=B.+Acha&amp;author=T.+Serrano-Gotarredona&amp;author=S.+Chen+&amp;publication_year=2013&amp;title=Mapping+from+frame-driven+to+frame-free+event-driven+vision+systems+by+low-rate+rate+coding+and+coincidence+processing%E2%80%93application+to+feedforward+convNets&amp;journal=IEEE+Trans.+Pattern+Anal.+Mach.+Intell.&amp;volume=35&amp;pages=2706-2719">Google Scholar</a></p>
<p>Petrovici, M. A., Schmitt, S., Klhn, J., Stckel, D., Schroeder, A., Bellec, G., et al. (2017). Pattern representation and recognition with accelerated analog neuromorphic systems, in <em>2017 IEEE International Symposium on Circuits and Systems (ISCAS)</em> (Baltimore, MD: ISCAS), 14.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=M.+A.+Petrovici&amp;author=S.+Schmitt&amp;author=J.+Kl%C3%A4hn&amp;author=D.+St%C3%B6ckel&amp;author=A.+Schroeder&amp;author=G.+Bellec+&amp;publication_year=2017&amp;title=%E2%80%9CPattern+representation+and+recognition+with+accelerated+analog+neuromorphic+systems,%E2%80%9D&amp;pages=1-4">Google Scholar</a></p>
<p>Pfeil, T., Grbl, A., Jeltsch, S., Mller, E., Mller, P., Petrovici, M. A., et al. (2013a). Six networks on a universal neuromorphic computing substrate. <em>Front. Neurosci.</em> 7:11. doi: 10.3389/fnins.2013.00011</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=23423583">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2013.00011">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=T.+Pfeil&amp;author=A.+Gr%C3%BCbl&amp;author=S.+Jeltsch&amp;author=E.+M%C3%BCller&amp;author=P.+M%C3%BCller&amp;author=M.+A.+Petrovici+&amp;publication_year=2013a&amp;title=Six+networks+on+a+universal+neuromorphic+computing+substrate&amp;journal=Front.+Neurosci.&amp;volume=7&amp;pages=11">Google Scholar</a></p>
<p>Pfeil, T., Potjans, T., Schrader, S., Potjans, W., Schemmel, J., Diesmann, M., et al. (2012). Is a 4-bit synaptic weight resolution enough? Constraints on enabling spike-timing dependent plasticity in neuromorphic hardware. <em>Front. Neurosci.</em> 6:90. doi: 10.3389/fnins.2012.00090</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=22822388">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2012.00090">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=T.+Pfeil&amp;author=T.+Potjans&amp;author=S.+Schrader&amp;author=W.+Potjans&amp;author=J.+Schemmel&amp;author=M.+Diesmann+&amp;publication_year=2012&amp;title=Is+a+4-bit+synaptic+weight+resolution+enough%3F+Constraints+on+enabling+spike-timing+dependent+plasticity+in+neuromorphic+hardware&amp;journal=Front.+Neurosci.&amp;volume=6&amp;pages=90">Google Scholar</a></p>
<p>Pfeil, T., Scherzer, A. C., Schemmel, J., and Meier, K. (2013b). Neuromorphic learning towards nano second precision, in <em>The 2013 International Joint Conference on Neural Networks (IJCNN)</em> (Dallas, TX: IJCNN), 15.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=T.+Pfeil&amp;author=A.+C.+Scherzer&amp;author=J.+Schemmel&amp;author=K.+Meier+&amp;publication_year=2013b&amp;title=%E2%80%9CNeuromorphic+learning+towards+nano+second+precision,%E2%80%9D&amp;pages=1-5">Google Scholar</a></p>
<p>Polino, A., Pascanu, R., and Alistarh, D. (2018). Model compression via distillation and quantization, in <em>International Conference on Learning Representations</em> (Vancouver, BC: ICLR).</p>
<p>Ponulak, F., and Kasiski, A. (2010). Supervised learning in spiking neural networks with resume: sequence learning, classification, and spike shifting. <em>Neural Comput.</em> 22, 467510. doi: 10.1162/neco.2009.11-08-901</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=19842989">PubMed Abstract</a> | <a href="https://doi.org/10.1162/neco.2009.11-08-901">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=F.+Ponulak&amp;author=A.+Kasi%C5%84ski+&amp;publication_year=2010&amp;title=Supervised+learning+in+spiking+neural+networks+with+resume%3A+sequence+learning,+classification,+and+spike+shifting&amp;journal=Neural+Comput.&amp;volume=22&amp;pages=467-510">Google Scholar</a></p>
<p>Ponulak, F., and Kasinski, A. (2011). Introduction to spiking neural networks: Information processing, learning and applications. <em>Acta Neurobiol. Exp.</em> 71, 409433.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=22237491">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=F.+Ponulak&amp;author=A.+Kasinski+&amp;publication_year=2011&amp;title=Introduction+to+spiking+neural+networks%3A+Information+processing,+learning+and+applications&amp;journal=Acta+Neurobiol.+Exp.&amp;volume=71&amp;pages=409-433">Google Scholar</a></p>
<p>Posch, C., Serrano-Gotarredona, T., Linares-Barranco, B., and Delbruck, T. (2014). Retinomorphic event-based vision sensors: bioinspired cameras with spiking output. <em>Proc. IEEE</em> 102, 14701484. doi: 10.1109/JPROC.2014.2346153</p>
<p><a href="https://doi.org/10.1109/JPROC.2014.2346153">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=C.+Posch&amp;author=T.+Serrano-Gotarredona&amp;author=B.+Linares-Barranco&amp;author=T.+Delbruck+&amp;publication_year=2014&amp;title=Retinomorphic+event-based+vision+sensors%3A+bioinspired+cameras+with+spiking+output&amp;journal=Proc.+IEEE&amp;volume=102&amp;pages=1470-1484">Google Scholar</a></p>
<p>Qiao, N., Mostafa, H., Corradi, F., Osswald, M., Stefanini, F., Sumislawska, D., et al. (2015). A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128k synapses. <em>Front. Neurosci.</em> 9:141. doi: 10.3389/fnins.2015.00141</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=25972778">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2015.00141">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=N.+Qiao&amp;author=H.+Mostafa&amp;author=F.+Corradi&amp;author=M.+Osswald&amp;author=F.+Stefanini&amp;author=D.+Sumislawska+&amp;publication_year=2015&amp;title=A+reconfigurable+on-line+learning+spiking+neuromorphic+processor+comprising+256+neurons+and+128k+synapses&amp;journal=Front.+Neurosci.&amp;volume=9&amp;pages=141">Google Scholar</a></p>
<p>Rao, R. P., and Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. <em>Nat. Neurosci.</em> 2:79.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=10195184">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+P.+Rao&amp;author=D.+H.+Ballard+&amp;publication_year=1999&amp;title=Predictive+coding+in+the+visual+cortex%3A+a+functional+interpretation+of+some+extra-classical+receptive-field+effects&amp;journal=Nat.+Neurosci.&amp;volume=2&amp;pages=79">Google Scholar</a></p>
<p>Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A. (2016). XNOR-Net: ImageNet classification using binary convolutional neural networks, in <em>Computer Vision  ECCV 2016</em>, eds B. Leibe, J. Matas, N. Sebe, and M. Welling (Cham: Springer International Publishing), 525542.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=M.+Rastegari&amp;author=V.+Ordonez&amp;author=J.+Redmon&amp;author=A.+Farhadi+&amp;publication_year=2016&amp;title=%E2%80%9CXNOR-Net%3A+ImageNet+classification+using+binary+convolutional+neural+networks,%E2%80%9D&amp;pages=525-542">Google Scholar</a></p>
<p>Rebecq, H., Gallego, G., Mueggler, E., and Scaramuzza, D. (2017). EMVS: Event-based multi-view stereo  3D reconstruction with an event camera in real-time. <em>Int. J. Comput.Vision</em> 121. doi: 10.1007/s11263-017-1050-6</p>
<p><a href="https://doi.org/10.1007/s11263-017-1050-6">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=H.+Rebecq&amp;author=G.+Gallego&amp;author=E.+Mueggler&amp;author=D.+Scaramuzza+&amp;publication_year=2017&amp;title=EMVS%3A+Event-based+multi-view+stereo+%E2%80%93+3D+reconstruction+with+an+event+camera+in+real-time&amp;journal=Int.+J.+Comput.Vision&amp;pages=1-21">Google Scholar</a></p>
<p>Rieke, F. (1999). <em>Spikes: Exploring the Neural Code</em>. Cambridge: MIT Press.</p>
<p>Riesenhuber, M., and Poggio, T. (1999). Hierarchical models of object recognition in cortex. <em>Nat. Neurosci.</em> 2, 1019.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=10526343">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Riesenhuber&amp;author=T.+Poggio+&amp;publication_year=1999&amp;title=Hierarchical+models+of+object+recognition+in+cortex&amp;journal=Nat.+Neurosci.&amp;volume=2&amp;pages=1019">Google Scholar</a></p>
<p>Rogister, P., Benosman, R., Ieng, S.-H., Lichtsteiner, P., and Delbruck, T. (2012). Asynchronous event-based binocular stereo matching. <em>IEEE Trans. Neural Netw. Learn. Syst.</em> 23, 347353. doi: 10.1109/TNNLS.2011.2180025</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=24808513">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TNNLS.2011.2180025">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=P.+Rogister&amp;author=R.+Benosman&amp;author=S.+-H.+Ieng&amp;author=P.+Lichtsteiner&amp;author=T.+Delbruck+&amp;publication_year=2012&amp;title=Asynchronous+event-based+binocular+stereo+matching&amp;journal=IEEE+Trans.+Neural+Netw.+Learn.+Syst.&amp;volume=23&amp;pages=347-353">Google Scholar</a></p>
<p>Rueckauer, B., Hu, Y., Lungu, I. A., Pfeiffer, M., and Liu, S. C. (2017). Conversion of continuous-valued deep networks to efficient event-driven networks for image classification. <em>Front. Neurosci.</em> 11:682. doi: 10.3389/fnins.2017.00682</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=29375284">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2017.00682">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=B.+Rueckauer&amp;author=Y.+Hu&amp;author=I.+A.+Lungu&amp;author=M.+Pfeiffer&amp;author=S.+C.+Liu+&amp;publication_year=2017&amp;title=Conversion+of+continuous-valued+deep+networks+to+efficient+event-driven+networks+for+image+classification&amp;journal=Front.+Neurosci.&amp;volume=11&amp;pages=682">Google Scholar</a></p>
<p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., et al. (2015). ImageNet large scale visual recognition challenge. <em>Int. J. Comput. Vision</em> 115, 211252. doi: 10.1007/s11263-015-0816-y</p>
<p><a href="https://doi.org/10.1007/s11263-015-0816-y">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=O.+Russakovsky&amp;author=J.+Deng&amp;author=H.+Su&amp;author=J.+Krause&amp;author=S.+Satheesh&amp;author=S.+Ma+&amp;publication_year=2015&amp;title=ImageNet+large+scale+visual+recognition+challenge&amp;journal=Int.+J.+Comput.+Vision&amp;volume=115&amp;pages=211-252">Google Scholar</a></p>
<p>Saghi, S., Mayr, C. G., Serrano-Gotarredona, T., Schmidt, H., Lecerf, G., Tomas, J., et al. (2015). Plasticity in memristive devices for spiking neural networks. <em>Front. Neurosci.</em> 9:51. doi: 10.3389/fnins.2015.00051</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=25784849">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2015.00051">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Sa%C3%AFghi&amp;author=C.+G.+Mayr&amp;author=T.+Serrano-Gotarredona&amp;author=H.+Schmidt&amp;author=G.+Lecerf&amp;author=J.+Tomas+&amp;publication_year=2015&amp;title=Plasticity+in+memristive+devices+for+spiking+neural+networks&amp;journal=Front.+Neurosci.&amp;volume=9&amp;pages=51">Google Scholar</a></p>
<p>Schemmel, J., Brderle, D., Grbl, A., Hock, M., Meier, K., and Millner, S. (2010). A wafer-scale neuromorphic hardware system for large-scale neural modeling, in <em>Proceedings of 2010 IEEE International Symposium on Circuits and Systems</em> (Paris: ISCAS), 19471950.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+Schemmel&amp;author=D.+Br%C3%BCderle&amp;author=A.+Gr%C3%BCbl&amp;author=M.+Hock&amp;author=K.+Meier&amp;author=S.+Millner+&amp;publication_year=2010&amp;title=%E2%80%9CA+wafer-scale+neuromorphic+hardware+system+for+large-scale+neural+modeling,%E2%80%9D&amp;pages=1947-1950">Google Scholar</a></p>
<p>Schemmel, J., Grbl, A., Meier, K., and Mueller, E. (2006). Implementing synaptic plasticity in a VLSI spiking neural network model, in <em>The 2006 IEEE International Joint Conference on Neural Network Proceedings</em> (Vancouver, BC: IJCNN), 16.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+Schemmel&amp;author=A.+Gr%C3%BCbl&amp;author=K.+Meier&amp;author=E.+Mueller+&amp;publication_year=2006&amp;title=%E2%80%9CImplementing+synaptic+plasticity+in+a+VLSI+spiking+neural+network+model,%E2%80%9D&amp;pages=1-6">Google Scholar</a></p>
<p>Schmitt, S., Klhn, J., Bellec, G., Grbl, A., Gttler, M., Hartel, A., et al. (2017). Neuromorphic hardware in the loop: training a deep spiking network on the BrainScaleS wafer-scale system, in <em>2017 International Joint Conference on Neural Networks (IJCNN)</em> (IJCNN), 22272234.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=S.+Schmitt&amp;author=J.+Kl%C3%A4hn&amp;author=G.+Bellec&amp;author=A.+Gr%C3%BCbl&amp;author=M.+G%C3%BCttler&amp;author=A.+Hartel+&amp;publication_year=2017&amp;title=%E2%80%9CNeuromorphic+hardware+in+the+loop%3A+training+a+deep+spiking+network+on+the+BrainScaleS+wafer-scale+system,%E2%80%9D&amp;pages=2227-2234">Google Scholar</a></p>
<p>Schmuker, M., Pfeil, T., and Nawrot, M. P. (2014). A neuromorphic network for generic multivariate data classification. <em>Proc. Natl. Acad. Sci. U.S.A.</em> 111, 20812086. doi: 10.1073/pnas.1303053111</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=24469794">PubMed Abstract</a> | <a href="https://doi.org/10.1073/pnas.1303053111">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Schmuker&amp;author=T.+Pfeil&amp;author=M.+P.+Nawrot+&amp;publication_year=2014&amp;title=A+neuromorphic+network+for+generic+multivariate+data+classification&amp;journal=Proc.+Natl.+Acad.+Sci.+U.S.A.&amp;volume=111&amp;pages=2081-2086">Google Scholar</a></p>
<p>Schrauwen, B., and Van Campenhout, J. (2004). Improving SpikeProp: enhancements to an error-backpropagation rule for spiking neural networks, in <em>Proceedings of the 15th ProRISC Workshop</em>, vol. 11 (Veldhoven).</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=B.+Schrauwen&amp;author=J.+Van+Campenhout+&amp;publication_year=2004&amp;title=%E2%80%9CImproving+SpikeProp%3A+enhancements+to+an+error-backpropagation+rule+for+spiking+neural+networks,%E2%80%9D&amp;journal=Proceedings+of+the+15th+ProRISC+Workshop&amp;volume=11">Google Scholar</a></p>
<p>Sengupta, A., Ye, Y., Wang, R., Liu, C., and Roy, K. (2018). Going deeper in spiking neural networks: VGG and residual architectures. <em>arXiv [Preprint]. arXiv:1802.02627</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Sengupta&amp;author=Y.+Ye&amp;author=R.+Wang&amp;author=C.+Liu&amp;author=K.+Roy+&amp;publication_year=2018&amp;title=Going+deeper+in+spiking+neural+networks%3A+VGG+and+residual+architectures&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1802.02627">Google Scholar</a></p>
<p>Serrano-Gotarredona, R., Oster, M., Lichtsteiner, P., Linares-Barranco, A., Paz-Vicente, R., Gmez-Rodrguez, F., et al. (2009). CAVIAR: A 45k neuron, 5M synapse, 12G connects/s AER hardware sensoryprocessinglearningactuating system for high-speed visual object recognition and tracking. <em>IEEE Trans. Neural Netw.</em> 20, 14171438. doi: 10.1109/TNN.2009.2023653</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=19635693">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TNN.2009.2023653">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+Serrano-Gotarredona&amp;author=M.+Oster&amp;author=P.+Lichtsteiner&amp;author=A.+Linares-Barranco&amp;author=R.+Paz-Vicente&amp;author=F.+G%C3%B3mez-Rodr%C3%ADguez+&amp;publication_year=2009&amp;title=CAVIAR%3A+A+45k+neuron,+5M+synapse,+12G+connects%2Fs+AER+hardware+sensory%E2%80%93processing%E2%80%93learning%E2%80%93actuating+system+for+high-speed+visual+object+recognition+and+tracking&amp;journal=IEEE+Trans.+Neural+Netw.&amp;volume=20&amp;pages=1417-1438">Google Scholar</a></p>
<p>Serrano-Gotarredona, T., and Linares-Barranco, B. (2015). Poker-DVS and MNIST-DVS. Their history, how they were made, and other details. <em>Front. Neurosci.</em> 9:481. doi: 10.3389/fnins.2015.00481</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=26733794">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2015.00481">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=T.+Serrano-Gotarredona&amp;author=B.+Linares-Barranco+&amp;publication_year=2015&amp;title=Poker-DVS+and+MNIST-DVS.+Their+history,+how+they+were+made,+and+other+details&amp;journal=Front.+Neurosci.&amp;volume=9&amp;pages=481">Google Scholar</a></p>
<p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., et al. (2016). Mastering the game of Go with deep neural networks and tree search. <em>Nature</em> 529, 484489. doi: 10.1038/nature16961</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=26819042">PubMed Abstract</a> | <a href="https://doi.org/10.1038/nature16961">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Silver&amp;author=A.+Huang&amp;author=C.+J.+Maddison&amp;author=A.+Guez&amp;author=L.+Sifre&amp;author=G.+Van+Den+Driessche+&amp;publication_year=2016&amp;title=Mastering+the+game+of+Go+with+deep+neural+networks+and+tree+search&amp;journal=Nature&amp;volume=529&amp;pages=484-489">Google Scholar</a></p>
<p>Singh Thakur, C., Molin, J., Cauwenberghs, G., Indiveri, G., Kumar, K., Qiao, N., et al. (2018). Large-scale neuromorphic spiking array processors: A quest to mimic the brain. <em>arXiv [Preprint]. arXiv:1805.08932</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=C.+Singh+Thakur&amp;author=J.+Molin&amp;author=G.+Cauwenberghs&amp;author=G.+Indiveri&amp;author=K.+Kumar&amp;author=N.+Qiao+&amp;publication_year=2018&amp;title=Large-scale+neuromorphic+spiking+array+processors%3A+A+quest+to+mimic+the+brain&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1805.08932">Google Scholar</a></p>
<p>Sironi, A., Brambilla, M., Bourdis, N., Lagorce, X., and Benosman, R. (2018). HATS: Histograms of averaged time surfaces for robust event-based object classification. <em>arXiv [Preprint]. arXiv:1803.07913</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Sironi&amp;author=M.+Brambilla&amp;author=N.+Bourdis&amp;author=X.+Lagorce&amp;author=R.+Benosman+&amp;publication_year=2018&amp;title=HATS%3A+Histograms+of+averaged+time+surfaces+for+robust+event-based+object+classification&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1803.07913">Google Scholar</a></p>
<p>Soudry, D., Hubara, I., and Meir, R. (2014). Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights, in <em>Advances in Neural Information Processing Systems 27</em>, eds Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (Curran Associates, Inc), 963971.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=D.+Soudry&amp;author=I.+Hubara&amp;author=R.+Meir+&amp;publication_year=2014&amp;title=%E2%80%9CExpectation+backpropagation%3A+Parameter-free+training+of+multilayer+neural+networks+with+continuous+or+discrete+weights,%E2%80%9D&amp;pages=963-971">Google Scholar</a></p>
<p>Stefanini, F., Neftci, E. O., Sheik, S., and Indiveri, G. (2014). PyNCS: a microkernel for high-level definition and configuration of neuromorphic electronic systems. <em>Front. Neuroinformatics</em> 8:73. doi: 10.3389/fninf.2014.00073</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=25232314">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fninf.2014.00073">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=F.+Stefanini&amp;author=E.+O.+Neftci&amp;author=S.+Sheik&amp;author=G.+Indiveri+&amp;publication_year=2014&amp;title=PyNCS%3A+a+microkernel+for+high-level+definition+and+configuration+of+neuromorphic+electronic+systems&amp;journal=Front.+Neuroinformatics&amp;volume=8&amp;pages=73">Google Scholar</a></p>
<p>Stemmler, M. (1996). A single spike suffices: the simplest form of stochastic resonance in model neurons. <em>Network Comput. Neural Syst.</em> 7, 687716.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=M.+Stemmler+&amp;publication_year=1996&amp;title=A+single+spike+suffices%3A+the+simplest+form+of+stochastic+resonance+in+model+neurons&amp;journal=Network+Comput.+Neural+Syst.&amp;volume=7&amp;pages=687-716">Google Scholar</a></p>
<p>Stromatias, E., Neil, D., Galluppi, F., Pfeiffer, M., Liu, S. C., and Furber, S. (2015). Scalable energy-efficient, low-latency implementations of trained spiking deep belief networks on SpiNNaker, in <em>2015 International Joint Conference on Neural Networks (IJCNN)</em> (Killarney: IJCNN), 18.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=E.+Stromatias&amp;author=D.+Neil&amp;author=F.+Galluppi&amp;author=M.+Pfeiffer&amp;author=S.+C.+Liu&amp;author=S.+Furber+&amp;publication_year=2015&amp;title=%E2%80%9CScalable+energy-efficient,+low-latency+implementations+of+trained+spiking+deep+belief+networks+on+SpiNNaker,%E2%80%9D&amp;pages=1-8">Google Scholar</a></p>
<p>Stromatias, E., Soto, M., Serrano-Gotarredona, T., and Linares-Barranco, B. (2017). An event-driven classifier for spiking neural networks fed with synthetic or dynamic vision sensor data. <em>Front. Neurosci.</em> 11:350. doi: 10.3389/fnins.2017.00350</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=28701911">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2017.00350">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=E.+Stromatias&amp;author=M.+Soto&amp;author=T.+Serrano-Gotarredona&amp;author=B.+Linares-Barranco+&amp;publication_year=2017&amp;title=An+event-driven+classifier+for+spiking+neural+networks+fed+with+synthetic+or+dynamic+vision+sensor+data&amp;journal=Front.+Neurosci.&amp;volume=11&amp;pages=350">Google Scholar</a></p>
<p>Sze, V., Chen, Y. H., Yang, T. J., and Emer, J. S. (2017). Efficient processing of deep neural networks: A tutorial and survey. <em>Proc. IEEE</em> 105, 22952329. doi: 10.1109/JPROC.2017.2761740</p>
<p><a href="https://doi.org/10.1109/JPROC.2017.2761740">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=V.+Sze&amp;author=Y.+H.+Chen&amp;author=T.+J.+Yang&amp;author=J.+S.+Emer+&amp;publication_year=2017&amp;title=Efficient+processing+of+deep+neural+networks%3A+A+tutorial+and+survey&amp;journal=Proc.+IEEE&amp;volume=105&amp;pages=2295-2329">Google Scholar</a></p>
<p>Tang, W., Hua, G., and Wang, L. (2017). How to train a compact binary neural network with high accuracy?, in <em>AAAI Conference on Artificial Intelligence</em> (San Francisco, CA: AAAI).</p>
<p>Tapson, J. C., Cohen, G. K., Afshar, S., Stiefel, K. M., Buskila, Y., Hamilton, T. J., et al. (2013). Synthesis of neural networks for spatio-temporal spike pattern recognition and processing. <em>Front. Neurosci.</em> 7:153. doi: 10.3389/fnins.2013.00153</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=24009550">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2013.00153">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+C.+Tapson&amp;author=G.+K.+Cohen&amp;author=S.+Afshar&amp;author=K.+M.+Stiefel&amp;author=Y.+Buskila&amp;author=T.+J.+Hamilton+&amp;publication_year=2013&amp;title=Synthesis+of+neural+networks+for+spatio-temporal+spike+pattern+recognition+and+processing&amp;journal=Front.+Neurosci.&amp;volume=7&amp;pages=153">Google Scholar</a></p>
<p>Tavanaei, A., Ghodrati, M., Kheradpisheh, S. R., Masquelier, T., and Maida, A. S. (2018). Deep learning in spiking neural networks. <em>arXiv</em>, abs/1804.08150.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Tavanaei&amp;author=M.+Ghodrati&amp;author=S.+R.+Kheradpisheh&amp;author=T.+Masquelier&amp;author=A.+S.+Maida+&amp;publication_year=2018&amp;title=Deep+learning+in+spiking+neural+networks&amp;journal=arXiv">Google Scholar</a></p>
<p>Teerapittayanon, S., McDanel, B., and Kung, H. T. (2016). BranchyNet: Fast inference via early exiting from deep neural networks, in <em>2016 23rd International Conference on Pattern Recognition (ICPR)</em> (Cancun: ICPR), 24642469.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=S.+Teerapittayanon&amp;author=B.+McDanel&amp;author=H.+T.+Kung+&amp;publication_year=2016&amp;title=%E2%80%9CBranchyNet%3A+Fast+inference+via+early+exiting+from+deep+neural+networks,%E2%80%9D&amp;pages=2464-2469">Google Scholar</a></p>
<p>Thiele, J. C., Bichler, O., and Dupret, A. (2018). Event-based, timescale invariant unsupervised online deep learning with STDP. <em>Front. Comput. Neurosci.</em> 12:46. doi: 10.3389/fncom.2018.00046</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=29962943">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fncom.2018.00046">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+C.+Thiele&amp;author=O.+Bichler&amp;author=A.+Dupret+&amp;publication_year=2018&amp;title=Event-based,+timescale+invariant+unsupervised+online+deep+learning+with+STDP&amp;journal=Front.+Comput.+Neurosci.&amp;volume=12&amp;pages=46">Google Scholar</a></p>
<p>Vidal, A. R., Rebecq, H., Horstschaefer, T., and Scaramuzza, D. (2018). Ultimate SLAM? Combining events, images, and IMU for robust visual SLAM in HDR and high-speed scenarios. <em>IEEE Robot. Autom. Lett.</em> 3, 9941001. doi: 10.1109/LRA.2018.2793357</p>
<p><a href="https://doi.org/10.1109/LRA.2018.2793357">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+R.+Vidal&amp;author=H.+Rebecq&amp;author=T.+Horstschaefer&amp;author=D.+Scaramuzza+&amp;publication_year=2018&amp;title=Ultimate+SLAM%3F+Combining+events,+images,+and+IMU+for+robust+visual+SLAM+in+HDR+and+high-speed+scenarios&amp;journal=IEEE+Robot.+Autom.+Lett.&amp;volume=3&amp;pages=994-1001">Google Scholar</a></p>
<p>Weikersdorfer, D., Adrian, D. B., Cremers, D., and Conradt, J. (2014). Event-based 3D SLAM with a depth-augmented dynamic vision sensor, <em>2014 IEEE International Conference on Robotics and Automation (ICRA)</em> (Hong Kong: ICRA), 359364.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=D.+Weikersdorfer&amp;author=D.+B.+Adrian&amp;author=D.+Cremers&amp;author=J.+Conradt+&amp;publication_year=2014&amp;title=%E2%80%9CEvent-based+3D+SLAM+with+a+depth-augmented+dynamic+vision+sensor,%E2%80%9D&amp;pages=359-364">Google Scholar</a></p>
<p>Wu, X., Wu, Y., and Zhao, Y. (2016). Binarized neural networks on the ImageNet classification task. <em>arXiv [Preprint]. arXiv:1604.03058</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=X.+Wu&amp;author=Y.+Wu&amp;author=Y.+Zhao+&amp;publication_year=2016&amp;title=Binarized+neural+networks+on+the+ImageNet+classification+task&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1604.03058">Google Scholar</a></p>
<p>Wu, Y., Deng, L., Li, G., Zhu, J., and Shi, L. (2017). Spatio-temporal backpropagation for training high-performance spiking neural networks. <em>arXiv [Preprint]. arXiv:1706.02609</em>.</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=29875621">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=Y.+Wu&amp;author=L.+Deng&amp;author=G.+Li&amp;author=J.+Zhu&amp;author=L.+Shi+&amp;publication_year=2017&amp;title=Spatio-temporal+backpropagation+for+training+high-performance+spiking+neural+networks&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1706.02609">Google Scholar</a></p>
<p>Wu, Z., Lin, D., and Tang, X. (2015). Adjustable bounded rectifiers: Towards deep binary representations. <em>arXiv [Preprint]. arXiv:1511.06201</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=Z.+Wu&amp;author=D.+Lin&amp;author=X.+Tang+&amp;publication_year=2015&amp;title=Adjustable+bounded+rectifiers%3A+Towards+deep+binary+representations&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1511.06201">Google Scholar</a></p>
<p>Yu, A. J., Giese, M. A., and Poggio, T. A. (2002). Biophysiologically plausible implementations of the maximum operation. <em>Neural Comput.</em> 14, 28572881. doi: 10.1162/089976602760805313</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=12487795">PubMed Abstract</a> | <a href="https://doi.org/10.1162/089976602760805313">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+J.+Yu&amp;author=M.+A.+Giese&amp;author=T.+A.+Poggio+&amp;publication_year=2002&amp;title=Biophysiologically+plausible+implementations+of+the+maximum+operation&amp;journal=Neural+Comput.&amp;volume=14&amp;pages=2857-2881">Google Scholar</a></p>
<p>Zambrano, D., and Bohte, S. M. (2016). Fast and efficient asynchronous neural computation with adapting spiking neural networks. <em>arXiv [Preprint]. arXiv:1609.02053</em>.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=D.+Zambrano&amp;author=S.+M.+Bohte+&amp;publication_year=2016&amp;title=Fast+and+efficient+asynchronous+neural+computation+with+adapting+spiking+neural+networks&amp;journal=arXiv+%5BPreprint%5D.+arXiv%3A1609.02053">Google Scholar</a></p>
<p>Zhang, Y., Li, P., Jin, Y., and Choe, Y. (2015). A digital liquid state machine with biologically inspired learning and its application to speech recognition. <em>IEEE Trans. Neural Netw. Learn. Syst.</em> 26, 26352649. doi: 10.1109/TNNLS.2015.2388544</p>
<p><a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=25643415">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TNNLS.2015.2388544">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Y.+Zhang&amp;author=P.+Li&amp;author=Y.+Jin&amp;author=Y.+Choe+&amp;publication_year=2015&amp;title=A+digital+liquid+state+machine+with+biologically+inspired+learning+and+its+application+to+speech+recognition&amp;journal=IEEE+Trans.+Neural+Netw.+Learn.+Syst.&amp;volume=26&amp;pages=2635-2649">Google Scholar</a></p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.51198bba.min.js"></script>
      
        <script src="../../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
      
    
  </body>
</html>