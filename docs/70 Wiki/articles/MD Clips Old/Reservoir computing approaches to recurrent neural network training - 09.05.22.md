---
created: 2022-05-09T16:47:04 (UTC +02:00)
tags: []
source: https://www.sciencedirect.com/science/article/pii/S1574013709000173
author: 
---

# Reservoir computing approaches to recurrent neural network training - ScienceDirect



> ## Excerpt
> Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reser…

---
[![Elsevier](https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/0f41bf0a82e6bb06d7c9c594fb6e5a3548742fb2/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/computer-science-review "Go to Computer Science Review on ScienceDirect")

[![Computer Science Review](https://ars.els-cdn.com/content/image/1-s2.0-S1574013709X00043-cov150h.gif)](https://www.sciencedirect.com/journal/computer-science-review/vol/3/issue/3)

## Abstract

Echo State Networks and Liquid State Machines introduced a new paradigm in artificial [recurrent neural network](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about recurrent neural network from ScienceDirect's AI-generated Topic Pages") (RNN) training, where an RNN (the *reservoir*) is generated randomly and only a readout is trained. The paradigm, becoming known as *reservoir computing*, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to *using different methods for training the reservoir and the readout*. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual [classification](https://www.sciencedirect.com/topics/computer-science/classification "Learn more about classification from ScienceDirect's AI-generated Topic Pages") of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.

-   [**Next**](https://www.sciencedirect.com/science/article/pii/S1574013709000136)

## 1\. Introduction

Artificial *[recurrent neural networks](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about recurrent neural networks from ScienceDirect's AI-generated Topic Pages")* (RNNs) represent a large and varied class of computational models that are designed by more or less detailed analogy with biological brain modules. In an RNN numerous abstract *neurons* (also called *units* or *processing elements*) are interconnected by likewise abstracted *synaptic connections* (or *links*), which enable *activations* to propagate through the network. The characteristic feature of RNNs that distinguishes them from the more widely used *[feedforward neural networks](https://www.sciencedirect.com/topics/computer-science/feedforward-neural-network "Learn more about feedforward neural networks from ScienceDirect's AI-generated Topic Pages")* is that the connection topology possesses cycles. The existence of cycles has a profound impact:

•

An RNN may develop a self-sustained temporal activation dynamics along its recurrent connection pathways, even in the absence of input. Mathematically, this renders an RNN to be a *[dynamical system](https://www.sciencedirect.com/topics/computer-science/dynamical-system "Learn more about dynamical system from ScienceDirect's AI-generated Topic Pages")*, while [feedforward networks](https://www.sciencedirect.com/topics/computer-science/feedforward-network "Learn more about feedforward networks from ScienceDirect's AI-generated Topic Pages") are *functions*.

•

If driven by an input signal, an RNN preserves in its internal state a [nonlinear transformation](https://www.sciencedirect.com/topics/computer-science/nonlinear-transformation "Learn more about nonlinear transformation from ScienceDirect's AI-generated Topic Pages") of the input history — in other words, it has a *dynamical memory*, and is able to process temporal context information.

This review article concerns a particular subset of RNN-based research in two aspects:

•

RNNs are used for a variety of scientific purposes, and at least two major classes of RNN models exist: they can be used for purposes of modeling biological brains, or as engineering tools for technical applications. The first usage belongs to the field of computational neuroscience, while the second frames RNNs in the realms of [machine learning](https://www.sciencedirect.com/topics/computer-science/machine-learning "Learn more about machine learning from ScienceDirect's AI-generated Topic Pages"), the theory of computation, and [nonlinear signal](https://www.sciencedirect.com/topics/computer-science/nonlinear-signal "Learn more about nonlinear signal from ScienceDirect's AI-generated Topic Pages") processing and control. While there are interesting connections between the two attitudes, this survey focuses on the latter, with occasional borrowings from the first.

•

From a dynamical systems perspective, there are two main classes of RNNs. Models from the first class are characterized by an energy-minimizing stochastic dynamics and symmetric connections. The best known instantiations are [Hopfield networks](https://www.sciencedirect.com/topics/computer-science/hopfield-network "Learn more about Hopfield networks from ScienceDirect's AI-generated Topic Pages") [\[1\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b1), [\[2\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b2), [Boltzmann machines](https://www.sciencedirect.com/topics/computer-science/boltzmann-machine "Learn more about Boltzmann machines from ScienceDirect's AI-generated Topic Pages") [\[3\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b3), [\[4\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b4), and the recently emerging Deep Belief Networks [\[5\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b5). These networks are mostly trained in some [unsupervised learning](https://www.sciencedirect.com/topics/computer-science/unsupervised-learning "Learn more about unsupervised learning from ScienceDirect's AI-generated Topic Pages") scheme. Typical targeted [network functionalities](https://www.sciencedirect.com/topics/computer-science/network-functionality "Learn more about network functionalities from ScienceDirect's AI-generated Topic Pages") in this field are [associative memories](https://www.sciencedirect.com/topics/computer-science/associative-memory "Learn more about associative memories from ScienceDirect's AI-generated Topic Pages"), [data compression](https://www.sciencedirect.com/topics/computer-science/data-compression "Learn more about data compression from ScienceDirect's AI-generated Topic Pages"), the unsupervised modeling of data distributions, and static pattern [classification](https://www.sciencedirect.com/topics/computer-science/classification "Learn more about classification from ScienceDirect's AI-generated Topic Pages"), where the model is run for multiple time steps per single input instance to reach some type of convergence or equilibrium (but see e.g., [\[6\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b6) for extension to temporal data). The mathematical background is rooted in statistical physics. In contrast, the second big class of RNN models typically features a deterministic update dynamics and directed connections. Systems from this class implement nonlinear filters, which transform an input time series into an output time series. The mathematical background here is nonlinear dynamical systems. The standard training mode is supervised. This survey is concerned only with RNNs of this second type, and when we speak of *RNNs* later on, we will exclusively refer to such systems.[1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fn1)

RNNs (of the second type) appear as highly promising and fascinating tools for nonlinear time series processing applications, mainly for two reasons. First, it can be shown that under fairly mild and general assumptions, such RNNs are universal approximators of dynamical systems [\[7\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b7). Second, biological brain modules almost universally exhibit recurrent connection pathways too. Both observations indicate that RNNs should potentially be powerful tools for engineering applications.

Despite this widely acknowledged potential, and despite a number of successful academic and practical applications, the impact of RNNs in nonlinear modeling has remained limited for a long time. The main reason for this lies in the fact that RNNs are difficult to train by gradient-descent-based methods, which aim at iteratively reducing the training error. While a number of training algorithms have been proposed (a brief overview is given in Section [2.5](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.5)), these all suffer from the following shortcomings:

•

The gradual change of network parameters during learning drives the network dynamics through bifurcations [\[8\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b8). At such points, the gradient information degenerates and may become ill-defined. As a consequence, convergence cannot be guaranteed.

•

A single parameter update can be computationally expensive, and many update cycles may be necessary. This results in long training times, and renders RNN training feasible only for relatively small networks (in the order of tens of units).

•

It is intrinsically hard to learn dependences requiring long-range memory, because the necessary gradient information exponentially dissolves over time [\[9\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b9) (but see the Long Short-Term Memory networks [\[10\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b10) for a possible escape).

•

Advanced training algorithms are mathematically involved and need to be parameterized by a number of global control parameters, which are not easily optimized. As a result, such algorithms need substantial skill and experience to be successfully applied.

In this situation of slow and difficult progress, in 2001 a fundamentally new approach to RNN design and training was proposed independently by Wolfgang Maass under the name of *Liquid State Machines* [\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11) and by Herbert Jaeger under the name of *Echo State Networks* [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12). This approach, which had predecessors in computational neuroscience [\[13\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b13) and subsequent ramifications in machine learning as the *Backpropagation-Decorrelation* [\[14\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b14) learning rule, is now increasingly often collectively referred to as *Reservoir Computing* (RC). The RC paradigm avoids the shortcomings of gradient-descent RNN training listed above, by setting up RNNs in the following way:

•

A recurrent neural network is *randomly* created and remains unchanged during training. This RNN is called the *reservoir*. It is passively excited by the input signal and maintains in its state a nonlinear transformation of the input history.

•

The desired output signal is generated as a [linear combination](https://www.sciencedirect.com/topics/computer-science/linear-combination "Learn more about linear combination from ScienceDirect's AI-generated Topic Pages") of the neuron’s signals from the input-excited reservoir. This linear combination is obtained by linear regression, using the teacher signal as a target.

[Fig. 1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fig1) graphically contrasts previous methods of RNN training with the RC approach.

![](https://ars.els-cdn.com/content/image/1-s2.0-S1574013709000173-gr1.jpg)

1.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S1574013709000173-gr1.jpg "Download full-size image")

Fig. 1. A. Traditional gradient-descent-based [RNN](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about RNN from ScienceDirect's AI-generated Topic Pages") training methods adapt all connection weights (bold arrows), including input-to-RNN, RNN-internal, and RNN-to-output weights. B. In Reservoir Computing, only the RNN-to-output weights are adapted.

Reservoir Computing methods have quickly become popular, as witnessed for instance by a theme issue of [Neural Networks](https://www.sciencedirect.com/topics/computer-science/neural-networks "Learn more about Neural Networks from ScienceDirect's AI-generated Topic Pages") [\[15\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b15), and today constitute one of the basic paradigms of RNN modeling [\[16\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b16). The main reasons for this development are the following:

**Modeling accuracy.** RC has starkly outperformed previous methods of nonlinear system identification, prediction and classification, for instance in predicting chaotic dynamics (three orders of magnitude improved accuracy [\[17\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17)), nonlinear wireless [channel equalization](https://www.sciencedirect.com/topics/computer-science/channel-equalization "Learn more about channel equalization from ScienceDirect's AI-generated Topic Pages") (two orders of magnitude improvement [\[17\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17)), the Japanese Vowel benchmark (zero test error rate, previous best: 1.8% [\[18\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18)), financial forecasting (winner of the international forecasting competition NN3[2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fn2)), and in isolated spoken digits recognition (improvement of word error rate on benchmark from 0.6% of previous best system to 0.2% [\[19\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b19), and further to 0% test error in recent unpublished work).

**Modeling capacity.** RC is computationally universal for continuous-time, continuous-value real-time systems modeled with bounded resources (including time and value resolution) [\[20\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b20), [\[21\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b21).

**Biological plausibility.** Numerous connections of RC principles to architectural and dynamical properties of [mammalian brains](https://www.sciencedirect.com/topics/computer-science/mammalian-brain "Learn more about mammalian brains from ScienceDirect's AI-generated Topic Pages") have been established. RC (or closely related models) provides explanations of why biological brains can carry out accurate computations with an “inaccurate” and noisy physical substrate [\[22\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b22), [\[23\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b23), especially accurate timing [\[24\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b24); of the way in which visual information is superimposed and processed in [primary visual cortex](https://www.sciencedirect.com/topics/computer-science/primary-visual-cortex "Learn more about primary visual cortex from ScienceDirect's AI-generated Topic Pages") [\[25\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b25), [\[26\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b26); of how cortico-basal pathways support the representation of sequential information; and RC offers a functional interpretation of the cerebellar circuitry [\[27\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b27), [\[28\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b28). A central role is assigned to an RC circuit in a series of models explaining sequential information processing in human and primate brains, most importantly of speech signals [\[13\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b13), [\[29\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b29), [\[30\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b30), [\[31\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b31).

**Extensibility and parsimony.** A notorious conundrum of [neural network research](https://www.sciencedirect.com/topics/computer-science/neural-network-research "Learn more about neural network research from ScienceDirect's AI-generated Topic Pages") is how to extend previously learned models by new items without impairing or destroying previously learned representations (*catastrophic interference* [\[32\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b32)). RC offers a simple and principled solution: new items are represented by new output units, which are appended to the previously established output units of a given reservoir. Since the output weights of different output units are independent of each other, catastrophic interference is a non-issue.

These encouraging observations should not mask the fact that RC is still in its infancy, and significant further improvements and extensions are desirable. Specifically, just simply creating a reservoir at random is unsatisfactory. It seems obvious that, when addressing a specific modeling task, a specific reservoir design that is adapted to the task will lead to better results than a naive random creation. Thus, the main stream of research in the field is today directed at understanding the effects of reservoir characteristics on task performance, and at developing suitable reservoir design and adaptation methods. Also, new ways of reading out from the reservoirs, including combining them into larger structures, are devised and investigated. While shifting from the initial idea of having a fixed randomly created reservoir and training only the readout, the current paradigm of reservoir computing remains (and differentiates itself from other RNN training approaches) as producing/training the reservoir and the readout separately and differently.

This review offers a conceptual classification and a comprehensive survey of this research.

As is true for many areas of machine learning, methods in reservoir computing converge from different fields and come with different names. We would like to make a distinction here between these differently named “tradition lines”, which we like to call *brands*, and the actual finer-grained ideas on producing good reservoirs, which we will call *recipes*. Since recipes can be useful and mixed across different brands, this review focuses on classifying and surveying them. To be fair, it has to be said that the authors of this survey associate themselves mostly with the Echo State Networks brand, and thus, willingly or not, are influenced by its mindset.

**Overview.** We start by introducing a generic notational framework in Section [2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2). More specifically, we define what we mean by *problem* or *task* in the context of machine learning in Section [2.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.1). Then we define a general notation for expansion (or kernel) methods for both non-temporal (Section [2.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.2)) and temporal (Section [2.3](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.3)) tasks, introduce our notation for recurrent neural networks in Section [2.4](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.4), and outline classical training methods in Section [2.5](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.5). In Section [3](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3) we detail the foundations of Reservoir Computing and proceed by naming the most prominent brands. In Section [4](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec4) we introduce our classification of the reservoir generation/adaptation recipes, which transcends the boundaries between the brands. Following this classification we then review universal (Section [5](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5)), unsupervised (Section [6](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6)), and supervised (Section [7](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7)) reservoir generation/adaptation recipes. In Section [8](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8) we provide a classification and review the techniques for reading the outputs from the reservoirs reported in literature, together with discussing various practical issues of readout training. A final discussion (Section [9](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec9)) wraps up the entire picture.

## 2\. Formalism

### 2.1. Formulation of the problem

Let a *problem* or a *task* in our context of [machine learning](https://www.sciencedirect.com/topics/computer-science/machine-learning "Learn more about machine learning from ScienceDirect's AI-generated Topic Pages") be defined as a problem of learning a functional relation between a given input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">u</mtext></mrow></msub></mrow></msup></math>$ and a desired output $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub></mrow></msup></math>$, where $<math><mi is="true">n</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">T</mi></math>$, and $<math><mi is="true">T</mi></math>$ is the number of data points in the training *dataset* $<math><mrow is="true"><mo is="true">{</mo><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">}</mo></mrow></math>$. A *non-temporal* task is where the data points are independent of each other and the goal is to learn a function $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mstyle mathvariant="italic" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></math>$ such that $<math><mi is="true">E</mi><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mo is="true">)</mo></mrow></math>$ is minimized, where $<math><mi is="true">E</mi></math>$ is an error measure, for instance, the normalized root-mean-square error (NRMSE) (1)$<math><mi is="true">E</mi><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><msqrt is="true"><mrow is="true"><mfrac is="true"><mrow is="true"><mrow is="true"><mo is="true">〈</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">−</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">‖</mo></mrow></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">〉</mo></mrow></mrow><mrow is="true"><mrow is="true"><mo is="true">〈</mo><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">‖</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">−</mo><mrow is="true"><mo is="true">〈</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">〉</mo></mrow><mo is="true">‖</mo></mrow></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">〉</mo></mrow></mrow></mfrac></mrow></msqrt><mtext is="true">,</mtext></math>$ where $<math><mrow is="true"><mo is="true">‖</mo><mo is="true">⋅</mo><mo is="true">‖</mo></mrow></math>$ stands for the [Euclidean distance](https://www.sciencedirect.com/topics/computer-science/euclidean-distance "Learn more about Euclidean distance from ScienceDirect's AI-generated Topic Pages") (or norm).

A *temporal* task is where $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle></math>$ and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub></math>$ are signals in a discrete time domain $<math><mi is="true">n</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">T</mi></math>$, and the goal is to learn a function $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mstyle mathvariant="italic" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mo is="true">…</mo><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></math>$ such that $<math><mi is="true">E</mi><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mo is="true">)</mo></mrow></math>$ is minimized. Thus the difference between the temporal and non-temporal task is that the function $<math><mstyle mathvariant="italic" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ we are trying to learn has memory in the first case and is memoryless in the second. In both cases the underlying assumption is, of course, that the functional dependence we are trying to learn actually exists in the data. For the temporal case this spells out as data adhering to an additive noise model of the form $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mstyle mathvariant="italic" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mo is="true">…</mo><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">+</mo><mstyle mathvariant="bold" is="true"><mi is="true">θ</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, where $<math><msub is="true"><mrow is="true"><mstyle mathvariant="italic" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ is the relation to be learned by $<math><mstyle mathvariant="italic" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ and $<math><mstyle mathvariant="bold" is="true"><mi is="true">θ</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub></mrow></msup></math>$ is a noise term, limiting the learning precision, i.e., the precision of matching the learned $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ to $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$.

Whenever we say that the task or the problem is learned *well*, or with good *accuracy* or *precision*, we mean that $<math><mi is="true">E</mi><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mo is="true">)</mo></mrow></math>$ is small. Normally one part of the $<math><mi is="true">T</mi></math>$ data points is used for training the model and another part (unseen during the training) for testing it. When speaking about output errors and *performance* or *precision* we will have *testing* errors in mind (if not explicitly specified otherwise). Also $<math><mi is="true">n</mi></math>$, denoting the discrete time, will often be used omitting its range $<math><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">T</mi></math>$.

### 2.2. Expansions and kernels in non-temporal tasks

Many tasks cannot be accurately solved by a simple [linear relation](https://www.sciencedirect.com/topics/computer-science/linear-relation "Learn more about linear relation from ScienceDirect's AI-generated Topic Pages") between the $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle></math>$ and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub></math>$, i.e., a linear model $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ (where $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">u</mtext></mrow></msub></mrow></msup></math>$) gives big errors $<math><mi is="true">E</mi><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mo is="true">)</mo></mrow></math>$ regardless of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$. In such situations one has to resort to *nonlinear* models. A number of generic and widely used approaches to nonlinear modeling are based on the idea of nonlinearly expanding the input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ into a high-dimensional feature vector $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></mrow></msup></math>$, and then utilizing those features using linear methods, for instance by linear regression or computing for a linear separation [hyperplane](https://www.sciencedirect.com/topics/computer-science/hyperplanes "Learn more about hyperplane from ScienceDirect's AI-generated Topic Pages"), to get a reasonable $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></math>$. Solutions of this kind can be expressed in the form (2)$<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mstyle mathvariant="italic" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mtext is="true">,</mtext></math>$ where $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></mrow></msup></math>$ are the trained output weights. Typically $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub><mo is="true">≫</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">u</mtext></mrow></msub></math>$, and we will often consider $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ as included in $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$. There is also typically a constant *bias* value added to [(2)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2), which is omitted here and in other equations for brevity. The bias can be easily implemented, having one of the features in $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ constant (e.g., $<math><mo is="true">=</mo><mn is="true">1</mn></math>$) and a corresponding column in $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$. Some models extend [(2)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2) to (3)$<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mstyle mathvariant="italic" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">[</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">]</mo></mrow><mo is="true">)</mo></mrow><mtext is="true">,</mtext></math>$ where $<math><msub is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ is some [nonlinear function](https://www.sciencedirect.com/topics/computer-science/nonlinear-function "Learn more about nonlinear function from ScienceDirect's AI-generated Topic Pages") (e.g., a sigmoid applied element-wise). For the sake of simplicity we will consider this definition as equivalent to [(2)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2), since $<math><msub is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ can be eliminated from $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></math>$ by redefining the target as $<math><msubsup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow><mrow is="true"><mo is="true">′</mo></mrow></msubsup><mo is="true">=</mo><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mo is="true">)</mo></mrow></math>$ (and the error function $<math><mi is="true">E</mi><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow><mrow is="true"><mo is="true">′</mo></mrow></msubsup><mo is="true">)</mo></mrow></math>$, if desired). Note that [(2)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2) is a special case of [(3)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3), with $<math><msub is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ being the identity.

Functions $<math><mstyle mathvariant="italic" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></math>$ that transform an input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ into a (higher-dimensional) vector $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ are often called *kernels* (and traditionally denoted $<math><mi is="true">ϕ</mi><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></math>$) in this context. Methods using kernels often employ the *kernel trick*, which refers to the option afforded by many kernels of computing inner products in the (high-dimensional, hence expensive) feature space of $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></math>$ more cheaply in the original space populated by $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle></math>$. The term *[kernel function](https://www.sciencedirect.com/topics/computer-science/kernel-function "Learn more about kernel function from ScienceDirect's AI-generated Topic Pages")* has acquired a close association with the kernel trick. Since here we will not exploit the kernel trick, in order to avoid confusion we will use the more neutral term of an *expansion* function for $<math><mstyle mathvariant="italic" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></math>$, and refer to methods using such functions as *expansion methods*. These methods then include *[Support Vector Machines](https://www.sciencedirect.com/topics/computer-science/support-vector-machine "Learn more about Support Vector Machines from ScienceDirect's AI-generated Topic Pages")* (which standardly do use the kernel trick), *[Feedforward Neural Networks](https://www.sciencedirect.com/topics/computer-science/feedforward-neural-network "Learn more about Feedforward Neural Networks from ScienceDirect's AI-generated Topic Pages")*, *[Radial Basis Function](https://www.sciencedirect.com/topics/computer-science/radial-basis-function "Learn more about Radial Basis Function from ScienceDirect's AI-generated Topic Pages")* approximators, *Slow Feature Analysis*, and various *Probability Mixture* models, among many others. Feedforward neural networks are also often referred to as *(multilayer) perceptrons* in the literature.

While training the output $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ is a well defined and understood problem, producing a good expansion function $<math><mstyle mathvariant="italic" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ generally involves more creativity. In many expansion methods, e.g., Support Vector Machines, the function is chosen “by hand” (most often through trial-and-error) and is fixed.

### 2.3. Expansions in temporal tasks

Many temporal methods are based on the same principle. The difference is that in a temporal task the function to be learned depends also on the history of the input, as discussed in Section [2.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.1). Thus, the expansion function has memory: $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mstyle mathvariant="italic" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mo is="true">…</mo><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></math>$, i.e., it is an expansion of the current input and its (potentially infinite) history. Since this function has an unbounded number of parameters, practical implementations often take an alternative, recursive, definition: (4)$<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mstyle mathvariant="italic" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mtext is="true">.</mtext></math>$

The output $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is typically produced in the same way as for non-temporal methods by [(2)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2) or [(3)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3).

In addition to the nonlinear expansion, as in the non-temporal tasks, such $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ could be seen as a type of a spatial embedding of the temporal information of $<math><mo is="true">…</mo><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$. This, for example, enables capturing higher-dimensional dynamical attractors $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mstyle mathvariant="italic" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mo is="true">…</mo><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">=</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></math>$ of the system being modeled by $<math><mstyle mathvariant="italic" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ from a series of lower-dimensional observations $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ the system is emitting, which is shown to be possible by *Takens’s theorem* [\[33\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b33).

### 2.4. Recurrent neural networks

The type of recurrent neural networks that we will consider most of the time in this review is a straightforward implementation of [(4)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd4). The nonlinear expansion with memory here leads to a *state vector* of the form (5)$<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mi is="true">f</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mtext is="true">,</mtext><mspace width="1em" class="quad" is="true"></mspace><mi is="true">n</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">T</mi><mtext is="true">,</mtext></math>$ where $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></mrow></msup></math>$ is a vector of reservoir neuron activations at a time step $<math><mi is="true">n</mi></math>$, $<math><mi is="true">f</mi><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ is the neuron [activation function](https://www.sciencedirect.com/topics/computer-science/activation-function "Learn more about activation function from ScienceDirect's AI-generated Topic Pages"), usually the symmetric $<math><mo is="true">tanh</mo><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$, or the positive logistic (or Fermi) sigmoid, applied element-wise, $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">u</mtext></mrow></msub></mrow></msup></math>$ is the input weight matrix, and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></mrow></msup></math>$ is a weight matrix of internal network connections. The network is usually started with the initial state $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mn is="true">0</mn><mo is="true">)</mo></mrow><mo is="true">=</mo><mstyle mathvariant="bold" is="true"><mi is="true">0</mi></mstyle></math>$. Bias values are again omitted in [(5)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5) in the same way as in [(2)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2). The readout $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ of the network is implemented as in [(3)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3).

Some models of RNNs extend [(5)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5) as (6)$<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mi is="true">f</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mtext is="true">,</mtext><mspace width="1em" class="quad" is="true"></mspace><mi is="true">n</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">T</mi><mtext is="true">,</mtext></math>$ where $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub></mrow></msup></math>$ is an optional output feedback weight matrix.

### 2.5. Classical training of RNNs

The classical approach to supervised training of RNNs, known as *[gradient descent](https://www.sciencedirect.com/topics/computer-science/gradient-descent "Learn more about gradient descent from ScienceDirect's AI-generated Topic Pages")*, is by iteratively adapting all weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$, $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$, $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$, and possibly $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub></math>$ (which as a whole we denote $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">all</mtext></mrow></msub></math>$ for brevity) according to their estimated gradients $<math><mi is="true">∂</mi><mi is="true">E</mi><mo is="true">/</mo><mi is="true">∂</mi><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">all</mtext></mrow></msub></math>$, in order to minimize the output error $<math><mi is="true">E</mi><mo is="true">=</mo><mi is="true">E</mi><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mo is="true">)</mo></mrow></math>$. A classical example of such methods is Real-Time Recurrent Learning [\[34\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b34), where the estimation of $<math><mi is="true">∂</mi><mi is="true">E</mi><mo is="true">/</mo><mi is="true">∂</mi><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">all</mtext></mrow></msub></math>$ is done recurrently, forward in time. Conversely, error [backpropagation](https://www.sciencedirect.com/topics/computer-science/backpropagation "Learn more about backpropagation from ScienceDirect's AI-generated Topic Pages") (BP) methods for training RNNs, which are derived as extensions of the BP method for feedforward neural networks [\[35\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b35), estimate $<math><mi is="true">∂</mi><mi is="true">E</mi><mo is="true">/</mo><mi is="true">∂</mi><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">all</mtext></mrow></msub></math>$ by propagating $<math><mi is="true">E</mi><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mo is="true">)</mo></mrow></math>$ backwards through network connections and time. The BP group of methods is arguably the most prominent in classical RNN training, with the classical example in this group being Backpropagation Through Time [\[36\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b36). It has a runtime complexity of $<math><mi is="true">O</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">)</mo></mrow></math>$ per weight update per time step for a single output $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></math>$, compared to $<math><mi is="true">O</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></mrow><mrow is="true"><mn is="true">4</mn></mrow></msup><mo is="true">)</mo></mrow></math>$ for Real-Time Recurrent Learning.

A systematic unifying overview of many classical gradient descent RNN training methods is presented in [\[37\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b37). The same contribution also proposes a new approach, often referred to by others as Atiya–Parlos Recurrent Learning (APRL). It estimates gradients with respect to neuron activations $<math><mi is="true">∂</mi><mi is="true">E</mi><mo is="true">/</mo><mi is="true">∂</mi><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></math>$ (instead of weights directly) and gradually adapts the weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">all</mtext></mrow></msub></math>$ to move the activations $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></math>$ into the desired directions. The method is shown to converge faster than previous ones. See Section [3.4](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.4) for more implications of APRL and bridging the gap between the classical gradient descent and the reservoir computing methods.

There are also other versions of supervised RNN training, formulating the training problem differently, such as using [Extended Kalman Filters](https://www.sciencedirect.com/topics/computer-science/extended-kalman-filter "Learn more about Extended Kalman Filters from ScienceDirect's AI-generated Topic Pages") [\[38\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b38) or the Expectation-Maximization algorithm [\[39\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b39), as well as dealing with special types of RNNs, such as Long Short-Term Memory [\[40\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b40) modular networks capable of learning long-term dependences.

There are many more, arguably less prominent, methods and their modifications for RNN training that are not mentioned here, as this would lead us beyond the scope of this review. The very fact of their multiplicity suggests that there is no clear winner in all aspects. Despite many advances that the methods cited above have introduced, they still have multiple common shortcomings, as pointed out in Section [1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec1).

## 3\. Reservoir methods

Reservoir computing methods differ from the “traditional” designs and learning techniques listed above in that they make a conceptual and computational separation between a dynamic *reservoir* — an [RNN](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about RNN from ScienceDirect's AI-generated Topic Pages") as a nonlinear temporal expansion function — and a recurrence-free (usually linear) *readout* that produces the desired output from the expansion.

This separation is based on the understanding (common with kernel methods) that $<math><mstyle mathvariant="italic" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ and $<math><mstyle mathvariant="italic" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ serve different purposes: $<math><mstyle mathvariant="italic" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ expands the input history $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mo is="true">…</mo></math>$ into a rich enough reservoir state space $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></mrow></msup></math>$, while $<math><mstyle mathvariant="italic" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ combines the neuron signals $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ into the desired output signal $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$. In the linear readout case [(2)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2), for each dimension $<math><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math>$ of $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></math>$ an output weight vector $<math><msub is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mo is="true">)</mo></mrow></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math>$ in the same space $<math><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></mrow></msup></math>$ is found such that (7)$<math><msub is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mo is="true">)</mo></mrow></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">≈</mo><msub is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mtext is="true">,</mtext></math>$ while the “purpose” of $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is to contain a rich enough representation to make this possible.

Since the expansion and the readout serve different purposes, training/generating them separately and even with different goal functions makes sense. The readout $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mstyle mathvariant="italic" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></math>$ is essentially a non-temporal function, learning which is relatively simple. On the other hand, setting up the reservoir such that a “good” state expansion $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ emerges is an ill-understood challenge in many respects. The “traditional” RNN training methods do not make the conceptual separation of a reservoir vs. a readout, and train both reservoir-internal and output weights in technically the same fashion. Nonetheless, even in traditional methods the ways of defining the [error gradients](https://www.sciencedirect.com/topics/computer-science/error-gradient "Learn more about error gradients from ScienceDirect's AI-generated Topic Pages") for the output $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ and the internal units $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ are inevitably different, reflecting that an explicit target $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is available only for the output units. Analyses of traditional training algorithms have furthermore revealed that the learning dynamics of internal vs. output weights exhibit systematic and striking differences. This theme will be expanded in Section [3.4](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.4).

Currently, reservoir computing is a vivid fresh RNN research stream, which has recently gained wide attention due to the reasons pointed out in Section [1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec1).

We proceed to review the most prominent “named” reservoir methods, which we call here *brands*. Each of them has its own history, a specific mindset, specific types of reservoirs, and specific insights.

### 3.1. Echo State Networks

*Echo State Networks* (ESNs) [\[16\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b16) represent one of the two pioneering reservoir computing methods. The approach is based on the observation that if a random RNN possesses certain algebraic properties, training only a linear readout from it is often sufficient to achieve excellent performance in practical applications. The untrained RNN part of an ESN is called a *dynamical reservoir*, and the resulting states $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ are termed *echoes* of its input history [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12)—this is where reservoir computing draws its name from.

ESNs standardly use simple sigmoid neurons, i.e., reservoir states are computed by [(5)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5) or [(6)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd6), where the [nonlinear function](https://www.sciencedirect.com/topics/computer-science/nonlinear-function "Learn more about nonlinear function from ScienceDirect's AI-generated Topic Pages") $<math><mi is="true">f</mi><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ is a sigmoid, usually the $<math><mo is="true">tanh</mo><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ function. Leaky integrator neuron models represent another frequent option for ESNs, which is discussed in depth in Section [5.5](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.5). Classical recipes of producing the ESN reservoir (which is in essence $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$) are outlined in Section [5.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.1), together with input-independent properties of the reservoir. Input-dependent measures of the quality of the activations $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ in the reservoir are presented in Section [6.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1).

The readout from the reservoir is usually linear [(3)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3), where $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is included as part of $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, which can also be spelled out in [(3)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3) explicitly as (8)$<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mrow is="true"><mo is="true">[</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">|</mo><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">]</mo></mrow><mo is="true">)</mo></mrow><mtext is="true">,</mtext></math>$ where $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub><mo is="true">×</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">u</mtext></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub><mo is="true">)</mo></mrow></mrow></msup></math>$ is the learned output weight matrix, $<math><msub is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ is the output neuron [activation function](https://www.sciencedirect.com/topics/computer-science/activation-function "Learn more about activation function from ScienceDirect's AI-generated Topic Pages") (usually the identity) applied component-wise, and $<math><mo is="true">⋅</mo><mo is="true">|</mo><mo is="true">⋅</mo></math>$ stands for a vertical concatenation of vectors. The original and most popular batch training method to compute $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ is linear regression, discussed in Section [8.1.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.1.1), or a computationally cheap online training discussed in Section [8.1.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.1.2).

The initial ESN publications [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12), [\[41\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b41), [\[42\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b42), [\[43\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b43), [\[17\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17) were framed in settings of [machine learning](https://www.sciencedirect.com/topics/computer-science/machine-learning "Learn more about machine learning from ScienceDirect's AI-generated Topic Pages") and [nonlinear signal](https://www.sciencedirect.com/topics/computer-science/nonlinear-signal "Learn more about nonlinear signal from ScienceDirect's AI-generated Topic Pages") processing applications. The original theoretical contributions of early ESN research concerned algebraic properties of the reservoir that make this approach work in the first place (the *echo state property* [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12) discussed in Section [5.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.1)) and analytical results characterizing the dynamical short-term memory capacity of reservoirs [\[41\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b41) discussed in Section [6.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1).

### 3.2. Liquid State Machines

*Liquid State Machines* (LSMs) [\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11) are the other pioneering reservoir method, developed independently from and simultaneously with ESNs. LSMs were developed from a computational neuroscience background, aiming at elucidating the principal computational properties of neural microcircuits [\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11), [\[20\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b20), [\[44\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b44), [\[45\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b45). Thus LSMs use more sophisticated and biologically realistic models of spiking integrate-and-fire neurons and dynamic synaptic connection models in the reservoir. The connectivity among the neurons often follows topological and metric constraints that are biologically motivated. In the LSM literature, the reservoir is often referred to as the *liquid*, following an intuitive metaphor of the excited states as ripples on the surface of a pool of water. Inputs to LSMs also usually consist of spike trains. In their readouts LSMs originally used multilayer feedforward [neural networks](https://www.sciencedirect.com/topics/computer-science/neural-networks "Learn more about neural networks from ScienceDirect's AI-generated Topic Pages") (of either spiking or sigmoid neurons), or linear readouts similar to ESNs [\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11). Additional mechanisms for averaging spike trains to get real-valued outputs are often employed.

RNNs of the LSM-type with spiking neurons and more sophisticated synaptic models are usually more difficult to implement, to correctly set up and tune, and typically more expensive to emulate on digital computers[3](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fn3) than simple ESN-type “weighted sum and nonlinearity” RNNs. Thus they are less widespread for engineering applications of RNNs than the latter. However, while the ESN-type neurons only emulate mean firing rates of biological neurons, spiking neurons are able to perform more complicated information processing, due to the time coding of the information in their signals (i.e., the exact timing of each firing also matters). Also findings on various mechanisms in natural neural circuits are more easily transferable to these more biologically-realistic models (there is more on this in Section [6.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2)).

The main theoretical contributions of the LSM brand to Reservoir Computing consist in analytical characterizations of the computational power of such systems [\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11), [\[21\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b21) discussed in Sections [6.1 “Goodness” measures of the reservoir activations](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1), [7.4 Trained auxiliary feedbacks](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7.4).

### 3.3. Evolino

*Evolino* [\[46\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b46) transfers the idea of ESNs from an RNN of simple sigmoidal units to a Long Short-Term Memory type of RNNs [\[40\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b40) constructed from units capable of preserving memory for long periods of time. In Evolino the weights of the reservoir are trained using evolutionary methods, as is also done in some extensions of ESNs, both discussed in Section [7.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7.2).

### 3.4. Backpropagation-Decorrelation

The idea of separation between a reservoir and a readout function has also been arrived at from the point of view of optimizing the performance of the RNN training algorithms that use error backpropagation, as already indicated in Section [2.5](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.5). In an analysis of the weight dynamics of an RNN trained using the APRL learning algorithm [\[47\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b47), it was revealed that the output weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$ of the network being trained change quickly, while the hidden weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ change slowly and in the case of a single output $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></math>$ the changes are column-wise coupled. Thus in effect APRL decouples the RNN into a quickly adapting output and a slowly adapting reservoir. Inspired by these findings a new iterative/online RNN training method, called *BackPropagation-DeCorrelation* (BPDC), was introduced [\[14\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b14). It approximates and significantly simplifies the APRL method, and applies it only to the output weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$, turning it into an online RC method. BPDC uses the reservoir update equation defined in [(6)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd6), where output feedbacks $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub></math>$ are essential, with the same type of units as ESNs. BPDC learning is claimed to be insensitive to the parameters of fixed reservoir weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$. BPDC boasts fast learning times and thus is capable of tracking quickly changing signals. As a downside of this feature, the trained network quickly forgets the previously seen data and is highly biased by the recent data. Some remedies for reducing this effect are reported in [\[48\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b48). Most of applications of BPDC in the literature are for tasks having one-dimensional outputs $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></math>$; however BPDC is also successfully applied to $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub><mo is="true">&gt;</mo><mn is="true">1</mn></math>$, as recently demonstrated in [\[49\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b49).

From a conceptual perspective we can define a range of RNN training methods that gradually bridge the gap between the classical BP and reservoir methods:

1.

Classical BP methods, such as [Backpropagation](https://www.sciencedirect.com/topics/computer-science/backpropagation "Learn more about Backpropagation from ScienceDirect's AI-generated Topic Pages") Through Time (BPTT) [\[36\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b36);

2.

Atiya–Parlos recurrent learning (APRL) [\[37\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b37);

3.

BackPropagation-DeCorrelation (BPDC) [\[14\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b14);

4.

Echo State Networks (ESNs) [\[16\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b16).

In each method of this list the focus of training gradually moves from the entire network towards the output, and convergence of the training is faster in terms of iterations, with only a single “iteration” in case 4. At the same time the potential expressiveness of the RNN, as per the same number of units in the NN, becomes weaker. All methods in the list primarily use the same type of simple sigmoid neuron model.

### 3.5. Temporal Recurrent Networks

This summary of RC brands would be incomplete without a spotlight directed at Peter F. Dominey’s decade-long research suite on cortico-striatal circuits in the human brain (e.g., [\[13\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b13), [\[29\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b29), [\[31\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b31), and many more). Although this research is rooted in empirical cognitive neuroscience and functional neuroanatomy and aims at elucidating complex neural structures rather than theoretical computational principles, it is probably Dominey who first clearly spelled out the RC principle: *“(…) there is no learning in the recurrent connections* \[within a [subnetwork](https://www.sciencedirect.com/topics/computer-science/subnetwork "Learn more about subnetwork from ScienceDirect's AI-generated Topic Pages") corresponding to a reservoir\]*, only between the State* \[i.e., reservoir\] *units and the Output units. Second, adaptation is based on a simple* *[associative learning](https://www.sciencedirect.com/topics/computer-science/associative-learning "Learn more about associative learning from ScienceDirect's AI-generated Topic Pages")* *mechanism (…)”* [\[50\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b50). It is also in this article where Dominey brands the neural reservoir module as a *Temporal Recurrent Network*. The learning algorithm, to which Dominey alludes, can be seen as a version of the Least Mean Squares discussed in Section [8.1.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.1.2). At other places, Dominey emphasizes the randomness of the connectivity in the reservoir: *“It is worth noting that the simulated recurrent prefrontal network relies on fixed randomized recurrent connections, (…)”* [\[51\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b51). Only in early 2008 did Dominey and “computational” RC researchers become aware of each other.

### 3.6. Other (exotic) types of reservoirs

As is clear from the discussion of the different reservoir methods so far, a variety of neuron models can be used for the reservoirs. Using different activation functions inside a single reservoir might also improve the richness of the echo states, as is illustrated, for example, by inserting some neurons with wavelet-shaped activation functions into the reservoir of ESNs [\[52\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b52). A hardware implementation friendly version of reservoirs composed of stochastic bitstream neurons was proposed in [\[53\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b53).

In fact the reservoirs do not necessarily need to be neural networks, governed by dynamics similar to [(5)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5). Other types of high-dimensional [dynamical systems](https://www.sciencedirect.com/topics/computer-science/dynamical-system "Learn more about dynamical systems from ScienceDirect's AI-generated Topic Pages") that can take an input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ and have an observable state $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ (which does not necessarily fully describe the state of the system) can be used as well. In particular this makes the reservoir paradigm suitable for harnessing the computational power of unconventional hardware, such as analog electronics [\[54\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b54), [\[55\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b55), biological neural tissue [\[26\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b26), optical [\[56\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b56), quantum, or physical “computers”. The last of these was demonstrated (taking the “reservoir” and “liquid” idea quite literally) by feeding the input via mechanical actuators into a reservoir full of water, recording the state of its surface optically, and successfully training a readout [multilayer perceptron](https://www.sciencedirect.com/topics/computer-science/multilayer-perceptron "Learn more about multilayer perceptron from ScienceDirect's AI-generated Topic Pages") on several [classification tasks](https://www.sciencedirect.com/topics/computer-science/classification-task "Learn more about classification tasks from ScienceDirect's AI-generated Topic Pages") [\[57\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b57). An idea of treating a computer-simulated gene regulation network of *Escherichia Coli* bacteria as the reservoir, a sequence of chemical stimuli as an input, and measures of protein levels and mRNAs as an output is explored in [\[58\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b58).

### 3.7. Other overviews of reservoir methods

An experimental comparison of LSM, ESN, and BPDC reservoir methods with different neuron models, even beyond the standard ones used for the respective methods, and different parameter settings is presented in [\[59\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b59). A brief and broad overview of reservoir computing is presented in [\[60\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b60), with an emphasis on applications and hardware implementations of reservoir methods. The editorial in the “Neural Networks” journal special issue on ESNs and LSMs [\[15\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b15) offers a short introduction to the topic and an overview of the articles in the issue (most of which are also surveyed here). An older and much shorter part of this overview, covering only reservoir adaptation techniques, is available as a technical report [\[61\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b61).

## 4\. Our classification of reservoir recipes

The successes of applying RC methods to benchmarks (see the listing in Section [1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec1)) outperforming classical fully trained [RNNs](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about RNNs from ScienceDirect's AI-generated Topic Pages") do not imply that randomly generated reservoirs are optimal and cannot be improved. In fact, “random” is almost by definition an antonym to “optimal”. The results rather indicate the need for some novel methods of training/generating the reservoirs that are very probably not a direct extension of the way the output is trained (as in BP). Thus besides application studies (which are not surveyed here), the bulk of current RC research on reservoir methods is devoted to optimal reservoir design, or reservoir optimization algorithms.

It is worth mentioning at this point that the general “no free lunch” principle in supervised [machine learning](https://www.sciencedirect.com/topics/computer-science/machine-learning "Learn more about machine learning from ScienceDirect's AI-generated Topic Pages") [\[62\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b62) states that there can exist no bias of a model which would universally improve the accuracy of the model for *all* possible problems. In our context this can be translated into a claim that no single type of reservoir can be optimal for all types of problems.

In this review we will try to survey all currently investigated ideas that help producing “good” reservoirs. We will classify those ideas into three major groups based on their universality:

•

*Generic* guidelines/methods of producing good reservoirs irrespective of the task (both the input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ and the desired output $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$);

•

*Unsupervised* pre-training of the reservoir with respect to the given input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, but not the target $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$;

•

*Supervised* pre-training of the reservoir with respect to both the given input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ and the desired output $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$.

These three classes of methods are discussed in the following three sections. Note that many of the methods to some extend transcend the boundaries of these three classes, but will be classified according to their main principle.

## 5\. Generic reservoir recipes

The most classical methods of producing reservoirs all fall into this category. All of them generate reservoirs randomly, with topology and weight characteristics depending on some preset parameters. Even though they are not optimized for a particular input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ or target $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, a good manual selection of the parameters is to some extent task-dependent, complying with the “no free lunch” principle just mentioned.

### 5.1. Classical ESN approach

Some of the most generic guidelines of producing good reservoirs were presented in the papers that introduced ESNs [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12), [\[42\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b42). Motivated by an intuitive goal of producing a “rich” set of dynamics, the recipe is to generate a **(i)***big*, **(ii)***sparsely* and **(iii)***randomly* connected, reservoir. This means that (i) $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></math>$ is sufficiently large, with order ranging from tens to thousands, (ii) the weight matrix $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ is sparse, with several to 20 per cent of possible connections, and (iii) the weights of the connections are usually generated randomly from a uniform distribution symmetric around the zero value. This design rationale aims at obtaining *many*, due to (i), reservoir activation signals, which are only *loosely coupled*, due to (ii), and *different*, due to (iii).

The input weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$ and the optional output feedback weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub></math>$ are usually dense (they can also be sparse like $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$) and generated randomly from a uniform distribution. The exact scaling of both matrices and an optional shift of the input (a constant value added to $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$) are the few other free parameters that one has to choose when “baking” an ESN. The rules of thumb for them are the following. The scaling of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$ and shifting of the input depends on how much [nonlinearity](https://www.sciencedirect.com/topics/computer-science/nonlinearities "Learn more about nonlinearity from ScienceDirect's AI-generated Topic Pages") of the processing unit the task needs: if the inputs are close to 0, the $<math><mo is="true">tanh</mo></math>$ neurons tend to operate with activations close to 0, where they are essentially linear, while inputs far from 0 tend to drive them more towards saturation where they exhibit more nonlinearity. The shift of the input may help to overcome undesired consequences of the symmetry around 0 of the $<math><mo is="true">tanh</mo></math>$ neurons with respect to the sign of the signals. Similar effects are produced by scaling the bias inputs to the neurons (i.e., the column of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$ corresponding to constant input, which often has a different scaling factor than the rest of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$). The scaling of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub></math>$ is in practice limited by a threshold at which the ESN starts to exhibit an unstable behavior, i.e., the output feedback loop starts to amplify (the errors of) the output and thus enters a diverging generative mode. In [\[42\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b42), these and related pieces of advice are given without a formal justification.

An important element for ESNs to work is that the reservoir should have the *echo state property* [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12). This condition in essence states that the effect of a previous state $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ and a previous input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ on a future state $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">+</mo><mi is="true">k</mi><mo is="true">)</mo></mrow></math>$ should vanish gradually as time passes (i.e., $<math><mi is="true">k</mi><mo is="true">→</mo><mi is="true">∞</mi></math>$), and not persist or even get amplified. For most practical purposes, the echo state property is assured if the reservoir weight matrix $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ is scaled so that its [spectral radius](https://www.sciencedirect.com/topics/computer-science/spectral-radius "Learn more about spectral radius from ScienceDirect's AI-generated Topic Pages") $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$ (i.e., the largest absolute eigenvalue) satisfies $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow><mo is="true">&lt;</mo><mn is="true">1</mn></math>$ [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12). Or, using another term, $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ is *contractive*. The fact that $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow><mo is="true">&lt;</mo><mn is="true">1</mn></math>$ almost always ensures the echo state property has led to an unfortunate misconception which is expressed in many RC publications, namely, that $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow><mo is="true">&lt;</mo><mn is="true">1</mn></math>$ amounts to a necessary and sufficient condition for the echo state property. This is wrong. The mathematically correct connection between the spectral radius and the echo state property is that the latter is violated if $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow><mo is="true">&gt;</mo><mn is="true">1</mn></math>$ *in reservoirs using the* $<math><mo is="true">tanh</mo></math>$ *function as neuron nonlinearity, and for zero input*. Contrary to widespread misconceptions, the echo state property can be obtained even if $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow><mo is="true">&gt;</mo><mn is="true">1</mn></math>$ for non-zero input (including bias inputs to neurons), and it may be lost even if $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow><mo is="true">&lt;</mo><mn is="true">1</mn></math>$, although it is hard to construct systems where this occurs (unless $<math><msup is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mn is="true">0</mn><mo is="true">)</mo></mrow><mo is="true">&gt;</mo><mn is="true">1</mn></math>$ for the nonlinearity $<math><mi is="true">f</mi></math>$), and in practice this does not happen.

The optimal value of $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$ should be set depending on the amount of memory and nonlinearity that the given task requires. A rule of thumb, likewise discussed in [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12), is that $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$ should be close to 1 for tasks that require long memory and accordingly smaller for the tasks where a too long memory might in fact be harmful. Larger $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$ also have the effect of driving signals $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ into more nonlinear regions of $<math><mo is="true">tanh</mo></math>$ units (further from 0) similarly to $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$. Thus scalings of both $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ have a similar effect on nonlinearity of the ESN, while their difference determines the amount of memory.

A rather conservative rigorous *sufficient* condition of the echo state property for any kind of inputs $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ (including zero) and states $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ (with $<math><mo is="true">tanh</mo></math>$ nonlinearity) being $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mo is="true">max</mo></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow><mo is="true">&lt;</mo><mn is="true">1</mn></math>$, where $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mo is="true">max</mo></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$ is the largest [singular value](https://www.sciencedirect.com/topics/computer-science/singular-value "Learn more about singular value from ScienceDirect's AI-generated Topic Pages") of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$, was proved in [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12). Recently, a less restrictive sufficient condition, namely, $<math><msub is="true"><mrow is="true"><mo is="true">inf</mo></mrow><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">D</mi></mstyle><mo is="true">∈</mo><mi mathvariant="script" is="true">D</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mo is="true">max</mo></mrow></msub><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">D</mi></mstyle><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">D</mi></mstyle></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup><mo is="true">)</mo></mrow><mo is="true">&lt;</mo><mn is="true">1</mn></math>$, where $<math><mstyle mathvariant="bold" is="true"><mi is="true">D</mi></mstyle></math>$ is an arbitrary matrix, minimizing the so-called $<math><mstyle mathvariant="bold" is="true"><mi is="true">D</mi></mstyle></math>$\-norm $<math><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mo is="true">max</mo></mrow></msub><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">D</mi></mstyle><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">D</mi></mstyle></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup><mo is="true">)</mo></mrow></math>$, from a set $<math><mi mathvariant="script" is="true">D</mi><mo is="true">⊂</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></mrow></msup></math>$ of [diagonal matrices](https://www.sciencedirect.com/topics/computer-science/diagonal-matrix "Learn more about diagonal matrices from ScienceDirect's AI-generated Topic Pages"), has been derived in [\[63\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b63). This sufficient condition approaches the necessary $<math><msub is="true"><mrow is="true"><mo is="true">inf</mo></mrow><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">D</mi></mstyle><mo is="true">∈</mo><mi mathvariant="script" is="true">D</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mo is="true">max</mo></mrow></msub><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">D</mi></mstyle><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">D</mi></mstyle></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup><mo is="true">)</mo></mrow><mo is="true">→</mo><mi is="true">ρ</mi><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">−</mo></mrow></msup></math>$, $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow><mo is="true">&lt;</mo><mn is="true">1</mn></math>$, e.g., when $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ is a normal or a triangular (permuted) matrix. A rigorous sufficient condition for the echo state property is rarely ensured in practice, with a possible exception being critical control tasks, where provable stability under any conditions is required.

### 5.2. Different topologies of the reservoir

There have been attempts to find topologies of the ESN reservoir different from sparsely randomly connected ones. Specifically, small-world [\[64\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b64), scale-free [\[65\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b65), and biologically inspired connection topologies generated by spatial growth [\[66\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b66) were tested for this purpose in a careful study [\[67\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b67), which we point out here due to its relevance although it was obtained only as a BSc thesis. The NRMS error [(1)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd1) of $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ as well as the eigenvalue spread of the cross-correlation matrix of the activations $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ (necessary for a fast online learning described in Section [8.1.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.1.2); see Section [6.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1) for details) were used as the performance measures of the topologies. This work also explored an exhaustive brute-force search of topologies of tiny networks (motifs) of four units, and then combining successful motives (in terms of the eigenvalue spread) into larger networks. The investigation, unfortunately, concludes that *“(…) none of the investigated network topologies was able to perform significantly better than simple random networks, both in terms of eigenvalue spread as well as testing error”* [\[67\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b67). This, however, does not serve as a proof that similar approaches are futile. An indication of this is the substantial variation in ESN performance observed among randomly created reservoirs, which is, naturally, more pronounced in smaller reservoirs (e.g., [\[68\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b68)).

In contrast, LSMs often use a biologically plausible connectivity structure and weight settings. In the original form they model a single cortical microcolumn [\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11). Since the model of both the connections and the neurons themselves is quite sophisticated, it has a large number of free parameters to be set, which is done manually, guided by biologically observed parameter ranges, e.g., as found in the rat somatosensory cortex [\[69\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b69). This type of model also delivers good performance for practical applications of speech recognition [\[69\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b69), [\[70\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b70) (and many similar publications by the latter authors). Since LSMs aim at accuracy of modeling natural neural structures, less biologically plausible connectivity patterns are usually not explored.

It has been demonstrated that much more detailed biological neural circuit models, which use anatomical and neurophysiological data-based laminar (i.e., cortical layer) connectivity structures and Hodgkin–Huxley model neurons, improve the information-processing capabilities of the models [\[23\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b23). Such highly realistic (for present-day standards) models *“perform significantly better than control circuits (which are lacking the laminar structures but are otherwise identical with regard to their components and overall connection statistics) for a wide variety of fundamental information-processing tasks”* [\[23\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b23).

Different from this direction of research, there are also explorations of using even simpler topologies of the reservoir than the classical ESN. It has been demonstrated that the reservoir can even be an unstructured feed-forward network with time-delayed connections if the finite limited memory window that it offers is sufficient for the task at hand [\[71\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b71). A degenerate case of a “reservoir” composed of linear units and a diagonalized $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ and unitary inputs $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$ was considered in [\[72\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b72). A one-dimensional lattice (ring) topology was used for a reservoir, together with an adaptation of the reservoir discussed in Section [6.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2), in [\[73\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b73). A special kind of excitatory and inhibitory neurons connected in a one-dimensional spatial arrangement was shown to produce interesting chaotic behavior in [\[74\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b74).

A tendency that higher ranks of the [connectivity matrix](https://www.sciencedirect.com/topics/computer-science/connectivity-matrix "Learn more about connectivity matrix from ScienceDirect's AI-generated Topic Pages") $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">mask</mtext></mrow></msub></math>$ (where $<math><msub is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mtext is="true">mask</mtext></mrow></msub></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></math>$ if $<math><msub is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"></mrow></msub></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></msub><mo is="true">≠</mo><mn is="true">0</mn></math>$, and $<math><mo is="true">=</mo><mn is="true">0</mn></math>$ otherwise, for $<math><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></math>$) correlate with lower ESN output errors was observed in [\[75\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b75). Connectivity patterns of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ such that $<math><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></mrow><mrow is="true"><mi is="true">∞</mi></mrow></msup><mo is="true">≡</mo><msub is="true"><mrow is="true"><mo is="true">lim</mo></mrow><mrow is="true"><mi is="true">k</mi><mo is="true">→</mo><mi is="true">∞</mi></mrow></msub><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></math>$ ($<math><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></math>$ standing for “$<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ to the power $<math><mi is="true">k</mi></math>$” and approximating weights of the cumulative indirect connections by paths of length $<math><mi is="true">k</mi></math>$ among the reservoir units) is neither fully connected, nor all-zero, are claimed to give a broader distribution of ESN prediction performances, thus including best performing reservoirs, than random sparse connectivities in [\[76\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b76). A [permutation matrix](https://www.sciencedirect.com/topics/computer-science/permutation-matrix "Learn more about permutation matrix from ScienceDirect's AI-generated Topic Pages") with a medium number and different lengths of connected cycles, or a general [orthogonal matrix](https://www.sciencedirect.com/topics/computer-science/orthogonal-matrix "Learn more about orthogonal matrix from ScienceDirect's AI-generated Topic Pages"), are suggested as candidates for such $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$s.

### 5.3. Modular reservoirs

One of the shortcomings of conventional ESN reservoirs is that, even though they are sparse, the activations are still coupled so strongly that the ESN is poor in dealing with different time scales simultaneously, e.g., predicting several superimposed generators. This problem was successfully tackled by dividing the reservoir into decoupled sub-reservoirs and introducing inhibitory connections among all the sub-reservoirs [\[77\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b77). For the approach to be effective, the inhibitory connections must predict the activations of the sub-reservoirs one time step ahead. To achieve this the inhibitory connections are heuristically computed from (the rest of) $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub></math>$, or the sub-reservoirs are updated in a sequence and the real activations of the already updated sub-reservoirs are used.

The Evolino approach introduced in Section [3.3](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.3) can also be classified as belonging to this group, as the LSTM [RNN](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about RNN from ScienceDirect's AI-generated Topic Pages") used for its reservoir consists of specific small memory-holding modules (which could alternatively be regarded as more complicated units of the network).

Approaches relying on combining outputs from several separate reservoirs will be discussed in Section [8.8](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.8).

### 5.4. Time-delayed vs. instantaneous connections

Another time-related limitation of the classical ESNs pointed out in [\[78\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78) is that no matter how many neurons are contained in the reservoir, it (like any other fully [recurrent network](https://www.sciencedirect.com/topics/computer-science/recurrent-network "Learn more about recurrent network from ScienceDirect's AI-generated Topic Pages") with all connections having a time delay) has only a single layer of neurons ([Fig. 2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fig2)). This makes it intrinsically unsuitable for some types of problems. Consider a problem where the mapping from $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ to $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is a very complex, nonlinear one, and the data in neighboring time steps are almost independent (i.e., little memory is required), as e.g., the “meta-learning” task in [\[79\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b79).[4](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fn4) Consider a single time step $<math><mi is="true">n</mi></math>$: signals from the input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ propagate only through one untrained layer of weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$, through the nonlinearity $<math><mi is="true">f</mi></math>$ influence the activations $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, and reach the output $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ through the trained weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ ([Fig. 2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fig2)). Thus ESNs are not capable of producing a very complex *instantaneous* mapping from $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ to $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ using a realistic number of neurons, which could (only) be effectively done by a multilayer FFNN (not counting some non-NN-based methods). Delaying the target $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub></math>$ by $<math><mi is="true">k</mi></math>$ time steps would in fact make the signals coming from $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ “cross” the nonlinearities $<math><mi is="true">k</mi><mo is="true">+</mo><mn is="true">1</mn></math>$ times before reaching $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">+</mo><mi is="true">k</mi><mo is="true">)</mo></mrow></math>$, but would mix the information from different time steps in $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">+</mo><mi is="true">k</mi><mo is="true">)</mo></mrow></math>$, breaking the required virtually independent mapping $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">→</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">+</mo><mi is="true">k</mi><mo is="true">)</mo></mrow></math>$, if no special structure of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ is imposed.

![](https://ars.els-cdn.com/content/image/1-s2.0-S1574013709000173-gr2.jpg)

1.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S1574013709000173-gr2.jpg "Download full-size image")

Fig. 2. Signal flow diagram of the standard ESN.

As a possible remedy Layered ESNs were introduced in [\[78\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78), where a part (up to almost half) of the reservoir connections can be instantaneous and the rest take one time step for the signals to propagate as in normal ESNs. Randomly generated Layered ESNs, however, do not offer a consistent improvement for large classes of tasks, and pre-training methods of such reservoirs have not yet been investigated.

The issue of standard ESNs not having enough trained layers is also discussed and addressed in a broader context in Section [8.8](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.8).

### 5.5. Leaky integrator neurons and speed of dynamics

In addition to the basic sigmoid units, leaky integrator neurons were suggested to be used in ESNs from the point of their introduction [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12). This type of neuron performs a leaky integration of its activation from previous time steps. Today a number of versions of leaky integrator neurons are often used in ESNs, which we will call here *leaky integrator ESNs* (LI-ESNs) where the distinction is needed. The main two groups are those using leaky integration before application of the [activation function](https://www.sciencedirect.com/topics/computer-science/activation-function "Learn more about activation function from ScienceDirect's AI-generated Topic Pages") $<math><mi is="true">f</mi><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$, and after. One example of the latter (in the discretized time case) has reservoir dynamics governed by (9)$<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><mi is="true">a</mi><mstyle mathvariant="normal" is="true"><mi is="true">Δ</mi></mstyle><mi is="true">t</mi><mo is="true">)</mo></mrow><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">+</mo><mstyle mathvariant="normal" is="true"><mi is="true">Δ</mi></mstyle><mi is="true">t</mi><mi is="true">f</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mtext is="true">,</mtext></math>$ where $<math><mstyle mathvariant="normal" is="true"><mi is="true">Δ</mi></mstyle><mi is="true">t</mi></math>$ is a compound time gap between two consecutive time steps divided by the time constant of the system and $<math><mi is="true">a</mi></math>$ is the decay (or leakage) rate [\[81\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b81). Another popular (and we believe, preferable) design can be seen as setting $<math><mi is="true">a</mi><mo is="true">=</mo><mn is="true">1</mn></math>$ and redefining $<math><mi is="true">δ</mi><mi is="true">t</mi></math>$ in Eq. [(9)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd9) as the leaking rate $<math><mi is="true">a</mi></math>$ to control the “speed” of the dynamics, (10)$<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><mi is="true">a</mi><mo is="true">)</mo></mrow><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">+</mo><mi is="true">a</mi><mi is="true">f</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mtext is="true">,</mtext></math>$ which in effect is an exponential moving average, has only one additional parameter and the [desirable property](https://www.sciencedirect.com/topics/computer-science/desirable-property "Learn more about desirable property from ScienceDirect's AI-generated Topic Pages") that neuron activations $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ never go outside the boundaries defined by $<math><mi is="true">f</mi><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$. Note that the simple ESN [(5)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5) is a special case of LI-ESNs [(9)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd9) or [(10)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd10) with $<math><mi is="true">a</mi><mo is="true">=</mo><mn is="true">1</mn></math>$ and $<math><mstyle mathvariant="normal" is="true"><mi is="true">Δ</mi></mstyle><mi is="true">t</mi><mo is="true">=</mo><mn is="true">1</mn></math>$. As a corollary, an LI-ESN with a good choice of the parameters can always perform *at least* as well as a corresponding simple ESN. With the introduction of the new parameter $<math><mi is="true">a</mi></math>$ (and $<math><mstyle mathvariant="normal" is="true"><mi is="true">Δ</mi></mstyle><mi is="true">t</mi></math>$), the condition for the echo state property is redefined [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12). A natural constraint on the two new parameters is $<math><mi is="true">a</mi><mstyle mathvariant="normal" is="true"><mi is="true">Δ</mi></mstyle><mi is="true">t</mi><mo is="true">∈</mo><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">]</mo></mrow></math>$ in [(9)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd9), and $<math><mi is="true">a</mi><mo is="true">∈</mo><mrow is="true"><mo is="true">[</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">]</mo></mrow></math>$ in [(10)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd10) — a neuron should neither retain, nor leak, more activation than it had. The effect of these parameters on the final performance of ESNs was investigated in [\[18\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18) and [\[82\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b82). The latter contribution also considers applying the leaky integrator in different places of the model and resampling the signals as an alternative.

The additional parameters of the LI-ESN control the “speed” of the reservoir dynamics. Small values of $<math><mi is="true">a</mi></math>$ and $<math><mstyle mathvariant="normal" is="true"><mi is="true">Δ</mi></mstyle><mi is="true">t</mi></math>$ result in reservoirs that react slowly to the input. By changing these parameters it is possible to shift the effective interval of frequencies in which the reservoir is working. Along these lines, time warping invariant ESNs (TWIESNs) — an architecture that can deal with strongly time-warped signals — were outlined in [\[81\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b81), [\[18\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18). This architecture varies $<math><mstyle mathvariant="normal" is="true"><mi is="true">Δ</mi></mstyle><mi is="true">t</mi></math>$ on-the-fly in [(9)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd9), directly depending on the speed at which the input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is changing.

From a signal processing point of view, the exponential moving average on the neuron activation [(10)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd10) does a simple *low-pass* filtering of its activations with the [cutoff frequency](https://www.sciencedirect.com/topics/computer-science/cutoff-frequency "Learn more about cutoff frequency from ScienceDirect's AI-generated Topic Pages") (11)$<math><msub is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mn is="true">2</mn><mi is="true">π</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><mi is="true">a</mi><mo is="true">)</mo></mrow><mstyle mathvariant="normal" is="true"><mi is="true">Δ</mi></mstyle><mi is="true">t</mi></mrow></mfrac><mtext is="true">,</mtext></math>$ where $<math><mstyle mathvariant="normal" is="true"><mi is="true">Δ</mi></mstyle><mi is="true">t</mi></math>$ is the [discretization](https://www.sciencedirect.com/topics/computer-science/discretization "Learn more about discretization from ScienceDirect's AI-generated Topic Pages") time step. This makes the neurons average out the frequencies above $<math><msub is="true"><mrow is="true"><mi is="true">f</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub></math>$ and enables tuning the reservoirs for particular frequencies. Elaborating further on this idea, *high-pass* neurons, that produce their activations by subtracting from the unfiltered activation [(5)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5) the low-pass filtered one [(10)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd10), and *band-pass* neurons, that combine the low-pass and high-pass ones, were introduced [\[83\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b83). The authors also suggested mixing neurons with different passbands inside a single ESN reservoir, and reported that a single reservoir of such kind is able to predict/generate signals having structure on different timescales.

Following this line of thought, [Infinite Impulse Response](https://www.sciencedirect.com/topics/computer-science/infinite-impulse-response "Learn more about Infinite Impulse Response from ScienceDirect's AI-generated Topic Pages") (IIR) band-pass filters having sharper cutoff characteristics were tried on neuron activations in ESNs with success in several types of signals [\[84\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b84). Since the filters often introduce an undesired phase shift to the signals, a time delay for the activation of each neuron was learned and applied before the linear readout from the reservoir. A successful application of Butterworth band-pass filters in ESNs is reported in [\[85\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b85).

Connections between neurons that have different time delays (more than one time step) can actually also be used inside the recurrent part, which enables the network to operate on different timescales simultaneously and learn longer-term dependences [\[86\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b86). This idea has been tried for RNNs trained by error [backpropagation](https://www.sciencedirect.com/topics/computer-science/backpropagation "Learn more about backpropagation from ScienceDirect's AI-generated Topic Pages"), but could also be useful for multi-timescale reservoirs. Long-term dependences can also be learned using the reservoirs mentioned in Section [3.3](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.3).

## 6\. Unsupervised reservoir adaptation

In this section we describe reservoir training/generation methods that try to optimize some measure defined on the activations $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ of the reservoir, for a given input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, but regardless of the desired output $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$. In Section [6.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1) we survey measures that are used to estimate the quality of the reservoir, irrespective of the methods optimizing them. Then local, Section [6.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2), and global, Section [6.3](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.3) unsupervised reservoir training methods are surveyed.

### 6.1. “Goodness” measures of the reservoir activations

The classical feature that reservoirs should possess is the echo state property, defined in Section [5.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.1). Even though this property depends on the concrete input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, usually in practice its existence is not measured explicitly, and only the [spectral radius](https://www.sciencedirect.com/topics/computer-science/spectral-radius "Learn more about spectral radius from ScienceDirect's AI-generated Topic Pages") $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$ is selected to be <1 irrespective of $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, or just tuned for the final performance. A measure of short-term *memory capacity*, evaluating how well $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ can be reconstructed by the reservoir as $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">+</mo><mi is="true">k</mi><mo is="true">)</mo></mrow></math>$ after various delays $<math><mi is="true">k</mi></math>$, was introduced in [\[41\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b41).

The two necessary and sufficient conditions for LSMs to work were introduced in [\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11). A *separation property* measures the distance between different states $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></math>$ caused by different input sequences $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle></math>$. The measure is refined for binary ESN-type reservoirs in [\[87\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b87) with a [generalization](https://www.sciencedirect.com/topics/computer-science/generalization "Learn more about generalization from ScienceDirect's AI-generated Topic Pages") in [\[88\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b88). An *approximation property* measures the capability of the readout to produce a desired output $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub></math>$ from $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></math>$, and thus is not an unsupervised measure, but is included here for completeness.

Methods for estimating the computational power and generalization capability of neural reservoirs were presented in [\[89\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b89). The proposed measure for computational power, or *kernel quality*, is obtained in the following way. Take $<math><mi is="true">k</mi></math>$ different input sequences (or segments of the same signal) $<math><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle></mrow><mrow is="true"><mi is="true">i</mi></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, where $<math><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">k</mi></math>$, and $<math><mi is="true">n</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">T</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub></math>$. For each input $<math><mi is="true">i</mi></math>$ take the resulting reservoir state $<math><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></mrow><mrow is="true"><mi is="true">i</mi></mrow></msup><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo is="true">)</mo></mrow></math>$, and collect them into a matrix $<math><mstyle mathvariant="bold" is="true"><mi is="true">M</mi></mstyle><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><mi is="true">k</mi><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></mrow></msup></math>$, where $<math><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub></math>$ is some fixed time after the appearance of $<math><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle></mrow><mrow is="true"><mi is="true">i</mi></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ in the input. Then the rank $<math><mi is="true">r</mi></math>$ of the matrix $<math><mstyle mathvariant="bold" is="true"><mi is="true">M</mi></mstyle></math>$ is the measure. If $<math><mi is="true">r</mi><mo is="true">=</mo><mi is="true">k</mi></math>$, this means that all the presented inputs can be separated by a linear readout from the reservoir, and thus the reservoir is said to have a *linear separation property*. For estimating the generalization capability of the reservoir, the same procedure can be performed with $<math><mi is="true">s</mi></math>$ ($<math><mi is="true">s</mi><mo is="true">≫</mo><mi is="true">k</mi></math>$) inputs $<math><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle></mrow><mrow is="true"><mi is="true">j</mi></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, $<math><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">s</mi></math>$, that represent the set of all possible inputs. If the resultant rank $<math><mi is="true">r</mi></math>$ is substantially smaller than the size $<math><mi is="true">s</mi></math>$ of the training set, the reservoir generalizes well. These two measures are more targeted to tasks of time series [classification](https://www.sciencedirect.com/topics/computer-science/classification "Learn more about classification from ScienceDirect's AI-generated Topic Pages"), but can also be revealing in predicting the performance of regression [\[90\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b90).

A much-desired measure to minimize is the eigenvalue spread (EVS, the ratio of the maximal eigenvalue to the minimal eigenvalue) of the cross-correlation matrix of the activations $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$. A small EVS is necessary for an online training of the ESN output by a computationally cheap and stable stochastic [gradient descent](https://www.sciencedirect.com/topics/computer-science/gradient-descent "Learn more about gradient descent from ScienceDirect's AI-generated Topic Pages") algorithm outlined in Section [8.1.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.1.2) (see, e.g., [\[91\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b91), chapter 5.3, for the mathematical reasons that render this mandatory). In classical ESNs the EVS sometimes reaches 1012 or even higher [\[92\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b92), which makes the use of stochastic gradient descent training unfeasible. Other commonly desirable features of the reservoir are small pairwise correlation of the reservoir activations $<math><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, or a large entropy of the $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ distribution (e.g., [\[92\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b92)). The latter is a rather popular measure, as discussed later in this review. A criterion for maximizing the local information transmission of each [individual neuron](https://www.sciencedirect.com/topics/computer-science/individual-neuron "Learn more about individual neuron from ScienceDirect's AI-generated Topic Pages") was investigated in [\[93\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b93) (more in Section [6.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2)).

The so-called *edge of chaos* is a region of parameters of a [dynamical system](https://www.sciencedirect.com/topics/computer-science/dynamical-system "Learn more about dynamical system from ScienceDirect's AI-generated Topic Pages") at which it operates at the boundary between the chaotic and non-chaotic behavior. It is often claimed (but not undisputed; see, e.g., [\[94\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b94)) that at the edge of chaos many types of dynamical systems, including binary systems and reservoirs, possess high computational power [\[87\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b87), [\[95\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b95). It is intuitively clear that the edge of chaos in reservoirs can only arise when the effect of inputs on the reservoir state does not die out quickly; thus such reservoirs can potentially have high memory capacity, which is also demonstrated in [\[95\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b95). However, this does not universally imply that such reservoirs are optimal [\[90\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b90). The edge of chaos can be empirically detected (even for biological networks) by measuring [Lyapunov exponents](https://www.sciencedirect.com/topics/computer-science/lyapunov-exponent "Learn more about Lyapunov exponents from ScienceDirect's AI-generated Topic Pages") [\[95\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b95), even though such measurements are not trivial (and often involve a degree of expert judgment) for high-dimensional noisy systems. For reservoirs of simple binary threshold units this can be done more simply by computing the [Hamming distances](https://www.sciencedirect.com/topics/computer-science/hamming-distance "Learn more about Hamming distances from ScienceDirect's AI-generated Topic Pages") between trajectories of the states [\[87\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b87). There is also an empirical observation that, while changing different parameter settings of a reservoir, the best performance in a given task correlates with a Lyapunov exponent specific to that task [\[59\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b59). The optimal exponent is related to the amount of memory needed for the task, as discussed in Section [5.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.1). It was observed in ESNs with no input that when $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$ is slightly greater than 1, the internally generated signals are periodic oscillations, whereas for larger values of $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$, the signals are more irregular and even chaotic [\[96\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b96). Even though stronger inputs $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ can push the dynamics of the reservoirs out of the chaotic regime and thus make them useful for computation, no reliable benefit of such a mode of operation was found in the last contribution.

In contrast to ESN-type reservoirs of real-valued units, simple binary threshold units exhibit a more immediate transition from damped to chaotic behavior without intermediate periodic oscillations [\[87\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b87). This difference between the two types of [activation functions](https://www.sciencedirect.com/topics/computer-science/activation-function "Learn more about activation functions from ScienceDirect's AI-generated Topic Pages"), including intermediate *quantized* ones, in ESN-type reservoirs was investigated more closely in [\[88\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b88). The investigation showed that reservoirs of binary units are more sensitive to the topology and the connection weight parameters of the network in their transition between damped and chaotic behavior, and computational performance, than the real-valued ones. This difference can be related to the similar apparent difference in sensitivity of the ESNs and LSM-type reservoirs of firing units, discussed in Section [5.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.2).

### 6.2. Unsupervised local methods

A natural strategy for improving reservoirs is to mimic biology (at a high level of abstraction) and count on *local* adaptation rules. “Local” here means that parameters pertaining to some neuron $<math><mi is="true">i</mi></math>$ are adapted on the basis of no other information than the activations of neurons directly connected with neuron $<math><mi is="true">i</mi></math>$. In fact all local methods are almost exclusively unsupervised, since the information on the performance $<math><mi is="true">E</mi></math>$ at the output is unreachable in the reservoir.

First attempts to decrease the eigenvalue spread in ESNs by classical Hebbian [\[97\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b97) (inspired by synaptic plasticity in biological brains) or Anti-Hebbian learning gave no success [\[92\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b92). A modification of Anti-Hebbian learning, called *Anti-Oja* learning, is reported to improve the performance of ESNs in [\[98\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b98).

On the more biologically realistic side of the RC research with spiking neurons, local unsupervised adaptations are very natural to use. In fact, LSMs had used synaptic connections with realistic short-term dynamic adaptation, as proposed by [\[99\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b99), in their reservoirs from the very beginning [\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11).

The Hebbian learning principle is usually implemented in spiking NNs as *spike-time-dependent plasticity* (STDP) of synapses. STDP is shown to improve the separation property of LSMs for real-world speech data, but not for random inputs $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle></math>$, in [\[100\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b100). The authors however were uncertain whether manually optimizing the parameters of the STDP adaptation (which they did) or the ones for generating the reservoir would result in a larger performance gain for the same effort spent. STDP is shown to work well with time-coded readouts from the reservoir in [\[101\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b101).

Biological neurons are widely observed to adapt their intrinsic excitability, which often results in [exponential distributions](https://www.sciencedirect.com/topics/computer-science/exponential-distribution "Learn more about exponential distributions from ScienceDirect's AI-generated Topic Pages") of firing rates, as observed in visual cortex (e.g., [\[102\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b102)). This homeostatic adaptation mechanism, called *intrinsic plasticity* (IP), has recently attracted a wide attention in the reservoir computing community. Mathematically, the exponential distribution maximizes the entropy of a non-negative random variable with a fixed mean; thus it enables the neurons to transmit maximal information for a fixed metabolic cost of firing. An IP learning rule for spiking model neurons aimed at this goal was first presented in [\[103\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b103).

For a more abstract model of the neuron, having a continuous Fermi sigmoid activation function $<math><mi is="true">f</mi><mo is="true">:</mo><mi mathvariant="double-struck" is="true">R</mi><mo is="true">→</mo><mrow is="true"><mo is="true">(</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></math>$, the IP rule was derived as a proportional control that changes the steepness and offset of the sigmoid to get an exponential-like output distribution in [\[104\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b104). A more elegant gradient IP learning rule for the same purpose was presented in [\[93\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b93), which is similar to the information maximization approach in [\[105\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b105). Applying IP with Fermi neurons in reservoir computing significantly improves the performance of BPDC-trained networks [\[106\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b106), [\[107\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b107), and is shown to have a positive effect on offline trained ESNs, but can cause stability problems for larger reservoirs [\[106\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b106). An ESN reservoir with IP-adapted Fermi neurons is also shown to enable predicting several superimposed oscillators [\[108\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b108).

An adaptation of the IP rule to $<math><mo is="true">tanh</mo></math>$ neurons ($<math><mi is="true">f</mi><mo is="true">:</mo><mi mathvariant="double-struck" is="true">R</mi><mo is="true">→</mo><mrow is="true"><mo is="true">(</mo><mo is="true">−</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></math>$) that results in a zero-mean Gaussian-like distribution of activations was first presented in [\[73\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b73) and investigated more in [\[55\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b55). The IP-adapted ESNs were compared with classical ones, both having Fermi and $<math><mo is="true">tanh</mo></math>$ neurons, in the latter contribution. IP was shown to (modestly) improve the performance in all cases. It was also revealed that ESNs with Fermi neurons have significantly smaller short-term memory capacity (as in Section [6.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1)) and worse performance in a synthetic NARMA prediction task, while having a slightly better performance in a speech recognition task, compared to $<math><mo is="true">tanh</mo></math>$ neurons. The same type of $<math><mo is="true">tanh</mo></math>$ neurons adapted by IP aimed at [Laplacian distributions](https://www.sciencedirect.com/topics/computer-science/laplacian-distribution "Learn more about Laplacian distributions from ScienceDirect's AI-generated Topic Pages") are investigated in [\[109\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b109). In general, IP gives more control on the working points of the reservoir [nonlinearity](https://www.sciencedirect.com/topics/computer-science/nonlinearities "Learn more about nonlinearity from ScienceDirect's AI-generated Topic Pages") sigmoids. The slope (first derivative) and the curvature (second derivative) of the sigmoid at the point around which the activations are centered by the IP rule affect the effective spectral radius and the nonlinearity of the reservoir, respectively. Thus, for example, centering $<math><mo is="true">tanh</mo></math>$ activations around points other than 0 is a good idea if no quasi-linear behavior is desired. IP has recently become employed in reservoirs as a standard practice by several research groups.

Overall, an information-theoretic view on adaptation of spiking neurons has a long history in computational neuroscience. Even better than maximizing just any information in the output of a neuron is maximizing *relevant* information. In other words, in its output the neuron should encode the inputs in such a way as to preserve maximal information about some (local) target signal. This is addressed in a general information-theoretical setting by the *Information Bottleneck* (IB) method [\[110\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b110). A learning rule for a spiking neuron that maximizes mutual information between its inputs and its output is presented in [\[111\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b111). A more general IB learning rule, transferring the general ideas of IB method to spiking neurons, is introduced in [\[112\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b112) and [\[113\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b113). Two *semi-local* training scenarios are presented in these two contributions. In the first, a neuron optimizes the mutual information of its output with outputs of some neighboring neurons, while minimizing the mutual information with its inputs. In the second, two neurons reading from the same signals maximize their information throughput, while keeping their inputs statistically independent, in effect performing Independent Component Analysis (ICA). A simplified online version of the IB training rule with a variation capable of performing [Principle Component Analysis](https://www.sciencedirect.com/topics/computer-science/principle-component-analysis "Learn more about Principle Component Analysis from ScienceDirect's AI-generated Topic Pages") (PCA) was recently introduced in [\[114\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b114). In addition, it assumes slow semi-local target signals, which is more biologically plausible. The approaches described in this paragraph are still waiting to be tested in the reservoir computing setting.

It is also of great interest to understand how different types of plasticity observed in biological brains interact when applied together and what effect this has on the quality of reservoirs. The interaction of the IP with Hebbian synaptic plasticity in a single Fermi neuron is investigated in [\[104\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b104) and further in [\[115\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b115). The synergy of the two plasticities is shown to result in a better specialization of the neuron that finds heavy-tail directions in the input. An interaction of IP with a neighborhood-based Hebbian learning in a layer of such neurons was also shown to maximize information transmission, perform nonlinear ICA, and result in an emergence of orientational Gabor-like receptive fields in [\[116\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b116). The interaction of STDP with IP in an LSM-like reservoir of simple sparsely spiking neurons was investigated in [\[117\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b117). The interaction turned out to be a non-trivial one, resulting in networks more robust to perturbations of the state $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ and having a better short-time memory and time series prediction performance.

A recent approach of combining STDP with a biologically plausible reinforcement signal is discussed in Section [7.5](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7.5), as it is not unsupervised.

### 6.3. Unsupervised global methods

Here we review [unsupervised methods](https://www.sciencedirect.com/topics/computer-science/unsupervised-method "Learn more about unsupervised methods from ScienceDirect's AI-generated Topic Pages") that optimize reservoirs based on *global* information of the reservoir activations induced by the given input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">x</mi><mo is="true">)</mo></mrow></math>$, but irrespective of the target $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, like for example the measures discussed in Section [6.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1). The intuitive goal of such methods is to produce good representations of (the history of) $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ in $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ for any (and possibly several) $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$.

A biologically inspired unsupervised approach with a reservoir trying to predict itself is proposed in [\[118\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b118). An additional output $<math><mstyle mathvariant="bold" is="true"><mi is="true">z</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></mrow></msup></math>$, $<math><mstyle mathvariant="bold" is="true"><mi is="true">z</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">z</mtext></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ from the reservoir is trained on the target $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">z</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></math>$, where $<math><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ are the activations of the reservoir before applying the neuron transfer function $<math><mo is="true">tanh</mo><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$, i.e., $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mo is="true">tanh</mo><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></math>$. Then, in the application phase of the trained networks, the original activations $<math><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, which result from $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$, and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$, are mixed with the self-predictions $<math><mstyle mathvariant="bold" is="true"><mi is="true">z</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></math>$ obtained from $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">z</mtext></mrow></msub></math>$, with a certain mixing ratio $<math><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><mi is="true">α</mi><mo is="true">)</mo></mrow><mo is="true">:</mo><mi is="true">α</mi></math>$. The coefficient $<math><mi is="true">α</mi></math>$ determines how much the reservoir is relying on the external input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ and how much on the internal self-prediction $<math><mstyle mathvariant="bold" is="true"><mi is="true">z</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$. With $<math><mi is="true">α</mi><mo is="true">=</mo><mn is="true">0</mn></math>$ we have the classical ESN and with $<math><mi is="true">α</mi><mo is="true">=</mo><mn is="true">1</mn></math>$ we have an “autistic” reservoir that does not react to the input. Intermediate values of $<math><mi is="true">α</mi></math>$ close to 1 were shown to enable reservoirs to generate slow, highly [nonlinear signals](https://www.sciencedirect.com/topics/computer-science/nonlinear-signal "Learn more about nonlinear signals from ScienceDirect's AI-generated Topic Pages") that are hard to get otherwise.

An algebraic unsupervised way of generating ESN reservoirs was proposed in [\[119\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b119). The idea is to linearize the ESN update equation [(5)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5) locally around its current state $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ at every time step $<math><mi is="true">n</mi></math>$ to get a linear approximation of [(5)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd5) as $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">=</mo><mstyle mathvariant="bold" is="true"><mi is="true">A</mi></mstyle><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><mstyle mathvariant="bold" is="true"><mi is="true">B</mi></mstyle><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, where $<math><mstyle mathvariant="bold" is="true"><mi is="true">A</mi></mstyle></math>$ and $<math><mstyle mathvariant="bold" is="true"><mi is="true">B</mi></mstyle></math>$ are time ($<math><mi is="true">n</mi></math>$)-dependent matrices corresponding to $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$ respectively. The approach aims at distributing the predefined complex eigenvalues of $<math><mstyle mathvariant="bold" is="true"><mi is="true">A</mi></mstyle></math>$ uniformly within the unit circle on the $<math><mi mathvariant="double-struck" is="true">C</mi></math>$ plane. The reservoir matrix $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ is obtained analytically from the set of these predefined eigenvalues and a given input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$. The motivation for this is, as for Kautz filters [\[120\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b120) in linear systems, that if the target $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is unknown, it is best to have something like an [orthogonal basis](https://www.sciencedirect.com/topics/computer-science/orthogonal-basis "Learn more about orthogonal basis from ScienceDirect's AI-generated Topic Pages") in $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, from which any $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ could, on average, be constructed well. The spectral radius of the reservoir is suggested to be set by hand (according to the correlation time of $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, which is an indication of a memory span needed for the task), or by adapting the bias value of the reservoir units to minimize the output error (which actually renders this method *supervised*, as in Section [7](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7)). Reservoirs generated this way are shown to yield higher average entropy of $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ distribution, higher short-term memory capacity (both measures mentioned in Section [6.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1)), and a smaller output error on a number of synthetic problems, using relatively small reservoirs ($<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub><mo is="true">=</mo><mn is="true">20</mn><mo is="true">,</mo><mn is="true">30</mn></math>$). However, a more extensive empirical comparison of this type of reservoir with the classical ESN one is still lacking.

## 7\. Supervised reservoir pre-training

In this section we discuss methods for training reservoirs to perform a specific given task, i.e., not only the concrete input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, but also the desired output $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is taken into account. Since a linear readout from a reservoir is quickly trained, the suitability of a candidate reservoir for a particular task (e.g., in terms of NRMSE [(1)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd1)) is inexpensive to check. Notice that even for most methods of this class the explicit target signal $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is not technically required for training the reservoir itself, but only for evaluating it in an outer loop of the adaptation process.

### 7.1. Optimization of global reservoir parameters

In Section [5.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.1) we discussed guidelines for the manual choice of global parameters for reservoirs of ESNs. This approach works well only with experience and a good intuitive grasp on nonlinear dynamics. A systematic [gradient descent method](https://www.sciencedirect.com/topics/computer-science/gradient-descent-method "Learn more about gradient descent method from ScienceDirect's AI-generated Topic Pages") of optimizing the global parameters of LI-ESNs (recalled from Section [5.5](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.5)) to fit them to a given task is presented in [\[18\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18). The investigation shows that the error surfaces in the combined global parameter and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ spaces may have very high curvature and multiple local minima. Thus, gradient descent methods are not always practical.

### 7.2. Evolutionary methods

As one can see from the previous sections of this review, optimizing reservoirs is generally challenging, and breakthrough methods remain to be found. On the other hand, checking the performance of a resulting ESN is relatively inexpensive, as said. This brings in *evolutionary* methods for the reservoir pre-training as a natural strategy.

Recall that the classical method generates a reservoir randomly; thus the performance of the resulting ESN varies slightly (and for small reservoirs not so slightly) from one instance to another. Then indeed, an “evolutionary” method as naive as “generate $<math><mi is="true">k</mi></math>$ reservoirs, pick the best” will outperform the classical method (“generate a reservoir”) with probability $<math><mrow is="true"><mo is="true">(</mo><mi is="true">k</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">/</mo><mi is="true">k</mi></math>$, even though the improvement might be not striking.

Several evolutionary approaches on optimizing reservoirs of ESNs are presented in [\[121\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b121). The first approach was to carry out an evolutionary search on the parameters for generating $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$: $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></math>$, $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$, and the connection density of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$. Then an [evolutionary algorithm](https://www.sciencedirect.com/topics/computer-science/evolutionary-algorithm "Learn more about evolutionary algorithm from ScienceDirect's AI-generated Topic Pages") [\[122\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b122) was used on individuals consisting of all the weight matrices $<math><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub><mo is="true">)</mo></mrow></math>$ of small ($<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub><mo is="true">=</mo><mn is="true">5</mn></math>$) reservoirs. A variant with a reduced search space was also tried where the weights, but not the topology, of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ were explored, i.e., elements of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ that were zero initially always stayed zero. The empirical results of modeling the motion of an underwater robot showed superiority of the methods over other state-of-art methods, and that the topology-restricted adaptation of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ is almost as effective as the full one.

Another approach of optimizing the reservoir $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ by a greedy evolutionary search is presented in [\[75\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b75). Here the same idea of separating the topology and weight sizes of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ to reduce the search space was independently used, but the search was, conversely, restricted to the connection topology. This approach also was demonstrated to yield on average 50% smaller (and much more stable) error in predicting the behavior of a mass–spring–damper system with small ($<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub><mo is="true">=</mo><mn is="true">20</mn></math>$) reservoirs than without the genetic optimization.

Yet another way of reducing the search space of the reservoir parameters is constructing a big reservoir weight matrix $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$ in a fractal fashion by repeatedly applying *Kronecker* self-multiplication to an initial small matrix, called the *Kronecker kernel* [\[123\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b123). This contribution showed that among $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$s constructed in this way some yield ESN performance similar to the best unconstrained $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$s; thus only the good weights of the small Kronecker kernel need to be found by evolutionary search for producing a well-performing reservoir.

Evolino [\[46\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b46), introduced in Section [3.3](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.3), is another example of adapting a reservoir (in this case an LSTM network) using a genetic search.

It has been recently demonstrated that by adapting only the slopes of the reservoir unit [activation functions](https://www.sciencedirect.com/topics/computer-science/activation-function "Learn more about activation functions from ScienceDirect's AI-generated Topic Pages") $<math><mi is="true">f</mi><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ by a state-of-art evolutionary algorithm, and having $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ random and fixed, a prediction performance of an ESN can be achieved close to the best of classical ESNs [\[68\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b68).

In addition to (or instead of) adapting the reservoirs, an evolutionary search can also be applied in training the readouts, such as readouts with no explicit $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, as discussed in Section [8.4](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.4).

### 7.3. Other types of supervised reservoir tuning

A greedy pruning of neurons from a big reservoir has been shown in a recent initial attempt [\[124\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b124) to often give a (bit) better classification performance for the same final $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></math>$ than just a randomly created reservoir of the same size. The effect of neuron removal to the reservoir dynamics, however, has not been addressed yet.

### 7.4. Trained auxiliary feedbacks

While reservoirs have a natural capability of performing complex real-time analog computations with fading memory [\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11), an analytical investigation has shown that they can approximate any $<math><mi is="true">k</mi></math>$\-order differential equation (with persistent memory) if extended with $<math><mi is="true">k</mi></math>$ trained feedbacks [\[21\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b21), [\[125\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b125). This is equivalent to simulating any [Turing machine](https://www.sciencedirect.com/topics/computer-science/turing-machines "Learn more about Turing machine from ScienceDirect's AI-generated Topic Pages"), and thus also means universal digital computing. In the presence of noise (or finite precision) the memory becomes limited in such models, but they still can simulate Turing machines with finite tapes.

This theory has direct implications for reservoir computing; thus different ideas on how the power of ESNs could be improved along its lines are explored in [\[78\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78). It is done by defining auxiliary targets, training additional outputs of ESNs on these targets, and feeding the outputs back to the reservoir. Note that this can be implemented in the usual model with feedback connections [(6)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd6) by extending the original output $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ with additional dimensions that are trained before training the original (final) output. The auxiliary targets are constructed from $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ and/or $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ or some additional knowledge of the modeled process. The intuition is that the feedbacks could shift the internal dynamics of $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ in the directions that would make them better linearly combinable into $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$. The investigation showed that for some types of tasks there are natural candidates for such auxiliary targets, which improve the performance significantly. Unfortunately, no universally applicable methods for producing auxiliary targets are known such that the targets would be both easy to learn and improve the accuracy of the final output $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$. In addition, training multiple outputs with feedback connections $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub></math>$ makes the whole procedure more complicated, as cyclical dependences between the trained outputs (one must take care of the order in which the outputs are trained) as well as stability issues discussed in Section [8.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.2) arise. Despite these obstacles, we perceive this line of research as having a big potential.

### 7.5. Reinforcement learning

In the line of biologically inspired local unsupervised adaptation methods discussed in Section [6.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2), an STDP modulated by a reinforcement signal has recently emerged as a powerful learning mechanism, capable of explaining some famous findings in neuroscience (biofeedback in monkeys), as demonstrated in [\[126\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b126), [\[127\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b127) and references thereof. The learning mechanism is also well biologically motivated as it uses a local unsupervised STDP rule and a reinforcement (i.e., reward) feedback, which is present in biological brains in a form of chemical signaling, e.g., by the level of dopamine. In the RC framework this learning rule has been successfully applied for training readouts from the reservoirs so far in [\[127\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b127), but could in principle be applied inside the reservoir too.

Overall the authors of this review believe that reinforcement learning methods are natural candidates for reservoir adaptation, as they can immediately exploit the knowledge of how well the output is learned inside the reservoir without the problems of error [backpropagation](https://www.sciencedirect.com/topics/computer-science/backpropagation "Learn more about backpropagation from ScienceDirect's AI-generated Topic Pages"). They can also be used in settings where no explicit target $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is available. We expect to see more applications of reinforcement learning in reservoir computing in the future.

## 8\. Readouts from the reservoirs

Conceptually, training a readout from a reservoir is a common supervised non-temporal task of mapping $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ to $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$. This is a well investigated domain in [machine learning](https://www.sciencedirect.com/topics/computer-science/machine-learning "Learn more about machine learning from ScienceDirect's AI-generated Topic Pages"), much more so than learning temporal mappings with memory. A large choice of methods is available, and in principle any of them can be applied. Thus we will only briefly go through the ones reported to be successful in the literature.

### 8.1. Single-layer readout

By far the most popular readout method from the ESN reservoirs is the originally proposed [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12) simple linear readout, as in [(3)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd3) (we will consider it as equivalent to [(8)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd8), i.e., $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ being part of $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$). It is shown to be often sufficient, as reservoirs provide a rich enough pool of signals for solving many application-relevant and benchmark tasks, and is very efficient to train, since optimal solutions can be found analytically.

#### 8.1.1. Linear regression

In batch mode, learning of the output weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$[(2)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2) can be phrased as solving a system of [linear equations](https://www.sciencedirect.com/topics/computer-science/linear-equations "Learn more about linear equations from ScienceDirect's AI-generated Topic Pages") (12)$<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><mo is="true">=</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">Y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub></math>$ with respect to $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$, where $<math><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub><mo is="true">×</mo><mi is="true">T</mi></mrow></msup></math>$ are all $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ produced by presenting the reservoir with $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">Y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub><mo is="true">×</mo><mi is="true">T</mi></mrow></msup></math>$ are all $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, both collected into respective matrices over the training period $<math><mi is="true">n</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mi is="true">T</mi></math>$. Usually $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ data from the beginning of the training run are discarded (they come before $<math><mi is="true">n</mi><mo is="true">=</mo><mn is="true">1</mn></math>$), since they are contaminated by initial transients.

Since typically the goal is minimizing a quadratic error $<math><mi is="true">E</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">Y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><mo is="true">)</mo></mrow></math>$ as in [(1)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd1) and $<math><mi is="true">T</mi><mo is="true">&gt;</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub></math>$, to solve [(12)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd12) one usually employs methods for finding *least square* solutions of *overdetermined* systems of linear equations (e.g., [\[128\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b128)), the problem also known as *linear regression*. One direct method is calculating the Moore–Penrose pseudoinverse $<math><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><mo is="true">+</mo></mrow></msup></math>$ of $<math><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></math>$, and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ as (13)$<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mo is="true">=</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">Y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><mo is="true">+</mo></mrow></msup><mtext is="true">.</mtext></math>$ Direct pseudoinverse calculations exhibit high numerical stability, but are expensive memory-wise for large state-collecting matrices $<math><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub><mo is="true">×</mo><mi is="true">T</mi></mrow></msup></math>$, thereby limiting the size of the reservoir $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub></math>$ and/or the number of training samples $<math><mi is="true">T</mi></math>$.

This issue is resolved in the *normal equations* formulation of the problem[5](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fn5): (14)$<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup><mo is="true">=</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">Y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup><mtext is="true">.</mtext></math>$ A naive solution of it would be (15)$<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mo is="true">=</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">Y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup><mtext is="true">.</mtext></math>$ Note that in this case $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">Y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub></mrow></msup></math>$ and $<math><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub></mrow></msup></math>$ do not depend on the length $<math><mi is="true">T</mi></math>$ of the training sequence, and can be calculated incrementally while the training data are passed through the reservoir. Thus, having these two matrices collected, the solution complexity of [(15)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd15) does not depend on $<math><mi is="true">T</mi></math>$ either in time or in space. Also, intermediate values of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ can be calculated in the middle of running through the training data, e.g., for an early assessment of the performance, making this a “semi-online” training method.

The method [(15)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd15) has lower numerical stability, compared to [(13)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd13), but the problem can be mitigated by using the pseudoinverse $<math><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">+</mo></mrow></msup></math>$ instead of the real inverse $<math><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup></math>$ (which usually also works faster). In addition, this method enables one to introduce *ridge*, or *Tikhonov*, [regularization](https://www.sciencedirect.com/topics/computer-science/regularization "Learn more about regularization from ScienceDirect's AI-generated Topic Pages") elegantly: (16)$<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mo is="true">=</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">Y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup><msup is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup><mo is="true">+</mo><msup is="true"><mrow is="true"><mi is="true">α</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mstyle mathvariant="bold" is="true"><mi is="true">I</mi></mstyle><mo is="true">)</mo></mrow></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup><mtext is="true">,</mtext></math>$ where $<math><mstyle mathvariant="bold" is="true"><mi is="true">I</mi></mstyle><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub></mrow></msup></math>$ is the [identity matrix](https://www.sciencedirect.com/topics/computer-science/identity-matrix "Learn more about identity matrix from ScienceDirect's AI-generated Topic Pages") and $<math><mi is="true">α</mi></math>$ is a regularization factor. In addition to improving the numerical stability, the regularization in effect reduces the magnitudes of entries in $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$, thus mitigating sensitivity to noise and overfitting; see Section [8.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.2) for more details. All this makes [(16)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd16) a highly recommendable choice for learning outputs from the reservoirs.

Another alternative for solving [(14)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd14) is decomposing the matrix $<math><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup></math>$ into a product of two [triangular matrices](https://www.sciencedirect.com/topics/computer-science/triangular-matrix "Learn more about triangular matrices from ScienceDirect's AI-generated Topic Pages") via Cholesky or LU decomposition, and solving [(14)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd14) by two steps of substitution, avoiding (pseudo-)inverses completely. The [Cholesky decomposition](https://www.sciencedirect.com/topics/computer-science/cholesky-decomposition "Learn more about Cholesky decomposition from ScienceDirect's AI-generated Topic Pages") is the more numerically stable of the two.

*Weighted* regression can be used for training linear readouts by multiplying both $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ and the corresponding $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ by different weights over time, thus emphasizing some time steps $<math><mi is="true">n</mi></math>$ over others. Multiplying certain recorded $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ and corresponding $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ by $<math><msqrt is="true"><mrow is="true"><mi is="true">k</mi></mrow></msqrt></math>$ has the same emphasizing effect as if they appeared in the training sequence $<math><mi is="true">k</mi></math>$ times.

When the reservoir is made from spiking neurons and thus $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ becomes a collection of spike trains, smoothing by low-pass filtering may be applied to it before doing the linear regression, or it can be done directly on $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ [\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11). For more on linear regression based on spike train data, see [\[129\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b129).

Evolutionary search for training linear readouts can also be employed. State-of-art evolutionary methods are demonstrated to be able to achieve the same record levels of precision for supervised tasks as with the best applications of linear regression in ESN training [\[68\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b68). Their much higher computational cost is justifiable in settings where no explicit $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is available, discussed in Section [8.4](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.4).

#### 8.1.2. Online adaptive output weight training

Some applications require online model adaptation, e.g., in online adaptive [channel equalization](https://www.sciencedirect.com/topics/computer-science/channel-equalization "Learn more about channel equalization from ScienceDirect's AI-generated Topic Pages") [\[17\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17). In such cases one typically minimizes an error that is exponentially discounted going back in time. $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ here acts as an adaptive [linear combiner](https://www.sciencedirect.com/topics/computer-science/linear-combiner "Learn more about linear combiner from ScienceDirect's AI-generated Topic Pages"). The simplest way to train $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ is to use stochastic [gradient descent](https://www.sciencedirect.com/topics/computer-science/gradient-descent "Learn more about gradient descent from ScienceDirect's AI-generated Topic Pages"). The method is familiar as the *Least Mean Squares* (LMS) algorithm in linear signal processing [\[91\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b91), and has many extensions and modifications. Its convergence performance is unfortunately severely impaired by [large eigenvalue](https://www.sciencedirect.com/topics/computer-science/largest-eigenvalue "Learn more about large eigenvalue from ScienceDirect's AI-generated Topic Pages") spreads of $<math><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup></math>$, as mentioned in Section [6.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.1).

An alternative to LMS, known in linear signal processing as the *Recursive Least Squares* (RLS) algorithm, is insensitive to the detrimental effects of eigenvalue spread and boasts a much faster convergence because it is a second-order method. The downside is that RLS is computationally more expensive (order $<math><mi is="true">O</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">)</mo></mrow></math>$ per time step instead of $<math><mi is="true">O</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$ for LMS, for $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">y</mtext></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></math>$) and notorious for numerical stability issues. Demonstrations of RLS are presented in [\[17\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17), [\[43\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b43). A careful and comprehensive comparison of variants of RLS is carried out in a Master’s thesis [\[130\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b130), which we mention here because it will be helpful for practitioners.

The BackPropagation-DeCorrelation (BPDC) algorithm discussed in Section [3.4](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec3.4) is another powerful method for online training of single-layer readouts with feedback connections from the reservoirs.

Simple forms of adaptive online learning, such as LMS, are also more biologically plausible than batch-mode training. From spiking neurons a firing time-coded (instead of a more common firing rate-coded) output for [classification](https://www.sciencedirect.com/topics/computer-science/classification "Learn more about classification from ScienceDirect's AI-generated Topic Pages") can also be trained by only adapting the delays of the output connections [\[101\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b101). And firing rate-coded readouts can be trained by a biologically-realistic reward-modulated STDP [\[127\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b127), mentioned in Section [6.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2).

#### 8.1.3. SVM-style readout

Continuing the analogy between the temporal and non-temporal expansion methods, discussed in Section [2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2), the reservoir can be considered a temporal kernel, and the standard linear readout $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ from it can be trained using the same loss functions and regularizations as in Support Vector Machines (SVMs) or [Support Vector Regression](https://www.sciencedirect.com/topics/computer-science/support-vector-regression "Learn more about Support Vector Regression from ScienceDirect's AI-generated Topic Pages") (SVR). Different versions of this approach are proposed and investigated in [\[131\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b131).

A standard [SVM](https://www.sciencedirect.com/topics/computer-science/support-vector-machine "Learn more about SVM from ScienceDirect's AI-generated Topic Pages") (having its own kernel) can also be used as a readout from a continuous-value reservoir [\[132\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b132). Similarly, special kernel types could be applied in reading out from spiking (LSM-type) reservoirs [\[133\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b133) (and references therein).

### 8.2. Feedbacks and stability issues

Stability issues (with reservoirs having the echo state property) usually only occur in *generative* setups where a model trained on (one step) signal prediction is later run in a generative mode, looping its output $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ back into the input as $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></math>$. Note that this is equivalent to a model with output feedbacks $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub></math>$[(6)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd6) and no input at all ($<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">u</mtext></mrow></msub><mo is="true">=</mo><mn is="true">0</mn></math>$), which is usually trained using *teacher forcing* (i.e., feeding $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ as $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ for the feedbacks during the training run) and later is run freely to generate signals as $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$. $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$ in the first case is equivalent to $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub></math>$ in the second one. Models having feedbacks $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub></math>$ may also suffer from instability while driven with external input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, i.e., not in a purely generative mode.

The reason for these instabilities is that even if the model can predict the signal quite accurately, going through the feedback loop of connections $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub></math>$ (or $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub></math>$) small errors get amplified, making $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ diverge from the intended $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$.

One way to look at this for trained linear outputs is to consider the feedback loop connections $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ and $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub></math>$ as part of the reservoir $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></math>$. Putting [(6)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd6), [(2)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd2) together we get (17)$<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mi is="true">f</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">in</mtext></mrow></msub><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><mrow is="true"><mo is="true">[</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub><mo is="true">]</mo></mrow><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mtext is="true">,</mtext></math>$ where $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">ofb</mtext></mrow></msub><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ forms the “extended reservoir” connections, which we will call $<math><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></mrow><mrow is="true"><mo is="true">∗</mo></mrow></msup></math>$ for brevity (as in [\[78\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78) Section 3.2). If the [spectral radius](https://www.sciencedirect.com/topics/computer-science/spectral-radius "Learn more about spectral radius from ScienceDirect's AI-generated Topic Pages") of the extended reservoir $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></mrow><mrow is="true"><mo is="true">∗</mo></mrow></msup><mo is="true">)</mo></mrow></math>$ is very large we can expect unstable behavior. A more detailed analysis using Laplace transformations and a sufficient condition for stability is presented in [\[134\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b134). On the other hand, for purely generative tasks, $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub></mrow><mrow is="true"><mo is="true">∗</mo></mrow></msup><mo is="true">)</mo></mrow><mo is="true">&lt;</mo><mn is="true">1</mn></math>$ would mean that the generated signal would die out, which is not desirable in most cases. Thus producing a generator with stable dynamics is often not trivial.

Quite generally, models trained with clean (noise-free) data for the best one-time-step prediction diverge fast in the generative mode, as they are too “sharp” and not noise-robust. A classical remedy is adding some noise to reservoir states $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ [\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b12) during the training. This way the generator forms a stable attractor by learning how to come to the desired next output $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ from a neighborhood of the current state $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, having seen it perturbed by noise during training. Setting the right amount of noise is a delicate balance between the sharpness (of the prediction) and the stability of the generator. Alternatively, adding noise to $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ can be seen as a form of regularization in training, as it in effect also emphasizes the diagonal of matrix $<math><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></mrow><mrow is="true"><msup is="true"><mrow is="true"></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">T</mi></mstyle></mrow></msup></mrow></msup></math>$ in [(16)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd16). A similar effect can be achieved using ridge regression [(16)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd16) [\[135\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b135), or to some extent even pruning of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ [\[136\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b136). Ridge regression [(16)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd16) is the least computationally expensive to do of the three, since the reservoir does not need to be rerun with the data to test different values of the regularization factor $<math><mi is="true">α</mi></math>$.

Using different modifications of signals for teacher forcing, like mixing $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ with noise, or in some cases using pure strong noise, during the training also has an effect on the final performance and stability, as discussed in Section 5.4 of [\[78\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78).

### 8.3. Readouts for classification/recognition

The time series classification or temporal pattern detection tasks that need a category indicator (as opposed to real values) as an output can be implemented in two main ways. The most common and straightforward way is having a real-valued output for each class (or a single output and a threshold for the two-class classifier), and interpreting the strengths of the outputs as votes for the corresponding classes, or even class probabilities (several options are discussed in [\[18\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18)). Often the most probable class is taken as the decision. A simple target $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub></math>$ for this approach is a constant $<math><msub is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mn is="true">1</mn></math>$ signal for the right class $<math><mi is="true">i</mi></math>$ and 0 for the others in the range of $<math><mi is="true">n</mi></math>$ where the indicating output is expected. More elaborate shapes of $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ can improve classification performance, depending on the task (e.g., [\[81\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b81)). With spiking neurons the direct classification based on time coding can be learned and done, e.g., the class is assigned depending on which output fires first [\[101\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b101).

The main alternative to direct class indications is to use predictive classifiers, i.e., train different predictors to predict different classes and assign a class to a new example corresponding to the predictor that predicts it best. Here the quality of each predictor serves as the output strength for the corresponding class. The method is quite popular in automated speech recognition (e.g., Section 6 in [\[137\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b137) for an overview). However, in Section 6.5 of [\[137\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b137) the author argues against this approach, at least in its straightforward form, pointing out some weaknesses, like the lack of specificity, and negative practical experience.

For both approaches a weighting scheme can be used for both training (like in weighted regression) and integrating the class votes, e.g., putting more emphasis on the end of the pattern when sufficient information has reached the [classifier](https://www.sciencedirect.com/topics/computer-science/classification-machine-learning "Learn more about classifier from ScienceDirect's AI-generated Topic Pages") to make the decision.

An advanced version of ESN-based predictive classifier, where for each class there is a set of competitively trained predictors and [dynamic programming](https://www.sciencedirect.com/topics/computer-science/dynamic-programming "Learn more about dynamic programming from ScienceDirect's AI-generated Topic Pages") is used to find the optimal sequence of them, is reported to be much more noise robust than a standard Hidden Markov Model in spoken word recognition [\[138\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b138).

### 8.4. Readouts beyond supervised learning

Even though most of the readout types from reservoirs reported in the literature are trained in a purely supervised manner, i.e., making $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ match an explicitly given $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, the reservoir computing paradigm lends itself to settings where no $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is available. A typical such setting is *[reinforcement learning](https://www.sciencedirect.com/topics/computer-science/reinforcement-learning "Learn more about reinforcement learning from ScienceDirect's AI-generated Topic Pages")* where only a feedback on the model’s performance is available. Note that an explicit $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ is not required for the reservoir adaptation methods discussed in Sections [5 Generic reservoir recipes](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5), [6 Unsupervised reservoir adaptation](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6) of this survey by definition. Even most of the adaptation methods classified as *supervised* in Section [7](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7) do not need an explicit $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, as long as one can evaluate the performance of the reservoir. Thus they can be used without modification, provided that unsupervised training and evaluation of the output is not prohibitively expensive or can be done simultaneously with reservoir adaptation. In this section we will give some pointers on training readouts using reinforcement learning.

A biologically inspired learning rule of Spike-Time-Dependent Plasticity (STDP) modulated by a reinforcement signal has been successfully applied for training a readout of firing neurons from the reservoirs of the same LSTM-type in [\[127\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b127).

[Evolutionary algorithms](https://www.sciencedirect.com/topics/computer-science/evolutionary-algorithm "Learn more about Evolutionary algorithms from ScienceDirect's AI-generated Topic Pages") are a natural candidate for training outputs in a non-supervised manner. Using a genetic search with crossover and mutation to find optimal output weights $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ of an ESN is reported in [\[139\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b139). Such an ESN is successfully applied for a hard reinforcement learning task of direct [adaptive control](https://www.sciencedirect.com/topics/computer-science/adaptive-control-systems "Learn more about adaptive control from ScienceDirect's AI-generated Topic Pages"), replacing a classical indirect controller.

ESNs trained with a simple “ (1+1)” evolution strategy for an unsupervised artificial embryogeny (the so-called “flag”) problem are shown to perform very well in [\[140\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b140).

An ESN trained with a state-of-art evolutionary continuous parameter optimization method (CMA-ES) shows comparable performance in a benchmark double pole balancing problem to the best [RNN](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about RNN from ScienceDirect's AI-generated Topic Pages") topology-learning methods in [\[68\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b68), [\[141\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b141). For this problem the best results are obtained when the spectral radius $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$ is adapted together with $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$. The same contributions also validate the CMA-ES readout training method on a standard supervised prediction task, achieving the same excellent precision (MSE of the order $<math><mn is="true">1</mn><msup is="true"><mrow is="true"><mn is="true">0</mn></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">15</mn></mrow></msup></math>$) as the state-of-art with linear regression. Conversely, the best results for this task were achieved with $<math><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"></mrow></msub><mo is="true">)</mo></mrow></math>$ fixed and training only $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$. An even more curious finding is that almost as good results were achieved by only adapting slopes of the reservoir [activation functions](https://www.sciencedirect.com/topics/computer-science/activation-function "Learn more about activation functions from ScienceDirect's AI-generated Topic Pages") $<math><mi is="true">f</mi><mrow is="true"><mo is="true">(</mo><mo is="true">⋅</mo><mo is="true">)</mo></mrow></math>$ and having $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ fixed, as mentioned in Section [7.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec7.2).

### 8.5. Multilayer readouts

[Multilayer perceptrons](https://www.sciencedirect.com/topics/computer-science/multilayer-perceptron "Learn more about Multilayer perceptrons from ScienceDirect's AI-generated Topic Pages") (MLPs) as readouts, trained by error backpropagation, were used from the very beginnings of LSMs [\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b11) and ESNs (unpublished). They are theoretically more powerful and expressive in their instantaneous mappings from $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ to $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ than linear readouts, and are thus suitable for particularly nonlinear outputs, e.g., in [\[142\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b142), [\[143\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b143). In both cases the MLP readouts are trained by error backpropagation. On the other hand they are significantly harder to train than an optimal single-layer linear regression, thus often giving inferior results compared to the latter in practice.

Some experience in training MLPs as ESN readouts, including network initialization, using stochastic, batch, and semi-batch gradients, adapting learning rates, and combining with regression-training of the last layer of the MLP, is presented in Section 5.3 of [\[78\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b78).

### 8.6. Readouts with delays

While the readouts from reservoirs are usually recurrence-free, this does not mean that they may not have memory. In some approaches they do, or rather some memory is inserted between the reservoir and the readout.

Learning a delay for each neuron in an ESN reservoir $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ in addition to the output weight from it is investigated in [\[84\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b84). Cross-correlation (simple or generalized) is used to optimally align activations of each neuron in $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ with $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$, and then activations with the delays $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></mrow><mrow is="true"><mtext is="true">delayed</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ are used to find $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ in a usual way. This approach potentially enables utilizing the computational power of the reservoir more efficiently. In a time-coded output from a spiking reservoir the output connection delays can actually be the only thing that is learned [\[101\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b101).

For time series [classification tasks](https://www.sciencedirect.com/topics/computer-science/classification-task "Learn more about classification tasks from ScienceDirect's AI-generated Topic Pages") the decision can be based on a readout from a joined reservoir state $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></mrow><mrow is="true"><mtext is="true">joined</mtext></mrow></msub><mo is="true">=</mo><mrow is="true"><mo is="true">[</mo><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><mo is="true">)</mo></mrow><mo is="true">]</mo></mrow></math>$ that is a concatenation of the reservoir states from different moments $<math><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub></math>$ in time during the time series [\[18\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18). This approach, compared to only using the last state of the given time series, moves the emphasis away from the ending of the series, depending on how the support times $<math><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math>$ are spread. It is also more expressive, since it has $<math><mi is="true">k</mi></math>$ times more trainable parameters in $<math><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">W</mi></mstyle></mrow><mrow is="true"><mtext is="true">out</mtext></mrow></msub></math>$ for the same size of the reservoir $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub></math>$. As a consequence, it is also more prone to overfitting. It is also possible to integrate intervals of states in some way, e.g., use $<math><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></mrow><mrow is="true"><mo is="true">∗</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo is="true">+</mo><mn is="true">1</mn></mrow></mfrac><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">m</mi><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></msubsup><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">m</mi><mo is="true">)</mo></mrow></math>$ instead of using a single snapshot of the states $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">)</mo></mrow></math>$.

An approach of treating a finite history of reservoir activations $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ (similar to $<math><mstyle mathvariant="bold" is="true"><mi is="true">X</mi></mstyle></math>$ in [(12)](https://www.sciencedirect.com/science/article/pii/S1574013709000173#fd12)) as a two-dimensional image, and training a minimum average correlations energy filter as the readout for dynamical pattern recognition is presented in [\[144\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b144).

Even though in Section [1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec1) we stated that the RNNs considered in this survey are used as nonlinear filters, which transform an input time series into an output time series, ESNs can also be utilized for non-temporal (defined in Section [2.1](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec2.1)) tasks $<math><mrow is="true"><mo is="true">{</mo><mrow is="true"><mo is="true">(</mo><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msub is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle></mrow><mrow is="true"><mtext is="true">target</mtext></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">}</mo></mrow></math>$ by presenting an ESN with the same input $<math><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ for many time steps, letting the ESN converge to a fixed-point [attractor state](https://www.sciencedirect.com/topics/computer-science/attractor-state "Learn more about attractor state from ScienceDirect's AI-generated Topic Pages") $<math><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></mrow><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">∞</mi><mo is="true">)</mo></mrow></math>$ (which it does if it possesses echo state property) and reading the output from the attractor state $<math><mstyle mathvariant="bold" is="true"><mi is="true">y</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mi is="true">y</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle></mrow><mrow is="true"><mstyle mathvariant="bold" is="true"><mi is="true">u</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">∞</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></math>$ [\[145\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b145), [\[146\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b146).

### 8.7. Combining several readouts

Segmenting of the spatially embedded trajectory of $<math><mstyle mathvariant="bold" is="true"><mi is="true">x</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></math>$ by $<math><mi is="true">k</mi></math>$\-means clustering and assigning a separate “responsible” linear readout for each cluster is investigated in [\[147\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b147). This approach increases the expressiveness of the ESN by having $<math><mi is="true">k</mi></math>$ linear readouts trained and an online switching mechanism among them. Bigger values of $<math><mi is="true">k</mi></math>$ are shown to compensate for smaller sizes $<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mtext is="true">x</mtext></mrow></msub></math>$ of the reservoirs to get the same level of performance.

A benchmark-record-breaking approach of taking an average of outputs from many (1000) different instances of tiny ($<math><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"></mrow></msub><mo is="true">=</mo><mn is="true">4</mn></math>$) trained ESNs is presented in Section 5.2.2 of [\[18\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b18). The approach is also combined with reading from different support times as discussed in Section [8.6](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.6) of this survey. Averaging outputs over 20 instances of ESNs was also shown to refine the prediction of chaotic time series in supporting online material of [\[17\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b17).

Using dynamic programing to find sequences in multiple sets of predicting readouts for classification [\[138\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b138) was already mentioned at the end of Section [8.3](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.3).

### 8.8. Hierarchies

Following the analogy between the ESNs and non-temporal [kernel methods](https://www.sciencedirect.com/topics/computer-science/kernel-method "Learn more about kernel methods from ScienceDirect's AI-generated Topic Pages"), ESNs would be called “type-1 shallow architectures” according to the classification proposed in [\[148\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b148). The reservoir adaptation techniques reviewed in our article would make ESNs “type-3 shallow architectures”, which are more expressive. However, the authors in [\[148\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b148) argue that any type of *shallow* (i.e., non-hierarchical) architectures is incapable of learning really complex intelligent tasks. This suggests that for demandingly complex tasks the adaptation of a single reservoir might not be enough and a hierarchical architecture of ESNs might be needed, e.g., such as presented in [\[149\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b149). Here the outputs of a higher level in the hierarchy serve as coefficients of mixing (or voting on) outputs from a lower one. The structure can have an arbitrary number of layers. Only the outputs from the reservoirs of each layer are trained simultaneously, using stochastic gradient descent and error backpropagation through the layers. The structure is demonstrated to discover features on different timescales in an unsupervised way when being trained for predicting a synthetic time series of interchanging generators. On the downside, such hierarchies require many epochs to train, and suffer from a similar problem of vanishing gradients, as deep [feedforward neural networks](https://www.sciencedirect.com/topics/computer-science/feedforward-neural-network "Learn more about feedforward neural networks from ScienceDirect's AI-generated Topic Pages") or gradient-descent methods for fully trained RNNs. They also do not scale-up yet to real-world demanding problems. Research on hierarchically structured RC models has only just begun.

## 9\. Discussion

The striking success of the original RC methods in outperforming fully trained [RNNs](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about RNNs from ScienceDirect's AI-generated Topic Pages") in many (though not all) tasks, established an important milestone, or even a turning point, in the research of RNN training. The fact that a randomly generated fixed RNN with only a linear readout trained consistently outperforms state-of-art RNN training methods had several consequences:

•

First of all it revealed that we do not really know how to train RNNs well, and something new is needed. The error [backpropagation](https://www.sciencedirect.com/topics/computer-science/backpropagation "Learn more about backpropagation from ScienceDirect's AI-generated Topic Pages") methods, which had caused a breakthrough in feedforward [neural network training](https://www.sciencedirect.com/topics/computer-science/neural-network-training "Learn more about RNN training from ScienceDirect's AI-generated Topic Pages") (up to a certain depth), and had also become the most popular training methods for RNNs, are hardly unleashing their full potential.

•

Neither are the classical RC methods yet exploiting the full potential of RNNs, since they use a random RNN, which is unlikely to be optimal, and a linear readout, which is quite limited by the quality of the signals it is combining. But they give a quite tough performance reference for more sophisticated methods.

•

The separation between the RNN reservoir and the readout provides a good *platform* to try out all kinds of RNN adaptation methods in the reservoir and see how much they can actually improve the performance over randomly created RNNs. This is particularly well suited for testing various biology-inspired RNN adaptation mechanisms, which are almost exclusively local and unsupervised, in how they can improve learning of a supervised task.

•

In parallel, it enables all types of powerful non-temporal methods to be applied for reading out of the reservoir.

This platform is the current *paradigm of RC*: using different methods for **(i)** producing/adapting the reservoir, and **(ii)** training different types of readouts. It enables looking for good (i) and (ii) methods independently, and combining the best practices from both research directions. The platform has been actively used by many researchers, ever since the first ESNs and LSMs appeared. This research in both (i) and (ii) directions, together with theoretical insights, like what characterizes a “good” reservoir, constitutes the modern field of RC.

In this review, together with motivating the new paradigm, we have provided a comprehensive survey of all this RC research. We introduced a natural taxonomy of the reservoir generation/adaptation techniques (i) with three big classes of methods (generic, unsupervised, and supervised), depending on their universality with respect to the input and desired output of the task. Inside each class, methods are also grouped into major directions of approaches, taking different inspirations. We also surveyed all types of readouts from the reservoirs (ii) reported in the literature, including the ones containing several layers of [nonlinearities](https://www.sciencedirect.com/topics/computer-science/nonlinearities "Learn more about nonlinearities from ScienceDirect's AI-generated Topic Pages"), combining several time steps, or several reservoirs, among others. We also briefly discussed some practical issues of training the most popular types of readouts in a tutorial-like fashion.

The survey is transcending the boundaries among several traditional methods that fall under the umbrella of RC, generalizing the results to the whole RC field and pointing out relations, where applicable.

Even though this review is quite extensive, we tried to keep it concise, outlining only the basic ideas of each contribution. We did not try to include *every* contribution relating to RC in this survey, but only the ones highlighting the main research directions. Publications only reporting applications of reservoir methods, but not proposing any interesting modifications of them, were left out. Since this review is aimed at a (fast) moving target, which RC is, some (especially very new) contributions might have been missed unintentionally.

In general, the RC field is still very young, but very active and quickly expanding. While the original first RC methods made an impact that could be called a small revolution, current RC research is more in a phase of a (rapid) evolution. The multiple new modifications of the original idea are gradually increasing the performance of the methods. While with no striking breakthroughs lately, the progress is steady, establishing some of the extensions as common practices to build on further. There are still many promising directions to be explored, hopefully leading to breakthroughs in the near future.

While the tasks for which RNNs are applied nowadays often are quite complex, hardly any of them could yet be called truly *intelligent*, as compared to the human level of intelligence. The fact that RC methods perform well in many of these simple tasks by no means indicates that there is little space left for their improvement. More complex tasks and adequate solutions are still to meet each other in RC. We further provide some of our (subjective, or even speculative) outlooks on the future of RC.

The elegant simplicity of the classical ESNs gives many benefits in these simple applications, but it also has some intrinsic limitations (as, for example, discussed in Section [5.4](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.4)) that must be overcome in some way or other. Since the RNN model is by itself biologically inspired, looking at real brains is a natural (literally) source of inspiration on how to do that. RC models may reasonably explain some aspects of how small portions of the brain work, but if we look at the bigger picture, the brain is far from being just a big blob of randomly connected neurons. It has a complex structure that is largely predefined before even starting to learn. In addition, there are many learning mechanisms observed in the real brain, as briefly outlined in Section [6.2](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec6.2). It is very probable that there is no single easily implementable underlying rule which can explain all learning.

The required complexity in the context of RC can be achieved in two basic ways: either **(i)** by giving the reservoir a more complex internal structure, like that discussed in Section [5.3](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec5.3) or **(ii)** externally building structures combining several reservoirs and readouts, like those discussed in Section [8.8](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.8). Note that the two ways correspond to the above-mentioned dichotomy of the RC research and are not mutually exclusive. An “externally” (ii) built structure can also be regarded as a single complex reservoir (i) and a readout from it all can be trained.

An internal auto-structuring of the reservoir (i) through an (unsupervised) training would be conceptually appealing and nature-like, but not yet quite feasible at the current state of knowledge. A robust realization of such a learning algorithm would signify a breakthrough in the generation/training of artificial NNs. Most probably such an approach would combine several competing learning mechanisms and goals, and require a careful parameter selection to balance them, and thus would not be easy to successfully apply. In addition, changing the structure of the RNN during the adaptive training would lead to bifurcations in the training process, as in [\[8\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b8), which makes learning very difficult.

Constructing external architectures or several reservoirs can be approached as more of an engineering task. The structures can be hand-crafted, based on the specifics of the application, and, in some cases, trained entirely supervised, each reservoir having a predefined function and a target signal for its readout. While such approaches are successfully being applied in practice, they are very case-specific, and not quite in the scope of the research reviewed here, since in essence they are just applications of (several instances of) the classical RC methods in bigger settings.

However, generic structures of multiple reservoirs (ii) that can be trained with no additional information, such as discussed in Section [8.8](https://www.sciencedirect.com/science/article/pii/S1574013709000173#sec8.8), are of high interest. Despite their current state being still an “embryo”, and the difficulties pointed out earlier, the authors of this review see this direction as highly promising.

Biological inspiration and progress of neuroscience in understanding how real brains work are beneficial for both (i) and (ii) approaches. Well understood natural principles of local neural adaptation and development can be relatively easily transfered to artificial reservoirs (i), and reservoirs internally structured to more closely resemble cortical microcolumns in the brain have been shown to perform better [\[23\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#b23). Understanding how different brain areas interact could also help in building external structures of reservoirs (ii) better suited for nature-like tasks.

In addition to processing and “understanding” multiple scales of time and abstraction in the data, which hierarchical models promise to solve, other features still lacking in the current RC (and overall RNN) methods include robustness and stability of pattern generation. A possible solution to this could be a homeostasis-like self-regulation in the RNNs. Other intelligence-tending features as selective longer-term memory or active attention are also not yet well incorporated.

In short, RC is not the end, but an important stepping-stone in the big journey of developing RNNs, ultimately leading towards building artificial and comprehending natural intelligence.

## Acknowledgments

This work is partially supported by Planet Intelligent Systems GmbH, a private company with an inspiring interest in fundamental research. The authors are also thankful to Benjamin Schrauwen, Michael Thon, and an anonymous reviewer of this journal for their helpful constructive feedback.

## References

[\[1\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb1)

John J. Hopfield

**Hopfield network**

Scholarpedia, 2 (5) (2007), p. 1977

[\[2\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb2)

John J. Hopfield

**Neural networks and physical systems with emergent collective computational abilities**

Proceedings of the National Academy of Sciences of the United States of America, 79 (1982), pp. 2554-2558

[\[3\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb3)

Geoffrey E. Hinton

**Boltzmann machine**

Scholarpedia, 2 (5) (2007), p. 1668

[\[4\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb4)

David H. Ackley, Geoffrey E. Hinton, Terrence J. Sejnowski

**A learning algorithm for Boltzmann machines**

Cognitive Science, 9 (1985), pp. 147-169

[\[5\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb5)

Geoffrey E. Hinton, Ruslan Salakhutdinov

**Reducing the dimensionality of data with neural networks**

Science, 313 (5786) (2006), pp. 504-507

[\[6\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb6)

Graham W. Taylor, Geoffrey E. Hinton, Sam Roweis

**Modeling human motion using binary latent variables**

Advances in Neural Information Processing Systems 19, NIPS 2006, MIT Press, Cambridge, MA (2007), pp. 1345-1352

[\[7\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb7)

Ken-ichi Funahashi, Yuichi Nakamura

**Approximation of dynamical systems by continuous time recurrent neural networks**

Neural Networks, 6 (1993), pp. 801-806

[\[8\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb8)

Kenji Doya, Bifurcations in the learning of recurrent neural networks, in: Proceedings of IEEE International Symposium on Circuits and Systems 1992, vol. 6, 1992, pp. 2777–2780

[\[9\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb9)

Yoshua Bengio, Patrice Simard, Paolo Frasconi

**Learning long-term dependencies with gradient descent is difficult**

IEEE Transactions on Neural Networks, 5 (2) (1994), pp. 157-166

[\[10\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb10)

Felix A. Gers, Jürgen Schmidhuber, Fred A. Cummins

**Learning to forget: Continual prediction with LSTM**

Neural Computation, 12 (10) (2000), pp. 2451-2471

[\[11\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb11)

Wolfgang Maass, Thomas Natschläger, Henry Markram

**Real-time computing without stable states: A new framework for neural computation based on perturbations**

Neural Computation, 14 (11) (2002), pp. 2531-2560

[\[12\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb12)

Herbert Jaeger, The “echo state” approach to analysing and training recurrent neural networks, Technical Report GMD Report 148, German National Research Center for Information Technology, 2001

[\[13\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb13)

Peter F. Dominey

**Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning**

Biological Cybernetics, 73 (1995), pp. 265-274

[\[14\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb14)

Jochen J. Steil, Backpropagation-decorrelation: Recurrent learning with O(N) complexity, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2004, IJCNN 2004, vol. 2, 2004, pp. 843–848

[\[15\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb15)

Herbert Jaeger, Wolfgang Maass, José C. Príncipe

**Special issue on echo state networks and liquid state machines — Editorial**

Neural Networks, 20 (3) (2007), pp. 287-289

[\[16\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb16)

Herbert Jaeger

**Echo state network**

Scholarpedia, 2 (9) (2007), p. 2330

[\[17\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb17)

Herbert Jaeger, Harald Haas

**Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication**

Science (2004), pp. 78-80

[\[18\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb18)

Herbert Jaeger, Mantas Lukoševičius, Dan Popovici, Udo Siewert

**Optimization and applications of echo state networks with leaky-integrator neurons**

Neural Networks, 20 (3) (2007), pp. 335-352

[\[19\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb19)

David Verstraeten, Benjamin Schrauwen, Dirk Stroobandt, Reservoir-based techniques for speech recognition, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2006, IJCNN 2006, 2006 pp. 1050–1053

[\[20\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb20)

Wolfgang Maass, Thomas Natschläger, Henry Markram

**A model for real-time computation in generic neural microcircuits**

Advances in Neural Information Processing Systems 15, NIPS 2002, MIT Press, Cambridge, MA (2003), pp. 213-220

[\[21\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb21)

Wolfgang Maass, Prashant Joshi, Eduardo D. Sontag

**Principles of real-time computing with feedback applied to cortical microcircuit models**

Advances in Neural Information Processing Systems 18, MIT Press, Cambridge, MA (2006), pp. 835-842

[\[22\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb22)

Dean V. Buonomano, Michael M. Merzenich

**Temporal information transformed into a spatial code by a neural network with realistic properties**

Science, 267 (1995), pp. 1028-1030

[\[23\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb23)

Stefan Haeusler, Wolfgang Maass

**A statistical analysis of information-processing properties of lamina-specific cortical microcircuit models**

Cerebral Cortex, 17 (1) (2007), pp. 149-162

[\[24\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb24)

Uma R. Karmarkar, Dean V. Buonomano

**Timing in the absence of clocks: Encoding time in neural network states**

Neuron, 53 (3) (2007), pp. 427-438

[\[25\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb25)

Garrett B. Stanley, Fei F. Li, Yang Dan

**Reconstruction of natural scenes from ensemble responses in the lateral genicualate nucleus**

Journal of Neuroscience, 19 (18) (1999), pp. 8036-8042

[\[26\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb26)

Danko Nikolić, Stefan Haeusler, Wolf Singer, Wolfgang Maass

**Temporal dynamics of information content carried by neurons in the primary visual cortex**

Advances in Neural Information Processing Systems 19, NIPS 2006, MIT Press, Cambridge, MA (2007), pp. 1041-1048

[\[27\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb27)

Werner M. Kistler, Chris I. De Zeeuw

**Dynamical working memory and timed responses: The role of reverberating loops in the olivo-cerebellar system**

Neural Computation, 14 (2002), pp. 2597-2626

[\[28\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb28)

Tadashi Yamazaki, Shigeru Tanaka

**The cerebellum as a liquid state machine**

Neural Networks, 20 (3) (2007), pp. 290-297

[\[29\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb29)

Peter F. Dominey, Michel Hoen, Jean-Marc Blanc, Taïssia Lelekov-Boissard

**Neurological basis of language and sequential cognition: Evidence from simulation, aphasia, and ERP studies**

Brain and Language, 86 (2003), pp. 207-225

[\[30\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb30)

Jean-Marc Blanc, Peter F. Dominey

**Identification of prosodic attitudes by atemporal recurrent network**

Cognitive Brain Research, 17 (2003), pp. 693-699

[\[31\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb31)

Peter F. Dominey, Michel Hoen, Toshio Inui

**A neurolinguistic model of grammatical construction processing**

Journal of Cognitive Neuroscience, 18 (12) (2006), pp. 2088-2107

[\[32\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb32)

Robert M. French

**Catastrophic interference in connectionist networks**

L. Nadel (Ed.), Encyclopedia of Cognitive Science, Volume 1, Nature Publishing Group (2003), pp. 431-435

[\[33\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb33)

Floris Takens

**Detecting strange attractors in turbulence**

Proceedings of a Symposium on Dynamical Systems and Turbulence, LNM, vol. 898, Springer (1981), pp. 366-381

[\[34\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb34)

Ronald J. Williams, David Zipser

**A learning algorithm for continually running fully recurrent neural networks**

Neural Computation, 1 (1989), pp. 270-280

[\[35\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb35)

David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams

**Learning internal representations by error propagation**

Neurocomputing: Foundations of research, MIT Press, Cambridge, MA, USA (1988), pp. 673-695

[\[36\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb36)

Paul J. Werbos

**Backpropagation through time: What it does and how to do it**

Proceedings of the IEEE, 78 (10) (1990), pp. 1550-1560

[\[37\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb37)

Amir F. Atiya, Alexander G. Parlos

**New results on recurrent network training: Unifying the algorithms and accelerating convergence**

IEEE Transactions on Neural Networks, 11 (3) (2000), pp. 697-709

[\[38\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb38)

Gintaras V. Puškorius, Lee A. Feldkamp

**Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks**

IEEE Transactions on Neural Networks, 5 (2) (1994), pp. 279-297

[\[39\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb39)

Sheng Ma, Chuanyi Ji

**Fast training of recurrent networks based on the EM algorithm**

IEEE Transactions on Neural Networks, 9 (1) (1998), pp. 11-26

[\[40\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb40)

Sepp Hochreiter, Jürgen Schmidhuber

**Long short-term memory**

Neural Computation, 9 (8) (1997), pp. 1735-1780

[\[41\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb41)

Herbert Jaeger, Short term memory in echo state networks, Technical Report GMD Report 152, German National Research Center for Information Technology, 2002

[\[42\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb42)

Herbert Jaeger, Tutorial on training recurrent neural networks, covering BPTT, RTRL, EKF and the “echo state network” approach, Technical Report GMD Report 159, German National Research Center for Information Technology, 2002

[\[43\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb43)

Herbert Jaeger

**Adaptive nonlinear system identification with echo state networks**

Advances in Neural Information Processing Systems 15, MIT Press, Cambridge, MA (2003), pp. 593-600

[\[44\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb44)

Thomas Natschläger, Henry Markram, Wolfgang Maass

**Computer models and analysis tools for neural microcircuits**

R. Kötter (Ed.), A Practical Guide to Neuroscience Databases and Associated Tools, Kluver Academic Publishers, Boston (2002)

[\[45\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb45)

Wolfgang Maass, Thomas Natschläger, Henry Markram

**Computational models for generic cortical microcircuits**

J. Feng (Ed.), Computational Neuroscience: A Comprehensive Approach, CRC-Press (2002)

[\[46\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb46)

Jürgen Schmidhuber, Daan Wierstra, Matteo Gagliolo, Faustino J. Gomez

**Training recurrent networks by Evolino**

Neural Computation, 19 (3) (2007), pp. 757-779

[\[47\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb47)

Ulf D. Schiller, Jochen J. Steil

**Analyzing the weight dynamics of recurrent learning algorithms**

Neurocomputing, 63C (2005), pp. 5-23

[\[48\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb48)

Jochen J. Steil

**Memory in backpropagation-decorrelation O(N) efficient online recurrent learning**

Proceedings of the 15th International Conference on Artificial Neural Networks, LNCS, vol. 3697, Springer (2005), pp. 649-654

[\[49\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb49)

Felix R. Reinhart, Jochen J. Steil, Recurrent neural autoassociative learning of forward and inverse kinematics for movement generation of the redundant PA-10 robot, in: Proceedings of the ECSIS Symposium on Learning and Adaptive Behaviors for Robotic Systems, LAB-RS, vol. 1, 2008, 35–40

[\[50\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb50)

Peter F. Dominey, Franck Ramus

**Neural network processing of natural language: I. Sensitivity to serial, temporal and abstract structure of language in the infant**

Language and Cognitive Processes, 15 (1) (2000), pp. 87-127

[\[51\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb51)

Peter F. Dominey

**From sensorimotor sequence to grammatical construction: Evidence from simulation and neurophysiology**

Adaptive Behaviour, 13 (4) (2005), pp. 347-361

[\[52\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb52)

Se Wang, Xiao-Jian Yang, Cheng-Jian Wei, Harnessing non-linearity by sigmoid-wavelet hybrid echo state networks (SWHESN), in: The 6th World Congress on Intelligent Control and Automation, WCICA 2006, 1, 2006, pp. 3014–3018

[\[53\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb53)

David Verstraeten, Benjamin Schrauwen, Dirk Stroobandt, Reservoir computing with stochastic bitstream neurons, in: Proceedings of the 16th Annual ProRISC Workshop, Veldhoven, The Netherlands, November 2005, pp. 454–459

[\[54\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb54)

Felix Schürmann, Karlheinz Meier, Johannes Schemmel

**Edge of chaos computation in mixed-mode VLSI - A hard liquid**

Advances in Neural Information Processing Systems 17, MIT Press, Cambridge, MA (2005), pp. 1201-1208

[\[55\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb55)

Benjamin Schrauwen, Marion Wardermann, David Verstraeten, Jochen J. Steil, Dirk Stroobandt

**Improving reservoirs using intrinsic plasticity**

Neurocomputing, 71 (2008), pp. 1159-1171

[\[56\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb56)

Kristof Vandoorne, Wouter Dierckx, Benjamin Schrauwen, David Verstraeten, Roel Baets, Peter Bienstman, Jan~Van Campenhout

**Toward optical signal processing using photonic reservoir computing**

Optics Express, 16 (15) (2008), pp. 11182-11192

[\[57\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb57)

Chrisantha Fernando, Sampsa Sojakka

**Pattern recognition in a bucket**

Proceedings of the 7th European Conference on Advances in Artificial Life, LNCS, vol. 2801, ECAL 2003, Springer (2003), pp. 588-597

[\[58\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb58)

Ben Jones, Dov Stekelo, Jon Rowe, Chrisantha Fernando, Is there a liquid state machine in the bacterium Escherichia coli?, in: Proceedings of the 1st IEEE Symposium on Artificial Life, ALIFE 2007, 1–5 April 2007, pp. 187–191

[\[59\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb59)

David Verstraeten, Benjamin Schrauwen, Michiel D’Haene, Dirk Stroobandt

**An experimental unification of reservoir computing methods**

Neural Networks, 20 (3) (2007), pp. 391-403

[\[60\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb60)

Benjamin Schrauwen, David Verstraeten, Jan Van Campenhout, An overview of reservoir computing: Theory, applications and implementations, in: Proceedings of the 15th European Symposium on Artificial Neural Networks, ESANN 2007, 2007, pp. 471–482

[\[61\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb61)

Mantas Lukoševičius, Herbert Jaeger, Overview of reservoir recipes, Technical Report No. 11, Jacobs University Bremen, 2007

[\[62\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb62)

David H. Wolpert, The supervised learning no-free-lunch theorems, in: Proceedings of the 6th Online World Conference on Soft Computing in Industrial Applications, WSC 2006, 2001, pp. 25–42

[\[63\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb63)

Michael Buehner, Peter Young

**A tighter bound for the echo state property**

IEEE Transactions on Neural Networks, 17 (3) (2006), pp. 820-824

[\[64\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb64)

Duncan J. Watts, Steven~H. Strogatz

**Collective dynamics of ‘small-world’ networks**

Nature, 393 (1998), pp. 440-442

[\[65\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb65)

Albert-Laszlo Barabasi, Reka Albert

**Emergence of scaling in random networks**

Science, 286 (1999), p. 509

[\[66\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb66)

Marcus Kaiser, Claus~C. Hilgetag

**Spatial growth of real-world networks**

Physical Review E, 69 (2004), p. 036103

[\[67\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb67)

Benjamin Liebald, Exploration of effects of different network topologies on the ESN signal crosscorrelation matrix spectrum, Bachelor’s Thesis, Jacobs University Bremen, 2004 [http://www.eecs.jacobs-university.de/archive/bsc-2004/liebald.pdf](http://www.eecs.jacobs-university.de/archive/bsc-2004/liebald.pdf)

[\[68\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb68)

Fei Jiang, Hugues Berry, Marc Schoenauer

**Supervised and evolutionary learning of echo state networks.**

Proceedings of 10th International Conference on Parallel Problem Solving from Nature, LNCS, vol. 5199, PPSN 2008, Springer (2008), pp. 215-224

[\[69\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb69)

Wolfgang Maass, Thomas Natschläger, Henry Markram

**Computational models for generic cortical microcircuits**

Computational Neuroscience: A Comprehensive Approach, Chapman & Hall/CRC (2004), pp. 575-605

[\[70\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb70)

David Verstraeten, Benjamin Schrauwen, Dirk Stroobandt, Jan~Van Campenhout

**Isolated word recognition with the liquid state machine: A case study**

Information Processing Letters, 95 (6) (2005), pp. 521-528

[\[71\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb71)

Michal Čerňanský, Matej Makula, Feed-forward echo state networks, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2005, IJCNN 2005, vol. 3, 2005, pp. 1479–1482

[\[72\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb72)

Georg Fette, Julian Eggert

**Short term memory and pattern matching with simple echo state network**

Proceedings of the 15th International Conference on Artificial Neural Networks, LNCS, vol. 3696, ICANN 2005, Springer (2005), pp. 13-18

[\[73\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb73)

David Verstraeten, Benjamin Schrauwen, Dirk Stroobandt, Adapting reservoirs to get Gaussian distributions, in: Proceedings of the 15th European Symposium on Artificial Neural Networks, ESANN 2007, 2007, pp. 495–500

[\[74\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb74)

Carlos Lourenço, Dynamical reservoir properties as network effects, in: Proceedings of the 14th European Symposium on Artificial Neural Networks, ESANN 2006, 2006, pp. 503–508

[\[75\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb75)

Keith Bush, Batsukh Tsendjav, Improving the richness of echo state features using next ascent local search, in: Proceedings of the Artificial Neural Networks In Engineering Conference, St. Louis, MO, 2005, pp. 227–232

[\[76\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb76)

Márton~Albert Hajnal, András Lőrincz

**Critical echo state networks**

Proceedings of the 16th International Conference on Artificial Neural Networks, LNCS, vol. 4131, Springer (2006), pp. 658-667

[\[77\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb77)

Yanbo Xue, Le Yang, Simon Haykin

**Decoupled echo state networks with lateral inhibition**

Neural Networks, 20 (3) (2007), pp. 365-376

[\[78\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb78)

Mantas Lukoševičius, Echo state networks with trained feedbacks, Technical Report No. 4, Jacobs University Bremen, 2007

[\[79\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb79)

Danil V. Prokhorov, Lee A. Feldkamp, Ivan Yu. Tyukin, Adaptive behavior with fixed weights in RNN: An overview, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2002, IJCNN 2002, 2002, pp. 2018–2023

[\[80\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb80)

Mohamed Oubbati, Paul Levi, Michael Schanz, Meta-learning for adaptive identification of non-linear dynamical systems, in: Proceedings of the IEEE International Joint Symposium on Intelligent Control, June 2005, pp. 473–478

[\[81\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb81)

Mantas Lukoševičius, Dan Popovici, Herbert Jaeger, Udo Siewert, Time warping invariant echo state networks, Technical Report No. 2, Jacobs University Bremen, 2006

[\[82\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb82)

Benjamin Schrauwen, Jeroen Defour, David Verstraeten, Jan M. Van Campenhout

**The introduction of time-scales in reservoir computing, applied to isolated digits recognition**

Proceedings of the 17th International Conference on Artificial Neural Networks, LNCS, vol. 4668, Springer (2007), pp. 471-479

[\[83\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb83)

Udo Siewert, Welf Wustlich, Echo-state networks with band-pass neurons: Towards generic time-scale-independent reservoir structures, Internal Status Report, PLANET intelligent systems GmbH, 2007. Available online at [http://snn.elis.ugent.be/](http://snn.elis.ugent.be/)

[\[84\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb84)

Georg Holzmann, Echo state networks with filter neurons and a delay&sum readout, Internal Status Report, Graz University of Technology, 2007. Available online at [http://grh.mur.at/data/misc.html](http://grh.mur.at/data/misc.html)

[\[85\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb85)

Francis wyffels, Benjamin Schrauwen, David Verstraeten, Stroobandt Dirk, Band-pass reservoir computing, in: Z. Hou, and N. Zhang (Eds.), Proceedings of the IEEE International Joint Conference on Neural Networks, 2008, IJCNN 2008, Hong Kong, 2008, pp. 3204–3209

[\[86\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb86)

Salah El Hihi, Yoshua Bengio

**Hierarchical recurrent neural networks for long-term dependencies**

Advances in Neural Information Processing Systems 8, MIT Press, Cambridge, MA (1996), pp. 493-499

[\[87\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb87)

Nils Bertschinger, Thomas Natschläger

**Real-time computation at the edge of chaos in recurrent neural networks**

Neural Computation, 16 (7) (2004), pp. 1413-1436

[\[88\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb88)

Benjamin Schrauwen, Lars Buesing, Robert Legenstein, On computational power and the order-chaos phase transition in reservoir computing, in: Advances in Neural Information Processing Systems 21, NIPS 2008, 2009, pp. 1425–1432

[\[89\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb89)

Wolfgang Maass, Robert A. Legenstein, Nils Bertschinger

**Methods for estimating the computational power and generalization capability of neural microcircuits**

Advances in Neural Information Processing Systems 17, NIPS 2004, MIT Press, Cambridge, MA (2005), pp. 865-872

[\[90\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb90)

Robert A. Legenstein, Wolfgang Maass

**Edge of chaos and prediction of computational performance for neural circuit models**

Neural Networks, 20 (3) (2007), pp. 323-334

[\[91\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb91)

Behrouz Farhang-Boroujeny

**Adaptive Filters: Theory and Applications**

Wiley (1998)

[\[92\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb92)

Herbert Jaeger, Reservoir riddles: suggestions for echo state network research, Proceedings of the IEEE International Joint Conference on Neural Networks, 2005, IJCNN 2005, vol. 3, 2005, pp. 1460–1462

[\[93\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb93)

Jochen Triesch, A gradient rule for the plasticity of a neuron’s intrinsic excitability, in: Proceedings of the 13th European Symposium on Artificial Neural Networks, ESANN 2005, 2005, pp. 65–70

[\[94\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb94)

Melanie Mitchell, James P. Crutchfield, Peter T. Hraber

**Dynamics, computation, and the “edge of chaos”: A re-examination**

G. Cowan, D. Pines, D. Melzner (Eds.), Complexity: Metaphors, Models, and Reality, Addison-Wesley, Reading, MA (1994), pp. 497-513

[\[95\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb95)

Robert Legenstein, Wolfgang Maass

**What makes a dynamical system computationally powerful?**

S. Haykin, J. Príncipe, T. Sejnowski, J. McWhirter (Eds.), New Directions in Statistical Signal Processing: From Systems to Brain, MIT Press (2007), pp. 127-154

[\[96\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb96)

Mustafa C. Ozturk, José C. Príncipe, Computing with transiently stable states, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2005, IJCNN 2005, vol. 3, 2005, pp. 1467–1472

[\[97\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb97)

Donald O. Hebb

**The Organization of Behavior: A Neuropsychological Theory**

Wiley, New York (1949)

[\[98\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb98)

Štefan Babinec, Jiří Pospíchal

**Improving the prediction accuracy of echo state neural networks by anti-Oja’s learning**

Proceedings of the 17th International Conference on Artificial Neural Networks, LNCS, vol. 4668, Springer (2007), pp. 19-28

[\[99\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb99)

Henry Markram, Yun Wang, Misha Tsodyks

**Differential signaling via the same axon of neocortical pyramidal neurons**

Proceedings of National Academy of Sciences USA, 95 (9) (1998), pp. 5323-5328

[\[100\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb100)

David Norton, Dan Ventura, Preparing more effective liquid state machines using Hebbian learning, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2006, IJCNN 2006, 2006, pp. 4243–4248

[\[101\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb101)

Hélène Paugam-Moisy, Regis Martinez, Samy Bengio

**Delay learning and polychronization for reservoir computing**

Neurocomputing, 71 (7–9) (2008), pp. 1143-1158

[\[102\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb102)

Roland Baddeley, Larry F. Abbott, Michael C.A. Booth, Frank Sengpeil, Toby Freeman, Edward A. Wakeman, Edmund T. Rolls

**Responses of neurons in primary and inferior temporal visual cortices to natural scenes**

Proceedings of the Royal Society of London B, 264 (1997), pp. 1775-1783

[\[103\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb103)

Martin Stemmler, Christof Koch

**How voltage-dependent conductances can adapt to maximize the information encoded by neuronal firing rate**

Nature Neuroscience, 2 (6) (1999), pp. 521-527

[\[104\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb104)

Jochen Triesch

**Synergies between intrinsic and synaptic plasticity in individual model neurons**

Advances in Neural Information Processing Systems 17, MIT Press, Cambridge, MA (2005), pp. 1417-1424

[\[105\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb105)

Anthony J. Bell, Terrence J. Sejnowski

**An information-maximization approach to blind separation and blind deconvolution**

Neural Computation, 7 (6) (1995), pp. 1129-1159

[\[106\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb106)

Jochen J. Steil

**Online reservoir adaptation by intrinsic plasticity for backpropagation-decorrelation and echo state learning**

Neural Networks, 20 (3) (2007), pp. 353-364

[\[107\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb107)

Marion Wardermann, Jochen J. Steil, Intrinsic plasticity for reservoir learning algorithms, in: Proceedings of the 15th European Symposium on Artificial Neural Networks, ESANN 2007, 2007, pp. 513–518

[\[108\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb108)

Jochen J. Steil, Several ways to solve the MSO problem, in: Proceedings of the 15th European Symposium on Artificial Neural Networks, ESANN 2007, 2007, pp. 489–494

[\[109\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb109)

Joschka Boedecker, Oliver Obst, Norbert Michael Mayer, Minoru Asada, Studies on reservoir initialization and dynamics shaping in echo state networks, in: Proceedings of the 17th European Symposium on Artificial Neural Networks, ESANN 2009, 2009 (in press)

[\[110\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb110)

Naftali Tishby, Fernando~C. Pereira, William Bialek

**The information bottleneck method**

Proceedings of the 37th Annual Allerton Conference on Communication, Control and Computing (1999), pp. 368-377

[\[111\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb111)

Taro Toyoizumi, Jean-Pascal Pfister, Kazuyuki Aihara, Wulfram Gerstner

**Generalized Bienenstock–Cooper–Munro rule for spiking neurons that maximizes information transmission**

Proceedings of National Academy of Sciences USA, 102 (2005), pp. 5239-5244

[\[112\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb112)

Stefan Klampfl, Robert Legenstein, Wolfgang Maass

**Information bottleneck optimization and independent component extraction with spiking neurons**

Advances in Neural Information Processing Systems 19, ICANN 2007, MIT Press, Cambridge, MA (2007), pp. 713-720

[\[113\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb113)

Stefan Klampfl, Robert Legenstein, Wolfgang Maass

**Spiking neurons can learn to solve information bottleneck problems and to extract independent components**

Neural Computation, 21 (4) (2008), pp. 911-959

[\[114\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb114)

Lars Buesing, Wolfgang Maass

**Simplified rules and theoretical analysis for information bottleneck optimization and PCA with spiking neurons**

Advances in Neural Information Processing Systems 20, MIT Press, Cambridge, MA (2008), pp. 193-200

[\[115\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb115)

Jochen Triesch

**Synergies between intrinsic and synaptic plasticity mechanisms**

Neural Computation, 19 (4) (2007), pp. 885-909

[\[116\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb116)

Nicholas J. Butko, Jochen Triesch

**Learning sensory representations with intrinsic plasticity**

Neurocomputing (70) (2007), pp. 1130-1138

[\[117\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb117)

Andreea Lazar, Gordon Pipa, Jochen Triesch

**Fading memory and time series prediction in recurrent networks with different forms of plasticity**

Neural Networks, 20 (3) (2007), pp. 312-322

[\[118\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb118)

Norbert M. Mayer, Matthew Browne, Echo state networks and self-prediction, in: Revised Selected Papers of Biologically Inspired Approaches to Advanced Information Technology, BioADIT 2004, 2004, pp. 40–48

[\[119\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb119)

Mustafa C. Ozturk, Dongming Xu, José C. Príncipe

**Analysis and design of echo state networks**

Neural Computation, 19 (1) (2007), pp. 111-138

[\[120\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb120)

William H. Kautz

**Transient synthesis in the time domain**

IRE Transactions on Circuit Theory, 1 (3) (1954), pp. 29-39

[\[121\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb121)

Kazuo Ishii, Tijn van der Zant, Vlatko Bečanović, Paul Plöger, Identification of motion with echo state network, in: Proceedings of the OCEANS 2004 MTS/IEEE–TECHNO-OCEAN 2004 Conference, vol. 3, 2004, pp. 1205–1210

[\[122\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb122)

John H. Holland

**Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology Control and Artificial Intelligence**

MIT Press, Cambridge, MA, USA (1992)

[\[123\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb123)

Ali Ajdari Rad, Mahdi Jalili, Martin Hasler

**Reservoir optimization in recurrent neural networks using Kronecker kernels**

Proceedings of IEEE International Symposium on Circuits and Systems 2008, IEEE (2008), pp. 868-871

[\[124\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb124)

Xavier Dutoit, Hendrik Van Brussel, Marnix Nutti, A first attempt of reservoir pruning for classification problems, in: Proceedings of the 15th European Symposium on Artificial Neural Networks, ESANN 2007, 2007, pp. 507–512

[\[125\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb125)

Wolfgang Maass, Prashant Joshi, Eduardo D. Sontag

**Computational aspects of feedback in neural circuits**

PLoS Computational Biology, 3 (1) (2007), p. e165+

[\[126\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb126)

Robert Legenstein, Dejan Pecevski, Wolfgang Maass

**Theoretical analysis of learning with reward-modulated spike-timing-dependent plasticity**

Advances in Neural Information Processing Systems 20, NIPS 2007, MIT Press, Cambridge, MA (2008), pp. 881-888

[\[127\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb127)

Robert Legenstein, Dejan Pecevski, Wolfgang Maass

**A learning theory for reward-modulated spike-timing-dependent plasticity with application to biofeedback**

PLoS Computational Biology, 4 (10) (2008), p. e1000180

[\[128\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb128)

A˚ke Björck

**Numerical Method for Least Squares Problems**

SIAM, Philadelphia, PA, USA (1996)

[\[129\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb129)

Andrew Carnell, Daniel Richardson, Linear algebra for time series of spikes, in: Proceedings of the 13th European Symposium on Artificial Neural Networks, ESANN 2005, 2005, pp. 363–368

[\[130\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb130)

Ali U. Küçükemre, Echo state networks for adaptive filtering, University of Applied Sciences Bohn-Rhein-Sieg, Germany, April 2006. [http://www.faculty.jacobs-university.de/hjaeger/pubs/Kucukemre.pdf](http://www.faculty.jacobs-university.de/hjaeger/pubs/Kucukemre.pdf)

[\[131\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb131)

Zhinwei Shi, Min Han

**Support vector echo-state machine for chaotic time-series prediction**

IEEE Transactions on Neural Networks, 18 (2) (2007), pp. 359-372

[\[132\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb132)

Jürgen Schmidhuber, Matteo Gagliolo, Daan Wierstra, Faustino J. Gomez, Evolino for recurrent support vector machines. Technical Report, 2006

[\[133\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb133)

Benjamin Schrauwen, Jan~Van Campenhout

**Linking non-binned spike train kernels to several existing spike train metrics**

M. Verleysen (Ed.), Proceedings of the 14th European Symposium on Artificial Neural Networks, ESANN 2006, d-side publications, Evere (2006), pp. 41-46

[\[134\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb134)

Jochen J. Steil, Stability of backpropagation-decorrelation efficient O(N) recurrent learning, in: Proceedings of the 13th European Symposium on Artificial Neural Networks, ESANN 2005, 2005, pp. 43–48

[\[135\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb135)

Francis wyffels, Benjamin Schrauwen, Dirk Stroobandt

**Stable output feedback in reservoir computing using ridge regression**

Proceedings of the 18th International Conference on Artificial Neural Networks, LNCS, vol. 5163, ICANN 2008, Springer (2008), pp. 808-817

[\[136\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb136)

Xavier Dutoit, Benjamin Schrauwen, Jan Van Campenhout, Dirk Stroobandt, Hendrik Van Brussel, Marnix Nuttin, Pruning and regularization in reservoir computing: A first insight, in: Proceedings of the 16th European Symposium on Artificial Neural Networks, ESANN 2008, 2008, pp. 1–6

[\[137\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb137)

Joe Tebelskis, Ph.D. Thesis, Speech Recognition using Neural Networks, School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, 1995

[\[138\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb138)

Mark D. Skowronski, John G. Harris

**Automatic speech recognition using a predictive echo state network classifier**

Neural Networks, 20 (3) (2007), pp. 414-423

[\[139\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb139)

Dongming Xu, Jing Lan, José C. Príncipe, Direct adaptive control: An echo state network and genetic algorithm approach, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2005, IJCNN 2005, vol. 3, 2005, pp. 1483–1486

[\[140\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb140)

Alexandre Devert, Nicolas Bredeche, Marc Schoenauer

**Unsupervised learning of echo state networks: a case study in artificial embryogeny**

Proceedings of the 8th International Conference on Artificial Evolution, LNCS, vol. 4926, Springer (2008), pp. 278-290

[\[141\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb141)

Fei Jiang, Hugues Berry, Marc Schoenauer

**Unsupervised learning of echo state networks: Balancing the double pole**

Proceedings of the 10th Genetic and Evolutionary Computation Conference, ACM (2008), pp. 869-870

[\[142\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb142)

Keith Bush, Charles Anderson, Modeling reward functions for incomplete state representations via echo state networks, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 2005, IJCNN 2005, vol. 5, 2005, pp. 2995–3000

[\[143\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb143)

Štefan Babinec, Jiří Pospíchal

**Merging echo state and feedforward neural networks for time series forecasting**

Proceedings of the 16th International Conference on Artificial Neural Networks, LNCS, vol. 4131, Springer (2006), pp. 367-375

[\[144\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb144)

Mustafa C. Ozturk, José C. Príncipe

**An associative memory readout for ESNs with applications to dynamical pattern recognition**

Neural Networks, 20 (3) (2007), pp. 377-390

[\[145\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb145)

Mark Embrechts, Luis Alexandre, Jonathan Linton, Reservoir computing for static pattern recognition, in: Proceedings of the 17th European Symposium on Artificial Neural Networks, ESANN 2009, 2009 (in press)

[\[146\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb146)

Felix R. Reinhart, Jochen J. Steil, Attractor-based computation with reservoirs for online learning of inverse kinematics, in: Proceedings of the 17th European Symposium on Artificial Neural Networks, ESANN 2009, 2009 (in press)

[\[147\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb147)

Keith Bush, Charles Anderson, Exploiting iso-error pathways in the N, k-plane to improve echo state network performance, 2006

[\[148\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb148)

Yoshua Bengio, Yann LeCun

**Scaling learning algorithms toward AI**

L. Bottou, O. Chapelle, D. DeCoste, J. Weston (Eds.), Large Scale Kernel Machines, MIT Press, Cambridge, MA (2007)

[\[149\]](https://www.sciencedirect.com/science/article/pii/S1574013709000173#bb149)

Herbert Jaeger, Discovering multiscale dynamical features with hierarchical echo state networks, Technical Report No. 9, Jacobs University Bremen, 2007

## Cited by (1641)

[View Abstract](https://www.sciencedirect.com/science/article/abs/pii/S1574013709000173)

Copyright © 2009 Elsevier Inc. All rights reserved.
