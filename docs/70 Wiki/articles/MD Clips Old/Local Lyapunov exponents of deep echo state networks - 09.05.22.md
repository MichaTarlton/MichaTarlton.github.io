---
created: 2022-05-09T16:49:28 (UTC +02:00)
tags: []
source: https://www.sciencedirect.com/science/article/pii/S0925231218302157
author: 
---

# Local Lyapunov exponents of deep echo state networks - ScienceDirect



> ## Excerpt
> The analysis of deep Recurrent Neural Network (RNN) models represents a research area of increasing interest. In this context, the recent introduction…

---
## 1\. Introduction

The extension of deep learning methodologies to the class of [Recurrent Neural Networks](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about Recurrent Neural Networks from ScienceDirect's AI-generated Topic Pages") (RNNs) is currently stimulating an increasing interest in the [machine learning](https://www.sciencedirect.com/topics/computer-science/machine-learning "Learn more about machine learning from ScienceDirect's AI-generated Topic Pages") community [\[1\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0001), [\[2\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0002). In this area, the study of hierarchically structured RNN architectures (see e.g. [\[3\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0003), [\[4\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0004), [\[5\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0005), [\[6\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0006), [\[7\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0007), [\[8\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0008)) paved the way to the design of models able to develop feature representations of temporal information at increasing levels of abstraction, enabling a natural approach to tasks on time-series featured by multiple time-scales (especially in the cognitive area). Besides, the elaboration of temporal information in a layered and recurrent fashion is also motivated by strong evidences of biological plausibility emerged from the area of [neuroscience](https://www.sciencedirect.com/topics/neuroscience/neurosciences "Learn more about neuroscience from ScienceDirect's AI-generated Topic Pages") [\[9\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0009), [\[10\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0010).

However, the analysis of deep RNNs is relatively young, and one of the major topics still deserving research attention is related to understanding and characterizing their dynamical behavior, especially in relation to the inherent role of the hierarchical composition of the recurrent units in layers. A useful methodology in this regard is provided by the Reservoir Computing (RC) [\[11\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0011), [\[12\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0012) paradigm and the Echo State Network (ESN) [\[13\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0013), [\[14\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0014) approach to RNNs modeling. In particular, allowing to taking apart all the effects due to learning, the recent introduction of the DeepESN model [\[15\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0015), [\[16\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0016) enabled the study of the intrinsic role played by the layering factor in deep RNN architectures. Moreover, by inheriting the training characterization typical of standard RC models, DeepESNs also provide an efficient methodology for designing and training [deep learning models](https://www.sciencedirect.com/topics/computer-science/deep-learning-model "Learn more about deep learning models from ScienceDirect's AI-generated Topic Pages") in the temporal domain.

A first mean to investigate the characteristics of [recurrent network](https://www.sciencedirect.com/topics/computer-science/recurrent-network "Learn more about recurrent network from ScienceDirect's AI-generated Topic Pages") dynamics is given in the RC area by the Echo State Property (ESP) [\[17\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0017), which has recently been extended to the case of deep networks in [\[18\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0018). The analysis provided by the study of the ESP conditions in [\[17\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0017) has started to reveal the natural characterizations of deep RNNs under a [dynamical system](https://www.sciencedirect.com/topics/computer-science/dynamical-system "Learn more about dynamical system from ScienceDirect's AI-generated Topic Pages") perspective [\[18\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0018), but it might result of reduced utility in practical cases as it basically neglects the influence of the external input on networks dynamics. By their very nature, recurrent neural models implement dynamical systems whose trajectories in the state space are influenced by initial conditions and by the external input signals, which practically realize a link between the system dynamics and the computational task at hand. In this context, the analysis of stability of deep RNNs dynamics when driven by an external input represents a topic of great importance and still demanded in literature.

In this paper, by pursuing the study of the dynamical behavior of recurrent models typical in the RC area, we provide a theoretical and practical tool that allows us to investigate and control the stability of deep recurrent networks driven by the input. Specifically, we extend the applicability of the study of local [Lyapunov exponents](https://www.sciencedirect.com/topics/computer-science/lyapunov-exponent "Learn more about Lyapunov exponents from ScienceDirect's AI-generated Topic Pages") [\[19\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0019), [\[20\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0020) from the case of shallow ESNs (see e.g. [\[12\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0012), [\[21\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0021), [\[22\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0022)) to the case of DeepESNs. In particular, the maximum among the local Lyapunov exponents is a useful mean to express the network’s sensibility to [small perturbations](https://www.sciencedirect.com/topics/computer-science/small-perturbation "Learn more about small perturbations from ScienceDirect's AI-generated Topic Pages") of its [state trajectories](https://www.sciencedirect.com/topics/computer-science/state-trajectory "Learn more about state trajectories from ScienceDirect's AI-generated Topic Pages"), and as such it can well quantify the degree of stability (or order) in the dynamical behavior of the system. Given the actual input for the system, the proposed methodology can be used to identify the different dynamical regimes that follow from different cases of networks design conditions, such as the RC scaling factors, the number of recurrent units and the depth of the network. The proposed tool is practically demonstrated on artificial data as well as on signals from real-world datasets.

While the developed tool could be certainly applied to the case of deep RNNs at any stage of training, its application in the RC context enables us to investigate the actual role of layering in RNNs and shed light on its natural effect on the richness and stability of the developed network’s dynamics. In this regard, a particularly interesting condition of dynamical behavior is represented by the stable-unstable transition where the maximum local Lyapunov exponent is null, a region of the state space known as the edge of [criticality](https://www.sciencedirect.com/topics/computer-science/criticality "Learn more about criticality from ScienceDirect's AI-generated Topic Pages"). Previous works in the RC literature already showed that the performance of recurrent models for tasks requiring a long memory span peaks near the criticality of their dynamical behavior [\[23\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0023), [\[24\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0024), [\[25\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0025), [\[26\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0026). Examples are represented by the benchmark tasks in the RC area (e.g. [\[12\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0012), [\[27\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0027), [\[28\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0028), [\[29\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0029), [\[30\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0030)), tasks in the domain of neural circuit models (e.g. [\[24\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0024), [\[25\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0025), [\[31\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0031)), as well as real-world tasks, e.g. in the area of speech processing [\[12\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0012) and mobile traffic load estimation [\[32\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0032). Although the methodology proposed in this paper is not put forward as a performance predictor for trained recurrent models, as an additional element of analysis here we use it to study the relation between the memory and the regimes of DeepESN behaviors through the short-term Memory Capacity task [\[33\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0033).

The rest of this paper is organized as follows. In [Section 2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0002) we introduce the basic elements of RC and describe the DeepESN model. In [Section 3](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0003) we provide the mathematical characterization of the stability analysis of DeepESNs in terms of the maximum local Lyapunov exponent. The outcomes of our experimental analysis are reported and discussed in [Section 4](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0004). Finally, conclusions are presented in [Section 5](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0006).

## 2\. Deep echo state networks

Within the framework of randomized [neural networks](https://www.sciencedirect.com/topics/neuroscience/neural-networks "Learn more about neural networks from ScienceDirect's AI-generated Topic Pages") [\[34\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0034), the RC paradigm [\[11\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0011), [\[12\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0012) has attested as a state-of-the-art methodology for efficient [RNN](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about RNN from ScienceDirect's AI-generated Topic Pages") modeling. The most widely known model in this context is represented by the ESN [\[13\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0013), [\[14\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0014), [\[35\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0035). From the [architectural perspective](https://www.sciencedirect.com/topics/computer-science/architectural-perspective "Learn more about architectural perspective from ScienceDirect's AI-generated Topic Pages"), an ESN comprises a recurrent hidden layer of non-linear units, called reservoir, and a feed-forward output layer of typically linear units, called readout. The essence of the ESN operation is that the reservoir part implements a set of randomized filters that serve to dynamically and non-linearly encode the input history into a high dimensional state space, where the task at hand can be approached satisfactorily even by a linear output tool.

From a [dynamical system](https://www.sciencedirect.com/topics/computer-science/dynamical-system "Learn more about dynamical system from ScienceDirect's AI-generated Topic Pages") point of view, the reservoir of an ESN computes a discrete-time input-driven non-linear dynamical system, such that at each time step the state evolution is ruled by the reservoir [state transition function](https://www.sciencedirect.com/topics/computer-science/state-transition-function "Learn more about state transition function from ScienceDirect's AI-generated Topic Pages"). By referring to the case of leaky integrator reservoir units [\[36\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0036), at each time step *t* the reservoir state update equation is given by: (1)$<math><mrow is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><mi is="true">a</mi><mo is="true">)</mo></mrow><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">+</mo><mi is="true">a</mi><mi is="true">tanh</mi><mo is="true">(</mo><msub is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">θ</mi></mrow><mo is="true">+</mo><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo><mo is="true">,</mo></mrow></math>$where $<math><mrow is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup></mrow></math>$ and $<math><mrow is="true"><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">U</mi></msub></msup></mrow></math>$ are respectively the reservoir state and the input at time step *t, a* ∈ \[0, 1\] is the leaking rate parameter, $<math><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub><mo is="true">×</mo><msub is="true"><mi is="true">N</mi><mi is="true">U</mi></msub></mrow></msup></mrow></math>$ is the input weight matrix, $<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">θ</mi></mrow><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup></mrow></math>$ is the weight vector corresponding to the unitary input bias, $<math><mrow is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub><mo is="true">×</mo><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></mrow></msup></mrow></math>$ is the recurrent reservoir weight matrix and tanh  denotes the element-wise application of the [hyperbolic tangent](https://www.sciencedirect.com/topics/computer-science/hyperbolic-tangent "Learn more about hyperbolic tangent from ScienceDirect's AI-generated Topic Pages") non-linearity. Typically, a null state is used as initial condition, i.e. $<math><mrow is="true"><mi mathvariant="bold" is="true">x</mi><mo is="true">(</mo><mn is="true">0</mn><mo is="true">)</mo><mo is="true">=</mo><mn mathvariant="bold" is="true">0</mn></mrow></math>$.

The output at time step *t* is computed by the readout as a [linear combination](https://www.sciencedirect.com/topics/computer-science/linear-combination "Learn more about linear combination from ScienceDirect's AI-generated Topic Pages") of the activation of the reservoir units, according to the following equation: (2)$<math><mrow is="true"><mi mathvariant="bold" is="true">y</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">o</mi><mi is="true">u</mi><mi is="true">t</mi></mrow></msub><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">θ</mi></mrow><mrow is="true"><mi is="true">o</mi><mi is="true">u</mi><mi is="true">t</mi></mrow></msub><mo is="true">,</mo></mrow></math>$where $<math><mrow is="true"><mi mathvariant="bold" is="true">y</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">Y</mi></msub></msup></mrow></math>$ is the output at time step *t*, $<math><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">o</mi><mi is="true">u</mi><mi is="true">t</mi></mrow></msub><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">Y</mi></msub><mo is="true">×</mo><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></mrow></msup></mrow></math>$ is the output weight matrix and $<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">θ</mi></mrow><mrow is="true"><mi is="true">o</mi><mi is="true">u</mi><mi is="true">t</mi></mrow></msub><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">Y</mi></msub></msup></mrow></math>$ is the vector of weights corresponding to the unitary input bias for the readout.

A major peculiarity of the ESN approach is that only the readout undergoes a training process, such that the weights in **W***out* and ***θ****out* are adjusted on a training set in order to solve a least squares problem, typically in an off-line fashion and in closed form, using of pseudo-inversion or [Tikhonov regularization](https://www.sciencedirect.com/topics/computer-science/tikhonov-regularization "Learn more about Tikhonov regularization from ScienceDirect's AI-generated Topic Pages"). The reservoir’s parameters are instead left untrained after initialization constrained to the dictates of the Echo State Property (ESP) [\[14\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0014). The ESP states that the reservoir’s dynamics should asymptotically depend only on the driving input signal, while dependencies on initial conditions should vanish with time such that the state of the network tends to represent an “echo” of the input. Essentially, the ESP links the asymptotic behavior of the reservoir dynamics to the input signal on which the reservoir is running. Although a certain research effort has being devoted in the last years to describe and understand more and more in depth the conditions under which the ESP holds (see e.g. [\[17\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0017), [\[37\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0037), [\[38\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0038)), two basic conditions are widely adopted in literature for this purpose. Specifically, a sufficient condition and a necessary condition are applied to the weight matrix $<math><mrow is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mo is="true">,</mo></mrow></math>$ requiring to respectively control its 2-norm (i.e. its maximum singular value) and its [spectral radius](https://www.sciencedirect.com/topics/computer-science/spectral-radius "Learn more about spectral radius from ScienceDirect's AI-generated Topic Pages") (i.e. the maximum among the eigenvalues in modulus) to be below unity. In the following, we will refer to the standard ESN model, as described by [Eq. (1)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0001) and [(2)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0002) as *shallow ESN*.

In this paper we are concerned with the extension of the shallow ESN model towards a deep architecture, in which the recurrent component is hierarchically organized into a stack of reservoir layers. The corresponding model is termed DeepESN, as introduced in [\[15\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0015), [\[16\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0016). From a general perspective, it is worth to note that, although several possible ways of constructing deep recurrent architectures have been investigated in literature [\[3\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0003), a stacked composition of recurrent hidden layers is likely to represent the most common choice (see e.g. [\[4\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0004), [\[5\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0005), [\[6\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0006), [\[8\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0008)).

Focusing on the recurrent part of the architecture of a DeepESN, graphically illustrated in [Fig. 1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0001), at each time step the state computation follows a pipeline from the external input towards the higher layer. Specifically, at time step *t* the first layer is fed by the external input, whereas each layer in the hierarchy at depth higher than 1 is fed by the output of the previous layer at the same time step *t*.

![Fig. 1](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr1.jpg)

1.  [Download : Download high-res image (168KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr1_lrg.jpg "Download high-res image (168KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr1.jpg "Download full-size image")

Fig. 1. Layered reservoir architecture of a DeepESN.

Keeping the basic notation introduced above for shallow ESNs, here we use *NL* to denote the number of reservoir layers in the stacked architecture, assuming for the ease of presentation that every layer has the same dimension (i.e. the same number of recurrent units), which we indicate by *NR*. Moreover, for every $<math><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">,</mo></mrow></math>$ we use $<math><mrow is="true"><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup></mrow></math>$ to indicate the state of the reservoir in the *i*th layer at time step *t*.

Viewing the DeepESN as a whole system, the global state space of the network can be considered as the product of the *NL* state spaces of the layers in the architecture. Accordingly, the global state of the DeepESN at time step *t* is represented by $<math><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">g</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></mrow></msup></mrow></math>$. From a dynamical system point of view, the global dynamics of a DeepESN is ruled by its global state transition function *F*: (3)$<math><mrow is="true"><mi is="true">F</mi><mo is="true">:</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">U</mi></msub></msup><mo is="true">×</mo><munder is="true"><munder accentunder="true" is="true"><mrow is="true"><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup><mo is="true">×</mo><mo is="true">…</mo><mo is="true">×</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup></mrow><mo is="true">︸</mo></munder><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mspace width="4.pt" is="true"></mspace><mtext is="true">times</mtext></mrow></munder><mo is="true">→</mo><munder is="true"><munder accentunder="true" is="true"><mrow is="true"><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup><mo is="true">×</mo><mo is="true">…</mo><mo is="true">×</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup></mrow><mo is="true">︸</mo></munder><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mspace width="4.pt" is="true"></mspace><mtext is="true">times</mtext></mrow></munder></mrow></math>$

which, given the external input, describes the evolution of the global system’s dynamics between two any consecutive time steps, i.e. for every *t* it results $<math><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">g</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">g</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></math>$. The global state transition function *F* can be conveniently considered in its layer-wise form, i.e. $<math><mrow is="true"><mi is="true">F</mi><mo is="true">=</mo><mo is="true">(</mo><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">)</mo></mrow></msup><mo is="true">)</mo><mo is="true">,</mo></mrow></math>$ where for each $<math><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">,</mo></mrow></math>$ *F*(*i*) represents the state transition function ruling the dynamics of the *i*th layer. In particular, the state transition function of the first layer, i.e. *F*(1), can be defined as follows: (4)$<math><mtable displaystyle="true" is="true"><mtr is="true"><mtd is="true"></mtd><mtd is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">:</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">U</mi></msub></msup><mo is="true">×</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup><mo is="true">→</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd><mtd is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="1em" is="true"></mspace><mo is="true">=</mo><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd><mtd is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="1em" is="true"></mspace><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">)</mo></mrow><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mi is="true">tanh</mi><mo is="true">(</mo></mrow><msub is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">θ</mi></mrow><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd><mtd is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="2.em" is="true"></mspace><mo is="true">+</mo><mspace width="0.16em" is="true"></mspace><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo><mo is="true">,</mo></mrow></mrow></mtd></mtr></mtable></math>$where *a*(1) ∈ \[0, 1\] is the leaking rate parameter of the first layer, $<math><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub><mo is="true">×</mo><msub is="true"><mi is="true">N</mi><mi is="true">U</mi></msub></mrow></msup></mrow></math>$ is the input weight matrix (as in [Eq. (1)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0001) for the shallow ESN case), $<math><mrow is="true"><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">θ</mi></mrow><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup></mrow></math>$ is the weight vector associated to the unitary input bias for the first layer and $<math><mrow is="true"><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub><mo is="true">×</mo><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></mrow></msup></mrow></math>$ is the recurrent reservoir weight matrix for the first layer.

For layer *i* > 1, the state at time step *t*, i.e. **x**(*i*)(*t*), directly depends on the state of the previous layer $<math><mrow is="true"><mi is="true">i</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></math>$ at the same time step, i.e. $<math><mrow is="true"><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$ and on the state of the layer *i* at the previous time step, i.e. $<math><mrow is="true"><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$ as can be seen in the third line of [Eq. (5)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0005). Apart from the dependence on the state of the same layer at the previous time step, we can further observe that **x**(*i*)(*t*) recursively depends on the activation at the previous time step $<math><mrow is="true"><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></math>$ of all the previous layers $<math><mrow is="true"><mi is="true">i</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">,</mo><mi is="true">i</mi><mo is="true">−</mo><mn is="true">2</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ back to the input at the present time step **u**(*t*). Thereby, the state transition function *F*(*i*), for *i* > 1, can be defined as follows: (5)$<math><mtable displaystyle="true" is="true"><mtr is="true"><mtd is="true"></mtd><mtd is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">:</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">U</mi></msub></msup><mo is="true">×</mo><munder is="true"><munder accentunder="true" is="true"><mrow is="true"><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup><mo is="true">×</mo><mo is="true">…</mo><mo is="true">×</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup></mrow><mo is="true">︸</mo></munder><mrow is="true"><mi is="true">i</mi><mspace width="4.pt" is="true"></mspace><mtext is="true">times</mtext></mrow></munder><mo is="true">→</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd><mtd is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="1em" is="true"></mspace><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd><mtd is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="1em" is="true"></mspace><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">)</mo></mrow><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mi is="true">tanh</mi><mo is="true">(</mo></mrow><msubsup is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">θ</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd><mtd is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="2.em" is="true"></mspace><mo is="true">+</mo><mspace width="0.16em" is="true"></mspace><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd><mtd is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="1em" is="true"></mspace><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">)</mo></mrow><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mi is="true">tanh</mi><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">(</mo></mrow><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd><mtd is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="2.em" is="true"></mspace><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mrow is="true"><mo is="true">)</mo><mo is="true">+</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">θ</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">+</mo><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></mtd></mtr></mtable></math>$where *a*(*i*) ∈ \[0, 1\] is the leaking rate parameter of the *i*\-th layer, $<math><mrow is="true"><msubsup is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub><mo is="true">×</mo><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></mrow></msup></mrow></math>$ is the inter-layer reservoir weight matrix for layer *i*, corresponding to the connections from the state of layer $<math><mrow is="true"><mi is="true">i</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ i.e $<math><mrow is="true"><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$ to the state of layer *i*, i.e. **x**(*i*)(*t*), $<math><mrow is="true"><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">θ</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></msup></mrow></math>$ is the weight vector associated to the unitary input bias for layer *i* and $<math><mrow is="true"><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub><mo is="true">×</mo><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></mrow></msup></mrow></math>$ is the recurrent reservoir weight matrix at layer *i*.

**Remark 1**

Note that a shallow ESN can be obtained as a special case of a DeepESN whose reservoir architecture has just one layer. In fact, whenever the hierarchy of reservoirs in the DeepESN contains only one layer, i.e. if $<math><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ then the state transition function *F* in [Eq. (3)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0003) reduces to *F*(1) in [Eq. (4)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0004), which in turn corresponds to the case of a shallow ESN in [Eq. (1)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0001).

**Remark 2**

It is interesting to observe that a DeepESN can be interpreted as obtained by applying some constraints to the architecture of a single-layer (shallow) ESN with the same total number of recurrent units. Specifically, the reservoir architecture of a DeepESN does not present connections from the input layer to layers at a level higher than 1, as well as connections from higher layers to lower layers and connections from each layer to higher layers different from the one that immediately follows in the pipeline. Under this viewpoint it is possible to see that the deepESN has a simpler architecture than a single-layer ESN, with a reduction in the number of reservoir weights that can be quantified. For instance, assuming full-connectivity in the involved matrices, this absolute reduction amounts to $<math><mrow is="true"><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">2</mn></mrow><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub></msubsup><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub><msub is="true"><mi is="true">N</mi><mi is="true">U</mi></msub><mo is="true">)</mo></mrow><mo is="true">+</mo><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">−</mo><mn is="true">1</mn></mrow></msubsup><msubsup is="true"><mi is="true">N</mi><mrow is="true"><mi is="true">R</mi></mrow><mn is="true">2</mn></msubsup><mi is="true">i</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">−</mo><mn is="true">2</mn></mrow></msubsup><msubsup is="true"><mi is="true">N</mi><mrow is="true"><mi is="true">R</mi></mrow><mn is="true">2</mn></msubsup><mi is="true">i</mi><mo is="true">)</mo></mrow></mrow></math>$$<math><mrow is="true"><mo is="true">=</mo><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><msub is="true"><mi is="true">N</mi><mi is="true">U</mi></msub><mo is="true">+</mo><msubsup is="true"><mi is="true">N</mi><mrow is="true"><mi is="true">R</mi></mrow><mn is="true">2</mn></msubsup><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mi is="true">N</mi><mrow is="true"><mi is="true">L</mi></mrow><mn is="true">2</mn></msubsup><mo is="true">−</mo><mn is="true">2</mn><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">+</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></math>$ which is quadratic in both the number of units per layer and in the number of layers. This peculiar architectural organization influences the way in which the temporal information is processed by different sub-parts of the hierarchical reservoir, composed by recurrent units that are progressively more distant from the input. Apart from this architectural simplification, notice that layering is implemented by using non-delayed connections between successive reservoir levels. The absence of delays in the transmission of the state information between successive layers enables to process the temporal information at each time step in a deep fashion, through a hierarchical composition of multiple levels of recurrent units. As an additional point, notice that the use of (*tanh*) non-linearities applied individually to each layer during the state computation does not allow to describe the DeepESN dynamics by means of an equivalent shallow system.

Overall, the combination of the above described constraints (i.e. architectural simplification and non-delayed connections between layers) realizes a specific type of model that is different from the standard ESN and that has distinctive characteristics, started to be analyzed in [\[15\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0015), [\[16\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0016), [\[18\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0018) and further investigated in this paper.

Training a DeepESN is a process that is carried out similarly to the case of a shallow ESN, i.e. by adjusting on a training set the parameters of a linear readout layer, whose operation is characterized as in [Eq. (2)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0002). The difference with respect to the shallow ESN case is that at each time step *t* the input for the readout is given by the global state of the DeepESN, i.e. by **x***g*(*t*). Apart from these considerations, in the rest of this paper we shall not further address the aspects related to the network training, keeping the focus of our investigations on the DeepESN dynamics.

We shall assume in our analysis that the input and the reservoir state spaces are compact sets. Moreover, from the notation viewpoint, we will use the symbol *ρ*(·) to denote the spectral radius of its matrix argument, and **I** to denote the [identity matrix](https://www.sciencedirect.com/topics/computer-science/identity-matrix "Learn more about identity matrix from ScienceDirect's AI-generated Topic Pages") (whose dimension can be obtained from the context).

Recently, the ESP for valid reservoir’s dynamics initialization has been extended to the case of DeepESNs in [\[18\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0018), where a sufficient condition and a necessary condition for the ESP to hold in case of hierarchical reservoir architectures have been provided. These conditions are briefly recalled in the following, while the reader is referred to [\[18\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0018) for their proofs. In particular, the sufficient condition for the ESP of a DeepESN is related to the study of contractivity of the state transition function in [Eq. (3)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0003) that rules the network’s dynamics. Basically, such condition states that the maximum rate of contraction among the dynamics of all the reservoir layers should be below unity, as reported in the following [Proposition 1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0003).

**Proposition 1**

*Consider a DeepESN whose dynamics is given by the state transition functions in*[*Eqs. (3)*](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0003)–[*(5)*](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0005). *A sufficient condition for the ESP to hold is then given by* (6)$<math><mrow is="true"><munder is="true"><mi is="true">max</mi><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub></mrow></munder><msup is="true"><mi is="true">C</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">&lt;</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$*where the C*(*i*) *values are Lipschitz constants for the state transition functions of the reservoir layers in the DeepESN architectures. Specifically,*

(a)

*for the first layer (i = 1)*

$<math><mrow is="true"><msup is="true"><mi is="true">C</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><msub is="true"><mrow is="true"><mo is="true">∥</mo><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">∥</mo></mrow><mn is="true">2</mn></msub></mrow></math>$

(b)

*for higher layers (i*  >  *1)*

$<math><mrow is="true"><msup is="true"><mi is="true">C</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo></mrow><msup is="true"><mi is="true">C</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">∥</mo></mrow><msubsup is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup><msub is="true"><mrow is="true"><mo is="true">∥</mo></mrow><mn is="true">2</mn></msub><mrow is="true"><mo is="true">+</mo><mo is="true">∥</mo></mrow><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><msub is="true"><mo is="true">∥</mo><mn is="true">2</mn></msub><mo is="true">)</mo></mrow></mrow></math>$*.*

The necessary condition for the ESP of DeepESNs is rooted in the analysis of [asymptotic stability](https://www.sciencedirect.com/topics/computer-science/asymptotic-stability "Learn more about asymptotic stability from ScienceDirect's AI-generated Topic Pages") of network’s dynamics. Essentially it says that the maximum among the effective spectral radii of the reservoir layers should be below unity, as stated by the following [Proposition 2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0004).

**Proposition 2**

*Consider a DeepESN whose dynamics is given by the state transition functions in*[*Eqs. (3)*](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0003)*–*[*(5)*](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0005)*, and assume that the set of admissible inputs for the system includes the null sequence. Then, a necessary condition for the ESP to hold is given by the following equation:* (7)$<math><mrow is="true"><munder is="true"><mi is="true">max</mi><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub></mrow></munder><mrow is="true"><mi is="true">ρ</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">)</mo></mrow><mi mathvariant="bold" is="true">I</mi><mo is="true">+</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo stretchy="true" is="true">)</mo></mrow></mrow><mo is="true">&lt;</mo><mn is="true">1</mn><mo is="true">.</mo></mrow></math>$

Notice that in [Propositions 1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0003) and [2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0004) we respectively make use of the 2-norm and of the spectral radius. Although these operators are clearly related [\[39\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0039), we recall that their values are generally different unless [symmetric matrices](https://www.sciencedirect.com/topics/computer-science/symmetric-matrix "Learn more about symmetric matrices from ScienceDirect's AI-generated Topic Pages") are considered, which is typically not the case in common reservoir [initialization procedures](https://www.sciencedirect.com/topics/computer-science/initialization-procedure "Learn more about initialization procedures from ScienceDirect's AI-generated Topic Pages").

**Remark 3**

Note that if the DeepESN contains only one reservoir layer, i.e. if $<math><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ then the sufficient and the necessary conditions reported respectively in [Eqs. (6)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0006) and [(7)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0007), reduce to the corresponding conditions for the case of shallow ESN, as usually reported in the RC literature [\[11\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0011), [\[14\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0014).

Here it is worth to observe that, although the necessary condition in [Eq. (7)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0007) is strictly related to the stability analysis of DeepESN dynamics, it actually covers only a limited aspect of the topic. Indeed, it deals with asymptotic stability of the null state as fixed point of the state transition function governing the system, under the assumption of a constant null input. In this sense, the analysis of RC dynamics through the spectral radii in the necessary condition expressed by [Eq. (7)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0007) is in fact “blind” to the external input. In more general cases, the stability of the dynamical system implemented by the (hierarchical) reservoir of a DeepESN should be analyzed by taking into consideration the actual trajectories in the state space in which the system is driven also in dependence of the actual input signal. This aspect in addressed in the following [Section 3](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0003).

## 3\. Stability of DeepESN dynamics

In this Section we go more in depth in the analysis of the stability of [dynamical systems](https://www.sciencedirect.com/topics/computer-science/dynamical-system "Learn more about dynamical systems from ScienceDirect's AI-generated Topic Pages") realized by hierarchically organized [RNNs](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about RNNs from ScienceDirect's AI-generated Topic Pages"). In particular, in an attempt to remove the constraints that are intrinsic in the analysis provided by the necessary condition for the ESP, we focus on the study of the [Lyapunov exponents](https://www.sciencedirect.com/topics/computer-science/lyapunov-exponent "Learn more about Lyapunov exponents from ScienceDirect's AI-generated Topic Pages") of the dynamical system implemented by the reservoir part of a DeepESN. Lyapunov exponents represent a mathematical tool that provides a measure of the sensitivity of a dynamical system to initial conditions, and as such, in the case of RC networks, are intuitively and intimately related to the true essence of the ESP. Recently, the link between Lyapunov exponents and the ESP has been further investigated in [\[40\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0040), in which results in the area of mean field theory are used to show the relevant role of Lyapunov exponents in delineating the domain of “local” validity of the ESP in relation to the input [\[40\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0040). In the RC area, our analysis extends the applicability of the study of the Lyapunov exponents from the context of shallow RC networks (see e.g. [\[12\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0012), [\[21\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0021), [\[22\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0022), [\[30\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0030), [\[41\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0041)) to the case of reservoirs with a structured hierarchical organization.

In dynamical system theory, Lyapunov exponents give a measure of the rate of exponential distortion (i.e. stretching or shrinking) of the trajectories of a dynamical system starting in infinitesimally close initial states [\[42\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0042), [\[43\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0043), [\[44\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0044). In general, an *N*\-dimensional dynamical system is featured by *N* Lyapunov exponents, each of which characterizes the rate of distortion along one of the directions in the state space in which the system’s trajectory is evolving. If a Lyapunov exponent is smaller than 0 it means that two neighboring trajectories will stay close to each other along the direction corresponding to that exponent. Conversely, if a Lyapunov exponent is greater than 0, then two neighboring trajectories will deviate from each other exponentially fast along the corresponding direction, and in this case predictability of the system can be lost in a relatively short time. The maximum Lyapunov exponent plays a major role in this context, as it dominates the rate of divergence or convergence in the state space and thereby represents a useful indicator of the stability of the whole system during its evolution. In particular, if the maximum Lyapunov exponent has a value that is below (resp. above) 0, then the dynamical system is characterized by stable (resp. unstable) dynamics, with the value of 0 denoting a transition condition known as the edge of [criticality](https://www.sciencedirect.com/topics/computer-science/criticality "Learn more about criticality from ScienceDirect's AI-generated Topic Pages") [\[29\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0029), [\[32\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0032), [\[45\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0045), the edge of stability [\[21\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0021), [\[46\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0046), [\[47\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0047), or the edge of chaos [\[23\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0023), [\[24\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0024), [\[25\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0025), [\[30\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0030). In common practical situations, it is useful to consider the local Lyapunov exponents [\[19\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0019), [\[20\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0020), i.e. local finite-time approximations evaluated over a trajectory followed by the dynamical system while it is driven by a real input sequence. The spectrum of the local Lyapunov exponents is strictly related to the Jacobian of the [state transition function](https://www.sciencedirect.com/topics/computer-science/state-transition-function "Learn more about state transition function from ScienceDirect's AI-generated Topic Pages") ruling the dynamics of the system, which describes the instantaneous rate of local distortion at each time step. In particular, such approximations can be computed from the logarithm of the eigenvalues (in modulus) of the Jacobian of the state transition function, typically averaging the outcomes over a long trajectory [\[42\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0042). In the following, we shall use the symbol *λmax* to denote the maximum among the local Lyapunov exponents, which in light of the considerations outlined above can be adopted as a practical indicator of the local stability/instability dynamical behavior of the system under consideration along a real trajectory in the state space.

For a DeepESN whose dynamics is described by the global state transition function *F* in [Eq. (3)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0003), and it is layer-wise implemented through [Eqs. (4)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0004) and [(5)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0005), the [Jacobian matrix](https://www.sciencedirect.com/topics/computer-science/jacobian-matrix "Learn more about Jacobian matrix from ScienceDirect's AI-generated Topic Pages") at time step *t*, denoted by $<math><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><mi is="true">F</mi><mo is="true">,</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">g</mi></msub></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$ can be written as a block matrix in the following form: (8)$<math><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><mi is="true">F</mi><mo is="true">,</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">g</mi></msub></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">(</mo><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="0.28em" is="true"></mspace><mo is="true">…</mo></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="0.28em" is="true"></mspace><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="0.28em" is="true"></mspace><mo is="true">…</mo></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="0.28em" is="true"></mspace><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mo is="true">⋮</mo></mtd><mtd columnalign="left" is="true"><mo is="true">⋮</mo></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="0.28em" is="true"></mspace><mo is="true">⋱</mo></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="0.28em" is="true"></mspace><mo is="true">⋮</mo></mrow></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="0.28em" is="true"></mspace><mo is="true">…</mo></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="0.28em" is="true"></mspace><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></mtd></mtr></mtable><mo stretchy="true" is="true">)</mo></mrow></mrow></mtd></mtr></mtable></math>$where for $<math><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">,</mo></mrow></math>$ the block in position (*i, j*), i.e. $<math><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">j</mi><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$ is the partial derivative of the state transition function of the *i*\-th layer with respect to the state of the *j*th layer at previous time step, i.e. (9)$<math><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">j</mi><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">∂</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">2</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mrow><mrow is="true"><mi is="true">∂</mi><mrow is="true"><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">j</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></mrow></mfrac><mo is="true">.</mo></mrow></math>$It is interesting to observe that the hierarchical organization of the reservoir layers in a DeepESN architecture is reflected into the shape of the Jacobian matrix, which results to be a lower triangular block matrix. This structure can be exploited in the computation of the Jacobian matrix in [Eq. (8)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0008), as stated by the following [Lemma 1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0006).

**Lemma 1**

*Consider a DeepESN with NL layers, and whose dynamics is given by*[*Eqs. (3)*](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0003)*–*[*(5)*](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0005). *For every* $<math><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub></mrow></math>$ *the block element (i, j) of the Jacobian matrix of the global state transition function of the DeepESN can be computed as follows:* (10)$<math><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">j</mi><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><mn mathvariant="bold" is="true">0</mn></mtd><mtd columnalign="left" is="true"><mrow is="true"><mtext is="true">for</mtext><mspace width="4.pt" is="true"></mspace><mi is="true">i</mi><mo is="true">&lt;</mo><mi is="true">j</mi></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">)</mo></mrow><mi mathvariant="bold" is="true">I</mi><mo is="true">+</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><msup is="true"><mi mathvariant="bold" is="true">D</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mtext is="true">for</mtext><mspace width="4.pt" is="true"></mspace><mi is="true">i</mi><mo is="true">=</mo><mi is="true">j</mi></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><munderover is="true"><mo is="true">∏</mo><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">0</mn></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">−</mo><mo is="true">(</mo><mi is="true">j</mi><mo is="true">+</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></munderover><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">−</mo><mi is="true">k</mi><mo is="true">)</mo></mrow></msup><msup is="true"><mi mathvariant="bold" is="true">D</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">−</mo><mi is="true">k</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><msubsup is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">−</mo><mi is="true">k</mi><mo is="true">)</mo></mrow></msubsup><mo stretchy="true" is="true">)</mo></mrow></mtd><mtd is="true"></mtd></mtr><mtr is="true"><mtd is="true"></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><mspace width="1em" is="true"></mspace><mo is="true">×</mo><mrow is="true"><mo stretchy="true" is="true">[</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">j</mi><mo is="true">)</mo></mrow></msup><mo is="true">)</mo></mrow><mi mathvariant="bold" is="true">I</mi><mo is="true">+</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">j</mi><mo is="true">)</mo></mrow></msup><msup is="true"><mi mathvariant="bold" is="true">D</mi><mrow is="true"><mo is="true">(</mo><mi is="true">j</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">j</mi><mo is="true">)</mo></mrow></msup><mo stretchy="true" is="true">]</mo></mrow></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mtext is="true">for</mtext><mspace width="4.pt" is="true"></mspace><mi is="true">i</mi><mo is="true">&gt;</mo><mi is="true">j</mi></mrow></mtd></mtr></mtable></mrow></mrow></math>$*where, for every* $<math><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">,</mo></mrow></math>$ **D**(*i*)(*t*) *is a* *[diagonal matrix](https://www.sciencedirect.com/topics/computer-science/diagonal-matrix "Learn more about diagonal matrix from ScienceDirect's AI-generated Topic Pages")* *whose non-zero entries are given by* $<math><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mover accent="true" is="true"><mi is="true">x</mi><mo is="true">˜</mo></mover><mn is="true">1</mn><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mn is="true">2</mn></msup><mo is="true">)</mo><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mover accent="true" is="true"><mi is="true">x</mi><mo is="true">˜</mo></mover><mn is="true">2</mn><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mn is="true">2</mn></msup><mo is="true">)</mo><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mo is="true">…</mo><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mover accent="true" is="true"><mi is="true">x</mi><mo is="true">˜</mo></mover><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mn is="true">2</mn></msup><mo is="true">)</mo><mo is="true">,</mo></mrow></math>$ *where the elements of the vector* $<math><mrow is="true"><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">x</mi><mo is="true">˜</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></math>$ *are defined as* (11)$<math><mrow is="true"><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">x</mi><mo is="true">˜</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><mi is="true">tanh</mi><mo is="true">(</mo><msub is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">+</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">θ</mi></mrow><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">+</mo><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mtext is="true">if</mtext><mspace width="4.pt" is="true"></mspace><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><mrow is="true"><mi is="true">tanh</mi><mo is="true">(</mo></mrow><msubsup is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold" is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup></mrow></mtd></mtr><mtr is="true"><mtd is="true"></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><mrow is="true"><mspace width="1em" is="true"></mspace><mo is="true">×</mo><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo><mo is="true">+</mo></mrow><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">θ</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">+</mo><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mtext is="true">if</mtext><mspace width="4.pt" is="true"></mspace><mi is="true">i</mi><mo is="true">&gt;</mo><mn is="true">1</mn><mo is="true">.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>$

**Proof**

The proof is given in [Appendix A](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0007). □

The knowledge of the Jacobian of the global state transition function of a DeepESN is useful, e.g. for the computation of local first-order approximations of the network’s state evolution over time. However, whenever we are interested in the computation of the eigenvalues of the Jacobian in [Eq. (8)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0008) we can exploit a result from matrix theory to simplify the computation and avoid explicit calculation of the lower triangular blocks of the Jacobian. This result is reported in the following [Lemma 2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0007).

**Lemma 2**

*Consider a DeepESN with NL layers, and whose dynamics is given by,* [(3)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0003), [(4)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0004), [(5)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0005). *Then the set of the eigenvalues of the Jacobian matrix of its global state transition function (in*[*Eq. (8)*](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0008)*) is given by the set of the eigenvalues of its diagonal block matrices, i.e. for every time step t it results* (12)$<math><mrow is="true"><mtext is="true">eig</mtext><mrow is="true"><mo is="true">(</mo><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><mi is="true">F</mi><mo is="true">,</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">g</mi></msub></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">{</mo><mtext is="true">eig</mtext><mrow is="true"><mo is="true">(</mo><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo is="true">|</mo><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">}</mo></mrow><mo is="true">,</mo></mrow></math>$*where* eig(·) *is used to denote the set of eigenvalues of its matrix argument.*

**Proof**

The proof is given in  [Appendix B](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0008). □

Given the results of the previous [Lemmas 1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0006) and [2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0007), we can now provide an analytical expression for the *λmax* of a DeepESN, as stated in the following [Theorem 1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0008).

**Theorem 1**

*Consider a DeepESN with NL layers, whose dynamics is given by*[(3)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0003), [(4)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0004), [(5)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0005)*, and that is driven by an input sequence of length Ns. Then the maximum local Lyapunov exponent of the dynamical system implemented by the (hierarchical) reservoir component of the DeepESN can be estimated as follows:* (13)$<math><mrow is="true"><msub is="true"><mi is="true">λ</mi><mrow is="true"><mi is="true">m</mi><mi is="true">a</mi><mi is="true">x</mi></mrow></msub><mo is="true">=</mo><munder is="true"><mi is="true">max</mi><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><mstyle scriptlevel="1" displaystyle="false" is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub></mrow></mstyle></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mstyle scriptlevel="1" displaystyle="false" is="true"><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></mrow></mstyle></mtd></mtr></mtable></munder><mfrac is="true"><mn is="true">1</mn><msub is="true"><mi is="true">N</mi><mi is="true">s</mi></msub></mfrac><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><msub is="true"><mi is="true">N</mi><mi is="true">s</mi></msub></munderover><mi is="true">ln</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mtext is="true">eig</mtext><mi is="true">k</mi></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">)</mo></mrow><mi mathvariant="bold" is="true">I</mi><mo is="true">+</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><msup is="true"><mi mathvariant="bold" is="true">D</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo stretchy="true" is="true">)</mo></mrow><mrow is="true"><mo is="true">|</mo></mrow><mo stretchy="true" is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$*where* |eig*k*(·)| *denotes the modulus of the kth eigenvalue of its matrix argument and* **D**(*i*)(*t*) *is defined as in* [*Lemma 1*](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0006)*.*

**Proof**

Given the input sequence of length *Ns* and the resulting network’s dynamics, the *λmax* value can be estimated by the quantity: (14)$<math><mrow is="true"><msub is="true"><mi is="true">λ</mi><mrow is="true"><mi is="true">m</mi><mi is="true">a</mi><mi is="true">x</mi></mrow></msub><mo is="true">=</mo><munder is="true"><mi is="true">max</mi><mrow is="true"><msup is="true"><mi is="true">k</mi><mo is="true">′</mo></msup><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub></mrow></munder><mfrac is="true"><mn is="true">1</mn><msub is="true"><mi is="true">N</mi><mi is="true">s</mi></msub></mfrac><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><msub is="true"><mi is="true">N</mi><mi is="true">s</mi></msub></munderover><mrow is="true"><mi is="true">ln</mi><mo is="true">(</mo><mo is="true">|</mo></mrow><msub is="true"><mtext is="true">eig</mtext><msup is="true"><mi is="true">k</mi><mo is="true">′</mo></msup></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><mi is="true">F</mi><mo is="true">,</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">g</mi></msub></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mrow is="true"><mo is="true">|</mo><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>$where the index *k*′ spans over the entire dimension of the whole Jacobian. By applying the results of [Lemmas 1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0006) and [2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0007) directly to the expression in [Eq. (14)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0014), we have that at each time step, the maximum of the $<math><mrow is="true"><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mtext is="true">eig</mtext><msup is="true"><mi is="true">k</mi><mo is="true">′</mo></msup></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><mi is="true">F</mi><mo is="true">,</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">g</mi></msub></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mrow is="true"><mo is="true">|</mo></mrow></mrow></math>$ values corresponds to the maximum of the $<math><mrow is="true"><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mtext is="true">eig</mtext><mi is="true">k</mi></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">−</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">)</mo></mrow><mi mathvariant="bold" is="true">I</mi><mo is="true">+</mo><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><msup is="true"><mi mathvariant="bold" is="true">D</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo stretchy="true" is="true">)</mo></mrow><mrow is="true"><mo is="true">|</mo></mrow></mrow></math>$ values. From this, the statement of the theorem easily follows: □

The value of the *λmax* estimate derived in [Theorem 1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0008) provides us with useful information about the quality of the DeepESN dynamics in terms of stability. In particular, from [Eq. (13)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0013) we can see that the relation to the driving input signal at each time step is embedded into the **D**(*i*)(*t*) matrices, through the actual values of the network’s states along the trajectory followed in response (also) to the history of the inputs that have excited the reservoir. In this sense, the condition *λmax* < 0 can provide a mean for assessing the network stability that is more complete than the one that is given by the necessary condition for the ESP in [Eq. (7)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0007). On the other hand, if we restrict our analysis around the null state and assume null input for the system, then the condition *λmax* < 0 reduces back to the necessary condition for the ESP in [Eq. (7)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0007).

**Remark 4**

Note that if the DeepESN contains only one reservoir layer, i.e. if $<math><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ then the formula for the computation of *λmax* in [Eq. (13)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0013) reduces to the case of shallow ESNs as reported in literature (see e.g. [\[21\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0021), [\[22\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0022), [\[46\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0046)).

**Remark 5**

Here it is worth noticing that *in general cases* the value of *λmax* should not be considered as a predictor of the network performance on computational tasks. On the one hand, values of *λmax* > 0 denote an unstable (chaotic) network’s behavior, which is undesirable as it would imply that two input sequences that only slightly deviate from each other will lead the same network towards completely different states, thus compromising the [generalization ability](https://www.sciencedirect.com/topics/computer-science/generalization-ability "Learn more about generalization ability from ScienceDirect's AI-generated Topic Pages") of the model. On the other hand, too small values of *λmax* could denote a dynamic regime that is restricted into a region of excessive stability (order) of network’s states, in which differences in the external input do not have a strong impact on the state. In such cases it is possible that the network forgets the previous inputs too fast, resulting in a system with limited memory that could not be suitable to tackle some kind of tasks (e.g. those requiring much in terms of memory). Essentially, the value of *λmax* that is more appropriate to solve a specific task results from a delicate trade-off between stability of dynamics and memory span, as already discussed in the RC literature in terms of the relation between pairwise separation property and fading memory of reservoirs [\[23\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0023). Thus, the result provided by [Theorem 1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0008) should not be interpreted as a mean to predict the performance on general tasks, instead it should be seen as a tool to accurately control the regime of system dynamics developed by a deep recurrent architecture.

**Remark 6**

By inspecting [Eq. (13)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0013) we can notice that the individual contributions given to the *λmax* computation by the different layers in the deep recurrent architecture are collectively aggregated by means of the maximum operator. This means that when we consider a deep recurrent architecture with progressively more layers, the resulting value of *λmax* can never decrease as the number of layers increases. In this sense, i.e. in the process of incremental network construction, we can see that the value of *λmax* is a monotonic non-decreasing function of the number of layers.

**Remark 7**

A closer look at the *λmax* formula in [Eq. (13)](https://www.sciencedirect.com/science/article/pii/S0925231218302157#eq0013) allows us to give an insight on the conditions under which adding reservoir layers to a DeepESN architecture actually increases the value of *λmax* and thereby lowers the degree of stability of the system. Assuming that all the layers in the network are featured by the same hyper-parameterization (leaking rate, inter-layer and recurrent weight matrices), then we can see that for each time step the eigenvalues (in modulus) of $<math><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">J</mi><mrow is="true"><msup is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">,</mo><msup is="true"><mi mathvariant="bold" is="true">x</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow></mrow></math>$ are upper bounded e.g. by its 2-norm. Following this line of reasoning, we can see that if the norm of the inter-layer weights is smaller than 1, then the bounds on the eigenvalues will tend to increase at each layer. Thereby, whenever one desires to have a *λmax* trend characterized as an increasing function of the number of reservoir layers, all with the same properties, our suggestion is to use “small” weights in the inter-layer connections.

**Remark 8**

The mathematical framework of [Theorem 1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0008), although proposed for deep RC models, can be applied also to the case of trained deep RNNs made up of stacked recurrent layers, at any stage of the training process, taking care of the evolution of the recurrent weight matrices over time.

## 4\. Numerical simulations

In this section we practically demonstrate the tool for stability analysis of DeepESN dynamics developed in the previous [Section 3](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0003). We first investigate the effect of the variation of DeepESN hyper-parameters on the resulting values of *λmax* for increasing number of network’s layers. Then, we focus on assessing the effect of layering itself on the *λmax* value, varying the conditions for network’s setting and in comparison with shallow ESN cases. Finally, in [Section 4.1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0005) we investigate the relations to the short-term Memory Capacity.

In our experimental investigation, we considered DeepESN where the weights in **W***in* and $<math><mrow is="true"><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msup><mo is="true">,</mo></mrow></math>$ as well as, for every layer *i* > 1, the weights in $<math><msubsup is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup></math>$ were chosen from a uniform distribution over $<math><mrow is="true"><mo is="true">[</mo><mo is="true">−</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">]</mo></mrow></math>$. For the sake of simplicity, all the bias terms for the reservoir layers were included into the corresponding input or inter-layer weight matrix. The values in $<math><msup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">W</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup></math>$ were randomly chosen in $<math><mrow is="true"><mo is="true">[</mo><mo is="true">−</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">]</mo><mo is="true">,</mo></mrow></math>$ and then re-scaled to meet a desired [spectral radius](https://www.sciencedirect.com/topics/computer-science/spectral-radius "Learn more about spectral radius from ScienceDirect's AI-generated Topic Pages") value, denoted by *ρ*(*i*), whereas the values in **W***in* were re-scaled to a desired value of its 2-norm, an input scaling parameter denoted by *scalein*, i.e. $<math><mrow is="true"><mrow is="true"><mo is="true">∥</mo></mrow><msub is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><msub is="true"><mrow is="true"><mo is="true">∥</mo></mrow><mn is="true">2</mn></msub><mo is="true">=</mo><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub></mrow></math>$. Analogously, the values in each $<math><msubsup is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup></math>$ were re-scaled to a desired 2-norm, an inter-layer scaling parameter denoted by $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msubsup is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup><mo is="true">,</mo></mrow></math>$ i.e. $<math><mrow is="true"><mrow is="true"><mo is="true">∥</mo></mrow><msubsup is="true"><mi mathvariant="bold" is="true">W</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup><msub is="true"><mrow is="true"><mo is="true">∥</mo></mrow><mn is="true">2</mn></msub><mo is="true">=</mo><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msubsup is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup></mrow></math>$. For the sake of simplicity, we used the same hyper-parameterization in all the layers, i.e. for all $<math><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">,</mo></mrow></math>$ we considered $<math><mrow is="true"><msup is="true"><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">=</mo><mi is="true">ρ</mi><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><msup is="true"><mi is="true">a</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msup><mo is="true">=</mo><mi is="true">a</mi></mrow></math>$ and $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msubsup is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow></msubsup><mo is="true">=</mo><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow></msub></mrow></math>$. We generally varied the considered values of the hyper-parameters, through the different experiments, in the following ranges: *ρ* ∈ \[0.5, 1.5\], *a* ∈ \[0.1, 1\], *scalein* ∈ \[0.1, 1\], and *scaleil* ∈ \[0.1, 1\], using in every case a step-size of 0.1. The values of *ρ, a, scalein* and *scaleil*, as well as the number of layers *NL*, were set in the different cases with the sole purpose of analysis and without any aim to optimize the achieved results. For every hyper-parameterization we independently generated 100 network realizations (i.e. guesses with different random seeds), and averaged the results over such realizations. As driving input signal for *λmax* computation[1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fn0001) we considered a one-dimensional time-series (*NU* = 1) of length 5000 ($<math><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">s</mi></msub><mo is="true">=</mo><mn is="true">5000</mn></mrow></math>$), whose elements were individually drawn from a uniform distribution in $<math><mrow is="true"><mo is="true">[</mo><mo is="true">−</mo><mn is="true">0.8</mn><mo is="true">,</mo><mn is="true">0.8</mn><mo is="true">]</mo><mo is="true">,</mo></mrow></math>$ and whose first 100 steps were used as initial transient for the reservoir states.

In a first set of experiments, we targeted the analysis of *λmax* variability resulting from changing the values of the DeepESN hyper-parameters, while increasing the number of layers and the total number of reservoir units. For these experiments we considered reservoir layers with $<math><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">R</mi></msub><mo is="true">=</mo><mn is="true">10</mn></mrow></math>$ units, increasing the number of layers from 1 to 10, i.e. $<math><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">L</mi></msub><mo is="true">∈</mo><mrow is="true"><mo is="true">{</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><mn is="true">10</mn><mo is="true">}</mo></mrow></mrow></math>$. The results of the *λmax* computation under the considered settings are graphically reported in [Fig. 2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0002), where progressively lighter colors correspond to higher values of *λmax*, i.e. to progressively less stable networks dynamics.

![Fig. 2](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr2.jpg)

1.  [Download : Download high-res image (317KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr2_lrg.jpg "Download high-res image (317KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr2.jpg "Download full-size image")

Fig. 2. Averaged values of *λmax* obtained by DeepESN for increasing number of reservoir layers. (a): Increasing values of *ρ*, $<math><mrow is="true"><mi is="true">a</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow></math>$. (b): Decreasing values of *a*, $<math><mrow is="true"><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow></math>$. (c): Decreasing values of *scalein*, $<math><mrow is="true"><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mi is="true">a</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow></math>$. (d): Decreasing values of *scaleil*, $<math><mrow is="true"><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mi is="true">a</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow></math>$.

As we can see, the developed tool allows us to identify regions in the network’s hyper-parameters space characterized by different degrees of stability of the involved state dynamics. Specifically, we can observe in [Fig. 2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0002)(a) that higher values of *ρ* result in higher values of *λmax*, confirming what has been observed also in shallow ESNs e.g. in [\[22\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0022). Moreover, we can see that the value of *λmax* tends to increase for increasing number of layers in the deep recurrent architecture, eventually switching from stability to an unstable behavior in correspondence of the higher values of *ρ* and of *NL*. From [Fig. 2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0002)(b), [2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0002)(c) and [2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0002)(d) we can appreciate the effect of the variation of *a, scalein* and *scaleil*, respectively, on the value of *λmax*. As a rule of thumb, we can see that for all these three hyper-parameters smaller values lead to higher values of *λmax*, with an increasing trend for increasing *NL*, although the general impact appears to be less strong than in the case of *ρ*, as it can be observed by comparing the ranges of *λmax* variation in the different cases. Results achieved on the synthetic data described so far are qualitatively confirmed by the experimental assessment on cases with real-world input, as reported in [Appendix C](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0009).

In a second set of experiments, we have analyzed more in depth the impact of the hyper-parameters that led to a higher excursion in the results illustrated in [Fig. 2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0002), namely the spectral radius and the number of recurrent units (layers in the DeepESN). We started by comparing the values of *λmax* obtained by DeepESNs for increasing values of the *ρ* hyper-parameter, varying the number of networks layers while keeping constant the number of total reservoir units to 100. [Fig. 3](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0003) shows the achieved values of *λmax* when the 100 reservoir units are arranged into 20, 10 and 5 layers, where in each case results are averaged (and standard deviations are computed) over the network realizations. For a further comparison, in the same figure we also plotted the results achieved with a shallow ESN with the same hyper-parameterization and total number of reservoir units. From [Fig. 3](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0003) the effect of layering emerges clearly: a higher number of layers leads to a higher value of *λmax*. This effect is amplified and becomes increasingly significant for increasing values of *ρ* (as can be seen by looking also at the standard deviations in [Fig. 3](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0003)). Moreover, from the same figure we can see that from a certain point on, the same values of *λmax* achieved by shallow ESNs are obtained by layered reservoirs in correspondence of smaller values of *ρ*. Although [Fig. 3](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0003) refers to one among the possible choices of the *a, scalein* and *scaleil* parameters, we observed that the emerging properties are qualitatively independent of such choice[2](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fn0002).

![Fig. 3](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr3.jpg)

1.  [Download : Download high-res image (174KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr3_lrg.jpg "Download high-res image (174KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr3.jpg "Download full-size image")

Fig. 3. Averaged values of *λmax* obtained by DeepESN with a total number of 100 reservoir units organized in 20, 10 and 5 layers. The considered hyper-parameterization is for increasing values of *ρ* and $<math><mrow is="true"><mi is="true">a</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow></math>$ and $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow></msub><mo is="true">=</mo><mn is="true">0.5</mn></mrow></math>$. Results achieved by a shallow ESN with the same hyper-parameterization and the same number of reservoir units are reported as well for the sake of comparison.

An analogous experimental assessment, but for increasing total number of recurrent reservoir units, is provided in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0004), which shows the *λmax* values obtained by DeepESNs in which the recurrent units are organized in a progressively higher number of layers, each composed of 10 units. For comparison, in the same figure we plotted the results achieved, under the same experimental conditions and number of reservoir units, by shallow ESNs and ESNs in which the reservoir units are arranged into an increasing number of non connected groups. The latter represents an architectural variant called groupedESN, which in practice implements a similar degree of [sparsity](https://www.sciencedirect.com/topics/computer-science/sparsity "Learn more about sparsity from ScienceDirect's AI-generated Topic Pages") than DeepESN, but neglecting the layering factor. Specifically, in the architecture of a groupedESN the recurrent units are organized in groups, or sub-reservoirs, each of which is fed by the external input, while connections among the units in different sub-reservoirs are avoided. The groupedESN has been introduced in [\[15\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0015), [\[16\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0016) as an architectural baseline for comparative assessment of the impact of the layering factor on the characterization of the state evolution in DeepESNs. For all the cases considered in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0004), the plot shows the values averaged (and the standard deviations computed) over the different network realizations. Results in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0004) clearly show that organizing the same number of recurrent units into a layered architecture naturally and systematically leads to an overall network’s dynamics that is characterized by higher values of *λmax*, with regimes closer to the edge of [criticality](https://www.sciencedirect.com/topics/computer-science/criticality "Learn more about criticality from ScienceDirect's AI-generated Topic Pages"). In this respect, the impact of layering can be observed already for networks with a few tens of recurrent units (i.e. with a small number of layers in the DeepESN experimental setting), while it becomes increasingly significant for increasing number of units (by looking at the standard deviations in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0004)). Moreover, we can observe that the results achieved for increasing number of units by groupedESNs and shallow ESNs are very close to each other, and, in comparison, a smaller number of units in DeepESNs leads to similar (and even higher) values of *λmax*. Looking at the standard deviations in the plot in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0004) we can also observe that the variability of the achieved results tends to decrease for increasing network’s sizes (a common effect in RC due to the randomized [initialization process](https://www.sciencedirect.com/topics/computer-science/initialization-process "Learn more about initialization process from ScienceDirect's AI-generated Topic Pages") on the reservoir’s weights, see e.g. [\[12\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0012)), with an even more noticeable effect in the case of DeepESN. Furthermore, it is worth noticing that while the case illustrated in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0004) is well representative of the emerging behavior, we observed the same characterization of the dynamics also for other choices of models hyper-parameters in the considered ranges[3](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fn0003).

![Fig. 4](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr4.jpg)

1.  [Download : Download high-res image (173KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr4_lrg.jpg "Download high-res image (173KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr4.jpg "Download full-size image")

Fig. 4. Averaged values of *λmax* obtained by DeepESN for increasing number of reservoir units, organized in layers of 10 units each. The considered hyper-parameterization corresponds to $<math><mrow is="true"><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mi is="true">a</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math>$ $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow></math>$ and $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow></msub><mo is="true">=</mo><mn is="true">0.5</mn></mrow></math>$. Results achieved by a shallow ESN and groupedESN with the same hyper-parameterization and the same number of reservoir units are reported as well for the sake of comparison.

Overall, the findings described in this section revealed the intrinsic role of layering in recurrent neural models in terms of increased values of the maximum local [Lyapunov exponent](https://www.sciencedirect.com/topics/computer-science/lyapunov-exponent "Learn more about Lyapunov exponent from ScienceDirect's AI-generated Topic Pages") *λmax*. This also implies that compared to the shallow case, deep [recurrent networks](https://www.sciencedirect.com/topics/computer-science/recurrent-network "Learn more about recurrent networks from ScienceDirect's AI-generated Topic Pages") can be brought more easily close to the critical region of stable-unstable transition of system dynamics. In this regard it is worth mentioning that, although in general not directly related to the [predictive performance](https://www.sciencedirect.com/topics/computer-science/predictive-performance "Learn more about predictive performance from ScienceDirect's AI-generated Topic Pages") on learning tasks, the edge of criticality identifies a region of dynamical regimes in which interesting and rich behaviors emerge. In fact, several evidences have been reported in literature that [RNNs](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about RNNs from ScienceDirect's AI-generated Topic Pages") whose dynamics is brought close to the edge of criticality are able to develop richer representations of their input history, with the ability to perform complex processing on temporal data and likely resulting in better performances on computational tasks that require longer short-term memory abilities (see e.g. [\[23\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0023), [\[24\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0024), [\[25\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0025), [\[26\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0026)). The fact that the performance of RNN models has a peak when their dynamics are nearby the critical transition is a known fact in literature for several classes of problems, e.g. in real-world tasks in the areas of speech processing [\[12\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0012) or telephone load predictions [\[32\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0032). Other examples are provided by benchmark tasks in the area of spiking neuron models, such as the computation of parity functions of spiking inputs or the [classification](https://www.sciencedirect.com/topics/computer-science/classification "Learn more about classification from ScienceDirect's AI-generated Topic Pages") of noisy spike patterns (see e.g. [\[24\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0024), [\[25\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0025), [\[31\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0031)), as well as in the ESN area, including the assessment of short-term memory and modeling of sinusoidal, Mackey-Glass, and NARMA systems (see e.g. [\[12\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0012), [\[29\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0029), [\[30\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0030), [\[32\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0032)). In particular, a well representative task in this regard is given by the short-term Memory Capacity task [\[33\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0033), whose relation to the critical regime of system dynamics has already been experimentally shown e.g. in [\[12\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0012), [\[27\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0027), [\[28\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0028), [\[29\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0029), [\[30\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0030). In this context, allowing to incorporate the influence of the driving input signal in the analysis of network stability, the control of the maximum local Lyapunov exponent represents a more accurate way to characterize the richness of reservoir dynamics in the critical regime than just controlling the spectral radius of the recurrent reservoir weight matrix, which in this respect acts just as an a-priori measure [\[12\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0012), [\[22\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0022), [\[27\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0027), [\[37\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0037), [\[46\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0046).

The experimental results illustrated in this section showed that organizing the same amount of recurrent units into layers has the inherent effect of accelerating the process of approaching the stability limit of network’s dynamics. In this sense, layering in recurrent networks can be seen as a sort of reservoir optimization methodology that is both simple and cheap, allowing a network with a smaller total number of recurrent units (than required in shallow cases) to develop a state behavior within a rich dynamical region. As already expressed in [Remark 5](https://www.sciencedirect.com/science/article/pii/S0925231218302157#enun0010), in general the performance on a computational task is strongly related to the properties of the task itself (and its target). Thereby, in general cases, dynamics at the edge of stability could neither guarantee nor be necessary to obtain good results (just like setting a spectral radius value close to 1 does not necessarily lead to a better performance in shallow ESNs). Our analysis suggests that whenever one knows that the temporal task at hand has a characterization that requires dynamics close to the edge of stability to be properly addressed, then it is well advisable to organize the recurrent units into layers and control the proximity to the boundary of stability using the mathematical framework proposed in this paper. Besides, note that also when the task at hand is known to require recurrent dynamics quite far from the border of stability, our framework can be used to control the dynamical regime developed by the network and keep it within the region of interest for the task. Moreover, in uncertain cases, when nothing is known on the task to be approached, the reservoir organization by layering could anyway represent a convenient choice for network construction. The results of our analysis, on the one hand, contribute to explain the potentiality of layered recurrent models in outperforming shallow networks with the same number of recurrent units [\[15\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0015), [\[16\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0016) in tasks for which reservoirs brought to the limit of stability have shown a performance maximization, such as the short-term Memory Capacity (see [\[30\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0030) for a discussion on this aspect in the case of shallow ESNs). On the other hand, as the increase in the number of layers could result in an undesired unstable network dynamics, they also emphasize the importance of the analysis and control of the *λmax* value through the proposed methodology.

The relation between *λmax* and the short-term Memory Capacity of DeepESNs is investigated in the following [Section 4.1](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0005).

### 4.1. Short-term memory capacity

The short-term Memory Capacity (MC) task has been introduced in [\[33\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0033) with the aim of assessing the ability of a reservoir network to precisely recall its previous input history from its state. Specifically, the task consists in training different readout units to reconstruct the input signal with an increasing delay. Using *yd*(*t*) to denote the output of the readout unit trained to recall the input signal with delay *d*, i.e. $<math><mrow is="true"><mi is="true">u</mi><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mi is="true">d</mi><mo is="true">)</mo><mo is="true">,</mo></mrow></math>$ the MC is defined as follows: (15)$<math><mrow is="true"><mi is="true">M</mi><mi is="true">C</mi><mo is="true">=</mo><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">d</mi><mo is="true">=</mo><mn is="true">0</mn></mrow><mi is="true">∞</mi></munderover><msup is="true"><mi is="true">r</mi><mn is="true">2</mn></msup><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mi is="true">d</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msub is="true"><mi is="true">y</mi><mi is="true">d</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></math>$where $<math><mrow is="true"><msup is="true"><mi is="true">r</mi><mn is="true">2</mn></msup><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mi is="true">d</mi><mo is="true">)</mo></mrow><mo is="true">,</mo><msub is="true"><mi is="true">y</mi><mi is="true">d</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">t</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow></mrow></math>$ is the squared correlation coefficient between the signals $<math><mrow is="true"><mi is="true">u</mi><mo is="true">(</mo><mi is="true">t</mi><mo is="true">−</mo><mi is="true">d</mi><mo is="true">)</mo></mrow></math>$ and *yd*(*t*). As input signal for the task we adopted the same sequence used for the experiments described in [Section 4](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0004), prolonging it to a total length of 6000 time steps. Specifically, the first 5000 time steps have been used for readout training using pseudo-inversion (with a transient of 100 steps), leaving the remaining 1000 time steps for test. Following a similar approach to e.g. [\[30\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0030), [\[48\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0048), we practically implemented the task by considering a finite number of 200 delays, equal to twice the maximum number of total reservoir units used in the experiments, i.e. 100 in our case. Also for the experiments described in this section, for each hyper-parameterization considered, the presented results were obtained as the average over 100 network realizations (i.e. guesses with different random seeds for initialization).

We started by considering DeepESNs with 10 layers of 10 units each, varying the value of *ρ* in \[0.5, 1.5\] and the value of *scaleil* in \[0.1, 1\], keeping fixed $<math><mrow is="true"><mi is="true">a</mi><mo is="true">=</mo><mn is="true">1</mn></mrow></math>$ and $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow></math>$. We computed the values of *λmax* for the different cases on the training sequence, plotting the achieved data versus the corresponding [test MC](https://www.sciencedirect.com/topics/neuroscience/memory-tests "Learn more about test MC from ScienceDirect's AI-generated Topic Pages") values. The result is illustrated in [Fig. 5](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0005). As it can be seen, the MC values tend to increase for increasing values of *λmax* with a peak close to the edge of stability (i.e. for *λmax* ≈ 0), after which the MC suddenly drops. We could also observe that the MC values tend to naturally cluster based on the values of the *ρ* hyper-parameter.

![Fig. 5](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr5.jpg)

1.  [Download : Download high-res image (68KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr5_lrg.jpg "Download high-res image (68KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr5.jpg "Download full-size image")

Fig. 5. [Test MC](https://www.sciencedirect.com/topics/neuroscience/memory-tests "Learn more about Test MC from ScienceDirect's AI-generated Topic Pages") values of DeepESNs plotted versus the *λmax* estimated on the training set.

A further set of experiments aimed at comparing the MC values obtained by DeepESNs with those obtained by shallow ESNs under the same range of settings, varying the number of recurrent units and the value of *ρ*. For these experiments we considered an increasing number of total reservoir units varying from 10 to 100, organized in layers of 10 units each in the case of DeepESNs, and in a single layer in the case of shallow ESN. Note that this choice allowed us to perform a fair comparison between the deep and shallow cases, under the condition of using the same number of trainable weights for the linear readout (i.e. the same number of free parameters for the learner). As regards the other aspects of networks hyper-parameterization, we considered values of *ρ* in \[0.5, 1.5\], while keeping fixed the values of $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">l</mi></mrow></msub><mo is="true">=</mo><mn is="true">0.5</mn></mrow></math>$ and $<math><mrow is="true"><mi is="true">s</mi><mi is="true">c</mi><mi is="true">a</mi><mi is="true">l</mi><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">i</mi><mi is="true">n</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow></math>$ (in order to ensure, for both DeepESNs and shallow ESNs, to have similar conditions of reservoir non-linearities operating sufficiently far from a linear regime, where the MC results could be biased towards higher values).

The results are illustrated in [Fig. 6](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0006), where lighter colors corresponds to higher values of the MC. A comparison between the results of DeepESN and shallow ESN, respectively in [Fig. 6](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0006)(a) and [6](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0006)(b), clearly highlights the advantage of the layered architecture and, in light of the results in [Fig. 5](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0005), it reflects the evidences reported in [Section 4](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0004). First of all, we can see that DeepESNs achieve higher values of MC than shallow ESNs under the same experimental conditions, which is in line with previous results on hierarchichal reservoir architectures [\[15\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0015), [\[18\]](https://www.sciencedirect.com/science/article/pii/S0925231218302157#bib0018). Secondly, and more importantly, results in [Fig. 6](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0006) point out that DeepESNs are able to reach higher values of MC “before” shallow ESNs, both in terms of smaller *ρ* values and in terms of a smaller total number of recurrent units (i.e. with a cheaper network’s architecture), which confirms the results on the *λmax* computation provided in [Section 4](https://www.sciencedirect.com/science/article/pii/S0925231218302157#sec0004) (see [Figs. 3](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0003) and [4](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0004)). Moreover, we can see that the drop in the MC value for higher values of *ρ* is rather abrupt for DeepESNs, while it is smoother for shallow ESNs. In light of our analysis, such observation can be explained in terms of the higher rate of *λmax* increase emerged in layered reservoir architectures for increasing values of *ρ* (see [Fig. 3](https://www.sciencedirect.com/science/article/pii/S0925231218302157#fig0003)).

![Fig. 6](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr6.jpg)

1.  [Download : Download high-res image (197KB)](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr6_lrg.jpg "Download high-res image (197KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S0925231218302157-gr6.jpg "Download full-size image")

Fig. 6. [Test MC](https://www.sciencedirect.com/topics/neuroscience/memory-tests "Learn more about Test MC from ScienceDirect's AI-generated Topic Pages") values for increasing number of total reservoir units and *ρ*. (a): DeepESNs with reservoir layers of 10 units; (b): shallow ESNs under the same hyper-parameterization and number of reservoir units.

## 5\. Conclusions

In this paper we have addressed the study of deep [RNNs](https://www.sciencedirect.com/topics/computer-science/recurrent-neural-network "Learn more about RNNs from ScienceDirect's AI-generated Topic Pages") from a [dynamical system](https://www.sciencedirect.com/topics/computer-science/dynamical-system "Learn more about dynamical system from ScienceDirect's AI-generated Topic Pages") viewpoint, focusing on the fundamental issue of stability. To this aim, we have developed a mathematical tool that extends the applicability of the analysis through the local [Lyapunov exponents](https://www.sciencedirect.com/topics/computer-science/lyapunov-exponent "Learn more about Lyapunov exponents from ScienceDirect's AI-generated Topic Pages") to the case of hierarchically organized recurrent neural models. In the analysis of the network’s stability, such a tool allows us to take into practical account also the external input signals that are actually influencing the system’s dynamics.

The proposed methodology has been introduced in the domain of deep RC networks, where its effectiveness has been shown on three datasets, covering both cases of artificial as well as real-world input signals. Moreover, the proposed tool allowed us to study the inherent role played by the layering factor in recurrent models. In particular, we have shown that increasing the number of layers in a recurrent architecture naturally leads the resulting systems dynamics towards progressively less stable and richer regimes. We have provided experimental evidence suggesting that the same amount of recurrent units has a richer dynamical behavior that is pushed closer to the edge of stability whenever the units are arranged into a hierarchy of layers. Results on the MC task pointed out that this phenomenon has interesting implications in cases in which the temporal task under consideration is better approached by a recurrent model operating close to the border of stability. Specifically, compared to shallow recurrent architectures, hierarchically organized recurrent models required a smaller number of recurrent units to achieve similar memory lengths, at the same time showing a higher peak of memory under analogous settings.

We hope that the approach developed in this paper would help to enhance the understanding on the theoretical properties of the dynamical behaviors developed by deep RNNs. At the same time, we believe that the proposed methodology represents a useful ground for a principled exploitation of the intrinsic potentiality of hierarchical recurrent models.
