
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../A%20model%20of%20multisecond%20timing%20behaviour%20under%20peak-interval%20procedures%20-%2011.07.22/">
      
      
        <link rel="next" href="../Action%20suppression%20reveals%20opponent%20parallel%20control%20via%20striatal%20circuits%20-%2012.07.22/">
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.6">
    
    
      
        <title>A review of learning in biologically plausible spiking neural networks - ScienceDirect - obsidian-mkdocs template</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.ded33207.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="pink" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#a-review-of-learning-in-biologically-plausible-spiking-neural-networks-sciencedirect" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="obsidian-mkdocs template" class="md-header__button md-logo" aria-label="obsidian-mkdocs template" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            obsidian-mkdocs template
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              A review of learning in biologically plausible spiking neural networks - ScienceDirect
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="pink" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="pink" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="obsidian-mkdocs template" class="md-nav__button md-logo" aria-label="obsidian-mkdocs template" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    obsidian-mkdocs template
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          10 Projects
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          10 Projects
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/10%20Projects/" class="md-nav__link">
        10 Projects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Active%20Projects%20Overview/" class="md-nav__link">
        Active Projects Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/All%20Projects%20Overview/" class="md-nav__link">
        All Projects Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Open%20Projects%20Overview/" class="md-nav__link">
        Open Projects Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Projects%20Overview%20Template/" class="md-nav__link">
        Projects Overview Template
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Projects%20Table/" class="md-nav__link">
        Projects Table
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7" id="__nav_2_7_label" tabindex="0">
          1 Main Research
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7">
          <span class="md-nav__icon md-icon"></span>
          1 Main Research
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_1" id="__nav_2_7_1_label" tabindex="0">
          Global Trigger Local Back Propagation
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_1">
          <span class="md-nav__icon md-icon"></span>
          Global Trigger Local Back Propagation
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Global%20Trigger%20Local%20Back-Propagation/Global%20Trigger%20Local%20Back-Propagation/" class="md-nav__link">
        Global Trigger Local Back Propagation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_2" id="__nav_2_7_2_label" tabindex="0">
          Make the fucking timing figure
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_2">
          <span class="md-nav__icon md-icon"></span>
          Make the fucking timing figure
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Make%20the%20fucking%20timing%20figure/Figure%20Descriptions/" class="md-nav__link">
        Figure Descriptions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Make%20the%20fucking%20timing%20figure/Make%20the%20fucking%20timing%20figure/" class="md-nav__link">
        Make the fucking timing figure
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_3" id="__nav_2_7_3_label" tabindex="0">
          PhD Proposal
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_3">
          <span class="md-nav__icon md-icon"></span>
          PhD Proposal
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/Nicolai%27s%20Comments%20on%20Draft%205/" class="md-nav__link">
        Nicolai's Comments on Draft 5
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20Proposal%20tasks/" class="md-nav__link">
        PhD Proposal tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20Proposal/" class="md-nav__link">
        PhD Proposal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Outline/" class="md-nav__link">
        PhD proposal Outline
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_3_5" id="__nav_2_7_3_5_label" tabindex="0">
          PhD proposal Drafts
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_7_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_3_5">
          <span class="md-nav__icon md-icon"></span>
          PhD proposal Drafts
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20%20Draft%208%20Research%20Plan/" class="md-nav__link">
        PhD Proposal  Draft 8 Research Plan
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%201/" class="md-nav__link">
        1 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%202%20-%20cut%20and%20paste%20palette/" class="md-nav__link">
        1 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%202/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%204/" class="md-nav__link">
        PhD Proposal Draft 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%205%20CUT%20AND%20PASTE%20PALETTE/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%205.1/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%205/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%206.1/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%206/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%207%20latex%20conversion/" class="md-nav__link">
        PhD Proposal Draft 7 latex conversion
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Draft%207/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20Mega%20Frakenstein%20Collage%20Draft%203/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20Proposal%20draft%202%20-%20outline/" class="md-nav__link">
        1 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/PhD%20proposal%20Drafts/" class="md-nav__link">
        PhD proposal Drafts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/PhD%20Proposal/PhD%20proposal%20Drafts/Research%20Question%20Section%20Draft%202/" class="md-nav__link">
        Research Question Section Draft 2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_4" id="__nav_2_7_4_label" tabindex="0">
          Recreating SBF Model
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_4">
          <span class="md-nav__icon md-icon"></span>
          Recreating SBF Model
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Abstract%20for%20SBFA%20-%2027.03.23/" class="md-nav__link">
        Abstract for SBFA   27.03.23
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Brief%20Overview%20of%20SBF%20Model/" class="md-nav__link">
        Brief Overview of SBF Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Outline%20of%20SBF%20Model%20for%20Supervision%20Meeting/" class="md-nav__link">
        Outline of SBF Model for Supervision Meeting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Outline%20of%20SBF%20Model/" class="md-nav__link">
        Outline of SBF Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Recreating%20SBF%20Model/Recreating%20SBF%20Model/" class="md-nav__link">
        Recreating SBF Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_5" id="__nav_2_7_5_label" tabindex="0">
          SBF   Automata Experiment
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_5">
          <span class="md-nav__icon md-icon"></span>
          SBF   Automata Experiment
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Algorithm%20Outline/" class="md-nav__link">
        Algorithm Outline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Experiment%20Environment%20Variants/" class="md-nav__link">
        Experiment Environment Variants
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/First%20python%20implementation%20-%20conditions%20and%20results/" class="md-nav__link">
        First python implementation   conditions and results
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Pseudocode%20for%20Modular%20Arrangement/" class="md-nav__link">
        Pseudocode for Modular Arrangement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Pseudocode%20for%20SBF%20Automata/" class="md-nav__link">
        Pseudocode for SBF Automata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBF%20-%20Automata%20-%20Design%201/" class="md-nav__link">
        The SBF Automata Design
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBF%20-%20Automata%20Algorithm%20Formalized/" class="md-nav__link">
        SBF   Automata Algorithm Formalized
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBF%20-%20Automata%20Experiment/" class="md-nav__link">
        SBF   Automata Experiment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20-%20Normalized%20Automata%20Vote/" class="md-nav__link">
        SBFA   Normalized Automata Vote
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20Development%20Log/" class="md-nav__link">
        SBFA Development Log
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20Experiment%20Development/" class="md-nav__link">
        SBFA Experiment Development
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/SBFA%20Experiment%20Results%2010.02.23/" class="md-nav__link">
        SBFA Experiment Results 10.02.23
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Weighted%20Majority%20Vote%20Version%2027.01.23/" class="md-nav__link">
        Weighted Majority Vote Version [[27.01.23]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_5_14" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_5_14" id="__nav_2_7_5_14_label" tabindex="0">
          Ooman Collab
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_7_5_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_5_14">
          <span class="md-nav__icon md-icon"></span>
          Ooman Collab
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SBF%20-%20Automata%20Experiment/Ooman%20Collab/Ooman%20Collab/" class="md-nav__link">
        Formerly the Automata Oscilllators Project
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_6" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_6" id="__nav_2_7_6_label" tabindex="0">
          SNN Models
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_6">
          <span class="md-nav__icon md-icon"></span>
          SNN Models
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/SNN%20Models/SNN%20Models%20/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_7" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_7" id="__nav_2_7_7_label" tabindex="0">
          Survey Paper
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_7">
          <span class="md-nav__icon md-icon"></span>
          Survey Paper
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Survey%20Paper/Survey%20Paper%20Tasks/" class="md-nav__link">
        Tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Survey%20Paper/Survey%20Paper/" class="md-nav__link">
        Survey Paper
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_7_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_7_3" id="__nav_2_7_7_3_label" tabindex="0">
          How to Conduct Survey
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_7_7_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_7_3">
          <span class="md-nav__icon md-icon"></span>
          How to Conduct Survey
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Survey%20Paper/How%20to%20Conduct%20Survey/How%20to%20Conduct%20Survey%20/" class="md-nav__link">
        How to Conduct Survey 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_8" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7_8" id="__nav_2_7_8_label" tabindex="0">
          Timing Tasks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7_8">
          <span class="md-nav__icon md-icon"></span>
          Timing Tasks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/1%20Main%20Research/Timing%20Tasks/Timing%20Tasks%20Research/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8" >
      
      
      
        <label class="md-nav__link" for="__nav_2_8" id="__nav_2_8_label" tabindex="0">
          2 Alt. Research
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_8">
          <span class="md-nav__icon md-icon"></span>
          2 Alt. Research
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_8_1" id="__nav_2_8_1_label" tabindex="0">
          Stable Diffusion Project
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_8_1">
          <span class="md-nav__icon md-icon"></span>
          Stable Diffusion Project
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/2%20Alt.%20Research/Stable%20Diffusion%20Project/Stable%20Diffusion%20Project/" class="md-nav__link">
        Stable Diffusion Project
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_8_2" id="__nav_2_8_2_label" tabindex="0">
          Vicente Collab
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_8_2">
          <span class="md-nav__icon md-icon"></span>
          Vicente Collab
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/2%20Alt.%20Research/Vicente%20Collab/Vicente%20Collab/" class="md-nav__link">
        Vicente Collab
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_9" >
      
      
      
        <label class="md-nav__link" for="__nav_2_9" id="__nav_2_9_label" tabindex="0">
          3 PhD Administrative Stuff
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_9">
          <span class="md-nav__icon md-icon"></span>
          3 PhD Administrative Stuff
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/Applying%20for%20trip%20expenses/" class="md-nav__link">
        How to apply for trip expenses
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/PhD%20Administrative%20Stuff/" class="md-nav__link">
        [Resources for Ph.D. candidates at TKD](https://ansatt.oslomet.no/en/ressursside-for-phd-kandidater-ved-tkd1)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/PhD%20Courses/" class="md-nav__link">
        PhD Courses
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/3%20PhD%20Administrative%20Stuff/welcome%20letter/" class="md-nav__link">
        ![[Velkomstbrev ph.d.-kandidater internt - engelsk 1.pdf]]
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10" id="__nav_2_10_label" tabindex="0">
          4 Conferences & Schools
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10">
          <span class="md-nav__icon md-icon"></span>
          4 Conferences & Schools
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_1" id="__nav_2_10_1_label" tabindex="0">
          DeepLearn 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_1">
          <span class="md-nav__icon md-icon"></span>
          DeepLearn 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/DeepLearn%202022%20Travel/" class="md-nav__link">
        DeepLearn 2022 budget proposal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/DeepLearn%202022/" class="md-nav__link">
        DeepLearn 2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/DeepLearn%20Lecture%20Selections/" class="md-nav__link">
        [[DeepLearn 2022]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/Sean%20Meyn%20-%20DeepLearn%202022%20summer%20-%2025.07.22/" class="md-nav__link">
        Sean Meyn   DeepLearn 2022 summer   25.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_1_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_1_5" id="__nav_2_10_1_5_label" tabindex="0">
          Schuller
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_10_1_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_1_5">
          <span class="md-nav__icon md-icon"></span>
          Schuller
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/DeepLearn%202022/schuller/Bjorn%20Schuller%20-%20DeepLearn%202022%20summer%20-%2025.07.22/" class="md-nav__link">
        Motivations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_2" id="__nav_2_10_2_label" tabindex="0">
          MLSS^N 2022 Krakow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_2">
          <span class="md-nav__icon md-icon"></span>
          MLSS^N 2022 Krakow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/MLSSN%20Review/" class="md-nav__link">
        MLSSN Review
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/MLSS%5EN%202022%20Krakow/" class="md-nav__link">
        Dataview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Our%20Contribution%20to%20Continual%20Learning/" class="md-nav__link">
        PDF
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_2_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_10_2_4" id="__nav_2_10_2_4_label" tabindex="0">
          Talk Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_10_2_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_10_2_4">
          <span class="md-nav__icon md-icon"></span>
          Talk Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Andrea%20Tagliasacchi/" class="md-nav__link">
        Andrea Tagliasacchi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Andrew%20Saxe/" class="md-nav__link">
        Andrew Saxe
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Ewa%20Szczurek/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Friedemann%20Zenke%20MLSSN%20Lecture%20-%2030.06.22/" class="md-nav__link">
        Lecture Material
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Grabaska-Barwinska%20MLSSN%20Lecture%20-%2001.07.22/" class="md-nav__link">
        A rapid and efficient learning rule
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Jan%20Chorowski%20MLSSN%20%20-%2002.07.22/" class="md-nav__link">
        Jan Chorowski MLSSN    02.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Joao%20Henriques/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Joao%20Sacremento%20Lecture%20-%2029.06.22/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Piotr%20Milos%20Lecture%20MLSS%20-%2002.07.22/" class="md-nav__link">
        Piotr Milos Lecture MLSS   02.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Poster%20Session%20notes/" class="md-nav__link">
        Poster Session notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Rafal%20Bogcaz/" class="md-nav__link">
        Materials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Razvan%20Pascnau%20Lecture%20-%2027.06.22/" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Rianne%20Van%20Den%20Berg%20Lecture/" class="md-nav__link">
        None
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Tomasz%20Trzcinski%20MLSS%20-%2002.07.22/" class="md-nav__link">
        Tomasz Trzcinski MLSS   02.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/4%20Conferences%20%26%20Schools/MLSS%5EN%202022%20Krakow/Talk%20Notes/Zieba%20MLSSN%20Lecture%20-%2001.07.22/" class="md-nav__link">
        Discrete normalizing flows
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11" id="__nav_2_11_label" tabindex="0">
          5 Presentations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11">
          <span class="md-nav__icon md-icon"></span>
          5 Presentations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/5%20Presentations/" class="md-nav__link">
        5 Presentations
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_2" id="__nav_2_11_2_label" tabindex="0">
          Present on SNN MAB
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_2">
          <span class="md-nav__icon md-icon"></span>
          Present on SNN MAB
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Present%20on%20SNN%20MAB/Present%20on%20SNN%20MAB/" class="md-nav__link">
        Present on SNN MAB
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_3" id="__nav_2_11_3_label" tabindex="0">
          Presenting on Petter 2018
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_3">
          <span class="md-nav__icon md-icon"></span>
          Presenting on Petter 2018
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20on%20Petter%202018/Petter%202018%20Slides%20-%20Draft%201/" class="md-nav__link">
        Petter 2018 Slides   Draft 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20on%20Petter%202018/Petter%202018%20Slides%20-%20Draft%202/" class="md-nav__link">
        Petter 2018 Slides   Draft 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20on%20Petter%202018/Presenting%20on%20Petter%202018/" class="md-nav__link">
        Presenting on Petter 2018
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_4" id="__nav_2_11_4_label" tabindex="0">
          Presenting to Ooman
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_4">
          <span class="md-nav__icon md-icon"></span>
          Presenting to Ooman
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/Presenting%20to%20Ooman/Presenting%20to%20Oomman/" class="md-nav__link">
        RL, SNNs, and Representing time
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_11_5" id="__nav_2_11_5_label" tabindex="0">
          Templates
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_11_5">
          <span class="md-nav__icon md-icon"></span>
          Templates
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/templates/tpl/" class="md-nav__link">
        Tpl
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/5%20Presentations/templates/tpl2/" class="md-nav__link">
        Tpl2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12" id="__nav_2_12_label" tabindex="0">
          6 Teaching
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12">
          <span class="md-nav__icon md-icon"></span>
          6 Teaching
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_1" id="__nav_2_12_1_label" tabindex="0">
          ACIT4420   Python Programming Course Autumn 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_1">
          <span class="md-nav__icon md-icon"></span>
          ACIT4420   Python Programming Course Autumn 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4420%20-%20Python%20Programming%20Course%20Autumn%202022/ACIT4420%20-%20Python%20Programming%20Course%20Autumn%202022/" class="md-nav__link">
        ACIT4420 - Python Scripting
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_2" id="__nav_2_12_2_label" tabindex="0">
          ACIT4620   Comp. Intel Course Autmun 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_2">
          <span class="md-nav__icon md-icon"></span>
          ACIT4620   Comp. Intel Course Autmun 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/" class="md-nav__link">
        [[ACIT4620 Log]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/ACIT4620%20Log/" class="md-nav__link">
        [[04.10.22]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/ACIT4620%20-%20Comp.%20Intel%20Course%20Autmun%202022/Recommended%20Changes%20to%20Course/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_3" id="__nav_2_12_3_label" tabindex="0">
          DATA3900 Bachelors Thesis Supervision
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_3">
          <span class="md-nav__icon md-icon"></span>
          DATA3900 Bachelors Thesis Supervision
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/DATA%203900%20Group%20-%20Microsoft%20Norge%20AS/" class="md-nav__link">
        DATA 3900 Group   Microsoft Norge AS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/DATA3900%20Bachelors%20Thesis%20Supervision/" class="md-nav__link">
        DATA3900 Bachelors Thesis Supervision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/Group%20-%20Intility%20AS/" class="md-nav__link">
        Group   Intility AS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/DATA3900%20Bachelors%20Thesis%20Supervision/MSFT%20Bacheloroppgave/" class="md-nav__link">
        MSFT Bacheloroppgave
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_12_4" >
      
      
      
        <label class="md-nav__link" for="__nav_2_12_4" id="__nav_2_12_4_label" tabindex="0">
          TA courses Autumn 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_12_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_12_4">
          <span class="md-nav__icon md-icon"></span>
          TA courses Autumn 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/6%20Teaching/TA%20courses%20Autumn%202022/TA%20courses%20Autumn%202022%20/" class="md-nav__link">
        TA Course for Fall 2022
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13" id="__nav_2_13_label" tabindex="0">
          7 Obsidian Vault Meta
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_13_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13">
          <span class="md-nav__icon md-icon"></span>
          7 Obsidian Vault Meta
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Body%20of%20Knowledge%20Without%20Organs/" class="md-nav__link">
        Body of Knowledge without Organs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Obsidian%20Vault%20Meta/" class="md-nav__link">
        Obsidian Vault Meta
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Obsidian%20web%20browsing%20-12.01.23/" class="md-nav__link">
        Obsidian web browsing  12.01.23
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/obsidian%20meta%20log/" class="md-nav__link">
        Obsidian meta log
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13_5" id="__nav_2_13_5_label" tabindex="0">
          Better Reading
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_13_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13_5">
          <span class="md-nav__icon md-icon"></span>
          Better Reading
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Better%20Reading/Better%20Reading%20%20/" class="md-nav__link">
        Better Reading  
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13_6" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13_6" id="__nav_2_13_6_label" tabindex="0">
          Showing off my obsidian wokflow
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_13_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13_6">
          <span class="md-nav__icon md-icon"></span>
          Showing off my obsidian wokflow
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Showing%20off%20my%20obsidian%20wokflow/Showing%20off%20my%20obsidian%20wokflow/" class="md-nav__link">
        Showing off my obsidian wokflow
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_13_7" >
      
      
      
        <label class="md-nav__link" for="__nav_2_13_7" id="__nav_2_13_7_label" tabindex="0">
          Tracking People
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_13_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_13_7">
          <span class="md-nav__icon md-icon"></span>
          Tracking People
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/7%20Obsidian%20Vault%20Meta/Tracking%20People/Tracking%20People/" class="md-nav__link">
        Tracking People
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_14" >
      
      
      
        <label class="md-nav__link" for="__nav_2_14" id="__nav_2_14_label" tabindex="0">
          AI Movie Series
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_14">
          <span class="md-nav__icon md-icon"></span>
          AI Movie Series
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI%20Movie%20Series/AI%20Movie%20Series/" class="md-nav__link">
        AI Movie Series
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_15" >
      
      
      
        <label class="md-nav__link" for="__nav_2_15" id="__nav_2_15_label" tabindex="0">
          AI Art Fair Project
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_15_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_15">
          <span class="md-nav__icon md-icon"></span>
          AI Art Fair Project
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Art%20Fair%20Project/AI-Art%20Fair%20Project/" class="md-nav__link">
        AI Art Fair Project
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_16" >
      
      
      
        <label class="md-nav__link" for="__nav_2_16" id="__nav_2_16_label" tabindex="0">
          AI Lab Retreat Dec. 5 7 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_16_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_16">
          <span class="md-nav__icon md-icon"></span>
          AI Lab Retreat Dec. 5 7 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/AI-Lab%20Retreat%20Dec.%205-7%202022/" class="md-nav__link">
        AI Lab Retreat Dec. 5 7 2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Contributions%20Challenge/" class="md-nav__link">
        Contributions Challenge
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Free%20writing%20test%202/" class="md-nav__link">
        Free writing assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Group%20collaboration%20for%20living%20documents/" class="md-nav__link">
        A New Publication Method for a Future of Shared Knowledge
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/Problem%20challenge/" class="md-nav__link">
        So here is a problem
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Retreat%20Dec.%205-7%202022/test%20free%20writing/" class="md-nav__link">
        Free writing exercises
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_17" >
      
      
      
        <label class="md-nav__link" for="__nav_2_17" id="__nav_2_17_label" tabindex="0">
          AI Lab Wiki
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_17_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_17">
          <span class="md-nav__icon md-icon"></span>
          AI Lab Wiki
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/AI-Lab%20Wiki/AI-Lab%20Wiki%20/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_18" >
      
      
      
        <label class="md-nav__link" for="__nav_2_18" id="__nav_2_18_label" tabindex="0">
          Hugging Face Deep Reinforcement Learning Course
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_18_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_18">
          <span class="md-nav__icon md-icon"></span>
          Hugging Face Deep Reinforcement Learning Course
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%201/" class="md-nav__link">
        RL Framework
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%202/" class="md-nav__link">
        HF DRL   Unit 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%203/" class="md-nav__link">
        HF DRL   Unit 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%204/" class="md-nav__link">
        HF DRL   Unit 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20-%20Unit%205/" class="md-nav__link">
        HF DRL   Unit 5
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/HF%20DRL%20Coding%20-%20Unit%204/" class="md-nav__link">
        HF DRL Coding   Unit 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/Hugging%20Face%20Deep%20Reinforcement%20Learning%20Course/" class="md-nav__link">
        Hugging Face Deep Reinforcement Learning Course
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_19" >
      
      
      
        <label class="md-nav__link" for="__nav_2_19" id="__nav_2_19_label" tabindex="0">
          IKT460 G Reinforcement Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_19_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_19">
          <span class="md-nav__icon md-icon"></span>
          IKT460 G Reinforcement Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/IKT460%20Lecture%201/" class="md-nav__link">
        IKT460 Lecture 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/IKT460%20Lecture%202/" class="md-nav__link">
        Top Comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/IKT460-G%20Reinforcement%20Learning/" class="md-nav__link">
        IKT460 G Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/IKT460-G%20Reinforcement%20Learning/Project%20Proposal%20Outline/" class="md-nav__link">
        Project Proposal Outline
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_20" >
      
      
      
        <label class="md-nav__link" for="__nav_2_20" id="__nav_2_20_label" tabindex="0">
          Lab Social Media
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_20_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_20">
          <span class="md-nav__icon md-icon"></span>
          Lab Social Media
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Lab%20Social%20Media/Lab%20Social%20Media/" class="md-nav__link">
        Lab Social Media
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_21" >
      
      
      
        <label class="md-nav__link" for="__nav_2_21" id="__nav_2_21_label" tabindex="0">
          Laptop Environemnt
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_21_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_21">
          <span class="md-nav__icon md-icon"></span>
          Laptop Environemnt
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Laptop%20Environemnt/Laptop%20Environment/" class="md-nav__link">
        Laptop Environment
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_22" >
      
      
      
        <label class="md-nav__link" for="__nav_2_22" id="__nav_2_22_label" tabindex="0">
          Learning Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_22_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_22">
          <span class="md-nav__icon md-icon"></span>
          Learning Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Learning%20Python/Learning%20Python/" class="md-nav__link">
        Learning Python
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_23" >
      
      
      
        <label class="md-nav__link" for="__nav_2_23" id="__nav_2_23_label" tabindex="0">
          Making Posters
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_23_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_23">
          <span class="md-nav__icon md-icon"></span>
          Making Posters
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Making%20Posters/Making%20Posters/" class="md-nav__link">
        Making Posters
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Making%20Posters/Neuromorphics%20Poster%20-%20Staging/" class="md-nav__link">
        Neuromorphics Poster   Staging
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Making%20Posters/Neuromorphics%20Poster%202nd%20draft/" class="md-nav__link">
        Title
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_24" >
      
      
      
        <label class="md-nav__link" for="__nav_2_24" id="__nav_2_24_label" tabindex="0">
          New Business
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_24_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_24">
          <span class="md-nav__icon md-icon"></span>
          New Business
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/New%20Business/New%20Business%20/" class="md-nav__link">
        New Business 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_25" >
      
      
      
        <label class="md-nav__link" for="__nav_2_25" id="__nav_2_25_label" tabindex="0">
          Norwegian Courses A1 and A2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_25_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_25">
          <span class="md-nav__icon md-icon"></span>
          Norwegian Courses A1 and A2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Norwegian%20Courses%20A1%20and%20A2/Norwegian%20Courses%20A1%20and%20A2%20/" class="md-nav__link">
        Norwegian Courses A1 and A2 
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_26" >
      
      
      
        <label class="md-nav__link" for="__nav_2_26" id="__nav_2_26_label" tabindex="0">
          Obsidian Addons and Todos
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_26_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_26">
          <span class="md-nav__icon md-icon"></span>
          Obsidian Addons and Todos
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/Example%20Slides%20-%20Miniml/" class="md-nav__link">
        Example Slides   Miniml
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/Example%20Slides/" class="md-nav__link">
        Example Slides
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/MD%20Slides/" class="md-nav__link">
        MD Slides
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Obsidian%20Addons%20and%20Todos/Obsidian%20Addons%20and%20Todos/" class="md-nav__link">
        Obsidian Addons and Todos
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_27" >
      
      
      
        <label class="md-nav__link" for="__nav_2_27" id="__nav_2_27_label" tabindex="0">
          PENG9560 Topics in Artificial Intelligence and Machine Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_27_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_27">
          <span class="md-nav__icon md-icon"></span>
          PENG9560 Topics in Artificial Intelligence and Machine Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Mandatory%20Assignment%20for%20Module%20I%20-%20Computational%20Intelligence/" class="md-nav__link">
        Mandatory Assignment for Module I   Computational Intelligence
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%201/" class="md-nav__link">
        PENG9560 Assignment Draft 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%202/" class="md-nav__link">
        PENG9560 Assignment Draft 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%203/" class="md-nav__link">
        PENG9560 Assignment Draft 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Assignment%20Draft%204/" class="md-nav__link">
        PENG9560 Assignment Draft 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Outline%20Assignment%201/" class="md-nav__link">
        Drafts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/" class="md-nav__link">
        PENG9560 Topics in Artificial Intelligence and Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_27_8" >
      
      
      
        <label class="md-nav__link" for="__nav_2_27_8" id="__nav_2_27_8_label" tabindex="0">
          Presentation
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_27_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_27_8">
          <span class="md-nav__icon md-icon"></span>
          Presentation
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20-%20Draft%201%20-%20PENG9560/" class="md-nav__link">
        Presentation   Draft 1   PENG9560
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20-%20Draft%202%20-%20PENG9560/" class="md-nav__link">
        Presentation   Draft 2   PENG9560
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20-%20Draft%203%20-%20PENG9560/" class="md-nav__link">
        Presentation   Draft 3   PENG9560
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/PENG9560%20Topics%20in%20Artificial%20Intelligence%20and%20Machine%20Learning/Presentation/Presentation%20Outline%20-%20PENG9560/" class="md-nav__link">
        Presentation Outline   PENG9560
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_28" >
      
      
      
        <label class="md-nav__link" for="__nav_2_28" id="__nav_2_28_label" tabindex="0">
          Video   How Critical is Brain Criticality
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_28_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_28">
          <span class="md-nav__icon md-icon"></span>
          Video   How Critical is Brain Criticality
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Video%20-%20How%20Critical%20is%20Brain%20Criticality/Video%20-%20How%20Critical%20is%20Brain%20Criticality/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_29" >
      
      
      
        <label class="md-nav__link" for="__nav_2_29" id="__nav_2_29_label" tabindex="0">
          Video Production
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_29_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_29">
          <span class="md-nav__icon md-icon"></span>
          Video Production
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Video%20Production/Video%20Production%20/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_30" >
      
      
      
        <label class="md-nav__link" for="__nav_2_30" id="__nav_2_30_label" tabindex="0">
          Workplan 2022 23 for Gustavo
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_30_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_30">
          <span class="md-nav__icon md-icon"></span>
          Workplan 2022 23 for Gustavo
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../10%20Projects/Workplan%202022-23%20for%20Gustavo/Workplan%202022-23%20for%20Gustavo/" class="md-nav__link">
        Workplan 2022 23 for Gustavo
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          50 Reading
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          50 Reading
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/50%20Reading/" class="md-nav__link">
        50 Reading
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          PDF Searches
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          PDF Searches
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/PDF%20Searches/" class="md-nav__link">
        Searching in pdfs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/fixed%20interval%201/" class="md-nav__link">
        Paton_Buonomano_2018_The Neural Basis of Timing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/fixed%20interval/" class="md-nav__link">
        Fixed interval
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/interval/" class="md-nav__link">
        Interval
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/PDF%20Searches/time%20interval/" class="md-nav__link">
        Time interval
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          Reading Lists
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          Reading Lists
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Lists/Recent%20Reading/" class="md-nav__link">
        Recent Reading
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
          Reading Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          Reading Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Gerstner%20et%20al.%202018/" class="md-nav__link">
        Gerstner et al. 2018
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mehonic%202022/" class="md-nav__link">
        Mehonic 2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Oomen%202017/" class="md-nav__link">
        Oomen 2017
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Parent%202004/" class="md-nav__link">
        Parent 2004
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Payeur%20et%20al.%202021/" class="md-nav__link">
        Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_6" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_6" id="__nav_3_4_6_label" tabindex="0">
          Buzsáki and Vöröslakos (2023)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_6">
          <span class="md-nav__icon md-icon"></span>
          Buzsáki and Vöröslakos (2023)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Brain%20rhythms%20have%20come%20of%20age%20-%20comments/" class="md-nav__link">
        Brain rhythms have come of age   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Brain%20rhythms%20have%20come%20of%20age%20-%20glossary/" class="md-nav__link">
        Brain rhythms have come of age   glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Brain%20rhythms%20have%20come%20of%20age%20-%20topics/" class="md-nav__link">
        Brain rhythms have come of age   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/Buzs%C3%A1ki%20and%20V%C3%B6r%C3%B6slakos%20%282023%29/" class="md-nav__link">
        Buzsáki and Vöröslakos (2023)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_7" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_7" id="__nav_3_4_7_label" tabindex="0">
          Chen 2022
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_7">
          <span class="md-nav__icon md-icon"></span>
          Chen 2022
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022%20-%20comments/" class="md-nav__link">
        Chen 2022   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022%20-%20glossary/" class="md-nav__link">
        Formulae
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022%20-%20topics/" class="md-nav__link">
        Chen 2022   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Chen%202022/Chen%202022/" class="md-nav__link">
        Deep Reinforcement Learning with Spiking Q-learning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_8" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_8" id="__nav_3_4_8_label" tabindex="0">
          Granmo 2010
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_8">
          <span class="md-nav__icon md-icon"></span>
          Granmo 2010
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Granmo%202010/Granmo%202010%20Comments/" class="md-nav__link">
        Personal Summary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Granmo%202010/Granmo%202010%20Topics/" class="md-nav__link">
        Learning Automata (LA)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Granmo%202010/Granmo%202010/" class="md-nav__link">
        Granmo 2010
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_9" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_9" id="__nav_3_4_9_label" tabindex="0">
          Hartcher O'Brien, Brighouse, Levitan (2016)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_9">
          <span class="md-nav__icon md-icon"></span>
          Hartcher O'Brien, Brighouse, Levitan (2016)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29%20-%20comments/" class="md-nav__link">
        Hartcher O'Brien, Brighouse, Levitan (2016)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29%20-%20glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29%20-%20topics/" class="md-nav__link">
        Hartcher O'Brien, Brighouse, Levitan (2016)   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/Hartcher-O%27Brien%2C%20Brighouse%2C%20Levitan%20%282016%29/" class="md-nav__link">
        Hartcher O'Brien, Brighouse, Levitan (2016)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_10" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_10" id="__nav_3_4_10_label" tabindex="0">
          Hasegawa and Sakata 2015
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_10">
          <span class="md-nav__icon md-icon"></span>
          Hasegawa and Sakata 2015
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hasegawa%20and%20Sakata%202015/Hasegawa%20and%20Sakata%202015/" class="md-nav__link">
        Hasegawa and Sakata 2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hasegawa%20and%20Sakata%202015/Hasegawa%2C%20and%20Sakata%202015%20-%20comments/" class="md-nav__link">
        Hasegawa, and Sakata 2015   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Hasegawa%20and%20Sakata%202015/Hasegawa%2C%20and%20Sakata%202015%20-%20topics/" class="md-nav__link">
        Hasegawa, and Sakata 2015   topics
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_11" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_11" id="__nav_3_4_11_label" tabindex="0">
          Mello 2016
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_11">
          <span class="md-nav__icon md-icon"></span>
          Mello 2016
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016%20-%20Glossary/" class="md-nav__link">
        ABBREVIATIONS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016%20-%20Topics/" class="md-nav__link">
        Mello 2016   Topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016%20-%20comments/" class="md-nav__link">
        Mello 2016   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%202016/Mello%202016/" class="md-nav__link">
        Mello 2016
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_12" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_12" id="__nav_3_4_12_label" tabindex="0">
          Mello, Soares, Paton 2015
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_12">
          <span class="md-nav__icon md-icon"></span>
          Mello, Soares, Paton 2015
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015%20-%20comments/" class="md-nav__link">
        Mello, Soares, Paton 2015   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015%20-%20glossary/" class="md-nav__link">
        Mello, Soares, Paton 2015   glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015%20-%20topics/" class="md-nav__link">
        Mello, Soares, Paton 2015   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Mello%2C%20Soares%2C%20Paton%202015/Mello%2C%20Soares%2C%20Paton%202015/" class="md-nav__link">
        Mello, Soares, Paton 2015
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_13" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_13" id="__nav_3_4_13_label" tabindex="0">
          O’Byrne & Jerbi (2022)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_13_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_13">
          <span class="md-nav__icon md-icon"></span>
          O’Byrne & Jerbi (2022)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/O%27Byrne%20%26%20Jerbi%20%282022%29%20-%20comments/" class="md-nav__link">
        O'Byrne & Jerbi (2022)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/O%27Byrne%20%26%20Jerbi%20%282022%29%20-%20topics/" class="md-nav__link">
        Topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/O%E2%80%99Byrne%20%26%20Jerbi%20%282022%29/" class="md-nav__link">
        O’Byrne & Jerbi (2022)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_14" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_14" id="__nav_3_4_14_label" tabindex="0">
          Petter, Gershman, Meck (2018)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_14">
          <span class="md-nav__icon md-icon"></span>
          Petter, Gershman, Meck (2018)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29%20-%20comments/" class="md-nav__link">
        Petter, Gershman, Meck (2018)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29%20-%20glossary/" class="md-nav__link">
        Pavlovian Conditioning Protocol
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29%20-%20topics/" class="md-nav__link">
        Reward prediction error (RPE)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Petter%2C%20Gershman%2C%20Meck%20%282018%29/Petter%2C%20Gershman%2C%20Meck%20%282018%29/" class="md-nav__link">
        Petter, Gershman, Meck (2018)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_15" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_15" id="__nav_3_4_15_label" tabindex="0">
          Ponulak 2011
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_15_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_15">
          <span class="md-nav__icon md-icon"></span>
          Ponulak 2011
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Ponulak%202011/Ponulak%202011%20-%20comments/" class="md-nav__link">
        Ponulak 2011   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Ponulak%202011/Ponulak%202011%20-%20topics/" class="md-nav__link">
        Ponulak 2011   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Ponulak%202011/Ponulak%202011/" class="md-nav__link">
        Introduction to spiking neural networks: Information processing, learning and applications
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_16" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_16" id="__nav_3_4_16_label" tabindex="0">
          Sun 2020
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_16_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_16">
          <span class="md-nav__icon md-icon"></span>
          Sun 2020
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020%20-%20comments/" class="md-nav__link">
        Sun 2020   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020%20-%20glossary/" class="md-nav__link">
        Sun 2020   glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020%20-%20topics/" class="md-nav__link">
        Sun 2020   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Sun%202020/Sun%202020/" class="md-nav__link">
        A Review of Designs and Applications of Echo State Networks
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_17" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_17" id="__nav_3_4_17_label" tabindex="0">
          Voelkner 2019
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_17_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_17">
          <span class="md-nav__icon md-icon"></span>
          Voelkner 2019
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Voelkner%202019/Voelkner%202019%20-%20comments/" class="md-nav__link">
        Voelkner 2019   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Voelkner%202019/Voelkner%202019%20-%20topics/" class="md-nav__link">
        Voelkner 2019   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Voelkner%202019/Voelkner%202019/" class="md-nav__link">
        Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_18" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_18" id="__nav_3_4_18_label" tabindex="0">
          Yazidi 2021
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_18_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_18">
          <span class="md-nav__icon md-icon"></span>
          Yazidi 2021
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yazidi%202021/Yazidi%202021%20-%20Comments/" class="md-nav__link">
        Yazidi 2021   Comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yazidi%202021/Yazidi%202021%20-%20Topics/" class="md-nav__link">
        load balancing (LB)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yazidi%202021/Yazidi%202021/" class="md-nav__link">
        Yazidi 2021
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_19" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_19" id="__nav_3_4_19_label" tabindex="0">
          Yin 2017
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_19_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_19">
          <span class="md-nav__icon md-icon"></span>
          Yin 2017
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%202017/Yin%202017%20-%20comments/" class="md-nav__link">
        Yin 2017   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%202017/Yin%202017%20-%20topics/" class="md-nav__link">
        Yin 2017   topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%202017/Yin%202017/" class="md-nav__link">
        Yin 2017
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_20" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4_20" id="__nav_3_4_20_label" tabindex="0">
          Yin et al. (2022)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_20_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4_20">
          <span class="md-nav__icon md-icon"></span>
          Yin et al. (2022)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29%20-%20comments/" class="md-nav__link">
        Yin et al. (2022)   comments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29%20-%20glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29%20-%20topics/" class="md-nav__link">
        Topics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Reading%20Notes/Yin%20et%20al.%20%282022%29/Yin%20et%20al.%20%282022%29/" class="md-nav__link">
        Yin et al. (2022)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
          Videos
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          Videos
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Cosyne%202022%20Tutorial%20on%20Spiking%20Neural%20Networks/" class="md-nav__link">
        Cosyne 2022 Tutorial on Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Eliasmith%20%20-%20Spiking%20Neural%20Networks%20for%20More%20Efficient%20AI%20Algorithms/" class="md-nav__link">
        https://www.youtube.com/watch?v=PeW-TN3P1hk
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Hopfield%20Networks%20is%20All%20You%20Need/" class="md-nav__link">
        Hopfield Networks is All You Need
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/JNS%20Lecture%20Will%20Dabney%20-%20A%20Distributional%20Code%20for%20Value%20in%20Dopamine-Based%20Reinforcement%20Learning/" class="md-nav__link">
        A Distributional Code for Value in Dopamine-Based Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Videos/Neural%20representations%20of%20time%2C%20space%20and%20other%20continuous%20variables/" class="md-nav__link">
        Neural representations of time, space and other continuous variables
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
      
      
      
        <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
          Zotero Papers
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          Zotero Papers
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/Zotero%20Papers/Zotero%20Papers/" class="md-nav__link">
        Zotero Papers
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
      
      
      
        <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
          Citations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_7">
          <span class="md-nav__icon md-icon"></span>
          Citations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/%40pateriaHierarchicalReinforcementLearning2021/" class="md-nav__link">
        @pateriaHierarchicalReinforcementLearning2021
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/%40voelkerLegendreMemoryUnits2019/" class="md-nav__link">
        @voelkerLegendreMemoryUnits2019
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20Anand%20Subramoney%2C%20Franz%20Scherr%2C%20Guillaume%20Bellec%2C%20Elias%20Hajek%2C%20Darjan%20Salaj%2C%20Robert%20Legenstein%2C%20Wolfgang%20Maass%20-%20/" class="md-nav__link">
        Citation Anand Subramoney, Franz Scherr, Guillaume Bellec, Elias Hajek, Darjan Salaj, Robert Legenstein, Wolfgang Maass   
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20Eleni%20Vasilaki%2C%20Nicolas%20Fr%C3%A9maux%2C%20Robert%20Urbanczik%2C%20Walter%20Senn%2C%20Wulfram%20Gerstner%20-%202009/" class="md-nav__link">
        Citation Eleni Vasilaki, Nicolas Frémaux, Robert Urbanczik, Walter Senn, Wulfram Gerstner   2009
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20Peter%20Diehl%2C%20Matthew%20Cook%20-%202015/" class="md-nav__link">
        Citation Peter Diehl, Matthew Cook   2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/citations/Citation%20SlowProcessesNeurons/" class="md-nav__link">
        Citation SlowProcessesNeurons
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
      
      
      
        <label class="md-nav__link" for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
          Tweets
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_8">
          <span class="md-nav__icon md-icon"></span>
          Tweets
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/tweets/Twitter%20-%20VnVrinda%20-%20How%20to%20search%20and%20read%20papers%20-%2011.07.22/" class="md-nav__link">
        Twitter   VnVrinda   How to search and read papers   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/tweets/Twitter%20-%20YanliangShi%20-%2020.07.22/" class="md-nav__link">
        Twitter   YanliangShi   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/tweets/Twitter%20-%20hisspikeness%20-%2023.07.22/" class="md-nav__link">
        Twitter   hisspikeness   23.07.22
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
      
      
      
        <label class="md-nav__link" for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
          Zot2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_9">
          <span class="md-nav__icon md-icon"></span>
          Zot2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40abelesCorticonicsNeuralCircuits1991/" class="md-nav__link">
        Corticonics: Neural Circuits of the Cerebral Cortex
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40allmanPathophysiologicalDistortionsTime2012/" class="md-nav__link">
        Pathophysiological distortions in time perception and timed performance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40balciPeakIntervalProcedure2020/" class="md-nav__link">
        The Peak Interval Procedure in Rodents: A Tool for Studying the Neurobiological Basis of Interval Timing and Its Alterations in Models of Human Disease
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40beggsCortexCriticalPoint2022/" class="md-nav__link">
        The Cortex and the Critical Point: Understanding the Power of Emergence.
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bellecSolutionLearningDilemma2020/" class="md-nav__link">
        A solution to the learning dilemma for recurrent networks of spiking neurons
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bendorBiasingContentHippocampal2012/" class="md-nav__link">
        Biasing the content of hippocampal replay during sleep
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bengioBiologicallyPlausibleDeep2016/" class="md-nav__link">
        Towards Biologically Plausible Deep Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40biTimeRepresentationNeural2019/" class="md-nav__link">
        Time representation in neural network models trained to perform interval timing tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40botvinickDeepReinforcementLearning2020/" class="md-nav__link">
        Deep Reinforcement Learning and its Neuroscientific Implications
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40brahlekTransportPropertiesTopological2015/" class="md-nav__link">
        @brahlekTransportPropertiesTopological2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bretteSimulationNetworksSpiking2007/" class="md-nav__link">
        Simulation of networks of spiking neurons: A review of tools and strategies
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40bronsteinGeometricDeepLearning2021/" class="md-nav__link">
        Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40buzsakiBrainRhythmsHave2023/" class="md-nav__link">
        Brain rhythms have come of age
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40caoExplanatoryModelsNeuroscience2021/" class="md-nav__link">
        Explanatory models in neuroscience: Part 1 -- taking mechanistic abstraction seriously
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40carvalhoTemporalBisectionProcedure2019/" class="md-nav__link">
        Temporal Bisection Procedure
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40chenDeepReinforcementLearning2022/" class="md-nav__link">
        Deep Reinforcement Learning with Spiking Q-learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40chilkuriParallelizingLegendreMemory2021/" class="md-nav__link">
        Parallelizing Legendre Memory Unit Training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40cramerSurrogateGradientsAnalog2022/" class="md-nav__link">
        Surrogate gradients for analog neuromorphic computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40cruzActionSuppressionReveals2022/" class="md-nav__link">
        Action suppression reveals opponent parallel control via striatal circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40deverettIntervalTimingDeep2019/" class="md-nav__link">
        Interval timing in deep reinforcement learning agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40erdemExploringRelationshipsEffort2020/" class="md-nav__link">
        Exploring relationships between effort, motion, and sound in new musical instruments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40eshraghianIntroduction2022/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40fangwei123456SpikingJelly2022/" class="md-nav__link">
        SpikingJelly
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40fremauxNeuromodulatedSpikeTimingDependentPlasticity2016/" class="md-nav__link">
        Neuromodulated Spike-Timing-Dependent Plasticity, and Theory of Three-Factor Learning Rules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40furberSpiNNakerProject2014/" class="md-nav__link">
        The SpiNNaker Project
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gallonialessandroSNUFA2022Behavioral/" class="md-nav__link">
        SNUFA 2022 - Behavioral Timescale Synaptic Plasticity (BTSP) for credit assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gaoCorticalColumnWholebrain/" class="md-nav__link">
        Cortical column and whole-brain imaging with molecular contrast and nanoscale resolution
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gershmanReinforcementLearningEpisodic2017/" class="md-nav__link">
        Reinforcement learning and episodic memory in humans and animals: an integrative framework
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gershmanTimeRepresentationReinforcement2014/" class="md-nav__link">
        Time representation in reinforcement learning models of the basal ganglia
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40gerstnerEligibilityTracesPlasticity2018/" class="md-nav__link">
        @gerstnerEligibilityTracesPlasticity2018
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40goudarEncodingSensoryMotor2018/" class="md-nav__link">
        Encoding sensory and motor patterns as time-invariant trajectories in recurrent neural networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40grondinTimingTimePerception2010/" class="md-nav__link">
        Timing and time perception: A review of recent behavioral and neuroscience findings and theoretical directions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40guOscillatoryMultiplexingNeural2015a/" class="md-nav__link">
        Oscillatory multiplexing of neural population codes for interval timing and working memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hamedaniDeepSpikingDelayed2020/" class="md-nav__link">
        Deep Spiking Delayed Feedback Reservoirs and Its Application in Spectrum Sensing of MIMO-OFDM Dynamic Spectrum Sharing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hardyEncodingTimeFeedforward2018/" class="md-nav__link">
        Encoding Time in Feedforward Trajectories of a Recurrent Neural Network Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hartcher-obrienSingleMechanismAccount2016/" class="md-nav__link">
        A single mechanism account of duration and rate processing via the pacemaker-accumulator and beat frequency models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hasegawaModelMultisecondTiming2015/" class="md-nav__link">
        A model of multisecond timing behaviour under peak-interval procedures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40hesselRainbowCombiningImprovements2017/" class="md-nav__link">
        Rainbow: Combining Improvements in Deep Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40howardEfficientNeuralComputation2019/" class="md-nav__link">
        Efficient Neural Computation in the Laplace Domain
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40jazayeriNeuralMechanismSensing2015/" class="md-nav__link">
        @jazayeriNeuralMechanismSensing2015
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40jensenSelfOrganizedCriticalityEmergent1998/" class="md-nav__link">
        Self-Organized Criticality: Emergent Complex Behavior in Physical and Biological Systems
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40jirsaEntropyFreeEnergy2022/" class="md-nav__link">
        @jirsaEntropyFreeEnergy2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40khajehabdollahiWhenBeCritical2022/" class="md-nav__link">
        When to Be Critical? Performance and Evolvability in Different Regimes of Neural Ising Agents
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40kononowiczTimingTimePerception2018/" class="md-nav__link">
        Timing and Time Perception
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40kumarSpikingActivityPropagation2010/" class="md-nav__link">
        Spiking activity propagation in neuronal networks: reconciling different perspectives on neural coding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40lentResourceSelectionCognitive2018/" class="md-nav__link">
        Resource Selection in Cognitive Networks With Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40liMiceInferProbabilistic2013/" class="md-nav__link">
        Mice infer probabilistic models for timing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40liuHumanLevelControlDirectly2022/" class="md-nav__link">
        Human-Level Control Through Directly Trained Deep Spiking $Q$-Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40liuMultiobjectiveReinforcementLearning2015/" class="md-nav__link">
        Multiobjective Reinforcement Learning: A Comprehensive Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40loefflerModularityMultitaskingNeuromemristive2021a/" class="md-nav__link">
        Modularity and multitasking in neuro-memristive reservoir networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40maassNetworksSpikingNeurons1997/" class="md-nav__link">
        Networks of spiking neurons: The third generation of neural network models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40marsiliQuantifyingRelevanceLearning2022/" class="md-nav__link">
        Quantifying Relevance in Learning and Inference
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40matellNeuropsychologicalMechanismsInterval2000/" class="md-nav__link">
        Neuropsychological mechanisms of interval timing behavior
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40mehonicBraininspiredComputingNeeds2022/" class="md-nav__link">
        Brain-inspired computing needs a master plan
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40melloNeuralBehavioralMechanisms2016/" class="md-nav__link">
        Neural and Behavioral Mechanisms of Interval Timing in the Striatum
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40melloScalablePopulationCode2015/" class="md-nav__link">
        A Scalable Population Code for Time in the Striatum
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40mnihHumanlevelControlDeep2015/" class="md-nav__link">
        Human-level control through deep reinforcement learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40montanoGridgraphModelingEmergent2022/" class="md-nav__link">
        Grid-graph modeling of emergent neuromorphic dynamics and heterosynaptic plasticity in memristive nanonetworks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40montemurroPhaseofFiringCodingNatural2008/" class="md-nav__link">
        Phase-of-Firing Coding of Natural Visual Stimuli in Primary Visual Cortex
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40murrayLearningMultipleVariablespeed2017/" class="md-nav__link">
        Learning multiple variable-speed sequences in striatum via cortical tutoring
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40naudBurstdependentSynapticPlasticity/" class="md-nav__link">
        Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40neftciSurrogateGradientLearning2019/" class="md-nav__link">
        Surrogate Gradient Learning in Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40nicolaSupervisedLearningSpiking2017/" class="md-nav__link">
        Supervised learning in spiking neural networks with FORCE training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40obyrneHowCriticalBrain2022/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40oneillPlayItAgain2010/" class="md-nav__link">
        Play it again: reactivation of waking experience and memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40parisottoStabilizingTransformersReinforcement2020/" class="md-nav__link">
        Stabilizing Transformers for Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40patelImprovedRobustnessReinforcement2019/" class="md-nav__link">
        Improved robustness of reinforcement learning policies upon conversion to spiking neuronal network platforms applied to Atari Breakout game
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40patonNeuralBasisTiming2018/" class="md-nav__link">
        The Neural Basis of Timing: Distributed Mechanisms for Diverse Functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40payeurBurstdependentSynapticPlasticity2021/" class="md-nav__link">
        Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40petterIntegratingModelsInterval2018/" class="md-nav__link">
        Integrating Models of Interval Timing and Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40petterTemporalProcessingIntrinsic2016/" class="md-nav__link">
        Temporal Processing by Intrinsic Neural Network Dynamics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pfeifferDeepLearningSpiking2018/" class="md-nav__link">
        Deep Learning With Spiking Neurons: Opportunities and Challenges
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40phuongFormalAlgorithmsTransformers2022/" class="md-nav__link">
        Formal Algorithms for Transformers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoAssessingRobustnessCritical2022/" class="md-nav__link">
        Assessing the robustness of critical behavior in stochastic cellular automata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoBidirectionalLearningRobust2019%20%282%29/" class="md-nav__link">
        Bidirectional Learning for Robust Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoBidirectionalLearningRobust2019/" class="md-nav__link">
        Bidirectional Learning for Robust Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40pontes-filhoGeneralRepresentationDynamical2019/" class="md-nav__link">
        A general representation of dynamical systems for reservoir computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40ponulakIntroductionSpikingNeural2011/" class="md-nav__link">
        Introduction to spiking neural networks: Information processing, learning and applications
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40princeCurrentStateFuture2022/" class="md-nav__link">
        Current State and Future Directions for Learning in Biological Recurrent Neural Networks: A Perspective Piece
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40ramsauerHopfieldNetworksAll2020/" class="md-nav__link">
        Hopfield Networks is All You Need
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40rolnickExperienceReplayContinual2019a/" class="md-nav__link">
        Experience Replay for Continual Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40rossbroichFluctuationdrivenInitializationSpiking2022/" class="md-nav__link">
        Fluctuation-driven initialization for spiking neural network training
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40rueckauerConversionContinuousValuedDeep2017/" class="md-nav__link">
        Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40schmittNeuromorphicHardwareLoop2017/" class="md-nav__link">
        Neuromorphic hardware in the loop: Training a deep spiking network on the BrainScaleS wafer-scale system
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40shankarScaleInvariantInternalRepresentation2012/" class="md-nav__link">
        A Scale-Invariant Internal Representation of Time
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40shiScalarTimingTheory2022/" class="md-nav__link">
        Beyond Scalar Timing Theory: Integrating Neural Oscillators with Computational Accessibility in Memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40shiSpatialTemporalCorrelations2022/" class="md-nav__link">
        Spatial and temporal correlations in neural networks with structured connectivity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40songRewardbasedTrainingRecurrent2017/" class="md-nav__link">
        Reward-based training of recurrent neural networks for cognitive and value-based tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40spinolaWhatHumansLearn2013/" class="md-nav__link">
        What do humans learn in a double, temporal bisection task: Absolute or relative stimulus durations?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40sunReviewDesignsApplications2020/" class="md-nav__link">
        A Review of Designs and Applications of Echo State Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40sungSimultaneousEmulationSynaptic2022/" class="md-nav__link">
        Simultaneous emulation of synaptic and intrinsic plasticity using a memristive synapse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40suvrathanSTDPDiverseFunctionally2019a/" class="md-nav__link">
        Beyond STDP—towards diverse and functionally relevant plasticity rules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40swearingenPatternRespondingPeakInterval2010/" class="md-nav__link">
        The Pattern of Responding in the Peak-Interval Procedure with Gaps: An Individual-Trials Analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tallotNeuralEncodingTime2020/" class="md-nav__link">
        Neural encoding of time in the animal brain
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tanStrategyBenchmarkConverting2020/" class="md-nav__link">
        Strategy and Benchmark for Converting Deep Q-Networks to Event-Driven Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tangDeepReinforcementLearning2020/" class="md-nav__link">
        Deep Reinforcement Learning with Population-Coded Spiking Neural Network for Continuous Control
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tavanaeiDeepLearningSpiking2019/" class="md-nav__link">
        Deep learning in spiking neural networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tekiPersistenceMemoryHow2017/" class="md-nav__link">
        The Persistence of Memory: How the Brain Encodes Time in Memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40tello-ramosTimePlaceLearning2015/" class="md-nav__link">
        Time–place learning in wild, free-living hummingbirds
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40varellaModelPeakintervalTask2019/" class="md-nav__link">
        A model for the peak-interval task based on neural oscillation-delimited states
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40vasilakiSpikeBasedReinforcementLearning2009/" class="md-nav__link">
        Spike-Based Reinforcement Learning in Continuous State and Action Space: When Policy Gradient Methods Fail
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40vigneronCriticalSurveySTDP2020/" class="md-nav__link">
        A critical survey of STDP in Spiking Neural Networks for Pattern Recognition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40voelkerDynamicalSystemsSpiking2019/" class="md-nav__link">
        Dynamical Systems in Spiking Neuromorphic Hardware
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40voelkerLegendreMemoryUnits2019/" class="md-nav__link">
        Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40voelkerSpikePerformanceTraining2021/" class="md-nav__link">
        A Spike in Performance: Training Hybrid-Spiking Neural Networks with Quantized Activation Functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40vogelsSignalPropagationLogic2005/" class="md-nav__link">
        Signal Propagation and Logic Gating in Networks of Integrate-and-Fire Neurons
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wangConvergentEfficientDeep2022/" class="md-nav__link">
        Convergent and Efficient Deep Q Network Algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wangDeepReinforcementLearning2022/" class="md-nav__link">
        Deep Reinforcement Learning: A Survey
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wengTianshouHighlyModularized2022/" class="md-nav__link">
        Tianshou: a Highly Modularized Deep Reinforcement Learning Library
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wiesnerMeasuringComplexity2020/" class="md-nav__link">
        Measuring complexity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40williamsNeuralBurstCodes2021/" class="md-nav__link">
        Neural burst codes disguised as rate codes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wilsonInfluenceMultipleTemporal2015/" class="md-nav__link">
        The influence of multiple temporal memories in the peak-interval procedure
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40wimmerRewardLearningWorking2022/" class="md-nav__link">
        Reward learning and working memory: Effects of massed versus spaced training and post-learning delay period
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinAccurateOnlineTraining2022/" class="md-nav__link">
        Accurate online training of dynamical spiking neural networks through Forward Propagation Through Time
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinEffectiveEfficientComputation2020/" class="md-nav__link">
        Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinIntervaltimingProtocolsTheir2017/" class="md-nav__link">
        Interval-timing Protocols and Their Relevancy to the Study of Temporal Cognition and Neurobehavioral Genetics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40yinOscillationCoincidenceDetectionModels2022/" class="md-nav__link">
        Oscillation/Coincidence-Detection Models of Reward-Related Timing in Corticostriatal Circuits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zenkeSuperSpikeSupervisedLearning2018/" class="md-nav__link">
        SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zenkeSynapticPlasticityNeural2013/" class="md-nav__link">
        Synaptic Plasticity in Neural Networks Needs Homeostasis with a Fast Rate Detector
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zenkeVisualizingJointFuture2021/" class="md-nav__link">
        Visualizing a joint future of neuroscience and neuromorphic engineering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/%40zhouEncodingTimeNeural2022/" class="md-nav__link">
        Encoding time in neural dynamic regimes with distinct computational tradeoffs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/Diehl%20and%20Cook%202015/" class="md-nav__link">
        Unsupervised learning of digit recognition using spike-timing-dependent plasticity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/goldwasserPlantingUndetectableBackdoors2022/" class="md-nav__link">
        goldwasserPlantingUndetectableBackdoors2022
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../50%20Reading/zot2/wiesnerMeasuringComplexity2020/" class="md-nav__link">
        ['Measuring complexity']
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          70 Wiki
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          70 Wiki
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
          Articles
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          Articles
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../A%20Comprehensive%20Guide%20to%20Convolutional%20Neural%20Networks%20%E2%80%94%20the%20ELI5%20way%20-%2003.04.22/" class="md-nav__link">
        A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way   03.04.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Strange%20Illusion%20Shows%20The%20Human%20Brain%20Mess%20With%20Time%20to%20Maintain%20Our%20Expectations%20-%2008.04.22/" class="md-nav__link">
        Strange Illusion Shows The Human Brain Mess With Time to Maintain Our Expectations   08.04.22
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_3" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4_1_3" id="__nav_4_1_3_label" tabindex="0">
          MD Clips Old
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_3_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4_1_3">
          <span class="md-nav__icon md-icon"></span>
          MD Clips Old
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20Neural%20Mechanism%20for%20Sensing%20and%20Reproducing%20a%20Time%20Interval%20-%2023.06.22/" class="md-nav__link">
        A Neural Mechanism for Sensing and Reproducing a Time Interval   23.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20Neural%20Mechanism%20for%20Sensing%20and%20Reproducing%20a%20Time%20Interval%20-%20PubMed%20-%2023.06.22/" class="md-nav__link">
        A Neural Mechanism for Sensing and Reproducing a Time Interval   PubMed   23.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20Scalable%20Population%20Code%20for%20Time%20in%20the%20Striatum%20-%2008.06.22/" class="md-nav__link">
        A Scalable Population Code for Time in the Striatum - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20model%20of%20multisecond%20timing%20behaviour%20under%20peak-interval%20procedures%20-%2011.07.22/" class="md-nav__link">
        A model of multisecond timing behaviour under peak interval procedures   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          A review of learning in biologically plausible spiking neural networks - ScienceDirect
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        A review of learning in biologically plausible spiking neural networks - ScienceDirect
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#excerpt" class="md-nav__link">
    Excerpt
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Action%20suppression%20reveals%20opponent%20parallel%20control%20via%20striatal%20circuits%20-%2012.07.22/" class="md-nav__link">
        Action suppression reveals opponent parallel control via striatal circuits   12.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../An%20experimental%20unification%20of%20reservoir%20computing%20methods%20-%2011.05.22/" class="md-nav__link">
        An experimental unification of reservoir computing methods - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Computational%20roles%20of%20plastic%20probabilistic%20synapses%20-%2010.05.22/" class="md-nav__link">
        Computational roles of plastic probabilistic synapses - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Deep%20Learning%20With%20Spiking%20Neurons%20Opportunities%20and%20Challenges%20-%2011.05.22/" class="md-nav__link">
        Frontiers | Deep Learning With Spiking Neurons: Opportunities and Challenges | Neuroscience
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Design%20of%20deep%20echo%20state%20networks%20-%2009.05.22/" class="md-nav__link">
        Design of deep echo state networks - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Encoding%20sensory%20and%20motor%20patterns%20as%20time-invariant%20trajectories%20in%20recurrent%20neural%20networks%20-%2020.07.22/" class="md-nav__link">
        Encoding sensory and motor patterns as time invariant trajectories in recurrent neural networks   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Ensemble%20Coding%20of%20Vocal%20Control%20in%20Birdsong%20-%2021.06.22/" class="md-nav__link">
        Ensemble Coding of Vocal Control in Birdsong   21.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Hopfield%20network%20-%20Scholarpedia%20-%2022.06.22/" class="md-nav__link">
        Hopfield network   Scholarpedia   22.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20Review%20Articles%20-%2009.06.22/" class="md-nav__link">
        How to Review Articles   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20conduct%20a%20review%20-%2009.06.22/" class="md-nav__link">
        How to conduct a review   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20write%20a%20review%20article%20%20Writing%20your%20paper%20%20Author%20Services%20-%2009.06.22/" class="md-nav__link">
        How to write a review article  Writing your paper  Author Services   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../How%20to%20write%20a%20superb%20literature%20review%20-%2009.06.22/" class="md-nav__link">
        How to write a superb literature review
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Human-level%20control%20through%20deep%20reinforcement%20learning%20-%2013.05.22/" class="md-nav__link">
        Human-level control through deep reinforcement learning | Nature
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Integrating%20Models%20of%20Interval%20Timing%20and%20Reinforcement%20Learning%20-%2011.07.22/" class="md-nav__link">
        Integrating Models of Interval Timing and Reinforcement Learning   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../L0%20Norm%2C%20L1%20Norm%2C%20L2%20Norm%20%26%20L-Infinity%20Norm%20-%20Sara%20Iris%20Garcia%20-%20Medium%20-%2013.06.22/" class="md-nav__link">
        L0 Norm, L1 Norm, L2 Norm & L Infinity Norm   Sara Iris Garcia   Medium   13.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Learning%20multiple%20variable-speed%20sequences%20in%20striatum%20via%20cortical%20tutoring%20-%2020.07.22/" class="md-nav__link">
        Learning multiple variable speed sequences in striatum via cortical tutoring   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Learning%20offline%20memory%20replay%20in%20biological%20and%20artificial%20reinforcement%20learning%20-%2010.05.22/" class="md-nav__link">
        Learning offline: memory replay in biological and artificial reinforcement learning - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Local%20Lyapunov%20exponents%20of%20deep%20echo%20state%20networks%20-%2009.05.22/" class="md-nav__link">
        Local Lyapunov exponents of deep echo state networks - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Pad%C3%A9%20approximant%20-%20Scholarpedia%20-%2009.05.22/" class="md-nav__link">
        Padé approximant   Scholarpedia   09.05.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Peer%20Review%20Writing%20Guide%20-%20Reviewer%20Resources%20-%20Volunteer%20-%2009.06.22/" class="md-nav__link">
        Peer Review Writing Guide   Reviewer Resources   Volunteer   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../RNN%2C%20LSTM%20%26%20GRU%20-%2009.05.22/" class="md-nav__link">
        RNN, LSTM & GRU
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Reinforcement%20learning%20and%20episodic%20memory%20in%20humans%20and%20animals%20an%20integrative%20framework%20-%2020.07.22/" class="md-nav__link">
        Reinforcement learning and episodic memory in humans and animals an integrative framework   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Reinforcement%20learning%20with%20Marr%20-%2020.07.22/" class="md-nav__link">
        Reinforcement learning with Marr   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Research%20Guides%20Publishing%20in%20the%20Sciences%20How%20to%20Write%20a%20Scientific%20Literature%20Review%20-%2009.06.22/" class="md-nav__link">
        Research Guides Publishing in the Sciences How to Write a Scientific Literature Review   09.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Reservoir%20computing%20and%20extreme%20learning%20machines%20for%20non-linear%20time-series%20data%20analysis%20-%2011.05.22/" class="md-nav__link">
        Reservoir computing and extreme learning machines for non-linear time-series data analysis - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Reservoir%20computing%20approaches%20to%20recurrent%20neural%20network%20training%20-%2009.05.22/" class="md-nav__link">
        Reservoir computing approaches to recurrent neural network training - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Scientists%20discover%20how%20the%20brain%20keeps%20the%20urge%20to%20act%20in%20check%20%20Champalimaud%20Foundation%20-%2012.07.22/" class="md-nav__link">
        Scientists discover how the brain keeps the urge to act in check  Champalimaud Foundation   12.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Spiking%20Neural%20Networks%20and%20online%20learning%20An%20overview%20and%20perspectives%20-%2010.05.22/" class="md-nav__link">
        Spiking Neural Networks and online learning: An overview and perspectives - ScienceDirect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action%20-%2006.05.22/" class="md-nav__link">
        State–action–reward–state–action   06.05.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Temporal%20Bisection%20Procedure%20-%2020.06.22/" class="md-nav__link">
        Temporal Bisection Procedure   20.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../The%20Pattern%20of%20Responding%20in%20the%20Peak-Interval%20Procedure%20with%20Gaps%20An%20Individual-Trials%20Analysis%20-%2011.07.22/" class="md-nav__link">
        The Pattern of Responding in the Peak Interval Procedure with Gaps An Individual Trials Analysis   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../The%20influence%20of%20multiple%20temporal%20memories%20in%20the%20peak-interval%20procedure%20-%2011.07.22%20%281%29/" class="md-nav__link">
        The influence of multiple temporal memories in the peak interval procedure   11.07.22 (1)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../The%20influence%20of%20multiple%20temporal%20memories%20in%20the%20peak-interval%20procedure%20-%2011.07.22/" class="md-nav__link">
        The influence of multiple temporal memories in the peak interval procedure   11.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Time%20representation%20in%20reinforcement%20learning%20models%20of%20the%20basal%20ganglia%20-%2020.07.22/" class="md-nav__link">
        Time representation in reinforcement learning models of the basal ganglia   20.07.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Unified%20pre-%20and%20postsynaptic%20long-term%20plasticity%20enables%20reliable%20and%20flexible%20learning%20-%2011.05.22/" class="md-nav__link">
        Unified pre- and postsynaptic long-term plasticity enables reliable and flexible learning | eLife
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../What%20do%20humans%20learn%20in%20a%20double%2C%20temporal%20bisection%20task%20Absolute%20or%20relative%20stimulus%20durations%20-%2020.06.22/" class="md-nav__link">
        What do humans learn in a double, temporal bisection task Absolute or relative stimulus durations   20.06.22
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Wikipedia%20-%20Adiabatic%20process%20-%2022.06.22/" class="md-nav__link">
        Wikipedia   Adiabatic process   22.06.22
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_4" id="__nav_4_1_4_label" tabindex="0">
          MD webclips
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_4">
          <span class="md-nav__icon md-icon"></span>
          MD webclips
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_4_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_4_1" id="__nav_4_1_4_1_label" tabindex="0">
          MD Clips New
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_4_1_4_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_4_1">
          <span class="md-nav__icon md-icon"></span>
          MD Clips New
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../MD_webclips/MD_Clips_New/Surrogate%20gradients%20for%20analog%20neuromorphic%20computing%20-%2006.12.22/" class="md-nav__link">
        Surrogate gradients for analog neuromorphic computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../MD_webclips/MD_Clips_New/What%20is%20the%20Difference%20Between%20Gradient%20Descent%20and%20Gradient%20Ascent%20-%2012.01.23/" class="md-nav__link">
        What is the Difference Between Gradient Descent and Gradient Ascent?
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_5" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_5" id="__nav_4_1_5_label" tabindex="0">
          Markdown Clips
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_5">
          <span class="md-nav__icon md-icon"></span>
          Markdown Clips
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips/A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning%20-%20MachineLearningMastery.com%20-%2008.02.23.md/" class="md-nav__link">
        A Gentle Introduction to Cross-Entropy for Machine Learning - MachineLearningMastery.com
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips/Human-level%20control%20through%20deep%20reinforcement%20learning%20-%2020.02.23.md/" class="md-nav__link">
        Human-level control through deep reinforcement learning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_6" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_6" id="__nav_4_1_6_label" tabindex="0">
          Markdown Clips old
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_6">
          <span class="md-nav__icon md-icon"></span>
          Markdown Clips old
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/A%20model%20for%20the%20peak-interval%20task%20based%20on%20neural%20oscillation-delimited%20states%20-%2017.11.22.md/" class="md-nav__link">
        A model for the peak-interval task based on neural oscillation-delimited states
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/A%20single%20mechanism%20account%20of%20duration%20and%20rate%20processing%20via%20the%20pacemaker-accumulator%20and%20beat%20frequency%20models%20-%2016.11.22.md/" class="md-nav__link">
        A single mechanism account of duration and rate processing via the pacemaker-accumulator and beat frequency models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Beyond%20STDP%20%E2%80%94%20towards%20diverse%20and%20functionally%20relevant%20plasticity%20rules%20-%2012.10.22/" class="md-nav__link">
        Beyond STDP — towards diverse and functionally relevant plasticity rules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Geometric%20foundations%20of%20Deep%20Learning%20-%20Towards%20Data%20Science%20-%2006.10.22/" class="md-nav__link">
        Geometric foundations of Deep Learning - Towards Data Science
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Grid-graph%20modeling%20of%20emergent%20neuromorphic%20dynamics%20and%20heterosynaptic%20plasticity%20in%20memristive%20nanonetworks%20-%2007.10.22/" class="md-nav__link">
        Grid-graph modeling of emergent neuromorphic dynamics and heterosynaptic plasticity in memristive nanonetworks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/How%20critical%20is%20brain%20criticality%20-%20%20Original/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/How%20critical%20is%20brain%20criticality%20-%2020.10.22/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/How%20critical%20is%20brain%20criticality%20-%2022.10.22/" class="md-nav__link">
        How critical is brain criticality?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Modularity%20and%20multitasking%20in%20neuro-memristive%20reservoir%20networks%20-%2007.10.22/" class="md-nav__link">
        Modularity and multitasking in neuro-memristive reservoir networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Oscillatory%20multiplexing%20of%20neural%20population%20codes%20for%20interval%20timing%20and%20working%20memory%20-%2021.11.22.md/" class="md-nav__link">
        Oscillatory multiplexing of neural population codes for interval timing and working memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/The%20Persistence%20of%20Memory%20How%20the%20Brain%20Encodes%20Time%20in%20Memory%20-%2021.11.22.md/" class="md-nav__link">
        The Persistence of Memory: How the Brain Encodes Time in Memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Timing%20Intervals%20Using%20Population%20Synchrony%20and%20Spike%20Timing%20Dependent%20Plasticity%20-%2017.11.22.md/" class="md-nav__link">
        Timing Intervals Using Population Synchrony and Spike Timing Dependent Plasticity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Markdown%20Clips_old/Weighted%20Majority%20Algorithm%20-%20A%20beautiful%20algorithm%20for%20Learning%20from%20Experts/" class="md-nav__link">
        Weighted Majority Algorithm: A beautiful algorithm for Learning from Experts
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
      
      
      
        <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          Pages
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          Pages
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Apical%20Dendrite/" class="md-nav__link">
        Apical Dendrite
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Echo%20State%20Networks/" class="md-nav__link">
        Echo State Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Grid%20Cell%20-%20Time%20Cell%20Interactions/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Hierarchical%20Reinforcement%20Learning/" class="md-nav__link">
        Hierarchical Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Legendre%20Memory%20Unit%20%28LMU%29/" class="md-nav__link">
        Legendre Memory Unit (LMU)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Liquid%20State%20Machine/" class="md-nav__link">
        Liquid State Machine
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Recurrent%20Neural%20Network%20%28RNN%29/" class="md-nav__link">
        Nomenclature around degree of recurrence or moment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Reservoir%20Computing/" class="md-nav__link">
        Reservoir Computing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Spiking%20Neural%20Networks%20%28SNN%29/" class="md-nav__link">
        Spiking Neural Networks (SNN)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Striatal%20Beat%20Frequency%20Model/" class="md-nav__link">
        Striatal Beat Frequency Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Striatal%20Beat%20Frequency/" class="md-nav__link">
        Striatal Beat Frequency
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Three-Factor%20Learning%20Rule/" class="md-nav__link">
        Search
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
      
      
      
        <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
          People
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          People
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Anna%20Levina/" class="md-nav__link">
        Anna Levina
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Friedemann%20Zenke/" class="md-nav__link">
        Friedemann Zenke
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Jan%20Tore%20L%C3%B8nning/" class="md-nav__link">
        Jan Tore Lønning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Jeffrey%20Allan%20Lugowe/" class="md-nav__link">
        Jeffrey Allan Lugowe
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Kai%20Olav%20Ellefsen/" class="md-nav__link">
        Kai Olav Ellefsen
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Katalin%20Vertes/" class="md-nav__link">
        Katalin Vertes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Klas%20Henning%20Pettersen/" class="md-nav__link">
        Klas Henning Pettersen
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/Sven%20Peter%20N%C3%A4sholm/" class="md-nav__link">
        Sven Peter Näsholm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../people/people/" class="md-nav__link">
        In this folder
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
          Topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/ADAM%20Optimizer/" class="md-nav__link">
        ADAM Optimizer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Adiabatic%20Process/" class="md-nav__link">
        Adiabatic Process
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Bellman%20Equation/" class="md-nav__link">
        From [[HF DRL - Unit 2]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Chaos/" class="md-nav__link">
        Chaos
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Complexity/" class="md-nav__link">
        Reading
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Continuous%20Markov%20Decision%20Process/" class="md-nav__link">
        Continuous Markov Decision Process
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Convolutional%20Neural%20Network%20%28CNN%29/" class="md-nav__link">
        Convolutional Neural Network (CNN)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Cortico-basal%20ganglia-thalamo-cortical%20loop/" class="md-nav__link">
        Cortico basal ganglia thalamo cortical loop
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Credit%20Assignment%20Problem/" class="md-nav__link">
        Credit Assignment Problem
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Criticality/" class="md-nav__link">
        Criticality
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Cross%20Entropy/" class="md-nav__link">
        Cross Entropy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Deep%20Q-Learning/" class="md-nav__link">
        Deep Q Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Double%20DQN/" class="md-nav__link">
        Double DQN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Dyna-Q/" class="md-nav__link">
        Dyna Q
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Echo%20State%20Property%20%28ESP%29/" class="md-nav__link">
        Echo State Property (ESP)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Eligibility%20Trace/" class="md-nav__link">
        Commentary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/End-to-End%20Reinforcement-Learning/" class="md-nav__link">
        End to End Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Epsilon%20Decay/" class="md-nav__link">
        ϵ-Decay Strategies
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Ergodicity/" class="md-nav__link">
        Ergodicity / Ergodic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Excitatory%20-%20Inhibitory%20Oscillation%20%28EIO-SBF%29/" class="md-nav__link">
        Excitatory   Inhibitory Oscillation (EIO SBF)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Excitatory%20-%20Inhibitory%20postsynaptic%20potential%20%28EPSP%29%20-%20%28IPSP%29/" class="md-nav__link">
        Excitatory   Inhibitory postsynaptic potential (EPSP)   (IPSP)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Firing-Rate%20RNN%20Model/" class="md-nav__link">
        Firing Rate RNN Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Fixed%20Interval%20%28FI%29%20procedure/" class="md-nav__link">
        Main
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Geometric%20Deep%20Learning/" class="md-nav__link">
        Geometric Deep Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Impairment%20effect/" class="md-nav__link">
        Impairment effect
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Interval%20Timing/" class="md-nav__link">
        Interval Timing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Izhikevich%20%20simple%20spiking%20neurons/" class="md-nav__link">
        Izhikevich  simple spiking neurons
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Kernel/" class="md-nav__link">
        Kernel
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/L2-Norm/" class="md-nav__link">
        L2 Norm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Leaky%20Integrate-and-Fire/" class="md-nav__link">
        Leaky Integrate and Fire
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Long%20Short-Term%20Memory%20%28LSTM%29/" class="md-nav__link">
        Long Short Term Memory (LSTM)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Marr%E2%80%99s%20three%20levels%20of%20analysis/" class="md-nav__link">
        Marr’s three levels of analysis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Membrane%20Time%20Constant/" class="md-nav__link">
        Membrane Time Constant
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Multi-Armed%20Bandit%20Problem/" class="md-nav__link">
        Multi Armed Bandit Problem
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Multiplicative%20Weights%20Update%20Algorithm/" class="md-nav__link">
        About
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neural%20Cellular%20Automata/" class="md-nav__link">
        Neural Cellular Automata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neural%20Oscillations/" class="md-nav__link">
        Gamma
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neuromorphics/" class="md-nav__link">
        Neuromorphics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Neuroscientific%20AI/" class="md-nav__link">
        Neuroscientific AI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Nomenclature%20for%20Experiments/" class="md-nav__link">
        Statement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Novelty%20Signal/" class="md-nav__link">
        Novelty Signal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Nullcline/" class="md-nav__link">
        Nullcline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/On%20Off%20Policy/" class="md-nav__link">
        On Off Policy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Optuna/" class="md-nav__link">
        Optuna
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Pad%C3%A9%20approximants/" class="md-nav__link">
        Padé approximants
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Pareto%20Optimality/" class="md-nav__link">
        Pareto Optimality
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Peak%20Interval%20%28PI%29%20procedure/" class="md-nav__link">
        Peak Interval (PI) procedure
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Physical%20Symbol%20System%20Hypothesis%20%28PSSH%29/" class="md-nav__link">
        Physical Symbol System Hypothesis (PSSH)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Power%20Law%20distribution/" class="md-nav__link">
        Power Law distribution
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Q-Learning%20v.%20SARSA/" class="md-nav__link">
        Q Learning v. SARSA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Q-Learning/" class="md-nav__link">
        Q Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/REINFORCE/" class="md-nav__link">
        REINFORCE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Randomness%2C%20Disorder%2C%20and%20Noise/" class="md-nav__link">
        Randomness, Disorder, and Noise
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/ReLU/" class="md-nav__link">
        ReLU
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Relevance/" class="md-nav__link">
        Relevance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Reward%20Prediction%20Error/" class="md-nav__link">
        Reward Prediction Error
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/SARSA%20Algorithm/" class="md-nav__link">
        SARSA Algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Scalar%20Expectancy%20Theory/" class="md-nav__link">
        Scalar Expectancy Theory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Semi-Markov%20Decision%20Process%20%28SMDP%29/" class="md-nav__link">
        Semi Markov Decision Process (SMDP)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Separation%20Property/" class="md-nav__link">
        Separation Property
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Softmax/" class="md-nav__link">
        Softmax
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Spike%20Response%20Model%20neuron/" class="md-nav__link">
        Extracts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Stochastic%20Gradient%20Descent/" class="md-nav__link">
        Stochastic Gradient Descent
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Striatal%20Beat%20Frequency%20model%20%28SBF%29/" class="md-nav__link">
        Referenced in:
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Surrogate%20Gradient%20Learning/" class="md-nav__link">
        Surrogate Gradient Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Target%20v%20behavior%20Policy/" class="md-nav__link">
        Target v behavior Policy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Temporal%20Bisection%20Task/" class="md-nav__link">
        Temporal Bisection Task
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Temporal%20Difference%20%28TD%29/" class="md-nav__link">
        Temporal Difference (TD)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Three-Factor%20Learning%20Rule/" class="md-nav__link">
        Three Factor Learning Rule
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Three-Factor%20Learning/" class="md-nav__link">
        Three Factor Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Tigerprop/" class="md-nav__link">
        Tigerprop
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Time%20Invariance/" class="md-nav__link">
        Definition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Timing%20Tasks/" class="md-nav__link">
        Timing Tasks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Topics%20-%20Striatal%20Beat%20Frequency%20model%20%28SBF%29/" class="md-nav__link">
        Referenced in:
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Vogels-Abbott%20Benchmark/" class="md-nav__link">
        Vogels Abbott Benchmark
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Weber%27s%20Law/" class="md-nav__link">
        (Weber’s law):
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Weighted%20majority%20algorithm/" class="md-nav__link">
        Weighted majority algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/Wilson-Cowan%20rate%20model/" class="md-nav__link">
        Wilson Cowan rate model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/behavior%20theory%20of%20timing%20%28BeT%29/" class="md-nav__link">
        [[@hasegawaModelMultisecondTiming2015|Takayuki Hasegawa, Shogo Sakata 2015]]
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/bias-variance%20trade-off/" class="md-nav__link">
        Bias variance trade off
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/catastrophic%20forgetting/" class="md-nav__link">
        Catastrophic forgetting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/central%20tendency%20effects/" class="md-nav__link">
        Central tendency effects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/derivative%20log%20trick/" class="md-nav__link">
        Derivative log trick
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/discount%20factor/" class="md-nav__link">
        Discount factor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/e-prop/" class="md-nav__link">
        E prop
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/learned%20embedding/" class="md-nav__link">
        Learned embedding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/local%20plasticity/" class="md-nav__link">
        Local plasticity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/multiple%20clock%20or%20flexible%20clock%20hypothesis/" class="md-nav__link">
        Multiple clock or flexible clock hypothesis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/pacemaker%20accumulator%20models%20%28PA%29/" class="md-nav__link">
        Reading
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/perceptual%20aliasing/" class="md-nav__link">
        Perceptual aliasing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/primitive%20actions/" class="md-nav__link">
        Primitive actions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/rescaling/" class="md-nav__link">
        Rescaling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/stride/" class="md-nav__link">
        Stride
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../topics/temporal%20abstraction/" class="md-nav__link">
        Temporal abstraction
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          AZ. Assets
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          AZ. Assets
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../AZ.%20Assets/Outline%20or%20Goals%20Draft/" class="md-nav__link">
        Outline or Goals Draft
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          Features
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Features
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Features/LaTeX%20Math%20Support/" class="md-nav__link">
        LaTeX Math Support
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Features/Mermaid%20Diagrams/" class="md-nav__link">
        Mermaid diagrams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Features/Text%20Formatting/" class="md-nav__link">
        Text Formatting
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          Topic 1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Topic 1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Topic%201/Note%201/" class="md-nav__link">
        Note 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../Topic%201/Note%202/" class="md-nav__link">
        Note 2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#excerpt" class="md-nav__link">
    Excerpt
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="a-review-of-learning-in-biologically-plausible-spiking-neural-networks-sciencedirect">A review of learning in biologically plausible spiking neural networks - ScienceDirect<a class="headerlink" href="#a-review-of-learning-in-biologically-plausible-spiking-neural-networks-sciencedirect" title="Permanent link">&para;</a></h1>
<blockquote>
<h2 id="excerpt">Excerpt<a class="headerlink" href="#excerpt" title="Permanent link">&para;</a></h2>
<p>Artificial neural networks have been used as a powerful processing tool in various areas such as pattern recognition, control, robotics, and bioinform…</p>
</blockquote>
<hr />
<h1 id="abstract">Abstract<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h1>
<p><a href="https://www.sciencedirect.com/topics/engineering/artificial-neural-network" title="Learn more about Artificial neural networks from ScienceDirect's AI-generated Topic Pages">Artificial neural networks</a> have been used as a powerful processing tool in various areas such as pattern recognition, control, robotics, and <a href="https://www.sciencedirect.com/topics/engineering/bioinformatics" title="Learn more about bioinformatics from ScienceDirect's AI-generated Topic Pages">bioinformatics</a>. Their wide applicability has encouraged researchers to improve artificial neural networks by investigating the biological brain. Neurological research has significantly progressed in recent years and continues to reveal new characteristics of biological neurons. New technologies can now capture temporal changes in the internal activity of the brain in more detail and help clarify the relationship between brain activity and the perception of a given stimulus. This new knowledge has led to a new type of artificial neural network, the Spiking <a href="https://www.sciencedirect.com/topics/neuroscience/neural-networks" title="Learn more about Neural Network from ScienceDirect's AI-generated Topic Pages">Neural Network</a> (SNN), that draws more faithfully on biological properties to provide higher processing abilities. A review of recent developments in learning of <a href="https://www.sciencedirect.com/topics/engineering/spiking-neuron" title="Learn more about spiking neurons from ScienceDirect's AI-generated Topic Pages">spiking neurons</a> is presented in this paper. First the biological background of SNN learning algorithms is reviewed. The important elements of a learning algorithm such as the neuron model, <a href="https://www.sciencedirect.com/topics/neuroscience/synaptic-plasticity" title="Learn more about synaptic plasticity from ScienceDirect's AI-generated Topic Pages">synaptic plasticity</a>, information encoding and SNN topologies are then presented. Then, a critical review of the state-of-the-art learning algorithms for SNNs using single and multiple spikes is presented. Additionally, deep spiking neural networks are reviewed, and challenges and opportunities in the SNN field are discussed.</p>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303120"><strong>Previous</strong></a></li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303351"><strong>Next</strong></a></li>
</ul>
<h2 id="keywords">Keywords<a class="headerlink" href="#keywords" title="Permanent link">&para;</a></h2>
<p>Spiking neural network (SNN)</p>
<p>Learning</p>
<p>Synaptic plasticity</p>
<h2 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h2>
<p>The human brain is a very complex system and is constructed of approximately 90 billion neurons (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b4">Azevedo et al., 2009</a>). It is structurally organized by trillions of interconnected synapses. Information is transferred between neurons by electrical impulses called spikes. The effect of a spike, which is sent by a <a href="https://www.sciencedirect.com/topics/engineering/presynaptic-neuron" title="Learn more about presynaptic neuron from ScienceDirect's AI-generated Topic Pages">presynaptic neuron</a> to a receiving neuron, depends on the strength of the synapse that connects the two neurons. The <a href="https://www.sciencedirect.com/topics/computer-science/synaptic-strength" title="Learn more about synaptic strengths from ScienceDirect's AI-generated Topic Pages">synaptic strengths</a> and the connection pattern between neurons have a significant role in the information processing capability of nervous systems. The brain’s processing ability to solve complex problems has inspired many researchers to investigate its processing function and learning mechanisms. Artificial <a href="https://www.sciencedirect.com/topics/neuroscience/neural-networks" title="Learn more about Neural Networks from ScienceDirect's AI-generated Topic Pages">Neural Networks</a> (ANNs), as a powerful and flexible computing means to solve complex problems, have emerged as a result of this research on the brain’s processing functionality.</p>
<p>ANNs are inspired by the biological nervous system and are successfully used in various applications (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b50">Hinton et al., 2012</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b51">Hinton et al., 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b52">Hinton and Salakhutdinov, 2006</a>). However, their high abstraction compared to their biological counterpart (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b111">Pham, Packianather, &amp; Charles, 2008</a>) and their inability to capture the complex temporal dynamics of biological neurons have resulted in a new area of ANNs where the focus is placed on more biologically plausible neuronal models known as Spiking Neural Networks (SNNs). Thanks to their ability to capture the rich dynamics of biological neurons and to represent and integrate different information dimensions such as time, frequency, and phase, SNNs offer a promising computing paradigm and are potentially capable of modelling complex information processing in the brain (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b18">Brette et al., 2007</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b35">Gerstner and Kistler, 2002</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b53">Hodgkin and Huxley, 1952</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b61">Izhikevich, 2004</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b62">Izhikevich, 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b68">Kasabov et al., 2013</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b87">Maass and Zador, 1999</a>). SNNs are also potentially capable of dealing with large volumes of data and using trains of spikes for information representation (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b68">Kasabov et al., 2013</a>). Additionally, SNNs are suitable for implementation on low power hardware.</p>
<p>It is broadly agreed that spikes (a.k.a pulses or action potentials), which represent short and sudden increases in the voltage of a neuron, are used to transfer information between neurons (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b35">Gerstner &amp; Kistler, 2002</a>). The encoding of information through spikes is still a matter of debate in the computational <a href="https://www.sciencedirect.com/topics/neuroscience/neurosciences" title="Learn more about neuroscience from ScienceDirect's AI-generated Topic Pages">neuroscience</a> community. Previously, it was supposed that the brain encodes information through spike rates (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b89">Masquelier &amp; Deco, 2013</a>). However, neurobiological research findings have shown high speed processing in the brain that cannot be performed by a rate coding scheme alone (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b17">Brette, 2015</a>). It has been shown that human visual processing can perform a recognition task in less than 100 ms by using neurons in multiple layers (from the retina to the temporal lobe). It takes about 10 ms processing time for each neuron. The time-window is thus too small for rate coding to occur (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b141">Thorpe et al., 2001</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b145">Vreeken, 2003</a>). The rapid information processing in the electro sensory system of electric fish (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b49">Heiligenberg, 1991</a>) and in the auditory system of echo-locating bats (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b76">Kuwabara &amp; Suga, 1993</a>) are other examples of high speed information processing in biological nervous systems. High speed processing tasks can be performed using precise timing of spikes (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b145">Vreeken, 2003</a>). Additionally, the firing of so many spikes in rate coding of a stimulus demands considerable energy and resources. Moreover, the precise timing of spikes has a higher information encoding capacity in a small set of <a href="https://www.sciencedirect.com/topics/engineering/spiking-neuron" title="Learn more about spiking neurons from ScienceDirect's AI-generated Topic Pages">spiking neurons</a> (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b106">Paugam-Moisy &amp; Bohte, 2012</a>). Therefore, it seems clear that the precise timing of individual spikes, and not just the number of spikes or firing rate, is likely to convey information.</p>
<p>However, the exact learning mechanism in which a neuron is trained is an open question. Recently, biologists have found various forms of biological <a href="https://www.sciencedirect.com/topics/neuroscience/synaptic-plasticity" title="Learn more about synaptic plasticity from ScienceDirect's AI-generated Topic Pages">synaptic plasticity</a>, which are governed by spikes (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b29">Feldman, 2012</a>). These various forms of <a href="https://www.sciencedirect.com/topics/engineering/synaptic-weight" title="Learn more about synaptic weight from ScienceDirect's AI-generated Topic Pages">synaptic weight</a> and delay learning (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b83">Lin &amp; Faber, 2002</a>) are compatible with the spiking neuron model, whereas there is considerable difficulty for their application in traditional models.</p>
<p>The activity of a biological neural system can be studied at various scales, levels and perspectives, for example genes and molecules, single-cell <a href="https://www.sciencedirect.com/topics/computer-science/electrophysiology" title="Learn more about electrophysiology from ScienceDirect's AI-generated Topic Pages">electrophysiology</a>, multi neuron recordings, and cognitive neuroscience and psychophysics. Simulation and <a href="https://www.sciencedirect.com/topics/computer-science/mathematical-theory" title="Learn more about mathematical theories from ScienceDirect's AI-generated Topic Pages">mathematical theories</a> are used in the literature to link the various levels. In the bottom-up approach of investigating the biological nervous system, the knowledge of lower levels (such as <a href="https://www.sciencedirect.com/topics/mathematics/sigma-property" title="Learn more about properties from ScienceDirect's AI-generated Topic Pages">properties</a> of ion channels) is used to describe the higher level phenomena such as the generation of an action potential or memory formation. Hodgkin and Huxley’s model of a biological neuron is an example of a bottom-up description of a neuron. In the biophysical neuron model, the properties of ion channels with different time constant and different dynamics in a cell membrane are modelled (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b37">Gerstner, Sprekeler, &amp; Deco, 2012</a>). The activity of a biological neuron system can be investigated in highly extended levels.</p>
<p>In this paper, the review starts from the level of a <a href="https://www.sciencedirect.com/topics/engineering/single-neuron" title="Learn more about single neuron from ScienceDirect's AI-generated Topic Pages">single neuron</a> and then progresses to biologically plausible learning algorithms for a single neuron as well as populations of neurons. First the biological background of SNN learning algorithms is reviewed. The important elements of a learning algorithm such as the spiking neuron model, synaptic plasticity, information encoding and SNN topologies are then studied. Subsequently, state of the art learning algorithms for SNNs are reviewed. Finally, challenges and opportunities in the SNN field are discussed.</p>
<h2 id="2-biological-background">2. Biological background<a class="headerlink" href="#2-biological-background" title="Permanent link">&para;</a></h2>
<p>Neurons represent the elementary processing units of the brain. They communicate by sending and receiving action potentials (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b35">Gerstner &amp; Kistler, 2002</a>). Neurons are connected to each other, through synapses, in an intricate pattern making specific structures. A review of the literature shows that the important considerations in the design of a learning algorithm for <a href="https://www.sciencedirect.com/topics/neuroscience/neural-networks" title="Learn more about SNNs from ScienceDirect's AI-generated Topic Pages">SNNs</a> are as follows: neuron models, communication through synapses, the topology of the network, and the information encoding/decoding schemes. The following review discusses the impact of these aspects.</p>
<h3 id="21-spiking-neuron-models">2.1. Spiking neuron models<a class="headerlink" href="#21-spiking-neuron-models" title="Permanent link">&para;</a></h3>
<p>In 1952, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b53">Hodgkin and Huxley (1952)</a> performed experiments on the giant <a href="https://www.sciencedirect.com/topics/neuroscience/axon" title="Learn more about axon from ScienceDirect's AI-generated Topic Pages">axon</a> of the squid and built a four dimensional (4D) detailed conductance-based neuron model which can reproduce electrophysiological measurements. However, the intrinsic computational complexity of this model increases its computational cost. Consequently, more simple, phenomenological spiking neuron models are used for simulating large scale SNNs, <a href="https://www.sciencedirect.com/topics/neuroscience/neural-coding" title="Learn more about neural coding from ScienceDirect's AI-generated Topic Pages">neural coding</a> and memory (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b35">Gerstner &amp; Kistler, 2002</a>). The biological <a href="https://www.sciencedirect.com/topics/mathematics/plausibility" title="Learn more about plausibility from ScienceDirect's AI-generated Topic Pages">plausibility</a> and the implementation cost of various spiking neuron models are compared in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b61">Izhikevich (2004)</a>. The Leaky Integrate-and-Fire (LIF) model (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b73">Koch &amp; Segev, 1998</a>) and the Spike Response Model (SRM) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b36">Gerstner, Kistler, Naud, &amp; Paninski, 2014</a>) are two popular 1D spiking <a href="https://www.sciencedirect.com/topics/engineering/neural-model" title="Learn more about neural models from ScienceDirect's AI-generated Topic Pages">neural models</a> with low computational cost, but they offer poor biological plausibility compared with the Hodgkin and <a href="https://www.sciencedirect.com/topics/engineering/huxley-model" title="Learn more about Huxley model from ScienceDirect's AI-generated Topic Pages">Huxley model</a>. The 2D model of Izhikevich (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b60">Izhikevich, 2003</a>) offers a good trade-off between biological plausibility and computational efficiency. Although it can produce various spiking dynamics, many of these characteristics such as Chaos and Bi-stability have not been used in current learning algorithms.</p>
<p>According to biological evidence, a neuron can operate as an <a href="https://www.sciencedirect.com/topics/engineering/integrator" title="Learn more about integrator from ScienceDirect's AI-generated Topic Pages">integrator</a> or Coincidence Detector (CD) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b75">König, Engel, &amp; Singer, 1996</a>). In an integrator model the neuron integrates incoming <a href="https://www.sciencedirect.com/topics/neuroscience/postsynaptic-potential" title="Learn more about Post Synaptic Potentials from ScienceDirect's AI-generated Topic Pages">Post Synaptic Potentials</a> (PSP) in a longer <a href="https://www.sciencedirect.com/topics/mathematics/time-interval-tau" title="Learn more about time interval from ScienceDirect's AI-generated Topic Pages">time interval</a> than in a CD. The integrator model takes advantage of the effect of not only input spikes with short inter spike intervals, but also input spikes that are relatively far from each other. However CDs use the effect of the input spikes that are near to each other (i.e. have short inter spike intervals) to generate the neuron total PSP (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b75">König et al., 1996</a>). Learning algorithms that use an integrator model focus on <a href="https://www.sciencedirect.com/topics/engineering/synaptic-weight" title="Learn more about synaptic weight from ScienceDirect's AI-generated Topic Pages">synaptic weight</a> plasticity, however, the CD learning algorithm exploits synaptic delay modulation to learn (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b111">Pham et al., 2008</a>). Thus, there is potential to improve biological plausibility and computational ability of SNN learning algorithms by adjusting both synaptic weights and delays to construct new learning algorithms.</p>
<p><strong>Leaky Integrate-and-Fire (LIF) neuron model:</strong> Detailed conductance-based neuron models (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b53">Hodgkin &amp; Huxley, 1952</a>) can reproduce electrophysiological signals to a high degree of accuracy, but they are computationally complex. Simple phenomenological spiking neuron models with low computational cost are highly popular for studies of neural coding, memory, and network dynamics. The LIF is a one dimensional spiking neural model with low computation cost (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b36">Gerstner et al., 2014</a>), that is commonly adopted in the literature. The sub threshold dynamics of the LIF neuron are defined by the following equation: (2.1)<span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfrac is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo linebreak="goodbreak" is="true"&gt;=&lt;/mo&gt;&lt;mo linebreak="goodbreak" is="true"&gt;−&lt;/mo&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo linebreak="goodbreak" is="true"&gt;+&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mi is="true"&gt;I&lt;/mi&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>where <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="normal" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is the membrane potential, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is the membrane time constant, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is the membrane rest potential which is a constant, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is the <a href="https://www.sciencedirect.com/topics/neuroscience/membrane-resistance" title="Learn more about membrane resistance from ScienceDirect's AI-generated Topic Pages">membrane resistance</a>, and <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;I&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is the sum of the current supplied by the input synapses. <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;I&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is calculated by the following equation: (2.2)<span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;I&lt;/mi&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo linebreak="goodbreak" is="true"&gt;=&lt;/mo&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;mi is="true"&gt;⋅&lt;/mi&gt;&lt;mi is="true"&gt;S&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>where <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;W&lt;/mi&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;=&lt;/mo&gt;&lt;mfenced open="[" close="]" is="true"&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;…&lt;/mi&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is the weight vector. <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;S&lt;/mi&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;=&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;[&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo is="true"&gt;;&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo is="true"&gt;;&lt;/mo&gt;&lt;mi is="true"&gt;…&lt;/mi&gt;&lt;mo is="true"&gt;;&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo is="true"&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is the spatiotemporal input spike pattern containing <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/math&gt;\)</span> input <a href="https://www.sciencedirect.com/topics/engineering/spike-train" title="Learn more about spike trains from ScienceDirect's AI-generated Topic Pages">spike trains</a>, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mspace width="1em" class="nbsp" is="true"&gt;&lt;/mspace&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mi is="true"&gt;o&lt;/mi&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;mspace width="1em" class="nbsp" is="true"&gt;&lt;/mspace&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;…&lt;/mi&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. (2.3)<span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo linebreak="goodbreak" is="true"&gt;=&lt;/mo&gt;&lt;munder is="true"&gt;&lt;mrow is="true"&gt;&lt;mo linebreak="badbreak" is="true"&gt;∑&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mi is="true"&gt;δ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>where <span class="arithmatex">\(&lt;math&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/math&gt;\)</span> is the firing time of the <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/math&gt;\)</span>th (<span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;mo is="true"&gt;,&lt;/mo&gt;&lt;mi is="true"&gt;…&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>) spike in the <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/math&gt;\)</span>th input spike train, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is applied to the <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/math&gt;\)</span>th synapse, and <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;δ&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="normal" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is a <a href="https://www.sciencedirect.com/topics/mathematics/dirac-function" title="Learn more about Dirac function from ScienceDirect's AI-generated Topic Pages">Dirac function</a>.</p>
<p>When the membrane voltage, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, reaches the threshold level, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mi is="true"&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, an <a href="https://www.sciencedirect.com/topics/engineering/output-spike" title="Learn more about output spike from ScienceDirect's AI-generated Topic Pages">output spike</a> is generated, and the membrane voltage resets to the rest potential, <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, and stays at the resting level for period of time <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;mi is="true"&gt;e&lt;/mi&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, called the <a href="https://www.sciencedirect.com/topics/engineering/refractory-period" title="Learn more about refractory period from ScienceDirect's AI-generated Topic Pages">refractory period</a>.</p>
<h3 id="22-synaptic-plasticity">2.2. Synaptic plasticity<a class="headerlink" href="#22-synaptic-plasticity" title="Permanent link">&para;</a></h3>
<p><a href="https://www.sciencedirect.com/topics/neuroscience/synaptic-plasticity" title="Learn more about Synaptic plasticity from ScienceDirect's AI-generated Topic Pages">Synaptic plasticity</a> (i.e. change in synaptic efficacy) is considered to be the biological <a href="https://www.sciencedirect.com/topics/engineering/underpinnings" title="Learn more about underpinning from ScienceDirect's AI-generated Topic Pages">underpinning</a> of learning and memory. The exact relationship between microscopic synaptic <a href="https://www.sciencedirect.com/topics/mathematics/sigma-property" title="Learn more about properties from ScienceDirect's AI-generated Topic Pages">properties</a> and macroscopic functional consequences remains highly controversial (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b97">Morrison, Diesmann, &amp; Gerstner, 2008</a>). Unsupervised, supervised, and reinforcement learning are the three known types of learning strategies and the following details these approaches with a focus on synaptic plasticity. However, there is also biological evidence that the synaptic delay is not always constant and can be modulated during synaptic plasticity (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b83">Lin &amp; Faber, 2002</a>).</p>
<h4 id="221-unsupervised-learning">2.2.1. Unsupervised learning<a class="headerlink" href="#221-unsupervised-learning" title="Permanent link">&para;</a></h4>
<p><a href="https://www.sciencedirect.com/topics/computer-science/unsupervised-learning" title="Learn more about Unsupervised learning from ScienceDirect's AI-generated Topic Pages">Unsupervised learning</a> is progressed according to local events, and the local events do not have any notion of the task to be solved, and also they do not have any notion of the change being ‘good’ or ‘bad’. Learning simply involves an adaptation according to local activity. Hebb’s in 1949 postulate, which describes how <a href="https://www.sciencedirect.com/topics/engineering/synaptic-connection" title="Learn more about synaptic connections from ScienceDirect's AI-generated Topic Pages">synaptic connections</a> should be modified, has inspired many unsupervised approaches (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b97">Morrison et al., 2008</a>). Unsupervised learning may be constructed from a combination of the following: (a) spontaneous growth or decay of weights in the absence of any activity (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b142">Turrigiano &amp; Nelson, 2004</a>); (b) effects caused by postsynaptic spikes alone independent of presynaptic spike arrival (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b3">Artola, Brocher, &amp; Singer, 1990</a>); (c) effects caused by presynaptic spikes, independent of postsynaptic variables — the case for short-term synaptic plasticity (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b143">Vasilaki &amp; Giugliano, 2013</a>); (d) effects caused by presynaptic spikes in conjunction with postsynaptic spikes or in conjunction with postsynaptic <a href="https://www.sciencedirect.com/topics/engineering/depolarization" title="Learn more about depolarization from ScienceDirect's AI-generated Topic Pages">depolarization</a> (Hebbian terms) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b22">Clopath, Büsing, Vasilaki, &amp; Gerstner, 2010</a>); (e) all of the above effects may depend on the current value of the synaptic weight (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b97">Morrison et al., 2008</a>), e.g. close to a maximum weight <a href="https://www.sciencedirect.com/topics/computer-science/synaptic-change" title="Learn more about synaptic changes from ScienceDirect's AI-generated Topic Pages">synaptic changes</a> could become smaller (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b97">Morrison et al., 2008</a>). Although Hebbian plasticity is used in the current SNN learning algorithms, other synaptic plasticity such as short-term plasticity is less used.</p>
<p><a href="https://www.sciencedirect.com/topics/mathematics/spike-timing-dependent-plasticity" title="Learn more about Spike Timing Dependent Plasticity from ScienceDirect's AI-generated Topic Pages">Spike Timing Dependent Plasticity</a> (STDP) is a variant of the Hebbian unsupervised learning algorithm. This rule is proposed to describe the changes of a synaptic weight according to the relative timing of pre and postsynaptic spikes. According to STDP, a synaptic weight is potentiated if a presynaptic spike comes shortly before a postsynaptic spike. If the time interval between the pre- and postsynaptic spike is <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;=&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;p&lt;/mi&gt;&lt;mi is="true"&gt;o&lt;/mi&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;−&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;p&lt;/mi&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;mi is="true"&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and t <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;&gt;&lt;/mo&gt;&lt;/math&gt;\)</span> 0 then the synaptic weight will be potentiated. The magnitude of the potentiation is a function of t which decays exponentially with a time constant <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> and can be calculated by <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mi mathvariant="normal" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;∕&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is the maximum synaptic change. Alternatively, if a pre synaptic spike occurs shortly after the postsynaptic spike, then the <a href="https://www.sciencedirect.com/topics/computer-science/synaptic-efficacy" title="Learn more about synaptic efficacy from ScienceDirect's AI-generated Topic Pages">synaptic efficacy</a> is decreased. The magnitude of the decrease can be calculated by <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;∕&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, where <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> represents the maximum depression, and <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is a time constant. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#fig1">Fig. 1</a> shows the synaptic changes as a function of time interval t. The shape of an STDP function does not have to be fixed across a network and different synapses can have differing shapes (parameters) for this function. According to <a href="https://www.sciencedirect.com/topics/engineering/physiological-evidence" title="Learn more about physiological evidence from ScienceDirect's AI-generated Topic Pages">physiological evidence</a>, the generality of STDP is debated because the order of pre- and post-synaptic spikes is only important in some situations, depending on the presynaptic activity such as firing rate (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b140">Tetzlaff, Kolodziejski, Markelic, &amp; Wörgötter, 2012</a>).</p>
<p>Biological experiments show that the standard pair-based STDP models (i.e. pre-before-post and post-before-pre) cannot give a full description of STDP in a biological neuron. There are experimental researches that investigate multiple-spike protocols (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b97">Morrison et al., 2008</a>). Symmetric triplets STDP in the form of pre-post-pre and post-pre-post, which are a simple example of multiple-spike STDP, are investigated experimentally in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b11">Bi and Wang, 2002</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b32">Froemke and Dan, 2002</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b33">Froemke et al., 2006</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b148">Wang, Gerkin, Nauen, and Bi (2005)</a>. There are different multiple-spike STDP models that are developed to predict the experimental results (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b97">Morrison et al., 2008</a>). One simple triplet STDP model is developed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b109">Pfister and Gerstner (2006)</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b109">Pfister and Gerstner (2006)</a> implemented a triplet STDP with local variables called traces. The trace related to a <a href="https://www.sciencedirect.com/topics/engineering/presynaptic-neuron" title="Learn more about presynaptic neuron from ScienceDirect's AI-generated Topic Pages">presynaptic neuron</a> j is shown by <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="normal" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, and the <a href="https://www.sciencedirect.com/topics/engineering/postsynaptic-neuron" title="Learn more about post synaptic neuron from ScienceDirect's AI-generated Topic Pages">post synaptic neuron</a> ‘i’ corresponds to two traces <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="normal" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="normal" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> with fast and slow dynamics respectively (<span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;&lt;&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>) as shown in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#fig2">Fig. 2</a> (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b97">Morrison et al., 2008</a>). The <a href="https://www.sciencedirect.com/topics/computer-science/term-depression" title="Learn more about LTD from ScienceDirect's AI-generated Topic Pages">LTD</a> in the triplet STDP is similar to standard paired based STDP. A weight is depressed in proportion to the fast postsynaptic trace, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="normal" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> as shown by unfilled circles in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#fig2">Fig. 2</a>. At the moment of a postsynaptic spike LTP is induced in proportion to the presynaptic trace <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="normal" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> (similar to the standard pair based STDP) and the slow postsynaptic trace <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="normal" is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="normal" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> as shown in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#fig2">Fig. 2</a> by filled circles.</p>
<p><img alt="" src="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr1.jpg" /></p>
<ol>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr1_lrg.jpg" title="Download high-res image (80KB)">Download : Download high-res image (80KB)</a></li>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr1.jpg" title="Download full-size image">Download : Download full-size image</a></li>
</ol>
<p>Fig. 1. <a href="https://www.sciencedirect.com/topics/mathematics/spike-timing-dependent-plasticity" title="Learn more about STDP from ScienceDirect's AI-generated Topic Pages">STDP</a> learning time window. If the post neuron fired after the presynaptic spike, the weight of <a href="https://www.sciencedirect.com/topics/engineering/synaptic-connection" title="Learn more about synaptic connection from ScienceDirect's AI-generated Topic Pages">synaptic connection</a> from pre- to <a href="https://www.sciencedirect.com/topics/engineering/postsynaptic-neuron" title="Learn more about postsynaptic neuron from ScienceDirect's AI-generated Topic Pages">postsynaptic neuron</a> is increased. The magnitude of change decreases as <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;∕&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. Reverse order results in a decrease of the <a href="https://www.sciencedirect.com/topics/engineering/synaptic-weight" title="Learn more about synaptic weight from ScienceDirect's AI-generated Topic Pages">synaptic weight</a> with magnitude <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;∕&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>.</p>
<p>The figure is adapted from <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b43">González-Nalda (2009)</a>.</p>
<p>The ability of the multiple spike STDP to model the synaptic plasticity of a biological neuron shows that the learning algorithms that work based on multiple spikes are more biologically plausible and multiple spike coding can be an appropriate choice for modelling the information processing in the brain.</p>
<p><strong>Local dendritic depolarization related STDP:</strong> Several recent studies have investigated how a synapse location within the dendritic tree influences STDP. Dendritic mechanisms can produce different learning rules in different dendritic domains of the same neuron (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b67">Kampa et al., 2007</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b82">Letzkus et al., 2006</a>). Nearby synapses within dendritic branches contribute to local associative plasticity. Therefore, local dendritic depolarization is the main tool to manage the plasticity (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b29">Feldman, 2012</a>). In other words local dendritic PSPs can change the STDP characteristic. This change can be in various forms. For example, anti-Hebbian LTD on cortical <a href="https://www.sciencedirect.com/topics/neuroscience/pyramidal-cell" title="Learn more about pyramidal cells from ScienceDirect's AI-generated Topic Pages">pyramidal cells</a> is converted into Hebbian STDP depending on the location of the plasticity within the dendrite tree. The association of local dendritic depolarization and STDP may be useful in improving the information processing ability of neurons by specifying different synapses for different types of input information and by providing dynamic control over plasticity. Spike timing will be a main factor of plasticity in some circumstances; however, it will not be a prominent factor in others (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b29">Feldman, 2012</a>). STDP has been used to design learning algorithms for spiking neural networks (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b113">Ponulak and Kasiński, 2010</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b127">Srinivasa and Cho, 2012</a>). However, different biological characteristics of STDP are not considered in the learning algorithm, and it seems that the consideration of the new biological property of STDP (Local dendritic depolarization related STDP) might contribute to the design of a new learning algorithm which is more biologically plausible and has new interesting computational characteristics.</p>
<p><img alt="" src="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr2.jpg" /></p>
<ol>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr2_lrg.jpg" title="Download high-res image (121KB)">Download : Download high-res image (121KB)</a></li>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr2.jpg" title="Download full-size image">Download : Download full-size image</a></li>
</ol>
<p>Fig. 2. Triplet <a href="https://www.sciencedirect.com/topics/mathematics/spike-timing-dependent-plasticity" title="Learn more about STDP from ScienceDirect's AI-generated Topic Pages">STDP</a> is governed by a trace corresponding to presynaptic spikes, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, and the two fast (<span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold" is="true"&gt;1&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>) and slow (<span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold" is="true"&gt;2&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>) traces related to postsynaptic spikes. LTD (Long Term Depression) takes place at the time of a presynaptic spike in proportion to the momentary value of <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold" is="true"&gt;1&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> as shown by unfilled circles. A <a href="https://www.sciencedirect.com/topics/engineering/synaptic-weight" title="Learn more about synaptic weight from ScienceDirect's AI-generated Topic Pages">synaptic weight</a> is potentiated at the time of a postsynaptic spike in proportion to the momentary value of presynaptic trace (<span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>) and value of the slow trace <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi mathvariant="bold" is="true"&gt;2&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi mathvariant="bold-italic" is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> as shown by black filled circles.</p>
<p>The figure is adapted from <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b97">Morrison et al. (2008)</a></p>
<h4 id="222-supervised-learning">2.2.2. Supervised learning<a class="headerlink" href="#222-supervised-learning" title="Permanent link">&para;</a></h4>
<p>There is evidence that confirms the existence of supervised or instruction-based learning in the brain (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b20">Carey et al., 2005</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b27">Doya, 1999</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b59">Ito, 2000</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b72">Knudsen, 2002</a>). One form of the supervised learning is governed by an instruction signal. It is believed that these signals are provided in the learning modules by sensory feedback (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b20">Carey et al., 2005</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b72">Knudsen, 2002</a>) or by other neuronal assemblies (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b27">Doya, 1999</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b59">Ito, 2000</a>). However, the exact mechanism of supervised learning in the brain is not clear (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea &amp; Grüning, 2013</a>). The <a href="https://www.sciencedirect.com/topics/neuroscience/cerebellum" title="Learn more about cerebellum from ScienceDirect's AI-generated Topic Pages">cerebellum</a> is thought to be the primary site for supervised learning in the brain (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b64">Jörntell and Hansel, 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b114">Ponulak and Kasinski, 2011</a>). Supervised learning at the level of a neuron has been demonstrated experimentally by <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b31">Fregnac and Shulz (1999)</a>. Naturally, a few inputs to strong synapses can drive a neuron response and therefore drive learning of other input synapses. If these strong inputs are controlled for a target-specific task, they act as a teacher for the postsynaptic neuron.</p>
<h4 id="223-reinforcement-learning">2.2.3. Reinforcement learning<a class="headerlink" href="#223-reinforcement-learning" title="Permanent link">&para;</a></h4>
<p>Behaviours are learnt not only through direct instructions, but more often by exploring available actions in the presence of reward signals. In reward-based or reinforcement learning the direction and amount of change of the learning free parameters depends on the presence or absence of a success signal (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b120">Schultz, Dayan, &amp; Montague, 1997</a>). Reinforcement learning initially is understood by psychological evidence. Recently, it has been shown that the concentration of dopamine which can act as a reward signal affects the synaptic change in various parts of the brain. Dopamine is a <a href="https://www.sciencedirect.com/topics/computer-science/neuromodulators" title="Learn more about neuromodulator from ScienceDirect's AI-generated Topic Pages">neuromodulator</a> which is emitted by <a href="https://www.sciencedirect.com/topics/neuroscience/dopaminergic" title="Learn more about dopaminergic from ScienceDirect's AI-generated Topic Pages">dopaminergic</a> cells (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b114">Ponulak &amp; Kasinski, 2011</a>).</p>
<h4 id="224-delay-learning">2.2.4. Delay learning<a class="headerlink" href="#224-delay-learning" title="Permanent link">&para;</a></h4>
<p>The existence of synaptic delay in the mammalian neocortex is proven by experimental research and depending on the type and location of the neurons, the synaptic delay could be as short as 0.1 ms and as long as 40 ms (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b62">Izhikevich, 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b107">Paugam-Moisy et al., 2008</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b129">Swadlow, 1992</a>). The effect of the time delay on the processing ability of the nervous system is well established (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b40">Gilson et al., 2012</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b42">Glackin et al., 2010</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang, 2013</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b155">Xu, Zeng, et al., 2013</a>). For instance, it has been shown that delays have an important role in the mammalian auditory pathway for perception of sound <a href="https://www.sciencedirect.com/topics/engineering/localisation" title="Learn more about localization from ScienceDirect's AI-generated Topic Pages">localization</a> (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b42">Glackin et al., 2010</a>). Additionally, according to biological evidence the synaptic delay can be changed according to input and output spikes (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b83">Lin &amp; Faber, 2002</a>). This biological evidence of delay learning provides an inspiration in developing new learning algorithms that exploit delay learning in addition to the usual weight learning.</p>
<h3 id="23-information-encoding">2.3. Information encoding<a class="headerlink" href="#23-information-encoding" title="Permanent link">&para;</a></h3>
<p>How neurons encode information using spikes is one of the important questions discussed in <a href="https://www.sciencedirect.com/topics/neuroscience/neurosciences" title="Learn more about neuroscience from ScienceDirect's AI-generated Topic Pages">neuroscience</a>. It is assumed that neural information is conveyed either in the firing rate, or in the precise timing of spikes (temporal coding). Various forms of firing rate encoding exist, such as spike count, spike density, or population activity (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b35">Gerstner &amp; Kistler, 2002</a>). Although rate coding is commonly used in traditional <a href="https://www.sciencedirect.com/topics/engineering/artificial-neural-network" title="Learn more about ANNs from ScienceDirect's AI-generated Topic Pages">ANNs</a>, such an approach may not convey all the information related to a rapid processing task such as colour, visual information, odour and sound quality processing as the information encapsulated in the precise timing of spikes is ignored (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b21">Cariani, 2004</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b54">Hopfield, 1995</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b96">Mohemmed et al., 2013</a>).</p>
<p>Examples of temporal coding methods include time to first spike, Rank-Order Coding (ROC) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b118">Rullen &amp; Thorpe, 2001</a>), latency code, phase coding, coding by <a href="https://www.sciencedirect.com/topics/mathematics/synchrony" title="Learn more about synchrony from ScienceDirect's AI-generated Topic Pages">synchrony</a> (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b114">Ponulak &amp; Kasinski, 2011</a>), and polychronisation (which is a group of neurons time-locked to fire in various precise times (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b62">Izhikevich, 2006</a>)). <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b158">Yu, Tang, Tan, and Li (2013b)</a> have shown that coding based on precise timing of spikes can convey more information than ROC which ignores the time differences between spikes. Additionally, ROC is more sensitive to noise, because, the rank of each spike is dependent on the rank of other spikes. If the rank of a single spike is changed as a result of a small noise component, then the ranks of the other spikes are subsequently changed. Consequently, the resulting pattern is completely different from the original pattern (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b158">Yu et al., 2013b</a>).</p>
<h3 id="24-snn-topologies">2.4. SNN topologies<a class="headerlink" href="#24-snn-topologies" title="Permanent link">&para;</a></h3>
<p>A common <a href="https://www.sciencedirect.com/topics/computer-science/classification" title="Learn more about classification from ScienceDirect's AI-generated Topic Pages">classification</a> of SNN topologies considers three types of topologies, namely feed-forward, <a href="https://www.sciencedirect.com/topics/engineering/recurrent" title="Learn more about recurrent from ScienceDirect's AI-generated Topic Pages">recurrent</a> and hybrid networks. Synfire chain and fault-tolerant SNN proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b127">Srinivasa and Cho (2012)</a> are two examples of hybrid networks where some subpopulations may be strictly feed-forward while others may have recurrent topologies.</p>
<p>According to statistical analysis, a cat’s cerebral cortex structure can be considered a clustered network (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b77">Lameu et al., 2012</a>). Each cluster is a scale-free network with highly connected hubs. Those hubs are strongly connected together, i.e. a high number of neurons in different hubs are connected (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b77">Lameu et al., 2012</a>). <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b48">Hazan and Manevitz (2012)</a> have shown that specifying certain kinds of topological constraints, which have been claimed as reasonably biologically plausible, can restore robustness in SNNs (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b48">Hazan &amp; Manevitz, 2012</a>).</p>
<p>It is well established that the topology of SNNs in a brain dynamically changes during the learning process. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b5">Bassett et al. (2012)</a> have shown that primary sensorimotor and visual regions have a relatively stiff core that changes little over time but they have flexible periphery regions which change more frequently (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b5">Bassett et al., 2012</a>). Evolving spiking neural network (eSNN) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b6">Belatreche et al., 2007</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b151">Wysoski et al., 2008</a>), dynamic evolving SNN (deSNN) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b68">Kasabov et al., 2013</a>), dynamic cluster formation using populations of spiking neurons (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b8">Belatreche &amp; Paul, 2012</a>), and the online supervised learning method with adaptive structure in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b147">Wang, Belatreche, Maguire, and McGinnity (2014)</a> are examples of SNNs with <a href="https://www.sciencedirect.com/topics/computer-science/dynamic-topology" title="Learn more about dynamic topology from ScienceDirect's AI-generated Topic Pages">dynamic topology</a>. The evolving structure of SNNs enhances their processing ability as well as improving their biological plausibility.</p>
<h2 id="3-review-of-some-state-of-the-art-learning-algorithms-for-snns">3. Review of some state of the art learning algorithms for SNNs<a class="headerlink" href="#3-review-of-some-state-of-the-art-learning-algorithms-for-snns" title="Permanent link">&para;</a></h2>
<p>In the previous section some biologically plausible elements that might be used to construct spiking neural network learning algorithms were introduced. This section presents a critical review of state of the art learning algorithms for <a href="https://www.sciencedirect.com/topics/engineering/spiking-neuron" title="Learn more about spiking neurons from ScienceDirect's AI-generated Topic Pages">spiking neurons</a>. First, the learning algorithms that can train each neuron to fire a single spike are reviewed. Then, the learning algorithms that train a <a href="https://www.sciencedirect.com/topics/engineering/single-neuron" title="Learn more about single neuron from ScienceDirect's AI-generated Topic Pages">single neuron</a> or a single layer of neurons to learn multiple spikes are discussed. After that, learning of multiple spikes in a multilayer spiking neural network are reviewed. The delay learning ability of spiking neuron is discussed in Section <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#sec3.4">3.4</a>. Finally, recent Deep Spiking Neural Networks are reviewed.</p>
<h3 id="31-learning-a-single-spike-per-neuron">3.1. Learning a single spike per neuron<a class="headerlink" href="#31-learning-a-single-spike-per-neuron" title="Permanent link">&para;</a></h3>
<p>SpikeProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b13">Bohte, Kok, &amp; La Poutre, 2002</a>) is one of the first supervised learning methods for spiking neurons. SpikeProp was inspired by the classical <a href="https://www.sciencedirect.com/topics/engineering/backpropagation-algorithm" title="Learn more about backpropagation algorithm from ScienceDirect's AI-generated Topic Pages">backpropagation algorithm</a>. SpikeProp is a multilayer spiking neural network, and it is applied successfully to <a href="https://www.sciencedirect.com/topics/computer-science/classification" title="Learn more about classification from ScienceDirect's AI-generated Topic Pages">classification</a> problems. In the network, two neurons are connected through multiple connections with different weights and delays (see <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#fig3">Fig. 3</a>). SpikeProp, much like other gradient-based methods, is based on the estimation of the gradient of an error function and thus has local minima problems. In addition, silent neurons or non-firing neurons are another problem which prevents the calculation of the gradient.</p>
<p>Back-propagation with momentum (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b91">McKennoch et al., 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b152">Xin and Embrechts, 2001</a>), QuickProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b91">McKennoch et al., 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b152">Xin and Embrechts, 2001</a>), Resilient propagation (RProp) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b38">Ghosh-Dastidar and Adeli, 2007</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b91">McKennoch et al., 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b125">Silva and Ruano, 2005</a>) Levenberg–Marquardt BP (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b125">Silva &amp; Ruano, 2005</a>), and the SpikeProp based on <a href="https://www.sciencedirect.com/topics/computer-science/adaptive-learning-rate" title="Learn more about adaptive learning rate from ScienceDirect's AI-generated Topic Pages">adaptive learning rate</a> (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b124">Shrestha &amp; Song, 2015</a>) are various learning algorithms proposed to improve the performance of SpikeProp. In the first supervised learning algorithms for multilayer spiking neural networks for learning the precise timing, each neuron is trained to fire only a single spike. In the mentioned learning methods all neurons in the input, output and hidden layers can only fire a single spike. The mentioned learning algorithms depend on the neuron model used in the network.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b7">Belatreche, Maguire, McGinnity, and Wu (2003)</a> used evolutionary strategies to train both <a href="https://www.sciencedirect.com/topics/engineering/synaptic-weight" title="Learn more about synaptic weights from ScienceDirect's AI-generated Topic Pages">synaptic weights</a> and delays for <a href="https://www.sciencedirect.com/topics/engineering/classification-task" title="Learn more about classification tasks from ScienceDirect's AI-generated Topic Pages">classification tasks</a>. The proposed approach has good performance when compared to Spike-Prop, yet the training algorithm is time consuming.</p>
<p>In <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b111">Pham et al. (2008)</a> a self-organizing spiking neural network is designed for pattern clustering. The spike response neuron model with constant weights (which are chosen randomly) is used as a CD. The parameter of the CD is chosen so that the CD fires if its input synapses have spikes close to each other. The output layer is constructed of a two-dimensional grid. A CD is considered for each node in the grid. The number of neurons in the input layer is equal to the dimension of the input pattern. A Hebbian based rule is used to shift the synaptic delays. In other words, the synaptic delays are the learning parameters, and the proposed SOM (Self-organizing map) adjusts the synaptic delay of the winner neuron and the neurons near to the winner during adaptation stage. The Hebbian based learning is designed to adjust the synaptic delay so that the peaks of the input spike responses coincide with each other which cause the receiving neuron to fire. In the model only the first spike of an output is considered. The neuron threshold level is set to a small value at the beginning of the learning and is then increased during different learning epochs. The CDs with a high degree of coincidence can fire in response to a specific input. During learning, an input pattern is applied to the network and the neuron that fires first is considered as the winning neuron. Then the neurons that are in a neighbourhood of the winner are considered to contribute in a <a href="https://www.sciencedirect.com/topics/engineering/hebbian-learning" title="Learn more about Hebbian learning from ScienceDirect's AI-generated Topic Pages">Hebbian learning</a> process. The contribution of a neuron in the learning process is a function of its spatial distance from the winner. The function has a <a href="https://www.sciencedirect.com/topics/engineering/gaussian-shape" title="Learn more about Gaussian shape from ScienceDirect's AI-generated Topic Pages">Gaussian shape</a> and the winner is the centre of the function. The training procedure is stopped when the total change of the connection delays falls below a minimum value, or the delay change remains constant.</p>
<p><img alt="" src="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr3.jpg" /></p>
<ol>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr3_lrg.jpg" title="Download high-res image (218KB)">Download : Download high-res image (218KB)</a></li>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr3.jpg" title="Download full-size image">Download : Download full-size image</a></li>
</ol>
<p>Fig. 3. (a) The Spiking <a href="https://www.sciencedirect.com/topics/neuroscience/neural-networks" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages">neural network</a> architecture used in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b38">Ghosh-Dastidar and Adeli, 2007</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b91">McKennoch et al., 2006</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b125">Silva and Ruano (2005)</a>; (b) each neuron is connected to the next neuron by multiple synapses with different delays.</p>
<p>The figure is adapted from <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b38">Ghosh-Dastidar and Adeli (2007)</a>.</p>
<p>The SNN approach proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b111">Pham et al. (2008)</a> can be compared to the <a href="https://www.sciencedirect.com/topics/mathematics/sigma-property" title="Learn more about properties from ScienceDirect's AI-generated Topic Pages">properties</a> of a traditional SOM (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b74">Kohonen, 2013</a>). In a basic SOM the neighbourhood <a href="https://www.sciencedirect.com/topics/engineering/gaussian-function" title="Learn more about Gaussian function from ScienceDirect's AI-generated Topic Pages">Gaussian function</a> has a time varying width which is decreased by the learning epochs. This shrinking property of the neighbourhood function is not used in the SNN approach proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b111">Pham et al. (2008)</a>. According to this property the first stage of learning should cover a large number of neurons around the winner and as the learning progresses it is decreased. The appropriate neurons are chosen to become more selective to the specific input pattern by this method. Furthermore, in the SOM for <a href="https://www.sciencedirect.com/topics/neuroscience/neural-networks" title="Learn more about SNNs from ScienceDirect's AI-generated Topic Pages">SNNs</a> a simple learning algorithm based on the delay shift is used, however to date there are various learning algorithms for SNN based on the weights learning such as the algorithms proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b13">Bohte et al. (2002)</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b95">Mohemmed, Schliebs, Matsuda, and Kasabov (2012)</a> that can be used to design SOM with higher computation ability than the computation ability of the algorithm mentioned in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b111">Pham et al. (2008)</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b151">Wysoski et al. (2008)</a> proposed an evolving Spiking Neural Network (eSNN) by using a Hebbian-based training. eSNN changes its structure in order to respond optimally to different input visual patterns. It uses hierarchical layers and can be used for online learning applications. Rank order coding (ROC) and a simplified type of integrate-and-fire neuron model are used in eSNN. The algorithm only uses the information in the first spike of each input synapse and it ignores the information that is in the following spikes. In eSNN learning procedure each neuron can fire a single spike. A latency code is used to convert a real value related to input pattern to temporal information in the input spikes. Despite the application of the latency code, it uses a neuron model that works based on order of the input spikes and the precise time of input spikes is not too important for the neuron. Additionally, two different input patterns generated by the latency code can have same order of input spikes; however, they can have completely different temporal patterns relating to different classes. The other problem of eSNN is that it works only based on the first spike and the effect of the other input spikes is not reflected on the synaptic weights adjustment. So the network can only work on the single spike per input and it cannot capture the information related to spatiotemporal input pattern with multiple spikes in each input <a href="https://www.sciencedirect.com/topics/engineering/spike-train" title="Learn more about spike train from ScienceDirect's AI-generated Topic Pages">spike train</a>. Although eSNN is composed of four layers, learning is only taking place in a single layer (layer L3 in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#fig4">Fig. 4</a>).</p>
<p>In <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b68">Kasabov et al. (2013)</a> a dynamic eSNN (deSNN) was proposed to capture the information in more complex spatiotemporal input patterns composed of multiple spikes. deSNN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b68">Kasabov et al., 2013</a>) uses rank-order learning (similar to eSNN <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b151">Wysoski et al., 2008</a>) as well as the spike driven <a href="https://www.sciencedirect.com/topics/neuroscience/synaptic-plasticity" title="Learn more about synaptic plasticity from ScienceDirect's AI-generated Topic Pages">synaptic plasticity</a> (SDSP) learning rule. deSNN initializes the input weights based on the rank of the first input spikes. The initial value of each synaptic weight is adjusted based on SDSP. SDSP adjusts each weight by using the other input spikes that come after the first one. A weight is potentiated when it receives an input spike, and it is depressed in each time step that it does not receive an input spike. By this strategy the information of the first spike as well as the following ones are captured (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b68">Kasabov et al., 2013</a>).</p>
<p>The neuron model that is used in eSNN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b151">Wysoski et al., 2008</a>) and deSNN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b68">Kasabov et al., 2013</a>) is different from the standard biologically plausible neuron model such as <a href="https://www.sciencedirect.com/topics/mathematics/leaky-integrate" title="Learn more about LIF from ScienceDirect's AI-generated Topic Pages">LIF</a> or SRM. The model works on the order of input spikes and it does not have a leakage in the generated <a href="https://www.sciencedirect.com/topics/neuroscience/postsynaptic-potential" title="Learn more about PSP from ScienceDirect's AI-generated Topic Pages">PSP</a>. The leakage can be an essential property of a biological neuron that is sensitive to the <a href="https://www.sciencedirect.com/topics/mathematics/time-interval-tau" title="Learn more about time interval from ScienceDirect's AI-generated Topic Pages">time interval</a> and consequently it can be sensitive to temporal information inside the spatiotemporal input pattern. Although, each <a href="https://www.sciencedirect.com/topics/engineering/synaptic-input" title="Learn more about synaptic input from ScienceDirect's AI-generated Topic Pages">synaptic input</a> in deSNN can have multiple spikes, the output can generate a single spike.</p>
<p><img alt="" src="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr4.jpg" /></p>
<ol>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr4_lrg.jpg" title="Download high-res image (436KB)">Download : Download high-res image (436KB)</a></li>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr4.jpg" title="Download full-size image">Download : Download full-size image</a></li>
</ol>
<p>Fig. 4. Although <a href="https://www.sciencedirect.com/topics/neuroscience/neural-networks" title="Learn more about eSNN from ScienceDirect's AI-generated Topic Pages">eSNN</a> (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b151">Wysoski et al., 2008</a>) has a four-layer architecture, only the <a href="https://www.sciencedirect.com/topics/engineering/synaptic-weight" title="Learn more about synaptic weights from ScienceDirect's AI-generated Topic Pages">synaptic weights</a> of the neurons in the third layer (L3) are trained and the other layers have constant synaptic weights.</p>
<p>The figure is adapted from <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b151">Wysoski et al. (2008)</a></p>
<p>The tempotron (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b45">Gütig &amp; Sompolinsky, 2006</a>), a neuron with supervised learning ability, can learn to separate two different classes. Tempotron learns to spike in response to ‘+’ patterns and to be silent in response to ‘−’ (there are two classes, and the two classes are shown by ‘+’ and ‘−’). During training, if no <a href="https://www.sciencedirect.com/topics/engineering/output-spike" title="Learn more about output spike from ScienceDirect's AI-generated Topic Pages">output spike</a> was elicited in response to a ‘+’ pattern, each <a href="https://www.sciencedirect.com/topics/computer-science/synaptic-efficacy" title="Learn more about synaptic efficacy from ScienceDirect's AI-generated Topic Pages">synaptic efficacy</a> is increased. Conversely, if an output spike appears in response to a ‘−’ pattern the synaptic efficacies are decreased. Although this model is considered to be biologically plausible, its processing ability is restricted to <a href="https://www.sciencedirect.com/topics/computer-science/binary-classification" title="Learn more about binary classification from ScienceDirect's AI-generated Topic Pages">binary classification</a>. Furthermore, the scalability aspect was not discussed and it is not clear how this method can be used to <a href="https://www.sciencedirect.com/topics/mathematics/real-world-process" title="Learn more about process real world from ScienceDirect's AI-generated Topic Pages">process real world</a> datasets.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b156">Yu et al., 2012</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b158">Yu et al., 2013b</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b159">Yu, Tang, Tan, and Yu (2014)</a> used Tempotron for pattern recognition and a latency code is used to encode input patterns. A three layer structure including encoding neurons, tempotron and a readout layer, is used. However, it has only a single learning layer in which each neuron is trained based on Tempotron. Each neuron can only fire once within the encoding window.</p>
<p>The learning algorithms mentioned in this section can train neurons to fire only a single spike in response to a set of inputs within a simulation time window. Some of them can learn spatiotemporal input patterns with multiple input spikes per input synapse.</p>
<h3 id="32-learning-multiple-spikes-in-a-single-neuron-or-a-single-layer-of-neurons">3.2. Learning multiple spikes in a single neuron or a single layer of neurons<a class="headerlink" href="#32-learning-multiple-spikes-in-a-single-neuron-or-a-single-layer-of-neurons" title="Permanent link">&para;</a></h3>
<p>Multiple spikes significantly increase the richness of the neural information representation (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b15">Borst and Theunissen, 1999</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b113">Ponulak and Kasiński, 2010</a>). Single-spike coding schemes limit the diversity and capacity of information transmission in a network of spiking neurons (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b155">Xu, Zeng, et al., 2013</a>). Moreover, training a neuron to fire multiple spikes is more biologically plausible compared to a single-spike learning scheme (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea and Grüning, 2013</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang, 2013</a>). Temporal encoding through multiple spikes transfers important information in biological neural assemblies and the information cannot be expressed by a single spike coding scheme or a rate coding scheme. Although the exact mechanisms of biological coding schemes in the brain are not well understood, the biological evidence shows that multiple spikes coding schemes have a <a href="https://www.sciencedirect.com/topics/engineering/pivotal-role" title="Learn more about pivotal role from ScienceDirect's AI-generated Topic Pages">pivotal role</a> in the brain. For example, in the neuronal circuits of zebra fish brain, spatiotemporal spiking sensory inputs composed of spike trains are mapped to precise timing of spikes to execute well-timed motor sequences (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b92">Memmesheimer, Rubin, Ölveczky, &amp; Sompolinsky, 2014</a>). This biological evidence has motivated the development of learning algorithms for spiking neurons to fire multiple spikes with precise timings (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, &amp; Wang, 2013</a>).</p>
<p>Supervised Hebbian learning rules were used in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b81">Legenstein, Naeger, and Maass (2005)</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b117">Ruf and Schmitt (1997)</a> for learning temporal patterns. Although this approach has interesting properties such as locality, scalability and the ability of on-line processing, the authors indicated that convergence cannot be guaranteed in a general case. Another problem with all supervised Hebbian methods is continuous <a href="https://www.sciencedirect.com/topics/computer-science/synaptic-change" title="Learn more about synaptic change from ScienceDirect's AI-generated Topic Pages">synaptic change</a> even if the neuron fires exactly at the right times.</p>
<p>Statistical methods optimize the weights in order to maximize the likelihood of getting postsynaptic spikes at the desired times (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b110">Pfister, Toyoizumi, Barber, &amp; Gerstner, 2006</a>). However, it is difficult to learn complex spike trains using statistical methods. The proposed method in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b110">Pfister et al. (2006)</a> has not been applied to <a href="https://www.sciencedirect.com/topics/mathematics/real-world-data" title="Learn more about real world data from ScienceDirect's AI-generated Topic Pages">real world data</a> processing.</p>
<p>ReSuMe (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b112">Ponulak, 2005</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b113">Ponulak and Kasiński, 2010</a>) is another supervised learning algorithm that is based on a combination of <a href="https://www.sciencedirect.com/topics/mathematics/spike-timing-dependent-plasticity" title="Learn more about STDP from ScienceDirect's AI-generated Topic Pages">STDP</a> and anti-STDP learning windows to produce multiple desired output spikes. It is a biologically plausible supervised learning algorithm that is designed to produce a desirable output spike train in response to a spatiotemporal input spike pattern. The input and output spike sequences are generated randomly and the LIF neuron model is used in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b112">Ponulak (2005)</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b113">Ponulak and Kasiński (2010)</a>, however the learning algorithm does not depend on the model of spiking neuron. ReSuMe is based on the Widrow-Hoff rule which comes from a simple derivation of an error function in classical neural networks. The STDP is driven by using a remote teacher spike train to enhance appropriate synaptic weights to force the neuron to fire at desired times. Using remote supervised teacher spikes enables ReSuMe to overcome the silent neuron problem existing in the gradient based learning methods such as SpikeProp. The ability of on-line processing and locality are two remarkable properties of ReSuMe. The capabilities of ReSuMe have inspired research into new learning algorithms (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b30">Florian, 2012</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b41">Glackin et al., 2011</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b96">Mohemmed et al., 2013</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea and Grüning, 2013</a>). ReSuMe works based on weight adjustment, and it is a well-known learning method.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b113">Ponulak and Kasiński (2010)</a> proposed ReSuMe to adjust the synaptic weights of a neuron to generate a desired spike train, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, in response to a spatiotemporal input spike pattern <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;S&lt;/mi&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;=&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;[&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo is="true"&gt;;&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo is="true"&gt;;&lt;/mo&gt;&lt;mi is="true"&gt;…&lt;/mi&gt;&lt;mo is="true"&gt;;&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo is="true"&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. ReSuMe weight adjustment is based on the input, actual output and desired output spike trains. Ponulak incorporated precise <a href="https://www.sciencedirect.com/topics/engineering/spike-time" title="Learn more about spike times from ScienceDirect's AI-generated Topic Pages">spike times</a> in the Widrow-Hoff rule and employed STDP and anti-STDP windows to adjust synaptic weights and enable supervised learning according to <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#fd3.4">(3.4)</a>. (3.4)<span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mfrac is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo linebreak="goodbreak" is="true"&gt;=&lt;/mo&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;[&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;o&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo is="true"&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;[&lt;/mo&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mo linebreak="badbreak" is="true"&gt;∫&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo linebreak="badbreak" is="true"&gt;+&lt;/mo&gt;&lt;mi is="true"&gt;∞&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;mo is="true"&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>where, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, is the weight of the <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/math&gt;\)</span>th synapse at time <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/math&gt;\)</span>. The constant <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;/math&gt;\)</span> is a non-Hebbian term. If the number of spikes in the actual output spike train, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;o&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, is more (less) than the number of spikes in the desired spike train, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, then the non-Hebbian term decreases (increases) the weights. This term speeds up the learning procedure. <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is the learning window and like STDP it has an exponential function that decays with a time constant. (3.5)<span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo linebreak="goodbreak" is="true"&gt;=&lt;/mo&gt;&lt;mfenced open="{" close="" is="true"&gt;&lt;mrow is="true"&gt;&lt;mtable align="axis" equalrows="false" columnlines="none" equalcolumns="false" class="array" is="true"&gt;&lt;mtr is="true"&gt;&lt;mtd class="array" columnalign="left" is="true"&gt;&lt;mi is="true"&gt;A&lt;/mi&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo linebreak="badbreak" is="true"&gt;−&lt;/mo&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;mo is="true"&gt;∕&lt;/mo&gt;&lt;mi is="true"&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mspace width="1em" is="true"&gt;&lt;/mspace&gt;&lt;/mtd&gt;&lt;mtd class="array" columnalign="left" is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mi is="true"&gt;o&lt;/mi&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;mspace width="0.16667em" is="true"&gt;&lt;/mspace&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;mo is="true"&gt;≥&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr is="true"&gt;&lt;mtd class="array" columnalign="left" is="true"&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;mspace width="1em" is="true"&gt;&lt;/mspace&gt;&lt;/mtd&gt;&lt;mtd class="array" columnalign="left" is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mi is="true"&gt;o&lt;/mi&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;mspace width="1em" class="nbsp" is="true"&gt;&lt;/mspace&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;mo is="true"&gt;&lt;&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>Where <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;τ&lt;/mi&gt;&lt;/math&gt;\)</span> is the exponential function decay time constant. The term <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;A&lt;/mi&gt;&lt;/math&gt;\)</span> represents the amplitude of <a href="https://www.sciencedirect.com/topics/mathematics/long-term-potentiation" title="Learn more about long term potentiation from ScienceDirect's AI-generated Topic Pages">long term potentiation</a>. The term <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;∫&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;+&lt;/mo&gt;&lt;mi is="true"&gt;∞&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> represents the <a href="https://www.sciencedirect.com/topics/mathematics/convolution" title="Learn more about convolution from ScienceDirect's AI-generated Topic Pages">convolution</a> of the learning window, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;T&lt;/mi&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, and the <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/math&gt;\)</span>th input spike train. As discussed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b113">Ponulak and Kasiński (2010)</a> the weight <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> is increased at the instance of spikes in the desired spike train, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, if its input contains a spike shortly before a desired spike, whereas it is decreased if its input contains a spike shortly before an actual output spike. When the actual spike train approaches the desired spike train the weight increments and decrements compensate each other and the weights become stable. In ReSuMe the synapses do not have delays.</p>
<p>The Spike Pattern Association Neuron (SPAN) learning algorithm (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b95">Mohemmed et al., 2012</a>) is similar to ReSuMe, in that it combines STDP and anti-STDP processes and is also derived from the Widrow-Hoff rule. The novelty of this algorithm is that it transforms spike trains into <a href="https://www.sciencedirect.com/topics/engineering/analog-signal" title="Learn more about analogue signals from ScienceDirect's AI-generated Topic Pages">analogue signals</a> such that common <a href="https://www.sciencedirect.com/topics/engineering/mathematical-operation" title="Learn more about mathematical operations from ScienceDirect's AI-generated Topic Pages">mathematical operations</a> can be performed on them. SPAN can learn multiple desired spikes and can only train a single neuron (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#fig5">Fig. 5</a>). Despite SPAN’s similarity to ReSuMe, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b95">Mohemmed et al. (2012)</a> did not compare its performance with ReSuMe. In <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b96">Mohemmed et al. (2013)</a> SPAN is extended to train a SNN consisting of multiple spiking neurons to perform a classification task. In the method the neurons construct a single layer of spiking neuron. Although SPAN can train a neuron to fire multiple spikes, only a single desired spike is trained in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b96">Mohemmed et al. (2013)</a> for spatiotemporal patterns corresponding to a class.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b30">Florian (2012)</a> proposed a learning algorithm to train a neuron to fire a desired spike train in response to a spatiotemporal input pattern. The algorithm is called Chronotron (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b30">Florian, 2012</a>). Chronotron has two versions: I-learning with high biological <a href="https://www.sciencedirect.com/topics/mathematics/plausibility" title="Learn more about plausibility from ScienceDirect's AI-generated Topic Pages">plausibility</a> and E-learning with high memory capacity and high computational cost. Florian et al. used a SRM for the neuron and a <a href="https://www.sciencedirect.com/topics/computer-science/kernel-function" title="Learn more about kernel function from ScienceDirect's AI-generated Topic Pages">kernel function</a> as a spike response. The total normalized PSP of a synapse <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/math&gt;\)</span>, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>, is defined as the summation of all the kernel functions related to past presynaptic spikes in the synapse <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/math&gt;\)</span> until <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/math&gt;\)</span>. <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is called a normalized PSP because the effect of the weights is removed in the PSP and it only reflects the presynaptic spikes effect (number of input spikes and the time interval of the spikes). <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is used to construct a <a href="https://www.sciencedirect.com/topics/computer-science/graphical-illustration" title="Learn more about graphical illustration from ScienceDirect's AI-generated Topic Pages">graphical illustration</a> of the chronotron. This graphical illustration gives a good overview of the spike learning problem, so that it can be used to investigate the effect of the various parameters on the learning procedure. In E-learning, an error function, <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;/math&gt;\)</span>, is calculated according to a desired spike train and the actual output of the neuron. The error function has a factor associated with the insertion, deletion and shift of the actual spikes toward the desired ones. The error is used to estimate the weight change, <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;=&lt;/mo&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;−&lt;/mo&gt;&lt;mfrac is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;∂&lt;/mi&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;∂&lt;/mi&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. After further simplification an approximate mathematical formulation for <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is obtained. <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> is influenced by the synapse normalized PSP at the time of some actual output spikes that should be removed and some desired spikes that should be inserted and also it is influenced by the time shift that is necessary to shift the other actual output spikes towards the corresponding desired spikes. During the calculation of <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;Δ&lt;/mi&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> some simplifications are applied; however the impact of such an approach is not justified (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b30">Florian, 2012</a>). The intrinsic complexity and discontinuity of the dynamics of the spiking neuron implies that the traditional <a href="https://www.sciencedirect.com/topics/engineering/gradient-descent-method" title="Learn more about gradient descent method from ScienceDirect's AI-generated Topic Pages">gradient descent method</a> cannot easily be used whereas some <a href="https://www.sciencedirect.com/topics/computer-science/heuristic-methods" title="Learn more about heuristic methods from ScienceDirect's AI-generated Topic Pages">heuristic methods</a> inspired by the biological evidence should be employed.</p>
<p>The other version of the chronotron, I-learning, is inspired by ReSuMe and a weight is increased at the desired spike instant and it is decreased at the time of output spikes. In ReSuMe a weight change is managed by a decaying exponential function which is associated with the usual STDP learning window. However, I-learning is managed by the difference between two exponentials similar to the <span class="arithmatex">\(&lt;math&gt;&lt;mo is="true"&gt;∝&lt;/mo&gt;&lt;/math&gt;\)</span> kernel with two time constants. One of the time constants is associated with the increasing part of the function and the other related to the decaying part. Chronotron does not have the non-Hebbian constant which is used in ReSuMe to speed up the learning procedure. Chronotron is a learning algorithm for a single neuron and it can train the neuron to fire multiple spikes at desired times in response to a corresponding spatiotemporal input pattern. Learning algorithms at the level of a neuron have been the subject of considerable recent research such as Tempoteron (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b45">Gütig &amp; Sompolinsky, 2006</a>), ReSuMe (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b112">Ponulak, 2005</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b113">Ponulak and Kasiński, 2010</a>) and SPAN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b95">Mohemmed et al., 2012</a>). PSD (Precise-Spike-Driven Synaptic Plasticity) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b157">Yu, Tang, Tan, &amp; Li, 2013a</a>) is another example of learning methods that can train a single neuron to fire multiple desired spikes.</p>
<p><img alt="" src="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr5.jpg" /></p>
<ol>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr5_lrg.jpg" title="Download high-res image (244KB)">Download : Download high-res image (244KB)</a></li>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr5.jpg" title="Download full-size image">Download : Download full-size image</a></li>
</ol>
<p>Fig. 5. SPAN trains a <a href="https://www.sciencedirect.com/topics/engineering/spiking-neuron" title="Learn more about spiking neuron from ScienceDirect's AI-generated Topic Pages">spiking neuron</a> to map a spatio-temporal input pattern to an output <a href="https://www.sciencedirect.com/topics/engineering/spike-train" title="Learn more about spike train from ScienceDirect's AI-generated Topic Pages">spike train</a> composed of a number of desired spikes. (b) The training effect on the development of the <a href="https://www.sciencedirect.com/topics/engineering/output-spike" title="Learn more about output spikes from ScienceDirect's AI-generated Topic Pages">output spikes</a> at desired times during different learning trials. (c) The reduction of the error between the actual output spike train and the desired spike train in addition to its standard deviation.</p>
<p>The figure is adapted from <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b95">Mohemmed et al. (2012)</a></p>
<p>The synaptic weight association training (SWAT) algorithm proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b146">Wade, McDaid, Santos, and Sayers (2010)</a> merges STDP and the Bienenstock–Cooper–Munro (BCM) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b63">Jedlicka, 2002</a>) learning rule to train neurons in a single layer of spiking neurons. In the BCM model, synaptic plasticity depends on the activity of the corresponding <a href="https://www.sciencedirect.com/topics/engineering/postsynaptic-neuron" title="Learn more about postsynaptic neuron from ScienceDirect's AI-generated Topic Pages">postsynaptic neuron</a>. It potentiates a synaptic weight if the firing rate of the post synaptic neuron is higher than a threshold level, and it depresses the synaptic weight if the postsynaptic neuron firing rate is less than the threshold level. In SWAT, BCM is used to modulate the height of an STDP learning window to stabilize the weight adjustment governed by STDP. While STDP and BCM are used to train the output layer, the hidden layer in SWAT is used as a frequency filter to extract features from input patterns. The method only can use rate coding in the input and output patterns.</p>
<p>DL-ReSuMe (A Delay Learning-Based Remote Supervised Method for Spiking Neurons) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b132">Taherkhani, Belatreche, Li, &amp; Maguire, 2015b</a>) integrates weight modulation with the synaptic delay shift to map a random spatiotemporal input spike pattern into a random desired output spike train in a supervised way. DL-ReSuMe can achieve up to 10% higher accuracy than ReSuMe and BPSL (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b130">Taherkhani, Belatreche, Li, &amp; Maguire, 2014</a>) (A biologically plausible supervised learning method for spiking neurons) at a much faster learning speed. DL-ReSuMe (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b132">Taherkhani et al., 2015b</a>) can learn input spike trains of shorter duration and smaller mean frequency with a higher accuracy and much faster learning speed than ReSuMe. One interesting feature of DL-ReSuMe method is the ability to solve the silent window problem in a spatiotemporal input pattern.</p>
<p>DL-ReSuMe (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b132">Taherkhani et al., 2015b</a>) adjusts each delay only once during learning, and after the change it stays fixed and its corresponding synaptic weight is continually changed in subsequent epochs to train the neuron to fire at desired times. However, it is possible that the single delay adjustment does not set the delay at an appropriate value and it needs more tuning. Specially, when multiple desired spikes are learnt by a neuron, the effect of delay adjustment on different desired spikes should be considered to train the delays precisely. Additionally, multiple changes of the weights alter the previous situation and it is more likely that the first adjustment of a delay which is appropriate for the previous weights is not suitable for the new configuration of the weights. So, the delays also should be updated based on the weight updates. EDL, an Extended Delay Learning based remote supervised method for spiking neurons (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b131">Taherkhani, Belatreche, Li, &amp; Maguire, 2015a</a>), was proposed to solve this problem of DL-ReSuMe by introducing multiple delay adjustments. The main property of EDL is its regular multiple adjustment of delay and weights. Irregular multiple adjustments of delays cause distraction not only in the delay training, but also in weight adjustments, and consequently it reduces the performance of the learning. Moreover, in EDL, the multiple delay adjustments become stable when the neuron reaches its goal.</p>
<p>The Liquid State Machine (LSM) provides an approach that consists of a dynamic filter. The dynamic filter is constructed by a <a href="https://www.sciencedirect.com/topics/engineering/recurrent" title="Learn more about recurrent from ScienceDirect's AI-generated Topic Pages">recurrent</a> SNN called reservoir. It maps input spike trains to an internal dynamic state which nonlinearly depend on current and previous inputs. The output of the LSM is fed to a readout layer (a simple classifier) which is trained to classify the internal dynamic state streams (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b86">Maass, Natschläger, &amp; Markram, 2004</a>). LSM can capture temporal information, so it can have promising results on the applications that deal with temporal information, i.e. the application that the exact sequence of the input occurring in time is important in addition to the value of the inputs (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b144">Verstraeten, Schrauwen, Stroobandt, &amp; Van Campenhout, 2005</a>). Because, the recurrent connections in the LSM give a short-term memory effect through the different loops generated in the <a href="https://www.sciencedirect.com/topics/computer-science/recurrent-network" title="Learn more about recurrent network from ScienceDirect's AI-generated Topic Pages">recurrent network</a> and it helps LSM to process temporal information in which the history of input is important (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b144">Verstraeten et al., 2005</a>). Speech recognition, robot control, object tracking and EEG recognition are examples of tasks that are intrinsically temporal. Speech recognition of isolated digits (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b144">Verstraeten et al., 2005</a>), real-time speech recognition (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b119">Schrauwen, D’Haene, Verstraeten, &amp; Campenhout, 2008</a>), and robotics (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b65">Joshi &amp; Maass, 2004</a>) are examples of applications that use LSM. LSM has comparable performance to state-of-the-art artificial intelligent systems on the mentioned tasks (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b66">Ju, Xu, Chong, &amp; VanDongen, 2013</a>). The mentioned learning algorithm for single neurons in Section <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#sec3.2">3.2</a> can be used as readout for a LSM.</p>
<p>The LSM proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b86">Maass et al. (2004)</a> has a fixed topology and fixed connection weights. Its biological plausibility and processing performance could be improved by employing other biologically plausible topologies, and other synaptic plasticity approaches such as short-term plasticity. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b48">Hazan and Manevitz (2012)</a> implemented an LSM with various topologies to investigate the effect of the topological constraints on the LSM’s robustness. Effects of the LIF and the Izhkevich neuron model (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b60">Izhikevich, 2003</a>) are also investigated and shown to be qualitatively similar. Different types of read out methods such as Widrow &amp; Hoff, Back-Propagation, SVM and Tempotron (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b45">Gütig &amp; Sompolinsky, 2006</a>) are used in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b86">Maass et al. (2004)</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b107">Paugam-Moisy et al. (2008)</a> proposed a multi-timescale learning rule for SNNs where the reservoir has unsupervised synaptic plasticity driven by STDP and <a href="https://www.sciencedirect.com/topics/neuroscience/nerve-conduction" title="Learn more about axonal conduction from ScienceDirect's AI-generated Topic Pages">axonal conduction</a> delays. Polychronous spiking patterns emerge inside the reservoir. A polychromous group is a specific group of fired spikes by neurons inside the reservoir. The spikes in a polychronous group are not synchronous, i.e. they are not fired in same time, however they have a time-locked firing pattern, i.e. there are a specific time intervals between the spikes. It is believed that in a biologically plausible SNN, where the synaptic plasticity of the network is governed by STDP and there are conduction delays between neurons, polychromous groups appear (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b62">Izhikevich, 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b107">Paugam-Moisy et al., 2008</a>). In <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b107">Paugam-Moisy et al. (2008)</a> it is supposed that applying input patterns from the same class can activate a specific polychronous group, however the polychronous group is not activated when patterns from other classes are applied. They proposed a supervised learning algorithm to adjust the delays related to readout neurons to learn firing patterns of different polychronous groups related to different classes. The SNN uses biologically plausible elements such as STDP synaptic plasticity and axonal conduction delays, and it is also used to classify a large dataset (US Postal Service: USPS, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b46">Hastie, Tibshirani, &amp; Friedman, 2001</a>). A layer of spiking neurons is used to classify an input pattern in a supervised manner. Only the delays of the readout neurons are adjusted during the supervised learning. The delays are adjusted to force the neuron related to the target class to fire before the neuron related to the other class. The learning method tries to maximize the time difference between the negative class and positive class during the classification of two classes. It has relatively poor performance in comparison with traditional classification methods. The synaptic weights of the readout neurons are not changed. The neurons in LSM can fire multiple spikes; however the readout neuron can fire a single spike. Using a readout that can fire multiple spikes can increase the biological plausibility of the method.</p>
<p>Although the mentioned supervised learning methods can train multiple output spikes, they work based on a single training neuron or single-layer of neurons. It is difficult to design a multilayer network of spiking neurons with supervised learning ability to fire multiple desired spikes, because the complexity of the learning task is increased by increasing the number of spikes and layers (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b39">Ghosh-Dastidar and Adeli, 2009</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang, 2013</a>). In this situation the learning algorithm should control various neurons to generate different desired spikes. However, a real biological system is composed of a large number of <a href="https://www.sciencedirect.com/topics/computer-science/interconnected-neuron" title="Learn more about interconnected neurons from ScienceDirect's AI-generated Topic Pages">interconnected neurons</a> (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b39">Ghosh-Dastidar and Adeli, 2009</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea and Grüning, 2013</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang, 2013</a>).</p>
<p><img alt="" src="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr6.jpg" /></p>
<ol>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr6_lrg.jpg" title="Download high-res image (306KB)">Download : Download high-res image (306KB)</a></li>
<li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0893608019303181-gr6.jpg" title="Download full-size image">Download : Download full-size image</a></li>
</ol>
<p>Fig. 6. (a) The structure of the multilayer <a href="https://www.sciencedirect.com/topics/neuroscience/neural-networks" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages">neural network</a> proposed by <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang (2013)</a> to train multiple spikes (b) In the network two neurons are connected by multiple sub connection with different delays (c) each neuron in the hidden layer or output layer can fire multiple spikes .</p>
<p>The figure is adapted from <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang (2013)</a></p>
<h3 id="33-learning-multiple-spikes-in-a-multilayer-spiking-neural-network">3.3. Learning multiple spikes in a multilayer spiking neural network<a class="headerlink" href="#33-learning-multiple-spikes-in-a-multilayer-spiking-neural-network" title="Permanent link">&para;</a></h3>
<p>A multilayer neural network has higher information processing ability than a single layer of neurons. It has been shown that a <a href="https://www.sciencedirect.com/topics/neuroscience/perceptron" title="Learn more about perceptron from ScienceDirect's AI-generated Topic Pages">perceptron</a> multilayer neural network has a higher processing ability than a single layer of neurons. A single layer perceptron is limited to the classification of linearly separable patterns. However, a multilayer <a href="https://www.sciencedirect.com/topics/engineering/feedforward" title="Learn more about feedforward from ScienceDirect's AI-generated Topic Pages">feedforward</a> perceptron neural network can overcome the limitation of the single layer network (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b47">Haykin, 2009</a>). The higher processing ability of a multilayer <a href="https://www.sciencedirect.com/topics/mathematics/neuronal-network" title="Learn more about neuronal network from ScienceDirect's AI-generated Topic Pages">neuronal network</a> is not only proven in the classical neuronal network, but is also confirmed in the spiking neural network. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea and Grüning (2013)</a> have shown that a multilayer spiking neural network can perform a nonlinear separable logical operation, i.e. <a href="https://www.sciencedirect.com/topics/mathematics/xor" title="Learn more about XOR from ScienceDirect's AI-generated Topic Pages">XOR</a>, however the task cannot be accomplished without the <a href="https://www.sciencedirect.com/topics/engineering/hidden-layer-neuron" title="Learn more about hidden layer neurons from ScienceDirect's AI-generated Topic Pages">hidden layer neurons</a>. So single neurons or single-layer of neurons cannot simulate the learning of a biological neural network with a high processing ability, and designing a learning algorithm for a multilayer network of spiking neuron with the ability of firing multiple spikes is essentially required (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b39">Ghosh-Dastidar and Adeli, 2009</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea and Grüning, 2013</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang, 2013</a>).</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b12">Bichler, Querlioz, Thorpe, Bourgoin, and Gamrat (2012)</a> introduced a two-layer network for spiking neuron capable of extraction of temporally overlapping features directly from unfiltered Address-Event Representation (AER) silicon retina data, using only a simple, local STDP rule and 10 parameters in all for the neurons (LIF neuron is used in this method). Although the proposed learning algorithm is unsupervised, its 10 parameters are optimized through a supervised manner. A simplified form of STDP is used in the method. In the special form of STDP used in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b12">Bichler et al. (2012)</a>, during <a href="https://www.sciencedirect.com/topics/computer-science/term-depression" title="Learn more about LTD from ScienceDirect's AI-generated Topic Pages">LTD</a> all the input weights are reduced by a constant value, regardless of the time difference between presynaptic spike and post synaptic spike times, i.e. <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;p&lt;/mi&gt;&lt;mi is="true"&gt;o&lt;/mi&gt;&lt;mi is="true"&gt;s&lt;/mi&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;−&lt;/mo&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;p&lt;/mi&gt;&lt;mi is="true"&gt;r&lt;/mi&gt;&lt;mi is="true"&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span>. However, in a biologically plausible form of STDP, the amplitude of a weight adjustment depends exponentially on the time difference. Additionally, the weight adjustment of the neurons in the hidden layer is independent of the activity of the output neurons.</p>
<p>The <a href="https://www.sciencedirect.com/topics/engineering/online-learning-algorithm" title="Learn more about online learning algorithm from ScienceDirect's AI-generated Topic Pages">online learning algorithm</a> for SNNs proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b147">Wang et al. (2014)</a> is used to classify real valued input data. It is composed of three layers. In the input layer the real valued data are encoded to the precise timing of spikes through population coding scheme. The structure of the hidden layer is changed dynamically by using a <a href="https://www.sciencedirect.com/topics/engineering/clustering-algorithm" title="Learn more about clustering algorithm from ScienceDirect's AI-generated Topic Pages">clustering algorithm</a>. STDP and anti-STDP are used to train neurons in the output layer. The method (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b147">Wang et al., 2014</a>) uses latency coding in the first layer and the time to the first spike in the hidden layer, and it uses rating coding in the output layer to associate the applied input pattern to the class of the neuron that fire more spikes. Additionally, the training in the hidden and output layers operate independently. Consequently, it is hard for the network to find appropriate interaction between the activity of the different layers. Moreover, it is not explained whether adding and removing neurons to the hidden layer is a property of the biological neural network or this characteristic is only added to the <a href="https://www.sciencedirect.com/topics/engineering/artificial-neural-network" title="Learn more about artificial neural network from ScienceDirect's AI-generated Topic Pages">artificial neural network</a> to improve its performance.</p>
<p>In the previous <a href="https://www.sciencedirect.com/topics/engineering/gradient-descent" title="Learn more about gradient descent from ScienceDirect's AI-generated Topic Pages">gradient descent</a> learning algorithms for network of spiking neurons like SpikeProp, QuickProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b91">McKennoch et al., 2006</a>) and RProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b38">Ghosh-Dastidar &amp; Adeli, 2007</a>), each neuron in the input, hidden and output layers can only fire a single spike. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b14">Booij and tat Nguyen (2005)</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b39">Ghosh-Dastidar and Adeli (2009)</a> extended the multilayer SpikeProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b13">Bohte et al., 2002</a>) to allow each neuron in the input and hidden layers to fire multiple spikes. However, each output neuron can fire only a single spike. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang (2013)</a> proposed the first supervised learning method based on the <a href="https://www.sciencedirect.com/topics/computer-science/classical-error" title="Learn more about classical error from ScienceDirect's AI-generated Topic Pages">classical error</a> backpropagation method that can train a multilayer network of spiking neurons to fire multiple spikes (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#fig6">Fig. 6</a>). In this supervised learning method all the neurons in the input, hidden and output layers can fire multiple spikes. The network has multiple neurons at the output layer and each of the neurons can learn to fire their desired spike trains.</p>
<p>SpikeProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b13">Bohte et al., 2002</a>), QuickProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b91">McKennoch et al., 2006</a>), RProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b38">Ghosh-Dastidar &amp; Adeli, 2007</a>), and the methods proposed by <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b14">Booij and tat Nguyen (2005)</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b39">Ghosh-Dastidar and Adeli (2009)</a> adjust synaptic weights of a SNN to train each output neuron to fire at a desired time. In the methods, the sum of the square error for output neurons is considered as the error function. i.e. <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;mo is="true"&gt;.&lt;/mo&gt;&lt;mn is="true"&gt;5&lt;/mn&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;∑&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;mo is="true"&gt;∈&lt;/mo&gt;&lt;mi is="true"&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> where <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/math&gt;\)</span> is the index of the <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/math&gt;\)</span>th output neuron from the output neuron set, J. <span class="arithmatex">\(&lt;math&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;a&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/math&gt;\)</span> are the firing times of the first actual output spike and the desired spike of the <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/math&gt;\)</span>th output neuron, respectively.</p>
<p>The difficulty of designing an algorithm which trains multiple spikes in the output neurons is summarized as follows: there is not a constant number of actual output spikes, especially when working with the multilayer neural network with multiple spikes for each neuron. In this situation it is difficult to construct an error function. Secondly, when the number of spikes is increased as a result of increasing the number of neurons, the interfering effect of the various desired spikes on the weight changes is also increased and consequently the learning becomes difficult. When the weights are adjusted to train a neuron to learn a spike, this adjustment affects the neuron that has been learnt from other desired spikes. This effect can be referred to the interfering effect. So, when the number of the spikes is increased, the limited resources (learning parameters) should be adjusted in response to many demands (desired spikes). In other words, different desired spikes may lead to adjustment of learning parameters in opposite directions and finding an optimum value to satisfy the different situations is difficult. In order to overcome the first problem, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang (2013)</a> supposed that the number of actual spikes and the number of desired spikes are identical, i.e. they are equal to <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> for <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/math&gt;\)</span>th output neuron, and they calculate the error function theoretically according to this assumption. They use <span class="arithmatex">\(&lt;math&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;E&lt;/mi&gt;&lt;mo linebreak="goodbreak" linebreakstyle="after" is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;0&lt;/mn&gt;&lt;mo is="true"&gt;.&lt;/mo&gt;&lt;mn is="true"&gt;5&lt;/mn&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;∑&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;∑&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;mo is="true"&gt;=&lt;/mo&gt;&lt;mn is="true"&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;msup is="true"&gt;&lt;mrow is="true"&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;(&lt;/mo&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo is="true"&gt;−&lt;/mo&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mover accent="true" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;ˆ&lt;/mo&gt;&lt;/mrow&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo is="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mn is="true"&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;\)</span> as the error function to calculate the square error of the output neurons, where <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;N&lt;/mi&gt;&lt;/math&gt;\)</span> is the number of output neurons in the output layer, and <span class="arithmatex">\(&lt;math&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/math&gt;\)</span> and <span class="arithmatex">\(&lt;math&gt;&lt;msubsup is="true"&gt;&lt;mrow is="true"&gt;&lt;mover accent="true" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mo is="true"&gt;ˆ&lt;/mo&gt;&lt;/mrow&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mfenced open="(" close=")" is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/math&gt;\)</span> are the firing times of the <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;f&lt;/mi&gt;&lt;/math&gt;\)</span>th actual output spike and desired spike of the <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;j&lt;/mi&gt;&lt;/math&gt;\)</span>th output neuron, respectively. Then during the experiment, if the number of spikes in the actual spike train is not equal to the number of spikes in the desired spike train, they consider all the spikes in the spike train that has the lower number of spikes, say <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span>, and they only consider the first <span class="arithmatex">\(&lt;math&gt;&lt;msub is="true"&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow is="true"&gt;&lt;mi is="true"&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\)</span> spikes of the longer spike train. It does not seem to be an effective method to solve the first difficulty, because the learning method might adjust the network learning parameters for a desired spike which already has an actual output spike at the desired time. Consider a situation that the number of actual output spikes is higher than the number of desired spikes and there is a high number of early actual output spikes. The high number of the early actual output spikes causes that the method associates an early undesired actual spike to a far desired spike and overlook the actual spike that already is at the desired time. Finding a method that works based on local time event or temporal event can improve the method. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang (2013)</a> believe that their learning algorithm overcomes the second problem by using the principle of “the Bigger PSP, the Bigger Adjustment (BPBA)” in their learning. According to the BPBA principle at the instant of weight adjustment, <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/math&gt;\)</span>, the changes of each synapse are in proportion to the height of the PSP produced in the synapse at the time <span class="arithmatex">\(&lt;math&gt;&lt;mi is="true"&gt;t&lt;/mi&gt;&lt;/math&gt;\)</span>. This is equivalent to what happens during STDP. In STDP the presynaptic spike near to the post synaptic spike has a big PSP and their synaptic weights also are changed by a big value. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang (2013)</a> defined an error function based on the time difference between actual output spike trains and desired spike trains. Then the derivation of the error function with respect to the synaptic weights is calculated, and a learning rule with a large mathematical equation for changing the synaptic weights is extracted. The constructive effect of one of the mathematical elements is described according to BPBA principle; they do not investigate the effect of other mathematical elements in the learning rule. It is not clear what happens if some of the mathematical elements in the learning rule are simplified, similar to what is done in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b45">Gütig and Sompolinsky (2006)</a> for the chronotron. The simulation results in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang (2013)</a> have shown that ReSuMe is more efficient and accurate than the proposed method of Xu et al. for simulation times that are less than 600 ms. However the algorithm can be used to train a multilayer neural network. If ReSuMe is improved for learning a multilayer SNN, it might have good accuracy and efficiency compared to the method that is proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang (2013)</a>.</p>
<p>Gradient based methods suffer from various problems. A sudden jump in the network training error, called surge, is considered as one of the major problems, and it can cause failure in learning (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b124">Shrestha &amp; Song, 2015</a>). The non-monotonic and <a href="https://www.sciencedirect.com/topics/engineering/nonlinear-behavior" title="Learn more about nonlinear behaviour from ScienceDirect's AI-generated Topic Pages">nonlinear behaviour</a> of the <a href="https://www.sciencedirect.com/topics/neuroscience/nerve-cell-membrane" title="Learn more about neuron membrane from ScienceDirect's AI-generated Topic Pages">neuron membrane</a> potential make it difficult to minimize the constructed error and consequently leads to these surges during training (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b123">Shrestha and Song, 2013</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b124">Shrestha and Song, 2015</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b136">Takase et al., 2009</a>). The problem increases when the output neurons are trained to fire more than a single spike. Additionally, construction of an error function becomes difficult, because, the number of actual output spikes is not usually the same as the number of desired spikes during learning (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, &amp; Wang, 2013</a>). Although, training multiple spikes is difficult, it is a biologically plausible coding scheme (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea and Grüning, 2013</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang, 2013</a>) and it can convey more neural information (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b15">Borst and Theunissen, 1999</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b113">Ponulak and Kasiński, 2010</a>). This implies that the decoding scheme has an important effect on the learning tasks. After investigation of <a href="https://www.sciencedirect.com/topics/mathematics/gradient-based-method" title="Learn more about gradient based methods from ScienceDirect's AI-generated Topic Pages">gradient based methods</a> and their application in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b1">Adeli and Hung, 1994</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b38">Ghosh-Dastidar and Adeli, 2007</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b56">Hung and Adeli, 1993</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b57">Hung and Adeli, 1994</a>, it is concluded that the application of STDP which works based on local events is worth further investigation to design learning algorithms for a multilayer network of spiking neurons (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b39">Ghosh-Dastidar &amp; Adeli, 2009</a>).</p>
<p>STDP and anti-STDP were used in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea and Grüning (2013)</a> as the first biologically plausible supervised learning algorithm for classification of real world data using a multilayer spiking neural network. Each neuron in the input, hidden and output layers of the network can fire multiple spikes. In the learning algorithm, the output spikes produced by the <a href="https://www.sciencedirect.com/topics/engineering/hidden-neuron" title="Learn more about hidden neurons from ScienceDirect's AI-generated Topic Pages">hidden neurons</a> are not considered during training of the learning parameter of the hidden neurons. In other words the output spikes of the hidden neuron are not considered during the STDP and anti-STDP related to the hidden neuron input weight adjustments. However, in a biological neuron, the pre and post synaptic spikes of a neuron are usually used in STDP. Additionally, the spikes fired by the hidden neurons have a significant effect on a training task in a multilayer spiking neural network. Applying the hidden neuron output spikes during the learning of the hidden neuron can lead to a more biologically plausible learning algorithm, and improve the accuracy of the method. Another negative aspect of the learning method used for the hidden layer in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea and Grüning (2013)</a> is that the same method was used for adjusting the input weights of both inhibitory and excitatory neurons. However, different weight adjustment strategies that reflect appropriately the effect of positive and negative PSP produced by the excitatory and inhibitory neurons in a hidden layer can be used to improve the performance of the method.</p>
<p>A biologically plausible supervised learning algorithm for spiking neural networks is proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b133">Taherkhani, Belatreche, et al. (2018)</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b134">Taherkhani, Cosma, and McGinnity (2018)</a>. It uses the precise timing of multiple spikes which is a biologically plausible coding scheme to transmit the information between neurons. The learning parameters of neurons in the hidden layer and output layer are adjusted in parallel to train the network. It uses biological concept such as STDP, anti-STDP and delays learning to train the network. Simulation results show that the proposed method in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b133">Taherkhani, Belatreche, et al. (2018)</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b134">Taherkhani, Cosma, and McGinnity (2018)</a> has improved performance compared to the fully supervised algorithm that trains multiple spikes in all layers proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea and Grüning (2013)</a>. The improvement of the proposed method is achieved because of some significant properties of the method. First, it has used the firing times of spikes fired by the hidden neurons to train the input weights of the hidden neurons. However, in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea and Grüning (2013)</a> the firing time of a hidden neuron is not considered. Another property of the proposed method is the application of different appropriate approaches to adjust the weights related to inhibitory and excitatory neurons in a hidden layer. Delay learning increases the complexity of the learning method and consequently the running time, however, it can improve the performance of the method. Additionally, delays are a biologically plausible property and it is a natural property of a real biological system.</p>
<p>The various algorithms discussed in this section are summarized in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#tbl1">Table 1</a> according to their ability to train a single desired spike or multiple desired spikes, and their structure (a single neuron, a single layer of neurons, and multiple layers of spiking neurons). The learning algorithms are highly concentrated on learning a single spike (the second row) or on a single neuron (the second column) which are comparably simple tasks. The rate coding, which works based on multiple spikes without considering the precise timing of the spikes, commonly is used in traditional ANNs. After finding the biological evidence that proves the encoding of information in the precise timing of the spikes, learning algorithms are devised to capture the information in the precise time of spikes. The first learning algorithms for spiking neurons such as SpikeProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b13">Bohte et al., 2002</a>) work based on a single spike. Other examples of the learning algorithms that learn a single spike are shown in the first row of <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#tbl1">Table 1</a>. However, the learning algorithms which work based on multiple spikes (the third and the fourth rows of <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#tbl1">Table 1</a>) train a more biologically plausible learning task. Investigation of biological neurons shows that the synaptic plasticity models working based on multiple spikes are a better representation of their biological counterparts based on experimental data (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b11">Bi and Wang, 2002</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b32">Froemke and Dan, 2002</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b33">Froemke et al., 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b97">Morrison et al., 2008</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b121">Seth, 2015</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b148">Wang et al., 2005</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b162">Zuo et al., 2015</a>). Additionally, multiple spikes convey more information (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b15">Borst and Theunissen, 1999</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b113">Ponulak and Kasiński, 2010</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang, 2013</a>). The fourth column of the table contains examples of learning algorithms for multilayer networks which are biologically plausible learning algorithms. Although, the multilayer learning algorithms that can train multiple spikes are more biologically plausible, training multiple spikes is a difficult learning task for a multilayer network. Therefore, finding a learning algorithm for multilayer network of spiking neurons to train multiple desired spikes remains a challenging task.</p>
<h3 id="34-learning-algorithms-with-delay-leaning-ability">3.4. Learning algorithms with delay leaning ability<a class="headerlink" href="#34-learning-algorithms-with-delay-leaning-ability" title="Permanent link">&para;</a></h3>
<p>Experimental research has proven the existence of synaptic delays in biological neural (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b69">Katz and Miledi, 1965</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b94">Minneci et al., 2012</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b105">Parnas and Parnas, 2010</a>). The synaptic delay influences the processing ability of the nervous system (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b40">Gilson et al., 2012</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b42">Glackin et al., 2010</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang, 2013</a>). There is biological evidence that the synaptic delay can be modulated (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b16">Boudkkazi et al., 2011</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b83">Lin and Faber, 2002</a>). ‘Delay Selection’ (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b38">Ghosh-Dastidar and Adeli, 2007</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b39">Ghosh-Dastidar and Adeli, 2009</a>) and ‘Delay Shift’ (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b2">Adibi, Meybodi, &amp; Safabakhsh, 2005</a>) are two approaches that have been developed for delay learning in SNNs. Two neurons are connected by multiple synapses (sub connections) with various time delays in the delay selection method. The weights related to appropriate delays are increased and the weights of connections with unsuitable delays for learning desired spikes are decreased. For example, SpikeProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b13">Bohte et al., 2002</a>) used a ‘Delay Selection’ method.. Similarly, other research (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b38">Ghosh-Dastidar and Adeli, 2007</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b39">Ghosh-Dastidar and Adeli, 2009</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b91">McKennoch et al., 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b124">Shrestha and Song, 2015</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea and Grüning, 2013</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, and Wang, 2013</a>) has constructed multiple synapses with different synaptic weights and delays between two neurons. Using sub-connections with various synaptic delays improves the learning performance by producing output spikes at different desired times. However, the number of learning parameters (synaptic weights) is increased and consequently the computational cost is increased.</p>
<p>The delay shift approach is used to train a CD. A CD fires in response to coincident input spikes. Synaptic delay adjustment in the model is an essential characteristic of the learning algorithm (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b2">Adibi et al., 2005</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b111">Pham et al., 2008</a>). <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b111">Pham et al. (2008)</a> proposed a self-organizing delay shift method to train a CD. A Radial Basis Function (RBF) network is trained in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b2">Adibi et al. (2005)</a> using a delay shift approach. In this method a synaptic delay vector makes the centre of the <a href="https://www.sciencedirect.com/topics/engineering/radial-base-function" title="Learn more about RBF from ScienceDirect's AI-generated Topic Pages">RBF</a> neuron. If input spike patterns are close to this centre, they will fire the neuron. In a CD, the weights are considered constant and are not modified. However, the synaptic weight modulation has a dominant effect on the processing ability of spiking neural networks. DL-ReSuMe (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b132">Taherkhani et al., 2015b</a>), EDL (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b131">Taherkhani et al., 2015a</a>), and the supervised method proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b133">Taherkhani, Belatreche, et al. (2018)</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b134">Taherkhani, Cosma, and McGinnity (2018)</a> for multilayer SNN are other examples that use delay shift method for training.</p>
<p>Table 1. Learning algorithms for spiking <a href="https://www.sciencedirect.com/topics/neuroscience/neural-networks" title="Learn more about neural networks from ScienceDirect's AI-generated Topic Pages">neural networks</a>.</p>
<table>
<thead>
<tr>
<th>Empty Cell</th>
<th>Single neuron</th>
<th>Single layer</th>
<th>Multilayer</th>
</tr>
</thead>
<tbody>
<tr>
<td>A single spike</td>
<td></td>
<td>Evolving spiking neural network (eSNN) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b6">Belatreche et al., 2007</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b151">Wysoski et al., 2008</a>), Tempotron (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b156">Yu et al., 2012</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b158">Yu et al., 2013b</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b159">Yu et al., 2014</a>),</td>
<td></td>
</tr>
<tr>
<td>Multiple SPAN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b96">Mohemmed et al., 2013</a>)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LSM (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b107">Paugam-Moisy et al., 2008</a>), Pattern Clustering (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b111">Pham et al., 2008</a>)</td>
<td>SpikeProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b13">Bohte et al., 2002</a>),</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Backpropagation with momentum (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b91">McKennoch et al., 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b152">Xin and Embrechts, 2001</a>), QuickProp (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b91">McKennoch et al., 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b152">Xin and Embrechts, 2001</a>), Resilient propagation (RProp) (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b38">Ghosh-Dastidar and Adeli, 2007</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b91">McKennoch et al., 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b125">Silva and Ruano, 2005</a>), Levenberg–Marquardt BP (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b125">Silva &amp; Ruano, 2005</a>), The SpikeProp based on adaptive learning rate (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b124">Shrestha &amp; Song, 2015</a>), Evolutionary strategy (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b7">Belatreche et al., 2003</a>)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>A single output spike &amp; multiple spikes in input or hidden layers</td>
<td>Tempotron (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b45">Gütig &amp; Sompolinsky, 2006</a>)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Supervised Hebbian learning (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b117">Ruf &amp; Schmitt, 1997</a>)</td>
<td>deSNN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b68">Kasabov et al., 2013</a>)</td>
<td>(<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b14">Booij &amp; tat Nguyen, 2005</a>),</td>
<td></td>
</tr>
<tr>
<td>(<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b39">Ghosh-Dastidar &amp; Adeli, 2009</a>)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Multiple spikes</td>
<td>ReSuMe (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b113">Ponulak &amp; Kasiński, 2010</a>),</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SPAN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b95">Mohemmed et al., 2012</a>),</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Chronotron (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b30">Florian, 2012</a>), Supervised Hebbian learning (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b81">Legenstein et al., 2005</a>),</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>PSD (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b157">Yu et al., 2013a</a>)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BPSL (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b130">Taherkhani et al., 2014</a>), DL-ReSuMe (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b132">Taherkhani et al., 2015b</a>), EDL (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b131">Taherkhani et al., 2015a</a>)</td>
<td>Statistical methods (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b110">Pfister et al., 2006</a>)</td>
<td>(<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b126">Sporea &amp; Grüning, 2013</a>),</td>
<td></td>
</tr>
<tr>
<td>Unsupervised (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b12">Bichler et al., 2012</a>),</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b153">Xu, Gong, &amp; Wang, 2013</a>),</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b133">Taherkhani, Belatreche, et al., 2018</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b134">Taherkhani, Cosma, and McGinnity, 2018</a>).</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="35-deep-spiking-neural-networks">3.5. Deep Spiking Neural Networks<a class="headerlink" href="#35-deep-spiking-neural-networks" title="Permanent link">&para;</a></h3>
<p><a href="https://www.sciencedirect.com/topics/computer-science/deep-learning-method" title="Learn more about Deep learning methods from ScienceDirect's AI-generated Topic Pages">Deep learning methods</a> have achieved successful performance in different applications especially in image recognition (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b52">Hinton and Salakhutdinov, 2006</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b78">LeCun et al., 2015</a>). The ability of deep learning to integrate the feature extraction and feature learning processes enables it to be applied to challenging problems that are difficult to be solved by traditional <a href="https://www.sciencedirect.com/topics/engineering/machine-learning-method" title="Learn more about machine learning methods from ScienceDirect's AI-generated Topic Pages">machine learning methods</a>, since traditional machine learning methods have separate feature engineering and feature learning processes (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b133">Taherkhani, Belatreche, et al., 2018</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b134">Taherkhani, Cosma, and McGinnity, 2018</a>). Although, deep learning architectures have shown promising results, their processing ability is far below their biological counterpart, the brain.</p>
<p>In a Deep Neural Network (DNN), several processing layers are staked to make a deep <a href="https://www.sciencedirect.com/topics/engineering/lamination" title="Learn more about multilayer structure from ScienceDirect's AI-generated Topic Pages">multilayer structure</a>. All or most of the stacked layers are trained to increase the selectivity ability of the overall deep learning method and make a robust representation of the input data on different layers (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b78">LeCun et al., 2015</a>). The deep learning methods that work with raw input data are considered as <a href="https://www.sciencedirect.com/topics/mathematics/representation-learning" title="Learn more about representation learning from ScienceDirect's AI-generated Topic Pages">representation learning</a> methods. In a representation deep learning method the raw inputs are applied to a network and the network extracts representations which are required for classification or detection from the raw input data (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b78">LeCun et al., 2015</a>).</p>
<h4 id="351-backpropagation-for-supervised-learning">3.5.1. Backpropagation for supervised learning<a class="headerlink" href="#351-backpropagation-for-supervised-learning" title="Permanent link">&para;</a></h4>
<p>Backpropagation for supervised learning method has an important role in the successful results of classical deep learning methods (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b78">LeCun et al., 2015</a>). Consequently, appropriate <a href="https://www.sciencedirect.com/topics/engineering/backpropagation-learning" title="Learn more about backpropagation learning from ScienceDirect's AI-generated Topic Pages">backpropagation learning</a> algorithms for Deep Spiking Neural Network (DSNN) can result in successful performance for DSNNs. Zenke and Ganguli (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b160">Zenke &amp; Ganguli, 2018</a>) have proposed a voltage based gradient decent approach, called SuperSpike, to train a multilayer network of deterministic integrate-and-fire neurons to process spatiotemporal spiking patterns. They proposed a biologically plausible strategy for credit assignment in a deep SNN. In the <a href="https://www.sciencedirect.com/topics/engineering/gradient-approach" title="Learn more about gradient approach from ScienceDirect's AI-generated Topic Pages">gradient approach</a>, the partial derivative of the hidden neurons is approximated by the product of a <a href="https://www.sciencedirect.com/topics/engineering/nonlinear-function" title="Learn more about nonlinear function from ScienceDirect's AI-generated Topic Pages">nonlinear function</a> of postsynaptic voltage and related filtered presynaptic spike train. Using a nonlinear function of the postsynaptic voltage, instead of the postsynaptic spike train as credit assignment for hidden neurons, overcomes the problem of silent neuron in the network without injecting noise which might reduce the method accuracy. Additionally, SuperSpike used synaptic eligibility traces to import temporal activity in the credit assignment rule. Experiments carried out by <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b102">Neftci, Mostafa, and Zenke (2019)</a> revealed that SuperSpike does not have good performance for large multilayer SNN. Additionally, it is not applied for <a href="https://www.sciencedirect.com/topics/engineering/real-world-application" title="Learn more about real world applications from ScienceDirect's AI-generated Topic Pages">real world applications</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b98">Mostafa (2018)</a> proposed a backpropagation learning method for SNNs composed of multiple layers. Nonleaky integrate-and-fire neurons were used in this network. The neuron synaptic current kernels are exponentially decaying functions. The time of the first spike is considered as the desired output. A <a href="https://www.sciencedirect.com/topics/mathematics/differentiable-function" title="Learn more about differentiable function from ScienceDirect's AI-generated Topic Pages">differentiable function</a> has been constructed to relate the times of input spikes to the time of the first spike of an output neuron using a transformation. The input spikes that have <a href="https://www.sciencedirect.com/topics/computer-science/causal-relation" title="Learn more about causal relation from ScienceDirect's AI-generated Topic Pages">causal relation</a> with an output spike are considered, i.e. the input spikes that fire before the output spike time. Then weights are updated using a derivable cost function on spike time. In this method the time of the first spike is considered, and it prevents the neuron to fire multiple times.</p>
<h4 id="352-spiking-convolutional-neural-networks">3.5.2. Spiking convolutional neural networks<a class="headerlink" href="#352-spiking-convolutional-neural-networks" title="Permanent link">&para;</a></h4>
<p>CNN is a well-known DL method. The early layers in CNN were inspired from neuron responses in the <a href="https://www.sciencedirect.com/topics/engineering/primary-visual-cortex" title="Learn more about primary visual cortex from ScienceDirect's AI-generated Topic Pages">primary visual cortex</a> (V1). Primary visual features, such as oriented edges, are detected by the neurons in the V1 area. In conventional CNN, input images are convolved with kernels or filters to extract features related to the edges in early layers and to extract more abstract features, i.e. features related to shapes in higher level layers. In the traditional CNN the parameters of the kernel filters are often trained using error backpropagation methods. There exist a number of Spiking CNNs (SCNNs) which utilize hand-crafted <a href="https://www.sciencedirect.com/topics/computer-science/convolution-kernel" title="Learn more about convolution kernels from ScienceDirect's AI-generated Topic Pages">convolution kernels</a> and these SCNNs have been applied in a number of classification tasks (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b70">Kheradpisheh et al., 2016</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b71">Kheradpisheh et al., 2018</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b90">Masquelier and Thorpe, 2007</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b138">Tavanaei et al., 2016</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b161">Zhao et al., 2015</a>). In hand-crafted convolution kernels, the <a href="https://www.sciencedirect.com/topics/computer-science/kernel-parameter" title="Learn more about kernel parameters from ScienceDirect's AI-generated Topic Pages">kernel parameters</a> are fixed whilst the network is being trained.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b71">Kheradpisheh et al. (2018)</a> have proposed a SCNN where in the first layer Difference of <a href="https://www.sciencedirect.com/topics/engineering/gaussians">Gaussians</a> (DoG) filters are used to extract edges, and hand-crafted DOG filters were used in the first layer. DOG filters were used to encode the contrast in the input image to latency spike code. Positive or negative contrasts are detected by applying a DOG filter over different parts of input images. The DOG cells emit spikes depending on the contrast, and they fire spikes in such way that the order in the spikes contains information about the input image contrast. It is suggested that this rank order code can be used to perform V1 like edge detection (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b23">Delorme et al., 2001</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b71">Kheradpisheh et al., 2018</a>). After the first layer there are three pairs of convolutional and pooling layers. The <a href="https://www.sciencedirect.com/topics/mathematics/convolutional-layer" title="Learn more about convolutional layers from ScienceDirect's AI-generated Topic Pages">convolutional layers</a> are trained by STDP. Additionally, there is a <a href="https://www.sciencedirect.com/topics/mathematics/lateral-inhibition">lateral inhibition</a> mechanism in all convolutional layers such that when a neuron is fired, it inhibits other local neurons by resetting their potentials to zero, and prevents their firing for the current applied input image. Moreover, each neuron can only fire once during recognition of an input image. The learning in a subsequent convolutional layer begins when the learning in the current convolutional layer is finalized. The SCNN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b71">Kheradpisheh et al., 2018</a>) used max pooling layers which allow the propagation of its first emitted input spike. Despite the rank order coding of input information in the hand-crafted DOG layer, in the pooling layer only the time of the first spike is considered and information in the other spikes is ignored. The final pooling layer in the SCNN is a global max pooling layer which pools on overall neuronal maps of the last convolutional layer. The output of the global pooling layer is feed to a non-spiking <a href="https://www.sciencedirect.com/topics/computer-science/classification-machine-learning" title="Learn more about classifier from ScienceDirect's AI-generated Topic Pages">classifier</a>, i.e. a linear SVM classifier, to determine the category of the applied input. Therefore, supervised learning takes place in a non-spiking method, i.e. SVM. Additionally, instead of using spiking activity of the last <a href="https://www.sciencedirect.com/topics/computer-science/convolution-layer" title="Learn more about convolution layer from ScienceDirect's AI-generated Topic Pages">convolution layer</a>, the threshold of the neurons in the last convolutional layer was set to infinite and the final potentials are used for the next steps. The performance of the SCNN on MNIST data was 98.4%. There exist other SCNNs where all convolutional layers are trained instead of using fixed initial <a href="https://www.sciencedirect.com/topics/engineering/gabor-filters" title="Learn more about Gabor filters from ScienceDirect's AI-generated Topic Pages">Gabor filters</a> (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b139">Tavanaei, Masquelier, &amp; Maida, 2018</a>).</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b58">Illing, Gerstner, and Brea (2019)</a> have shown that localized receptive fields, i.e. Gabor filters, improve the accuracy of a networks compared to all-to all connectivity. They have used integrate-and-fire neurons and STDP to train a shallow SNN that contains a single hidden layer and a readout layer. The hidden layer has fixed weights which correspond to random Gabor filters, and they used a STDP based learning rule to train the readout layer. The SNN reached a testing accuracy comparable to other deep SNN on MNIST, i.e. 98.6%. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b58">Illing et al. (2019)</a> suggest that novel biologically plausible deep learning methods should reach better performance than their shallow counterpart networks. Additionally, novel biologically plausible deep learning methods should be tested on more complicated data than MNIST.</p>
<h4 id="353-semi-supervised-learning-methods">3.5.3. Semi-supervised learning methods<a class="headerlink" href="#353-semi-supervised-learning-methods" title="Permanent link">&para;</a></h4>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b104">Panda and Roy (2016)</a> proposed a convolutional Auto-Encoder (AE) learning method for SNN. <a href="https://www.sciencedirect.com/topics/mathematics/encoder" title="Learn more about AE from ScienceDirect's AI-generated Topic Pages">AE</a> is a well-known <a href="https://www.sciencedirect.com/topics/computer-science/unsupervised-learning" title="Learn more about unsupervised learning from ScienceDirect's AI-generated Topic Pages">unsupervised learning</a> method in classical neural networks. AE maps input to a lower <a href="https://www.sciencedirect.com/topics/engineering/dimensional-feature-space" title="Learn more about dimensional feature space from ScienceDirect's AI-generated Topic Pages">dimensional feature space</a>, then it reconstructs the input data from the low dimensional features. The first procedure is called encoding and the last procedure that reconstructs the input from the low dimension features is called decoding. It is supposed that AE can extract robust and <a href="https://www.sciencedirect.com/topics/computer-science/discriminative-feature" title="Learn more about discriminative features from ScienceDirect's AI-generated Topic Pages">discriminative features</a> in a low dimensional feature space that can be used to improve <a href="https://www.sciencedirect.com/topics/engineering/classification-accuracy" title="Learn more about classification accuracy from ScienceDirect's AI-generated Topic Pages">classification accuracy</a>. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b104">Panda and Roy (2016)</a> have used a backpropagation algorithm for layer wise training of different convolutional layers. They have used the membrane potential of spiking neurons to construct an error function. The convolutional layers are trained independently and are staked in a network. Additionally, they have used an <a href="https://www.sciencedirect.com/topics/computer-science/average-pooling" title="Learn more about average pooling from ScienceDirect's AI-generated Topic Pages">average pooling</a> layer after each convolutional layer. The pooling layer calculates the average of membrane potential of convolutional neurons within a <a href="https://www.sciencedirect.com/topics/computer-science/sampling-window" title="Learn more about sampling window from ScienceDirect's AI-generated Topic Pages">sampling window</a>. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b104">Panda and Roy (2016)</a> have used a backpropagation method to train the final fully connected output layer. The output layer maps extracted features from multiple convolutional and pooling layers to corresponding spiking labels using labelled training data. The CSNN achieved a high classification accuracy of 99.08% on MNIST dataset.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b79">Lee, Panda, Srinivasan, and Roy (2018)</a> proposed a learning algorithm for SCNN. The SCNN pretrains convolutional kernels using STDP in a layer wise manner. During the pretraining procedure, synaptic weights and neuronal thresholds are trained. At the time of a post-spike, the shared kernel weights are adjusted. Average STDP induced weight adjustments are applied when there are multiple post-neuronal spikes in a feature map. Additionally, at the time of a post-spike in a feature map, the firing thresholds of all the neurons in the feature map are uniformly increased. The threshold of neurons in feature maps exponentially decay over non-spiking periods. It is supposed that this threshold adjustment resembles homoeostasis in biological neurons. After the pretraining procedure, all the layers in the SCNN are trained using a gradient descent BP algorithm.</p>
<p>The <a href="https://www.sciencedirect.com/topics/engineering/deep-belief-network" title="Learn more about Deep Belief Networks from ScienceDirect's AI-generated Topic Pages">Deep Belief Networks</a> (DBNs) proposed by <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b52">Hinton and Salakhutdinov (2006)</a> and the Deep <a href="https://www.sciencedirect.com/topics/computer-science/boltzmann-machine">Boltzmann Machines</a> (DBMs) proposed by <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b128">Srivastava and Salakhutdinov (2014)</a> are two classical deep learning methods that have an unsupervised pretraining procedure. They have a layer-wise pretraining procedure and the deep structures of the networks enable them to learn high-level representation of features. The <a href="https://www.sciencedirect.com/topics/mathematics/restricted-boltzmann-machine" title="Learn more about Restricted Boltzmann Machine from ScienceDirect's AI-generated Topic Pages">Restricted Boltzmann Machine</a> (RBM) is a two-layer neural network that constitutes the <a href="https://www.sciencedirect.com/topics/computer-science/building-blocks" title="Learn more about building block from ScienceDirect's AI-generated Topic Pages">building block</a> of DBNs and DBMs. RBMs are trained by a method called Contrastive Divergence. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b101">Neftci, Das, Pedroni, Kreutz-Delgado, and Cauwenberghs (2014)</a> have proposed a spiking version of Contrastive Divergence to train a spiking RBM composed of integrate-and-fire neurons. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b101">Neftci et al. (2014)</a> have used a STDP for Contrastive Divergence. The accuracy of the spiking DBN on the MNIST data has been compared with feedforward fully connected multi-layer SNNs and SCNNs in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b137">Tavanaei, Ghodrati, and Reza (2019)</a>, and the comparison results show that the spiking DBN has lower accuracy than the other SNNs.</p>
<h4 id="354-converted-classical-dnn-as-dsnn">3.5.4. Converted classical DNN as DSNN<a class="headerlink" href="#354-converted-classical-dnn-as-dsnn" title="Permanent link">&para;</a></h4>
<p>There are a group of DSNNs which are directly constructed from the conventional DNN. In the group of DSNNs, first a classical DNN which is composed of neurons with continuous activation values is trained, then the classical DNN is deployed to a DSNN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b19">Cao et al., 2015</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b116">Rueckauer et al., 2017</a>). By this approach the state-of-the-art methods for training DNN can be used to construct DSNN to achieve a competitive performance. This conversion might cause loss of accuracy in the DSNN compared to the original DNN. Different techniques (such as introducing extra limitations on neuron firing rates or network parameters, weight scaling, adding noise, and using probabilistic weights) have been used to reduce the loss of accuracy during the conversion from ANN to SNN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b122">Shrestha, 2018</a>). The generated DSNN needs a large number of time steps to perform the input–output mapping, and the constructed DSNN cannot capture temporal dynamics of <a href="https://www.sciencedirect.com/topics/computer-science/spatiotemporal-data" title="Learn more about spatiotemporal data from ScienceDirect's AI-generated Topic Pages">spatiotemporal data</a> (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b80">Lee, Sarwar, &amp; Roy, 2019</a>). <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b137">Tavanaei et al. (2019)</a> compared a number of DSNNs, and their result shows that deep SNNs which are converted from a classical DNN can achieve the highest accuracy on MNIST data compared to methods that train DSNNs directly. DSNNs which are constructed by converting a classical DNN to DSNN usually use rate coding to encode an analog output of a classical neuron. The rate coding can mask the temporal information that can be processed by DSNN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b80">Lee et al., 2019</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b98">Mostafa, 2018</a>).</p>
<h4 id="355-deep-recurrent-spiking-neural-networks">3.5.5. Deep recurrent spiking neural networks<a class="headerlink" href="#355-deep-recurrent-spiking-neural-networks" title="Permanent link">&para;</a></h4>
<p><a href="https://www.sciencedirect.com/topics/engineering/recurrent-neural-network" title="Learn more about Recurrent neural networks from ScienceDirect's AI-generated Topic Pages">Recurrent neural networks</a> are a class of neural networks whose internal states evolve with time, and they have been used in temporal processing tasks such as noisy <a href="https://www.sciencedirect.com/topics/mathematics/time-series-prediction" title="Learn more about time series prediction from ScienceDirect's AI-generated Topic Pages">time series prediction</a>, <a href="https://www.sciencedirect.com/topics/computer-science/translation-languages" title="Learn more about language translation from ScienceDirect's AI-generated Topic Pages">language translation</a>, and <a href="https://www.sciencedirect.com/topics/engineering/automatic-speech-recognition" title="Learn more about automatic speech recognition from ScienceDirect's AI-generated Topic Pages">automatic speech recognition</a> (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b9">Bellec et al., 2018</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b102">Neftci et al., 2019</a>). Training large RNNs is a difficult task because during training, functions with long-range temporal and spatial dependencies should be optimized. The non-derivable dynamics of Spiking RNNs increase training difficulties. Training a deep RNN is a challenging task, as the dependency in space is extended in addition to the dependency in time (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b102">Neftci et al., 2019</a>). Th gradient descent method is a powerful method to train learning parameters in RNNs. In a multilayer network with hidden units the adjustment of learning parameters (credit assignment) for hidden units are obtained using the chain rule of derivatives. Special and temporal credit assignments are two problems that should be solved to train a RNN. The spatial credit assignment is a common problem for RNNs and multi-layer perceptrons, and it is assigned spatially across layers to update learning parameters.</p>
<p>In the gradient descent method, a method similar to the spatial credit assignment is used for temporal credit assignment through unrolling the network in time. This method is called Backpropagation Through Time (BPTT). The non-linearity in the activity of spiking neurons makes it difficult to apply the classical BBTT to SNN. Different methods have been proposed to address this challenge. Using biologically inspired local learning rules, converting trained classical NNs to SNNs, smoothing the network to make it continuously derivable, and using approximated gradient descent for SNN are different techniques that have been used to overcome the problem related to nonlinearity in gradient descent learning methods for SNNs (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b102">Neftci et al., 2019</a>).</p>
<p>The smoothing method can be used to overcome nonlinearity issues of the gradient decent learning method for SNNs. Smoothing models are used in SNN to construct a network with well-behaved gradients. For instance <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b55">Huh and Sejnowski (2018)</a> used an extended version of integrate-and-fire model in which the nonlinearity of a neuron is replaced by a continuous-valued gating function. Then a RNN with those neurons was trained using a standard BPTT. Another group of smoothing models are probabilistic models which use the log-likelihood of a spike train as a smooth quantity. Although there are some probabilistic models that can learn precise output spike timing (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b34">Gardner, Sporea, &amp; Grüning, 2015</a>), probabilistic models usually used rate-coding at output level and firing <a href="https://www.sciencedirect.com/topics/mathematics/probability-density-rho" title="Learn more about probability densities from ScienceDirect's AI-generated Topic Pages">probability densities</a>. These properties reduce their ability to capture temporal information. Although BBTT is a standard learning method for recurrent neural networks, BBTT is not a biologically realistic method. Because in BBTT, error signals should be transmitted backward in time and space. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b10">Bellec et al. (2019)</a> proposed a biologically plausible learning method using locally available information to make a biologically plausible approximation of BBTT. They have used feedforward eligibility traces of synapses that are available in a real time at the location of synapses to design the learning method (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b10">Bellec et al., 2019</a>).</p>
<h2 id="4-challenges-and-opportunities">4. Challenges and opportunities<a class="headerlink" href="#4-challenges-and-opportunities" title="Permanent link">&para;</a></h2>
<p>Recently, classical <a href="https://www.sciencedirect.com/topics/computer-science/deep-neural-network" title="Learn more about DNNs from ScienceDirect's AI-generated Topic Pages">DNNs</a> have achieved successful results in different applications. Despite this, DSNNs are still in the developmental stage and more advanced learning algorithms are required to train them. The main challenge in training DSNNs is to find methods to train neurons in a hidden layer of <a href="https://www.sciencedirect.com/topics/neuroscience/neural-networks" title="Learn more about DSNN from ScienceDirect's AI-generated Topic Pages">DSNN</a> in interaction with an output layer and other hidden layers. Supervised learning using a <a href="https://www.sciencedirect.com/topics/engineering/backpropagation" title="Learn more about backpropagation from ScienceDirect's AI-generated Topic Pages">backpropagation</a> method is a common approach to train a classical deep (multilayer) <a href="https://www.sciencedirect.com/topics/engineering/artificial-neural-network" title="Learn more about artificial neural network from ScienceDirect's AI-generated Topic Pages">artificial neural network</a> (ANN). However, the nonlinearity discontinuity of SNN activities makes it difficult to adopt existing backpropagation methods to train SNNs (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b137">Tavanaei et al., 2019</a>). Additionally, it is not biologically plausible to train a DSNN with the error backpropagation method (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b58">Illing et al., 2019</a>), and <a href="https://www.sciencedirect.com/topics/engineering/backpropagation-learning" title="Learn more about backpropagation learning from ScienceDirect's AI-generated Topic Pages">backpropagation learning</a> methods do not mimic the learning of the human brain (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b149">Whittington &amp; Bogacz, 2019</a>). It is an important challenge to understand what are the best biologically plausible <a href="https://www.sciencedirect.com/topics/computer-science/network-architecture" title="Learn more about network architectures from ScienceDirect's AI-generated Topic Pages">network architectures</a> and learning rules to train DSNNs for information processing. Local learning rules inspired by <a href="https://www.sciencedirect.com/topics/mathematics/spike-timing-dependent-plasticity" title="Learn more about STDP from ScienceDirect's AI-generated Topic Pages">STDP</a> are more biological plausible and can be investigated to design new learning algorithms for SNNs. One interesting structural property of a biological NN is the <a href="https://www.sciencedirect.com/topics/engineering/recurrent" title="Learn more about recurrent from ScienceDirect's AI-generated Topic Pages">recurrent</a> connections in the network. BBTT (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b102">Neftci et al., 2019</a>) is a classical method which has been applied to train a RNN. Finding a biologically plausible alternative for BBTT to train RNN using biologically plausible local learning rule can be consider as an important challenge for SNNs.</p>
<p>Neural encoding is an important aspect of learning in a SNN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b150">Wu et al., 2019</a>). Finding appropriate encoding mechanisms for spiking activity and designing compatible learning algorithms for the encoding mechanisms are new challenges in the SNN field. An optimization method for output <a href="https://www.sciencedirect.com/topics/engineering/spike-train" title="Learn more about spike train from ScienceDirect's AI-generated Topic Pages">spike train</a> encoding has been proposed in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b135">Taherkhani, Cosma, and Mcginnity (2019)</a>, and it has been shown that optimizing the output encoding during the learning phase in <a href="https://www.sciencedirect.com/topics/engineering/classification-task" title="Learn more about classification tasks from ScienceDirect's AI-generated Topic Pages">classification tasks</a> can increase <a href="https://www.sciencedirect.com/topics/engineering/classification-accuracy" title="Learn more about classification accuracy from ScienceDirect's AI-generated Topic Pages">classification accuracy</a> by 16.5% compared to when using a non-adapted output encoding. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b154">Xu, Yang, and Zeng (2019)</a> have shown that the time distances of input spikes related to actual and desired <a href="https://www.sciencedirect.com/topics/engineering/output-spike" title="Learn more about output spikes from ScienceDirect's AI-generated Topic Pages">output spikes</a> have an important effect on the accuracy of SNN, and finding an optimal <a href="https://www.sciencedirect.com/topics/mathematics/time-interval-tau" title="Learn more about time interval from ScienceDirect's AI-generated Topic Pages">time interval</a> for input spikes to be involved in the <a href="https://www.sciencedirect.com/topics/engineering/synaptic-weight" title="Learn more about synaptic weight from ScienceDirect's AI-generated Topic Pages">synaptic weight</a> adjustments can improve learning accuracy by 55% in a SNN. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b150">Wu et al. (2019)</a> have shown that input spike encoding has a significant effect on improving the accuracy of a learning algorithm for SNNs. In a classification task a simple output encoding is to assign an applied input to the class corresponding to the output neuron that has the highest firing rate. <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b103">Orchard et al. (2015)</a> have assigned an applied input to the class related to the output neuron that fires first, and they have reported this output encoding has achieved a good performance. On other hand, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b25">Diehl et al. (2015)</a> have shown that an increase in the number of output spikes in the output encoding will increase the classification accuracy. A population of neurons can be used instead of a <a href="https://www.sciencedirect.com/topics/engineering/single-neuron" title="Learn more about single neuron from ScienceDirect's AI-generated Topic Pages">single neuron</a> for each class to reduce the variance of the output (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b108">Pfeiffer &amp; Pfeil, 2018</a>). Therefore, spike encoding has a prominent effect on the performance of SNNs and appropriate encoding strategies should be employed. Additionally, learning algorithms should be compatible with the selected encoding strategy. This is a challenge in biologically plausible learning methods because it is not clear which encoding method (or methods) is used in the brain.</p>
<p>Time plays an important role in activity of SNNs. SNNs generate spatiotemporal spike patterns whereas classical NNs work with spatial activation. Consequently, SNNs need a specific loss function that generates an error related to time which is different from the loss function of a classical NN. There are a considerable number of DSNNs that use rate-based approach (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b24">Diehl and Cook, 2015</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b28">Eliasmith et al., 2012</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b44">Guerguiev et al., 2017</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b93">Mesnard et al., 2016</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b100">Neftci et al., 2016</a>). However, these DSNNs are close to classical neural networks which work based on continuous values, and they neglect the information carried by individual spikes that can lead to a fast computation (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b160">Zenke &amp; Ganguli, 2018</a>). For instance, a widely used method to convert a real value to a spike train is using the spike train that is drawn from a <a href="https://www.sciencedirect.com/topics/engineering/poisson-process" title="Learn more about Poisson process from ScienceDirect's AI-generated Topic Pages">Poisson process</a> with a firing rate in proportion to the real value. In this conversion only the average firing rate of the spike train is important and the precise timing of spikes is not considered (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b10">Bellec et al., 2019</a>). This can limit the capability of SNNs to process the precise timing of spikes.</p>
<p>DSNNs are often used for processing the data which has been used by classical DNNs. For instance, image samples in the MNIST and CIFAR10 datasets which are constructed from pixels with continuous values have been used for a long time with classical DNN, and DSNNs cannot currently outperform classical DNNs on this data. The nature of these benchmarks is close to the activation of classical neural network (i.e. they have continuous values) and they can directly be used by the methods. However, such data should be converted into spike trains before it can be used by DSNN. This conversion might destroy some parts of information in the images, and it can result in a reduction of accuracy of DSNN (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b10">Bellec et al., 2019</a>). Therefore, applying DSNNs for processing new datasets that have <a href="https://www.sciencedirect.com/topics/mathematics/sigma-property" title="Learn more about properties from ScienceDirect's AI-generated Topic Pages">properties</a> which are compatible with SNNs can lead to improved performance rather than when processing these datasets using non spiking neural networks. Research on this type of data can lead to the emergence of new SNN that can perform processing tasks which are difficult for conventional DNN. For instance the data obtained by event-based cameras in <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b115">Ramesh et al. (2019)</a> or the spiking activity that is recorded from real biological nervous system (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b88">Maggi, Peyrache, &amp; Humphries, 2018</a>) originate from original spatiotemporal spiking activity and they can be a good candidate to be used by DSNN. There has been progress in event-based vision and audio sensors (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b84">Liu et al., 2015</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b108">Pfeiffer and Pfeil, 2018</a>) and the data extracted from them can be processed by DSNNs. However, currently, there is a lack of appropriate benchmark data for evaluating SNNs. SNNs have the ability to achieve good performance when trained on suitable datasets that have properties which are compatible with SNNs, and currently there is an urgent need to develop such benchmark datasets.</p>
<p>A major capability of classical <a href="https://www.sciencedirect.com/topics/computer-science/deep-learning-method" title="Learn more about deep learning methods from ScienceDirect's AI-generated Topic Pages">deep learning methods</a>, especially deep CNNs, is their capability for hierarchical feature discovery, where discriminative, abstract, and invariant features are extracted. Bio-inspired SNNs have brain-like representation ability, and they potentially have higher representation capability than traditional rate-based networks (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b85">Maass, 1996</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b137">Tavanaei et al., 2019</a>). The SNNs ability to process temporal data can be mixed with the hierarchical feature representation capability of classical deep neural networks to construct a neural network with a high processing ability.</p>
<p>Advancements in <a href="https://www.sciencedirect.com/topics/engineering/regularization-method" title="Learn more about regularization methods from ScienceDirect's AI-generated Topic Pages">regularization methods</a> for training classical deep neural networks have improved the performance of these methods. However, SNNs have different characteristics, and finding appropriate regularization methods for learning parameter adjustment in SNN can be another interesting challenge to improve the performance of DSNNs.</p>
<p>Processing of the future big data, which is exponentially expanding as a result of advancements in technology, demands huge computing resources. This demand requires a novel scalable computing framework. Neuromorphic hardware is believed to be a paradigm that can potentially satisfy such a demand (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b99">Neftci, 2018</a>), because they mimic the brain which has significant processing ability. The brain can perform information processing with a high level of robustness, efficiency, and adaptivity. Neuromorphic engineering tries to produce hardware that can emulate the cognitive and adaptive ability of the brain. The exact principles of computation in the brain, which contains a massively parallel self-organizing neural architecture, have not yet been discovered (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b26">Douglas &amp; Martin, 2004</a>). In a situation such as real-time adaptability, autonomy, or privacy, it is required that computation performs close to sensors (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b99">Neftci, 2018</a>). In this case computing systems such as neuromorphic hardware with <a href="https://www.sciencedirect.com/topics/engineering/low-power-consumption" title="Learn more about low power consumption from ScienceDirect's AI-generated Topic Pages">low power consumption</a> have advantage.</p>
<p>Biological neurons have constraints on communication, and <a href="https://www.sciencedirect.com/topics/engineering/electric-power-utilization" title="Learn more about power consumption from ScienceDirect's AI-generated Topic Pages">power consumption</a>, and biological neurons communicate through sparse spiking activity which reduce energy consumption (<a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#b35">Gerstner &amp; Kistler, 2002</a>). Spike events consume energy and a low number of spikes in sparse spiking activity means a low power consumption. The state of other activities in the neuron such as membrane potentials, synaptic states, and <a href="https://www.sciencedirect.com/topics/neuroscience/neurotransmitters" title="Learn more about neurotransmitter from ScienceDirect's AI-generated Topic Pages">neurotransmitter</a> concentrations are local to the neuron. The sparse communication and the local processing generate a highly scalable structure. A computational strategy that works based on locally available information and sparse global communication is required to construct neuromorphic hardware with a scalable structure to solve real-world problems. Designing algorithms using local events for learning precise timing of spikes in a SNN can be useful for designing a learning strategy for neuromorphic hardware. SNNs are compatible for implementation in hardware as they can be more energy efficient compared to classical neural networks that work based on continuous values. Additionally, the nature of information communication in SNNs, which is based on spike activities, is close to the binary processing in a hardware platform. However, most of the state-of-the art <a href="https://www.sciencedirect.com/topics/engineering/machine-learning-method" title="Learn more about machine learning methods from ScienceDirect's AI-generated Topic Pages">machine learning methods</a> work on non-local information that restricts their scalability when they are implemented in hardware. Consequently, the methods cannot perform online and incremental fast and energy efficient learning, similar to the learning observed in humans and animals. New biological plausible learning algorithms for SNNs are required to design neuromorphic hardware that performs accurate high speed and scalable low energy computation.</p>
<p>In summary, there is a number of challenges for designing learning algorithms for DSNN. Designing learning algorithms to train <a href="https://www.sciencedirect.com/topics/engineering/hidden-neuron" title="Learn more about hidden neurons from ScienceDirect's AI-generated Topic Pages">hidden neurons</a> in an interconnected SNN by overcoming the discontinues and <a href="https://www.sciencedirect.com/topics/engineering/nonlinear-behavior" title="Learn more about nonlinear behaviour from ScienceDirect's AI-generated Topic Pages">nonlinear behaviour</a> of SNN to improve their accuracy is one of the main challenges. It is believed that local learning rules such as STDP performed in the biological nervous system can be a good option to design new biologically plausible learning algorithms for SNNs. Another challenge in training SNNs is neural encoding. It has been shown that neural encoding has significant effect on performance of a SNN. However, it is a challenge to find what is the exact encoding mechanism in the brain and how to design a learning algorithm to be compatible with the encoding mechanism. Another challenge in designing learning algorithms for SNN is the existence of time in the activity of a SNN. SNNs directly work with time which is a new characteristic compared to traditional NN. This offers an ability to work directly with time, however it causes discontinuity in neuron activity that increases the difficulty of learning. Another challenge is to make new dataset that use the ability of spiking times, such as the data generated from event-based vision and audio sensors. Designing biologically plausible learning algorithms that work based on local events offers a good opportunity to design learning algorithms for power efficient neuromorphic hardware.</p>
<h2 id="5-conclusion">5. Conclusion<a class="headerlink" href="#5-conclusion" title="Permanent link">&para;</a></h2>
<p>In this paper, the biological background of <a href="https://www.sciencedirect.com/topics/engineering/spiking-neuron" title="Learn more about spiking neurons from ScienceDirect's AI-generated Topic Pages">spiking neurons</a> was first considered, and then state of the art learning algorithms for <a href="https://www.sciencedirect.com/topics/neuroscience/neural-networks" title="Learn more about SNNs from ScienceDirect's AI-generated Topic Pages">SNNs</a> critically reviewed. According to the literature, there are different mathematical models for biological neurons. The models simulate behaviour of a biological neuron in different <a href="https://www.sciencedirect.com/topics/mathematics/level-of-detail" title="Learn more about level of details from ScienceDirect's AI-generated Topic Pages">level of details</a>. <a href="https://www.sciencedirect.com/topics/mathematics/leaky-integrate" title="Learn more about LIF from ScienceDirect's AI-generated Topic Pages">LIF</a> model is a simple one dimensional model of a biological neuron that needs less computational effort for modelling a biological neuron and Hodgkin and Huxley is a four dimensional model that can simulate the dynamic of a biological neuron with more details. It has a high computational cost.</p>
<p>The review shows that <a href="https://www.sciencedirect.com/topics/neuroscience/synaptic-plasticity" title="Learn more about synaptic plasticity from ScienceDirect's AI-generated Topic Pages">synaptic plasticity</a> is supposed to be the base of learning in the brain and there are different models of synaptic plasticity for a biological <a href="https://www.sciencedirect.com/topics/mathematics/neuronal-system" title="Learn more about neuronal system from ScienceDirect's AI-generated Topic Pages">neuronal system</a>. The models try to simulate the behaviour of biological synapses based on experimental data. Literature review shows that standard pair-based <a href="https://www.sciencedirect.com/topics/mathematics/spike-timing-dependent-plasticity" title="Learn more about STDP from ScienceDirect's AI-generated Topic Pages">STDP</a> model was used to design biological plausible learning algorithms for spiking neurons. However, the standard pair-based STDP model is not the only model for the synaptic plasticity. It has been shown that multiple-spike STDP models are biologically plausible models, and they can predict experimental data captured from biological neurons with a higher precision. It is not clear how multiple-spike STDP models can be used to design learning algorithm for artificial <a href="https://www.sciencedirect.com/topics/mathematics/neuronal-network" title="Learn more about neuronal network from ScienceDirect's AI-generated Topic Pages">neuronal network</a> for <a href="https://www.sciencedirect.com/topics/computer-science/machine-learning" title="Learn more about machine learning from ScienceDirect's AI-generated Topic Pages">machine learning</a> purpose. Application of the multi-spike STDP models can lead to design of a more biologically plausible learning algorithm for spiking neuron and potentially leads to design a more powerful intelligent system.</p>
<p>According to the review, delays are a natural <a href="https://www.sciencedirect.com/topics/mathematics/sigma-property" title="Learn more about property from ScienceDirect's AI-generated Topic Pages">property</a> of a biological neural network. On the other hand information conveys between neurons through precise timing of spikes. Delays can have direct effect on the precise timing of spikes. Therefore, designing a learning algorithm that merges the usual weight adjustment methods with a proper delay learning approach, can lead to a more biologically plausible learning algorithm with a higher processing ability.</p>
<p>Traditional neural networks work based on rate coding. The idea of encoding of information in precise timing of spikes motivated research into the development of learning algorithms such as SpikeProp that work based on a single spike per neuron. However, coding scheme based on precise timing of multiple spikes can convey more temporal information between neurons and it is more biologically plausible. However, designing a learning algorithm for a network of spiking neurons that conveys information between neurons through precise timing of multiple spikes is a difficult task and it demands more research.</p>
<p>A single biological neuron has interestingly different learning characteristics and many of the learning ability of a single biological neuron are not considered in their artificial counterparts. Various learning algorithms for <a href="https://www.sciencedirect.com/topics/engineering/single-neuron" title="Learn more about single neurons from ScienceDirect's AI-generated Topic Pages">single neurons</a> were reviewed in this paper. The review shows that designing new learning algorithms for a single neuron with new biological properties is an ongoing research, and it can lead to generation of new biologically plausible learning algorithms with higher processing abilities. The review also shows that multilayer neuronal networks have higher processing ability compared to a single neuron or single layer of neurons. It is not clear how the different learning algorithms for a single neuron with new biologically plausible characteristics can be extended to train a multilayer network of spiking neurons. A challenging task remains to design a more biologically plausible learning algorithm for multilayer spiking neural networks.</p>
<h2 id="acknowledgements">Acknowledgements<a class="headerlink" href="#acknowledgements" title="Permanent link">&para;</a></h2>
<p>Dr. Aboozar Taherkhani, Dr. Georgina Cosma, and Prof. T.M. McGinnity acknowledge the financial support of The Leverhulme Trust, UK (Research Project Grant No: <a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#GS1">RPG-2016-252</a>, Title: Novel Approaches for Constructing Optimized Multimodal Data Spaces).</p>
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb1">Adeli and Hung, 1994</a></p>
<p>Adeli H., Hung S.</p>
<p><strong>An adaptive conjugate gradient learning algorithm for efficient training of neural networks</strong></p>
<p>Applied Mathematics and Computation, 62 (1) (1994), pp. 81-102</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb2">Adibi et al., 2005</a></p>
<p>Adibi P., Meybodi M.R., Safabakhsh R.</p>
<p><strong>Unsupervised learning of synaptic delays based on learning automata in an RBF-like network of spiking neurons for data clustering</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb3">Artola et al., 1990</a></p>
<p>Artola A., Brocher S., Singer W.</p>
<p><strong>Different voltage-dependent thresholds for inducing long-term depression and long-term potentiation in slices of rat visual cortex</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb4">Azevedo et al., 2009</a></p>
<p>Azevedo F.A., Carvalho L.R., Grinberg L.T., Farfel J.M., Ferretti R.E., Leite R.E., <em>et al.</em></p>
<p><strong>Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain</strong></p>
<p>Journal of Comparative Neurology, 513 (5) (2009), pp. 532-541</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb5">Bassett et al., 2012</a></p>
<p>Bassett D.S., Wymbs N.F., Rombach M.P., Porter M.A., Mucha P.J., Grafton S.T.</p>
<p><strong>Core-periphery organisation of human brain dynamics</strong></p>
<p>(2012)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb6">Belatreche et al., 2007</a></p>
<p>Belatreche A., Maguire L., McGinnity M.</p>
<p><strong>Advances in design and application of spiking neural networks</strong></p>
<p>Soft Computing - A Fusion of Foundations, Methodologies and Applications, 11 (3) (2007), pp. 239-248, <a href="https://doi.org/10.1007/s00500-006-0065-7">10.1007/s00500-006-0065-7</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb7">Belatreche et al., 2003</a></p>
<p>Belatreche, A., Maguire, L., McGinnity, M., &amp; Wu, Q. (2003). A method for supervised training of spiking neural networks. In <em>Paper presented at the Proc. IEEE conf. cybernetics intelligence–challenges and advances</em> (pp. 39–44).</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb8">Belatreche and Paul, 2012</a></p>
<p>Belatreche, A., &amp; Paul, R. (2012). Dynamic cluster formation using populations of spiking neurons. In <em>Paper presented at the Neural networks (IJCNN), the 2012 international joint conference on</em> (pp. 1–6).</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb9">Bellec et al., 2018</a></p>
<p>Bellec, G., Salaj, D., Subramoney, A., Legenstein, R., &amp; Maass, W. (2018). Long short-term memory and learning-to-learn in networks of spiking neurons. In <em>Paper presented at the Advances in neural information processing systems</em> (pp. 787–797). Retrieved from <a href="http://arxiv.org/abs/1803.09574">http://arxiv.org/abs/1803.09574</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb10">Bellec et al., 2019</a></p>
<p>Bellec G., Scherr F., Hajek E., Salaj D., Legenstein R., Maass W.</p>
<p><strong>Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets</strong></p>
<p>(2019)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb11">Bi and Wang, 2002</a></p>
<p>Bi G., Wang H.</p>
<p><strong>Temporal asymmetry in spike timing-dependent synaptic plasticity</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb12">Bichler et al., 2012</a></p>
<p>Bichler O., Querlioz D., Thorpe S.J., Bourgoin J., Gamrat C.</p>
<p><strong>Extraction of temporally correlated features from dynamic vision sensors with spike-timing-dependent plasticity</strong></p>
<p>Neural Networks, 32 (Aug.) (2012), pp. 339-348</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb13">Bohte et al., 2002</a></p>
<p>Bohte S.M., Kok J.N., La Poutre H.</p>
<p><strong>Error-backpropagation in temporally encoded networks of spiking neurons</strong></p>
<p>Neurocomputing, 48 (1) (2002), pp. 17-37</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb14">Booij and tat Nguyen, 2005</a></p>
<p>Booij O., tat Nguyen H.</p>
<p><strong>A gradient descent rule for spiking neurons emitting multiple spikes</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb15">Borst and Theunissen, 1999</a></p>
<p>Borst A., Theunissen F.E.</p>
<p><strong>Information theory and neural coding</strong></p>
<p>Nature Neuroscience, 2 (11) (1999), pp. 947-957</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb16">Boudkkazi et al., 2011</a></p>
<p>Boudkkazi S., Fronzaroli-Molinieres L., Debanne D.</p>
<p><strong>Presynaptic action potential waveform determines cortical synaptic latency</strong></p>
<p>The Journal of Physiology, 589 (5) (2011), pp. 1117-1131</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb17">Brette, 2015</a></p>
<p>Brette R.</p>
<p><strong>Philosophy of the spike: Rate-based vs. spike-based theories of the brain</strong></p>
<p>Frontiers in Systems Neuroscience, 9 (Nov.) (2015), p. 151</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb18">Brette et al., 2007</a></p>
<p>Brette R., Rudolph M., Carnevale T., Hines M., Beeman D., Bower J.M., <em>et al.</em></p>
<p><strong>Simulation of networks of spiking neurons: A review of tools and strategies</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb19">Cao et al., 2015</a></p>
<p>Cao Y., Chen Y., Khosla D.</p>
<p><strong>Spiking deep convolutional neural networks for energy-efficient object recognition</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb20">Carey et al., 2005</a></p>
<p>Carey M.R., Medina J.F., Lisberger S.G.</p>
<p><strong>Instructive signals for motor learning from visual cortical area MT</strong></p>
<p>Nature Neuroscience, 8 (6) (2005), pp. 813-819</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb21">Cariani, 2004</a></p>
<p>Cariani P.A.</p>
<p><strong>Temporal codes and computations for sensory representation and scene analysis</strong></p>
<p>IEEE Transactions on Neural Networks, 15 (5) (2004), pp. 1100-1111</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb22">Clopath et al., 2010</a></p>
<p>Clopath C., Büsing L., Vasilaki E., Gerstner W.</p>
<p><strong>Connectivity reflects coding: A model of voltage-based STDP with homeostasis</strong></p>
<p>Nature Neuroscience, 13 (3) (2010), pp. 344-352</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb23">Delorme et al., 2001</a></p>
<p>Delorme A., Perrinet L., Thorpe S.J.</p>
<p><strong>Networks of integrate-and-fire neurons using rank order coding B: Spike timing dependent plasticity and emergence of orientation selectivity</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb24">Diehl and Cook, 2015</a></p>
<p>Diehl P.U., Cook M.</p>
<p><strong>Unsupervised learning of digit recognition using spike-timing-dependent plasticity</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb25">Diehl et al., 2015</a></p>
<p>Diehl, P. U., Neil, D., Binas, J., Cook, M., Liu, S. C., &amp; Pfeiffer, M. (2015). Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing. In <em>Proceedings of the international joint conference on neural networks</em>. 2015-Septe. <a href="http://dx.doi.org/10.1109/IJCNN.2015.7280696">http://dx.doi.org/10.1109/IJCNN.2015.7280696</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb26">Douglas and Martin, 2004</a></p>
<p>Douglas R.J., Martin K.A.C.</p>
<p><strong>Neuronal circuits of the neocortex</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb27">Doya, 1999</a></p>
<p>Doya K.</p>
<p><strong>What are the computations of the cerebellum the basal ganglia and the cerebral cortex?</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb28">Eliasmith et al., 2012</a></p>
<p>Eliasmith C., Stewart T.C., Choo X., Bekolay T., Dewolf T., Tang Y., <em>et al.</em></p>
<p><strong>A large-scale model of the functioning brain</strong></p>
<p>Science, 338 (2012), pp. 1202-1205</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb29">Feldman, 2012</a></p>
<p>Feldman D.E.</p>
<p><strong>The spike-timing dependence of plasticity</strong></p>
<p>Neuron, 75 (4) (2012), pp. 556-571</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb30">Florian, 2012</a></p>
<p>Florian R.V.</p>
<p><strong>The chronotron: A neuron that learns to fire temporally precise spike patterns</strong></p>
<p>PLoS One, 7 (8) (2012), Article e40233</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb31">Fregnac and Shulz, 1999</a></p>
<p>Fregnac Y., Shulz D.E.</p>
<p><strong>Activity-dependent regulation of receptive field properties of cat area 17 by supervised hebbian learning</strong></p>
<p>Journal of Neurobiology, 41 (1) (1999), pp. 69-82</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb32">Froemke and Dan, 2002</a></p>
<p>Froemke R.C., Dan Y.</p>
<p><strong>Spike-timing-dependent synaptic modification induced by natural spike trains</strong></p>
<p>Nature, 416 (6879) (2002), pp. 433-438</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb33">Froemke et al., 2006</a></p>
<p>Froemke R.C., Tsay I.A., Raad M., Long J.D., Dan Y.</p>
<p><strong>Contribution of individual spikes in burst-induced long-term synaptic modification</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb34">Gardner et al., 2015</a></p>
<p>Gardner B., Sporea I., Grüning A.</p>
<p><strong>Learning spatiotemporally encoded pattern transformations in structured spiking neural networks</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb35">Gerstner and Kistler, 2002</a></p>
<p>Gerstner W., Kistler W.M.</p>
<p><strong>Spiking neuron models: Single neurons, populations, plasticity</strong></p>
<p>Cambridge University Press (2002)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb36">Gerstner et al., 2014</a></p>
<p>Gerstner W., Kistler W.M., Naud R., Paninski L.</p>
<p><strong>Neuronal dynamics: From single neurons to networks and models of cognition</strong></p>
<p>Cambridge University Press (2014)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb37">Gerstner et al., 2012</a></p>
<p>Gerstner W., Sprekeler H., Deco G.</p>
<p><strong>Theory and simulation in neuroscience</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb38">Ghosh-Dastidar and Adeli, 2007</a></p>
<p>Ghosh-Dastidar S., Adeli H.</p>
<p><strong>Improved spiking neural networks for EEG classification and epilepsy and seizure detection</strong></p>
<p>Integrated Computer-Aided Engineering, 14 (3) (2007), pp. 187-212</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb39">Ghosh-Dastidar and Adeli, 2009</a></p>
<p>Ghosh-Dastidar S., Adeli H.</p>
<p><strong>A new supervised learning algorithm for multiple spiking neural networks with application in epilepsy and seizure detection</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb40">Gilson et al., 2012</a></p>
<p>Gilson M., Bürck M., Burkitt A.N., van Hemmen J.L.</p>
<p><strong>Frequency selectivity emerging from spike-timing-dependent plasticity</strong></p>
<p>Neural Computation, 24 (9) (2012), pp. 2251-2279</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb41">Glackin et al., 2011</a></p>
<p>Glackin C., Maguire L., McDaid L., Sayers H.</p>
<p><strong>Receptive field optimisation and supervision of a fuzzy spiking neural network</strong></p>
<p>Neural Networks, 24 (3) (2011), pp. 247-256</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb42">Glackin et al., 2010</a></p>
<p>Glackin B., Wall J.A., McGinnity T.M., Maguire L.P., McDaid L.J.</p>
<p><strong>A spiking neural network model of the medial superior olive using spike timing dependent plasticity for sound localization</strong></p>
<p>Frontiers in Computational Neuroscience, 4 (2010)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb43">González-Nalda, 2009</a></p>
<p>González-Nalda P.</p>
<p><strong>STDP learning time window</strong></p>
<p>(2009)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb44">Guerguiev et al., 2017</a></p>
<p>Guerguiev J., Lillicrap T.P., Richards B.A.</p>
<p><strong>Biologically feasible deep learning with segregated dendrites</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb45">Gütig and Sompolinsky, 2006</a></p>
<p>Gütig R., Sompolinsky H.</p>
<p><strong>The tempotron: A neuron that learns spike timing–based decisions</strong></p>
<p>Nature Neuroscience, 9 (3) (2006), pp. 420-428</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb46">Hastie et al., 2001</a></p>
<p>Hastie T., Tibshirani R., Friedman J.</p>
<p><strong>The elements of statistical learning: Data mining, inference, and prediction</strong></p>
<p>(2001)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb47">Haykin, 2009</a></p>
<p>Haykin S.S.</p>
<p><strong>Neural networks and learning machines</strong></p>
<p>Prentice Hall, New York (2009)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb48">Hazan and Manevitz, 2012</a></p>
<p>Hazan H., Manevitz L.M.</p>
<p><strong>Topological constraints and robustness in liquid state machines</strong></p>
<p>Expert Systems with Applications, 39 (2) (2012), pp. 1597-1606</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb49">Heiligenberg, 1991</a></p>
<p>Heiligenberg W.</p>
<p><strong>Neural nets in electric fish</strong></p>
<p>MIT Press (1991)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb50">Hinton et al., 2012</a></p>
<p>Hinton G., Deng L., Yu D., Dahl G., Mohamed A., Jaitly N., <em>et al.</em></p>
<p><strong>Deep neural networks for acoustic modeling in speech recognition</strong></p>
<p>IEEE Signal Processing Magazine, 29 (Nov.) (2012)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb51">Hinton et al., 2006</a></p>
<p>Hinton G.E., Osindero S., Teh Y.</p>
<p><strong>A fast learning algorithm for deep belief nets</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb52">Hinton and Salakhutdinov, 2006</a></p>
<p>Hinton G.E., Salakhutdinov R.R.</p>
<p><strong>Reducing the dimensionality of data with neural networks</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb53">Hodgkin and Huxley, 1952</a></p>
<p>Hodgkin A.L., Huxley A.F.</p>
<p><strong>A quantitative description of membrane current and its application to conduction and excitation in nerve</strong></p>
<p>The Journal of Physiology, 117 (4) (1952), p. 500</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb54">Hopfield, 1995</a></p>
<p>Hopfield J.</p>
<p><strong>Pattern recognition computation using action potential timing for stimulus representation</strong></p>
<p>Nature, 376 (6535) (1995), pp. 33-36</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb55">Huh and Sejnowski, 2018</a></p>
<p>Huh D., Sejnowski T.J.</p>
<p>Bengio S., Wallach H., Larochelle H., Grauman K., Cesa-Bianchi N., Garnett R. (Eds.), Gradient descent for spiking neural networks, Curran Associates, Inc (2018), pp. 1433-1443</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb56">Hung and Adeli, 1993</a></p>
<p>Hung S., Adeli H.</p>
<p><strong>Parallel backpropagation learning algorithms on cray Y-MP8/864 supercomputer</strong></p>
<p>Neurocomputing, 5 (6) (1993), pp. 287-302</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb57">Hung and Adeli, 1994</a></p>
<p>Hung S., Adeli H.</p>
<p><strong>Object-oriented backpropagation and its application to structural design</strong></p>
<p>Neurocomputing, 6 (1) (1994), pp. 45-55</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb58">Illing et al., 2019</a></p>
<p>Illing B., Gerstner W., Brea J.</p>
<p><strong>Biologically plausible deep learning — but how far can we go with shallow networks?</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb59">Ito, 2000</a></p>
<p>Ito M.</p>
<p><strong>Mechanisms of motor learning in the cerebellum1</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb60">Izhikevich, 2003</a></p>
<p>Izhikevich E.M.</p>
<p><strong>Simple model of spiking neurons</strong></p>
<p>IEEE Transactions on Neural Networks, 14 (6) (2003), pp. 1569-1572</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb61">Izhikevich, 2004</a></p>
<p>Izhikevich E.M.</p>
<p><strong>Which model to use for cortical spiking neurons?</strong></p>
<p>IEEE Transactions on Neural Networks, 15 (5) (2004), pp. 1063-1070</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb62">Izhikevich, 2006</a></p>
<p>Izhikevich E.M.</p>
<p><strong>Polychronization: Computation with spikes</strong></p>
<p>Neural Computation, 18 (2) (2006), pp. 245-282</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb63">Jedlicka, 2002</a></p>
<p>Jedlicka P.</p>
<p><strong>Synaptic plasticity metaplasticity and BCM theory</strong></p>
<p>Bratislavské Lekárske Listy, 103 (4/5) (2002), pp. 137-143</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb64">Jörntell and Hansel, 2006</a></p>
<p>Jörntell H., Hansel C.</p>
<p><strong>Synaptic memories upside down: Bidirectional plasticity at cerebellar parallel fiber-purkinje cell synapses</strong></p>
<p>Neuron, 52 (2) (2006), p. 227</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb65">Joshi and Maass, 2004</a></p>
<p>Joshi P., Maass W.</p>
<p><strong>Movement generation and control with generic neural microcircuits</strong></p>
<p>Biologically inspired approaches to advanced information technology, Springer (2004), pp. 258-273</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb66">Ju et al., 2013</a></p>
<p>Ju H., Xu J., Chong E., VanDongen A.M.J.</p>
<p><strong>Effects of synaptic connectivity on liquid state machine performance</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb67">Kampa et al., 2007</a></p>
<p>Kampa B.M., Letzkus J.J., Stuart G.J.</p>
<p><strong>Dendritic mechanisms controlling spike-timing-dependent synaptic plasticity</strong></p>
<p>Trends in Neurosciences, 30 (9) (2007), pp. 456-463</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb68">Kasabov et al., 2013</a></p>
<p>Kasabov N., Dhoble K., Nuntalid N., Indiveri G.</p>
<p><strong>Dynamic evolving spiking neural networks for on-line spatio- and spectro-temporal pattern recognition</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb69">Katz and Miledi, 1965</a></p>
<p>Katz B., Miledi R.</p>
<p><strong>The measurement of synaptic delay, and the time course of acetylcholine release at the neuromuscular junction</strong></p>
<p>Proceedings of the Royal Society of London. Series B. Biological Sciences, 161 (985) (1965), pp. 483-495</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb70">Kheradpisheh et al., 2016</a></p>
<p>Kheradpisheh S.R., Ganjtabesh M., Masquelier T.</p>
<p><strong>Bio-inspired unsupervised learning of visual features leads to robust invariant object recognition</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb71">Kheradpisheh et al., 2018</a></p>
<p>Kheradpisheh S.R., Ganjtabesh M., Thorpe S.J., Masquelier T.</p>
<p><strong>STDP-based spiking deep convolutional neural networks for object recognition</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb72">Knudsen, 2002</a></p>
<p>Knudsen E.I.</p>
<p><strong>Instructed learning in the auditory localization pathway of the barn owl</strong></p>
<p>Nature, 417 (6886) (2002), pp. 322-328</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb73">Koch and Segev, 1998</a></p>
<p>Koch C., Segev I.</p>
<p><strong>Methods in neuronal modeling: From ions to networks</strong></p>
<p>MIT press (1998)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb74">Kohonen, 2013</a></p>
<p>Kohonen T.</p>
<p><strong>Essentials of the self-organizing map</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb75">König et al., 1996</a></p>
<p>König P., Engel A.K., Singer W.</p>
<p><strong>Integrator or coincidence detector? the role of the cortical neuron revisited</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb76">Kuwabara and Suga, 1993</a></p>
<p>Kuwabara N., Suga N.</p>
<p><strong>Delay lines and amplitude selectivity are created in subthalamic auditory nuclei: The brachium of the inferior colliculus of the mustached bat</strong></p>
<p>Journal of Neurophysiology, 69 (5) (1993), p. 1713</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb77">Lameu et al., 2012</a></p>
<p>Lameu E., Batista C., Batista A., Iarosz K., Viana R., Lopes S., <em>et al.</em></p>
<p><strong>Suppression of bursting synchronization in clustered scale-free (rich-club) neuronal networks</strong></p>
<p>Chaos-Woodbury, 22 (4) (2012), Article 043149</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb78">LeCun et al., 2015</a></p>
<p>LeCun Y., Bengio Y., Hinton G.</p>
<p><strong>Deep learning</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb79">Lee et al., 2018</a></p>
<p>Lee C., Panda P., Srinivasan G., Roy K.</p>
<p><strong>Training deep spiking convolutional neural networks with STDP-based unsupervised pre-training followed by supervised fine-tuning</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb80">Lee et al., 2019</a></p>
<p>Lee, C., Sarwar, S. S., &amp; Roy, K. (2019). Enabling spike-based backpropagation in state-of-the-art deep neural network architectures. <em>113</em>(1), 54–66, This paper is avilable Online in: <a href="https://arxiv.org/abs/1903.06379v3">https://arxiv.org/abs/1903.06379v3</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb81">Legenstein et al., 2005</a></p>
<p>Legenstein R., Naeger C., Maass W.</p>
<p><strong>What can a neuron learn with spike-timing-dependent plasticity?</strong></p>
<p>Neural Computation, 17 (11) (2005), pp. 2337-2382</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb82">Letzkus et al., 2006</a></p>
<p>Letzkus J.J., Kampa B.M., Stuart G.J.</p>
<p><strong>Learning rules for spike timing-dependent plasticity depend on dendritic synapse location</strong></p>
<p>The Journal of Neuroscience, 26 (41) (2006), pp. 10420-10429</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb83">Lin and Faber, 2002</a></p>
<p>Lin J., Faber D.S.</p>
<p><strong>Modulation of synaptic delay during synaptic plasticity</strong></p>
<p>Trends in Neurosciences, 25 (9) (2002), pp. 449-455</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb84">Liu et al., 2015</a></p>
<p>Liu S., Delbruck T., Indiveri G., Whatley A., Douglas R.</p>
<p><strong>Event-based neuromorphic systems</strong></p>
<p>John Wiley &amp; Sons (2015)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb85">Maass, 1996</a></p>
<p>Maass W.</p>
<p><strong>Lower bounds for the computational power of networks of spiking neurons</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb86">Maass et al., 2004</a></p>
<p>Maass W., Natschläger T., Markram H.</p>
<p><strong>Computational models for generic cortical microcircuits</strong></p>
<p>Computational neuroscience: A comprehensive approach (vol. 18) (2004), pp. 575-605</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb87">Maass and Zador, 1999</a></p>
<p>Maass W., Zador A.</p>
<p><strong>Computing and learning with dynamic synapses</strong></p>
<p>Pulsed Neural Networks, 6 (May) (1999), pp. 321-336</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb88">Maggi et al., 2018</a></p>
<p>Maggi S., Peyrache A., Humphries M.D.</p>
<p><strong>An ensemble code in medial prefrontal cortex links prior events to outcomes during learning</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb89">Masquelier and Deco, 2013</a></p>
<p>Masquelier T., Deco G.</p>
<p><strong>Learning and coding in neural networks</strong></p>
<p>Principles of neural coding, CRC Press (2013), pp. 513-526</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb90">Masquelier and Thorpe, 2007</a></p>
<p>Masquelier T., Thorpe S.J.</p>
<p><strong>Unsupervised learning of visual features through spike timing dependent plasticity</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb91">McKennoch et al., 2006</a></p>
<p>McKennoch, S., Liu, D., &amp; Bushnell, L. G. (2006). Fast modifications of the spikeprop algorithm. In <em>Paper presented at the IJCNN’06. International joint conference on neural networks</em> (pp. 3970–3977).</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb92">Memmesheimer et al., 2014</a></p>
<p>Memmesheimer R., Rubin R., Ölveczky B., Sompolinsky H.</p>
<p><strong>Learning precisely timed spikes</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb93">Mesnard et al., 2016</a></p>
<p>Mesnard T., Gerstner W., Brea J.</p>
<p><strong>Towards deep learning with spiking neurons in energy based models with contrastive hebbian plasticity</strong></p>
<p>(2016)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb94">Minneci et al., 2012</a></p>
<p>Minneci F., Kanichay R.T., Silver R.A.</p>
<p><strong>Estimation of the time course of neurotransmitter release at central synapses from the first latency of postsynaptic currents</strong></p>
<p>Journal of Neuroscience Methods, 205 (1) (2012), pp. 49-64</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb95">Mohemmed et al., 2012</a></p>
<p>Mohemmed A., Schliebs S., Matsuda S., Kasabov N.</p>
<p><strong>SPAN: Spike pattern association neuron for learning spatio-temporal spike patterns</strong></p>
<p>International Journal of Neural Systems, 22 (04) (2012)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb96">Mohemmed et al., 2013</a></p>
<p>Mohemmed A., Schliebs S., Matsuda S., Kasabov N.</p>
<p><strong>Training spiking neural networks to associate spatio-temporal input–output spike patterns</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb97">Morrison et al., 2008</a></p>
<p>Morrison A., Diesmann M., Gerstner W.</p>
<p><strong>Phenomenological models of synaptic plasticity based on spike timing</strong></p>
<p>Biological Cybernetics, 98 (6) (2008), pp. 459-478</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb98">Mostafa, 2018</a></p>
<p>Mostafa H.</p>
<p><strong>Supervised learning based on temporal coding in spiking neural networks</strong></p>
<p>IEEE Transactions on Neural Networks and Learning Systems, 29 (7) (2018), pp. 3227-3235, <a href="https://doi.org/10.1109/TNNLS.2017.2726060">10.1109/TNNLS.2017.2726060</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb99">Neftci, 2018</a></p>
<p>Neftci E.O.</p>
<p><strong>Data and power efficient intelligence with neuromorphic learning machines</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb100">Neftci et al., 2016</a></p>
<p>Neftci E., Augustine C., Paul S., Detorakis G.</p>
<p><strong>Event-driven random back-propagation: Enabling neuromorphic deep learning machines</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb101">Neftci et al., 2014</a></p>
<p>Neftci E., Das S., Pedroni B., Kreutz-Delgado K., Cauwenberghs G.</p>
<p><strong>Event-driven contrastive divergence for spiking neuromorphic systems</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb102">Neftci et al., 2019</a></p>
<p>Neftci E.O., Mostafa H., Zenke F.</p>
<p><strong>Surrogate gradient learning in spiking neural networks</strong></p>
<p>(2019)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb103">Orchard et al., 2015</a></p>
<p>Orchard G., Meyer C., Etienne-Cummings R., Posch C., Thakor N., Benosman R.</p>
<p><strong>Hfirst: A temporal approach to object recognition</strong></p>
<p>IEEE Transactions on Pattern Analysis and Machine Intelligence, 37 (10) (2015), pp. 2028-2040, <a href="https://doi.org/10.1109/TPAMI.2015.2392947">10.1109/TPAMI.2015.2392947</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb104">Panda and Roy, 2016</a></p>
<p>Panda, P., &amp; Roy, K. (2016). Unsupervised regenerative learning of hierarchical features in spiking deep networks for object recognition. In <em>Paper presented at the IEEE international joint conference on neural networks</em> (pp. 299–306). Retrieved from <a href="https://arxiv.org/abs/1602.01510">https://arxiv.org/abs/1602.01510</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb105">Parnas and Parnas, 2010</a></p>
<p>Parnas I., Parnas H.</p>
<p><strong>Control of neurotransmitter release: From Ca2 to voltage dependent G-protein coupled receptors</strong></p>
<p>Pflügers Archiv-European Journal of Physiology, 460 (6) (2010), pp. 975-990</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb106">Paugam-Moisy and Bohte, 2012</a></p>
<p>Paugam-Moisy H., Bohte S.</p>
<p><strong>Computing with spiking neuron networks</strong></p>
<p>Handbook of natural computing, Springer (2012), pp. 335-376</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb107">Paugam-Moisy et al., 2008</a></p>
<p>Paugam-Moisy H., Martinez R., Bengio S.</p>
<p><strong>Delay learning and polychronization for reservoir computing</strong></p>
<p>Neurocomputing, 71 (7) (2008), pp. 1143-1158</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb108">Pfeiffer and Pfeil, 2018</a></p>
<p>Pfeiffer M., Pfeil T.</p>
<p><strong>Deep learning with spiking neurons: Opportunities and challenges</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb109">Pfister and Gerstner, 2006</a></p>
<p>Pfister J.P., Gerstner W.</p>
<p><strong>Triplets of spikes in a model of spike timing-dependent plasticity</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb110">Pfister et al., 2006</a></p>
<p>Pfister J., Toyoizumi T., Barber D., Gerstner W.</p>
<p><strong>Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning</strong></p>
<p>Neural Computation, 18 (6) (2006), pp. 1318-1348</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb111">Pham et al., 2008</a></p>
<p>Pham D., Packianather M., Charles E.</p>
<p><strong>Control chart pattern clustering using a new self-organizing spiking neural network</strong></p>
<p>Proceedings of the Institution of Mechanical Engineers, Part B (Management and Engineering Manufacture), 222 (10) (2008), pp. 1201-1211</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb112">Ponulak, 2005</a></p>
<p>Ponulak F.</p>
<p><strong>ReSuMe—New supervised learning method for spiking neural networks. (no. 1)</strong></p>
<p>Institute of Control and Information Engineering, Poznan University of Technology (2005)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb113">Ponulak and Kasiński, 2010</a></p>
<p>Ponulak F., Kasiński A.</p>
<p><strong>Supervised learning in spiking neural networks with ReSuMe: Sequence learning, classification, and spike shifting</strong></p>
<p>Neural Computation, 22 (2) (2010), pp. 467-510</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb114">Ponulak and Kasinski, 2011</a></p>
<p>Ponulak F., Kasinski A.</p>
<p><strong>Introduction to spiking neural networks: Information processing, learning and applications</strong></p>
<p>Acta Neurobiologiae Experimentalis, 71 (4) (2011), pp. 409-433</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb115">Ramesh et al., 2019</a></p>
<p>Ramesh B., Yang H., Orchard G.M., Le Thi N.A., Zhang S., Xiang C.</p>
<p><strong>DART: Distribution aware retinal transform for event-based cameras</strong></p>
<p>IEEE Transactions on Pattern Analysis and Machine Intelligence, 8828 (2019), p. 1, <a href="https://doi.org/10.1109/tpami.2019.2919301">10.1109/tpami.2019.2919301</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb116">Rueckauer et al., 2017</a></p>
<p>Rueckauer B., Lungu I., Hu Y., Pfeiffer M.</p>
<p><strong>Conversion of continuous-valued deep networks to efficient event-driven networks for image classification</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb117">Ruf and Schmitt, 1997</a></p>
<p>Ruf B., Schmitt M.</p>
<p><strong>Learning temporally encoded patterns in networks of spiking neurons</strong></p>
<p>Neural Processing Letters, 5 (1) (1997), pp. 9-18</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb118">Rullen and Thorpe, 2001</a></p>
<p>Rullen R.V., Thorpe S.J.</p>
<p><strong>Rate coding versus temporal order coding: What the retinal ganglion cells tell the visual cortex</strong></p>
<p>Neural Computation, 13 (6) (2001), pp. 1255-1283</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb119">Schrauwen et al., 2008</a></p>
<p>Schrauwen B., D’Haene M., Verstraeten D., Campenhout J.V.</p>
<p><strong>Compact hardware liquid state machines on FPGA for real-time speech recognition</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb120">Schultz et al., 1997</a></p>
<p>Schultz W., Dayan P., Montague P.R.</p>
<p><strong>A neural substrate of prediction and reward</strong></p>
<p>Science, 275 (5306) (1997), pp. 1593-1599</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb121">Seth, 2015</a></p>
<p>Seth A.</p>
<p><strong>Neural coding: Rate and time codes work together</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb122">Shrestha, 2018</a></p>
<p>Shrestha, S. B. (2018). SLAYER : Spike layer error reassignment in time. In <em>Paper presented at the Advances in neural information processing systems</em> (pp. 1412–1421). Retrieved from <a href="http://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time">http://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb123">Shrestha and Song, 2013</a></p>
<p>Shrestha, S. B., &amp; Song, Qing (2013). Weight convergence of SpikeProp and adaptive learning rate. In <em>Paper presented at the Communication, control, and computing (Allerton), 2013 51st annual allerton conference on</em> (pp. 506–511) <a href="http://dx.doi.org/10.1109/Allerton.2013.6736567">http://dx.doi.org/10.1109/Allerton.2013.6736567</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb124">Shrestha and Song, 2015</a></p>
<p>Shrestha S.B., Song Q.</p>
<p><strong>Adaptive learning rate of SpikeProp based on weight convergence analysis</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb125">Silva and Ruano, 2005</a></p>
<p>Silva, S. M., &amp; Ruano, A. E. (2005). Application of Levenberg–Marquardt method to the training of spiking neural networks. In <em>Paper presented at the Neural networks and brain, 2005. ICNN &amp; B’05. International conference on (vol. 3)</em> (pp. 1354–1358).</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb126">Sporea and Grüning, 2013</a></p>
<p>Sporea I., Grüning A.</p>
<p><strong>Supervised learning in multilayer spiking neural networks</strong></p>
<p>Neural Computation, 25 (2) (2013), pp. 473-509</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb127">Srinivasa and Cho, 2012</a></p>
<p>Srinivasa N., Cho Y.</p>
<p><strong>Self-organizing spiking neural model for learning fault-tolerant spatio-motor transformations</strong></p>
<p>IEEE Transactions on Neural Networks and Learning Systems, 23 (10) (2012), pp. 1526-1538</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb128">Srivastava and Salakhutdinov, 2014</a></p>
<p>Srivastava N., Salakhutdinov R.</p>
<p><strong>Multimodal learning with deep Boltzmann machines</strong></p>
<p>Journal of Machine Learning Research (JMLR), 15 (1) (2014), pp. 2949-2980</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb129">Swadlow, 1992</a></p>
<p>Swadlow H.A.</p>
<p><strong>Monitoring the excitability of neocortical efferent neurons to direct activation by extracellular current pulses</strong></p>
<p>Journal of Neurophysiology, 68 (2) (1992), pp. 605-619</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb130">Taherkhani et al., 2014</a></p>
<p>Taherkhani, A., Belatreche, A., Li, Y., &amp; Maguire, L. P. (2014). A new biologically plausible supervised learning method for spiking neurons. In <em>Paper presented at the Proc. ESANN</em> (pp. 11–16).</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb131">Taherkhani et al., 2015a</a></p>
<p>Taherkhani A., Belatreche A., Li Y., Maguire L.P.</p>
<p>Arik S., Huang T., Lai W.K., Liu Q. (Eds.), EDL: An extended delay learning based remote supervised method for spiking neurons, Springer International Publishing (2015), pp. 190-197</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb132">Taherkhani et al., 2015b</a></p>
<p>Taherkhani A., Belatreche A., Li Y., Maguire L.P.</p>
<p><strong>DL-ReSuMe: A delay learning based remote supervised method for spiking neurons</strong></p>
<p>IEEE Transactions on Neural Networks and Learning Systems, 26 (12) (2015), pp. 3137-3149, <a href="https://doi.org/10.1109/TNNLS.2015.2404938">10.1109/TNNLS.2015.2404938</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb133">Taherkhani, Belatreche, et al., 2018</a></p>
<p>Taherkhani A., Belatreche A., Li Y., Member S., Maguire L.P.</p>
<p><strong>A supervised learning algorithm for learning precise timing of multiple spikes in multilayer</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb134">Taherkhani, Cosma, and McGinnity, 2018</a></p>
<p>Taherkhani A., Cosma G., McGinnity T.M.</p>
<p><strong>Deep-FS: A feature selection algorithm for deep boltzmann machines</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb135">Taherkhani et al., 2019</a></p>
<p>Taherkhani A., Cosma G., Mcginnity T.M.</p>
<p><strong>Optimization of output spike train encoding for a spiking neuron based on its spatiotemporal input pattern</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb136">Takase et al., 2009</a></p>
<p>Takase, H., Fujita, M., Kawanaka, H., Tsuruoka, S., Kita, H., &amp; Hayashi, T. (2009). Obstacle to training SpikeProp networks — cause of surges in training process —. In <em>Paper presented at the Neural networks, 2009. IJCNN 2009. International joint conference on</em> (pp. 3062–3066). <a href="http://dx.doi.org/10.1109/IJCNN.2009.5178756">http://dx.doi.org/10.1109/IJCNN.2009.5178756</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb137">Tavanaei et al., 2019</a></p>
<p>Tavanaei A., Ghodrati M., Reza S.</p>
<p><strong>Deep learning in spiking neural networks</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb138">Tavanaei et al., 2016</a></p>
<p>Tavanaei, A., Masquelier, T., &amp; Maida, A. S. (2016). Acquisition of visual features through probabilistic spike-timing-dependent plasticity. In <em>Proceedings of the international joint conference on neural networks, 2016-Octob</em> (pp. 307–314). <a href="http://dx.doi.org/10.1109/IJCNN.2016.7727213">http://dx.doi.org/10.1109/IJCNN.2016.7727213</a>.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb139">Tavanaei et al., 2018</a></p>
<p>Tavanaei A., Masquelier T., Maida A.</p>
<p><strong>Representation learning using event-based STDP</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb140">Tetzlaff et al., 2012</a></p>
<p>Tetzlaff C., Kolodziejski C., Markelic I., Wörgötter F.</p>
<p><strong>Time scales of memory, learning, and plasticity</strong></p>
<p>Biological Cybernetics, 106 (11–12) (2012), pp. 715-726</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb141">Thorpe et al., 2001</a></p>
<p>Thorpe S., Delorme A., Van Rullen R.</p>
<p><strong>Spike-based strategies for rapid processing</strong></p>
<p>Neural Networks, 14 (6) (2001), pp. 715-725</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb142">Turrigiano and Nelson, 2004</a></p>
<p>Turrigiano G.G., Nelson S.B.</p>
<p><strong>Homeostatic plasticity in the developing nervous system</strong></p>
<p>Nature Reviews Neuroscience, 5 (2) (2004), pp. 97-107</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb143">Vasilaki and Giugliano, 2013</a></p>
<p>Vasilaki E., Giugliano M.</p>
<p><strong>Emergence of connectivity motifs in networks of model neurons with short-and long-term plastic synapses</strong></p>
<p>(2013)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb144">Verstraeten et al., 2005</a></p>
<p>Verstraeten D., Schrauwen B., Stroobandt D., Van Campenhout J.</p>
<p><strong>Isolated word recognition with the liquid state machine: A case study</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb145">Vreeken, 2003</a></p>
<p>Vreeken J.</p>
<p><strong>Spiking neural networks, an introduction: Technical Report UU-CS, (2003-008)</strong></p>
<p>(2003), pp. 1-5</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb146">Wade et al., 2010</a></p>
<p>Wade J.J., McDaid L.J., Santos J.A., Sayers H.M.</p>
<p><strong>SWAT: A spiking neural network training algorithm for classification problems</strong></p>
<p>IEEE Transactions on Neural Networks, 21 (11) (2010), pp. 1817-1830</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb147">Wang et al., 2014</a></p>
<p>Wang J., Belatreche A., Maguire L.p., McGinnity T.M.</p>
<p><strong>An online supervised learning method for spiking neural networks with adaptive structure</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb148">Wang et al., 2005</a></p>
<p>Wang H., Gerkin R.C., Nauen D.W., Bi G.</p>
<p><strong>Coactivation and timing-dependent integration of synaptic potentiation and depression</strong></p>
<p>Nature Neuroscience, 8 (2) (2005), pp. 187-193</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb149">Whittington and Bogacz, 2019</a></p>
<p>Whittington J.C.R., Bogacz R.</p>
<p><strong>Theories of error back-propagation in the brain</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb150">Wu et al., 2019</a></p>
<p>Wu J., Chua Y., Zhang M., Yang Q., Li G., Li H.</p>
<p><strong>Deep spiking neural network with spike count based learning rule</strong></p>
<p>(2019)</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb151">Wysoski et al., 2008</a></p>
<p>Wysoski S.G., Benuskova L., Kasabov N.</p>
<p><strong>Fast and adaptive network of spiking neurons for multi-view visual pattern recognition</strong></p>
<p>Neurocomputing, 71 (13) (2008), pp. 2563-2575</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb152">Xin and Embrechts, 2001</a></p>
<p>Xin, J., &amp; Embrechts, M. J. (2001). Supervised learning with spiking neural networks. In <em>Paper presented at the Neural networks, 2001. Proceedings. IJCNN’01. International joint conference on (vol. 3)</em> (pp. 1772–1777).</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb153">Xu, Gong, and Wang, 2013</a></p>
<p>Xu B., Gong Y., Wang B.</p>
<p><strong>Delay-induced firing behavior and transitions in adaptive neuronal networks with two types of synapses</strong></p>
<p>Science China Chemistry, 56 (2) (2013), pp. 222-229</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb154">Xu et al., 2019</a></p>
<p>Xu Y., Yang J., Zeng X.</p>
<p><strong>An optimal time interval of input spikes involved in synaptic adjustment of spike sequence learning</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb155">Xu, Zeng, et al., 2013</a></p>
<p>Xu Y., Zeng X., Han L., Yang J.</p>
<p><strong>A supervised multi-spike learning algorithm based on gradient descent for spiking neural networks</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb156">Yu et al., 2012</a></p>
<p>Yu, Q., Tan, K. C., &amp; Tang, H. (2012). Pattern recognition computation in a spiking neural network with temporal encoding and learning. In <em>Paper presented at the Neural networks (IJCNN), the 2012 international joint conference on</em> (pp. 1–7).</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb157">Yu et al., 2013a</a></p>
<p>Yu Q., Tang H., Tan K.C., Li H.</p>
<p><strong>Precise-spike-driven synaptic plasticity: Learning hetero-association of spatiotemporal spike patterns</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb158">Yu et al., 2013b</a></p>
<p>Yu Q., Tang H., Tan K.C., Li H.</p>
<p><strong>Rapid feedforward computation by temporal encoding and learning with spiking neurons</strong></p>
<p>IEEE Transactions on Neural Networks and Learning Systems, 24 (10) (2013), pp. 1539-1552, <a href="https://doi.org/10.1109/TNNLS.2013.2245677">10.1109/TNNLS.2013.2245677</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb159">Yu et al., 2014</a></p>
<p>Yu Q., Tang H., Tan K.C., Yu H.</p>
<p><strong>A brain-inspired spiking neural network model with temporal encoding and learning</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb160">Zenke and Ganguli, 2018</a></p>
<p>Zenke F., Ganguli S.</p>
<p><strong>SuperSpike: Supervised learning in multilayer spiking neural networks</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb161">Zhao et al., 2015</a></p>
<p>Zhao B., Ding R., Chen S., Linares-Barranco B., Tang H.</p>
<p><strong>Feedforward categorization on AER motion events using cortex-like features in a spiking neural network</strong></p>
<p>IEEE Transactions on Neural Networks and Learning Systems, 26 (9) (2015), pp. 1963-1978, <a href="https://doi.org/10.1109/TNNLS.2014.2362542">10.1109/TNNLS.2014.2362542</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019303181#bb162">Zuo et al., 2015</a></p>
<p>Zuo Y., Safaai H., Notaro G., Mazzoni A., Panzeri S., Diamond M.</p>
<p><strong>Complementary contributions of spike timing and spike rate to perceptual decisions in rat S1 and S2 cortex</strong></p>
<h2 id="cited-by-74">Cited by (74)<a class="headerlink" href="#cited-by-74" title="Permanent link">&para;</a></h2>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608019303181">View Abstract</a></p>
<p>© 2019 Elsevier Ltd. All rights reserved.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.51198bba.min.js"></script>
      
        <script src="../../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
      
    
  </body>
</html>